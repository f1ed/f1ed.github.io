<!DOCTYPE html>
<html  lang="zh">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="baidu-site-verification" content="9ce18CEmjH" />
<meta name="google-site-verification" content="GSUThrU_AtZE-dgdz1QWWouv0L2teKqHWrZg7DfHbXo" />
<meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.1" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>「机器学习-李宏毅」:Semi-supervised Learning - fred&#39;s blog</title>


    <meta name="description" content="这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？ 再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。 对于Generative Model，文章重点讲述了如何用EM算法来训练模型。 对于">
<meta property="og:type" content="article">
<meta property="og:title" content="「机器学习-李宏毅」:Semi-supervised Learning">
<meta property="og:url" content="https://f1ed.github.io/2020/07/03/semi-supervised/index.html">
<meta property="og:site_name" content="fred&#39;s blog">
<meta property="og:description" content="这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？ 再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。 对于Generative Model，文章重点讲述了如何用EM算法来训练模型。 对于">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://f1ed.github.io/gallery/thumbnails/81648714_p0.jpg">
<meta property="article:published_time" content="2020-07-02T16:00:00.000Z">
<meta property="article:modified_time" content="2020-07-03T11:32:00.705Z">
<meta property="article:author" content="f1ed">
<meta property="article:tag" content="Machine-Learning">
<meta property="article:tag" content="open-classes">
<meta property="article:tag" content="Semi-supervised">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://f1ed.github.io/gallery/thumbnails/81648714_p0.jpg">





<link rel="alternative" href="/atom.xml" title="「机器学习-李宏毅」:Semi-supervised Learning" type="application/atom+xml">



<link rel="icon" href="/images/heart.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171512660-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-171512660-1');
</script>

    
    <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?b1d53a77276f7e423bc7cf8fafd95b75";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body class="is-3-column">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」:Semi-supervised Learning" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/atom.xml">RSS</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    <a class="navbar-item" target="_blank" rel="noopener" title="My GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="/gallery/thumbnails/81648714_p0.jpg" alt="「机器学习-李宏毅」:Semi-supervised Learning">
        </span>
    </div>
    
    <div class="card-content article ">
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <i class="fas fa-bars"></i>「机器学习-李宏毅」:Semi-supervised Learning
            
        </h1>
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-07-02T16:00:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-07-03</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2020-07-03T11:32:00.705Z"><i class="far fa-calendar-check">&nbsp;</i>2020-07-03</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/">机器学习-李宏毅</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    <i class="far fa-clock"></i>&nbsp;
                    
                    
                    29 分钟 读完 (大约 4394 个字)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span>次访问
                </span>
                
            </div>
        </div>
        
        <div class="content">
            <p>这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？</p>
<p>再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。</p>
<p>对于Generative Model，文章重点讲述了如何用EM算法来训练模型。</p>
<p>对于Low-density，文章重点讲述了如何让模型进行Self-training，并且在训练中引入Entropy-based Regularization term来尽可能low-density的假设。</p>
<p>对于Smoothness Assumption，文章重点讲述了Graph-based Approach（基于图的方法），并且在训练中引入Smoothness Regularization term来尽可能满足Smoothness Assumption的假设。</p>
<p>对于Better Representation，本篇文章只是简单阐述了其思想，具体介绍见这篇博客。</p>
<a id="more"></a>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>什么是Semi-supervised learning(半监督学习)？和Supervised learning（监督式学习）的区别在哪？</p>
<p><strong>Supervised learning（监督式学习）</strong>：</p>
<p>用来训练的数据集 $R$  中的数据labeled data，即 ${(x^r,\hat{y}^r)}_{r=1}^R$ .</p>
<p>比如在图像分类数据集中： $x^r$ 是image，对应的target output $y^r$ 是分类的label。</p>
<p>而<strong>Semi-supervised learning（半监督式学习）</strong>：</p>
<p>用来的训练的数据集由两部分组成 $\{(x^r,\hat{y}^r)\}_{r=1}^R$   ,    $\{x^u\}_{u=R}^{R+U}$   ，即labeled data和unlabeled data，而且通常情况下，unlabeled data的数量远远高于labeled data是数量，即 $U&gt;&gt;R$ .</p>
<p>Semi-supervised learning 又分为两种，Transductive learning （转导/推论推导）和 Inductive learning（归纳推理）</p>
<ul>
<li>Transductive learing: unlabeled data is the testing data. 即测试数据在训练中用过。</li>
<li>Inductive learning: unlabeled data is not the testing data.测试数据是训练中没有用过的数据。</li>
<li>这里的使用testing data是指用testing data的feature，而不是使用testing data的label。</li>
</ul>
<hr>
<p>为什么会有semi-supervised learning？</p>
<ul>
<li><p>Collecting data is easy, but collecting “labelled” data is expensive.</p>
<p>【收集数据很简单，但收集有label的数据很难】</p>
</li>
<li><p>We do semi-supervised learning in our lives</p>
<p>【在生活中，更多的也是半监督式学习，我们能明白少量看到的事物，但看到了更多我们不懂的，即unlabeled data】</p>
</li>
</ul>
<h2 id="Why-Semi-supervised-learning-helps"><a href="#Why-Semi-supervised-learning-helps" class="headerlink" title="Why Semi-supervised learning helps"></a>Why Semi-supervised learning helps</h2><p>为什么半监督学习能帮助解决一些问题？</p>
<p>如上图所示，如果只有labeled data，分类所画的boundary可能是一条竖线。</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRNz6.md.png" alt="NXRNz6.md.png" style="zoom:75%;" />

<p>但如果有一些unlabeled data（如灰色的点），分类所画的boundary可能是一条斜线。</p>
<p>The distribution of the unlabeled data tell us something.</p>
<p>半监督式学习之所以有用，是因为这些unlabeled data的分布能告诉我们一些东西。</p>
<p>通常这也伴随着一些假设，所以半监督式学习是否有用往往取决于这些假设是否合理。</p>
<h1 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h1><h2 id="Supervised-Generative-Model"><a href="#Supervised-Generative-Model" class="headerlink" title="Supervised Generative Model"></a>Supervised Generative Model</h2><p>在<a href="/2020/03/21/Classification1/" title="这篇">这篇</a>文章中，有详细讲述分类问题中的generative model。</p>
<p>给定一个labelled training data $x^r\in C_1,C_2$ 训练集。</p>
<p>prior probability（先验概率）有 $P(C_i)$ 和 $P(x|C_i)$ ，假设是Gaussian模型，则 $P(x|C_i)$ 由Gaussian模型中的 $\mu^i,\Sigma$ 参数决定。</p>
<p>根据已有的labeled data，计算出假设的Gaussian模型的参数（如下图），从而得出prior probability。</p>
<img src="https://s1.ax1x.com/2020/07/03/NXonAS.md.png" alt="NXonAS.md.png" style="zoom:75%;" />

<p>即可算出posterior probability  $P\left(C_{1} \mid x\right)=\frac{P\left(x \mid C_{1}\right) P\left(C_{1}\right)}{P\left(x \mid C_{1}\right) P\left(C_{1}\right)+P\left(x \mid C_{2}\right) P\left(C_{2}\right)}$ </p>
<h2 id="Semi-supervised-Generative-Model"><a href="#Semi-supervised-Generative-Model" class="headerlink" title="Semi-supervised Generative Model"></a>Semi-supervised Generative Model</h2><p>在只有labeled data的图中，算出来的 $\mu,\Sigma$ 参数如下图所示：</p>
<img src="https://s1.ax1x.com/2020/07/03/NXonAS.md.png" alt="NXonAS.md.png" style="zoom:75%;" />

<p>但如果有unlabeled data（绿色点），会发现分布的模型参数更可能是是下图：</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRtRx.md.png" alt="NXRtRx.md.png" style="zoom:75%;" />

<p>The unlabeled data $x^u$ help re-estimate $P(C_1),P(C_2),\mu^1,\mu^2,\Sigma$ .</p>
<p>因此，unlabeled data会影响分布，从而影响prior probability，posterior probability，最终影响 boundary。</p>
<h3 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h3><p>所以有unlabeled data, 这个Semi-supervised 的算法怎么做呢？</p>
<p>其实就是<strong>EM</strong>（Expected-maximization algorithm，期望最大化算法。）</p>
<ol>
<li><p>Initialization : $\theta={P(C_1),P(C_2),\mu^1,\mu^2,\Sigma}$ .</p>
<p>初始化Gaussian模型参数，可以随机初始，也可以通过labeled data得出。</p>
<p>虽然这个算法最终会收敛，但是初始化的参数影响收敛结果，就像gradient descent一样。</p>
</li>
<li><p>E：Step 1: compute the posterior probability of unlabeled data $P_\theta(C_1|x^u)$ (depending on model $\theta$ )</p>
<p>根据当前model的参数，计算出unlabeled data的posterior probability $P(C_1|x^u)$ .(以$P(C_1|x^u)$ 为例) </p>
</li>
<li><p>M：Step 2: update model. Back to step1 until the algorithm converges enventually.</p>
<p>用E步得到unlabeled data的posterior probability来最大化极大似然函数，更新得到新的模型参数，公式很直觉。(以 $C_1$ 为例)</p>
<p>（$N$ ：data 的总数，包括unlabeled data; $N_1$ :label= $C_1$ 的data数）</p>
<ul>
<li><p>$P(C_1)=\frac{N_1+\Sigma_{x^u}P(C_1|x^u)}{N}$  </p>
<p>对比没有unlabeled data之前的式子， $P(C_1)=\frac{N_1}{N}$ ，除了已有label= $C_1$ ，还多了一部分，即unlabeled data中属于 $C_1$ 的概率和。</p>
</li>
<li>$\mu^{1}=\frac{1}{N_{1}} \sum_{x^{r} \in C_{1}} x^{r}+\frac{1}{\sum_{x^{u}} P\left(C_{1} \mid x^{u}\right)} \sum_{x^{u}} P\left(C_{1} \mid x^{u}\right) x^{u}$  

<p>对比没有unlabeled data的式子 ，$\mu^{1}=\frac{1}{N_{1}} \sum_{x^{r} \in C_{1}} x^{r}$ ，除了已有的label= $C_1$ ，还多了一部分，即unlabeled data的 $x^u$ 的加权平均（权重为 $P(C_1\mid x^u)$ ，即属于 $C_1$ 的概率）。</p>
</li>
<li><p>$\Sigma$ 公式也包括了unlabeled data.</p>
</li>
</ul>
</li>
</ol>
<p>所以这个算法的Step 1就是EM算法的Expected期望部分，根据已有的labeled data得出极大似然函数的估计值；</p>
<p>Step 2就是EM算法的Maximum部分，利用unlabeled data（通过已有模型的参数）最大化E步的极大似然函数，更新模型参数。</p>
<p>最后反复迭代Step 1和Step 2，直至收敛。</p>
<h3 id="Why-EM"><a href="#Why-EM" class="headerlink" title="Why EM"></a>Why EM</h3><p>[1]挖坑EM详解。</p>
<p>为什么可以用EM算法来解决Semi-supervised?</p>
<ul>
<li><p>只有labeled data</p>
<p>极大似然函数 $\log{L(\theta)}=\sum_{x^r}\log{P_\theta(x^r,\hat{y}^r)}$ , 其中 $P_\theta(x^r,\hat{y}^r)=P_\theta(x^r\mid \hat{y}^r)P(\hat{y}^r)$ .</p>
<p>对上式子求导是有closed-form solution的。</p>
</li>
<li><p>有labeled data和unlabeled data</p>
<p>极大似然函数增加了一部分  $\log L(\theta)=\sum_{x^{r}} \log P_{\theta}\left(x^{r}, \hat{y}^{r}\right)+\sum_{x^{u}} \log P_{\theta}\left(x^{u}\right)$ .</p>
<p>将后部分用全概率展开， $P_{\theta}\left(x^{u}\right)=P_{\theta}\left(x^{u} \mid C_{1}\right) P\left(C_{1}\right)+P_{\theta}\left(x^{u} \mid C_{2}\right) P\left(C_{2}\right)$  .</p>
<p>如果要求后部分，因为是unlabeled data, 所以模型 $\theta$ 需要得知unlabeled data的label，即 $P(C_1\mid x^u)$ ,而求这个式子，也需要得到 prior probability $P(x^u\mid C_1)$ ,但这个式子需要事先得知模型 $\theta$ ，因此陷入了死循环。</p>
<p>因此这个极大似然函数不是convex（凸），不能直接求解，因此用迭代的EM算法逐步maximum极大似然函数。</p>
</li>
</ul>
<h1 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h1><p>另一种假设是Low-density Separation的假设，即这个世界是非黑即白的”Black-or-white”。</p>
<p>两种类别之间是low-density，交界处有明显的鸿沟，因此要么是类别1，要么是类别2，没有第三种情况。</p>
<h2 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h2><p>对于Low-density Separation Assumption的假设，使用Self-training的方法。</p>
<p>Given：labeled data set $={(x^r,\hat{y}^r}<em>{r=1}^R$ ,unlabeled data set $={x^u}</em>{u=l}^R+U$ .</p>
<p><strong>Repeat：</strong> </p>
<ol>
<li><p>Train model $f^<em>$ from labeled data set. ($f^</em>$ is independent to the model)</p>
<p>从labeled data set中训练出一个模型</p>
</li>
<li><p>Apply $f^*$ to the unlabeled data set. Obtain pseudo-label ${(x^u,y^u}_{u=l}^{R+U}$ .</p>
<p>用这个模型 $f^*$ 来预测unlabeled data set， 获得伪label</p>
</li>
<li><p>Remove a set of data from unlabeled data set, and add them into the labeled data set.</p>
<p>拿出一些unlabeled data(pseudo-label)，放到labeled data set中，回到步骤1，再训练。</p>
<ul>
<li><p>how to choose the data set remains open</p>
<p>如何选择unlabeled data 是自设计的</p>
</li>
<li><p>you can also provide a weight to each data.</p>
<p>训练中可以对unlabeled data(pseudo-label)和labeled data 赋予不同的权重.</p>
</li>
</ul>
</li>
</ol>
<p><strong>注意：</strong> Regression模型是不能self-training的，因为unlabeled data和其pseudo-label放在模型中的loss为0，无法再minimize。</p>
<h2 id="Hard-Label"><a href="#Hard-Label" class="headerlink" title="Hard Label"></a>Hard Label</h2><p><strong>V.S.  semi-supervised learning for generative model</strong> </p>
<p>Semi-supervised learning for generative model和Low-density Separation的区别其实是soft label 和soft label的区别。</p>
<p>generative model是利用来unlabeled data的 $P(C_1|x^u)$ posterior probability来计算新的prior probability，迭代更新模型。</p>
<p>而low-density是计算出unlabeled data的pseudo-label，选择性扩大labeled data set(即加入部分由pseudo-label的unlabeled data)来迭代训练模型。</p>
<p>因此，如果考虑Neural Network：</p>
<p>($\theta^*$ 是labeled data计算所得的network parameters)</p>
<p>如下图，unlabeled data $x^u$ 放入模型中预测，得到 $\begin{bmatrix} 0.7 \ 0.3\end{bmatrix}$ .</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRYJ1.md.png" alt="NXRYJ1.md.png" style="zoom:75%;" />

<p>如果是使用hard label，则 $x^u$ 的target是 $\begin{bmatrix} 1 \ 0\end{bmatrix}$ .</p>
<p>如果是使用soft label，则 $x^u$ 的target是 $\begin{bmatrix} 0.7 \ 0.3\end{bmatrix}$ .</p>
<p>如果是使用soft label，则self-training不会有效，因为新的data加进去，不会增大模型的loss，也就无法再minimize.</p>
<p><strong>所以基于Low-density Separation的假设，是非黑即白的，需要使用hard label来self-training。</strong> </p>
<h2 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h2><p>在训练模型中，我们需要尽量保证unlabeled data在模型中的分布是low-density separation。</p>
<p>即下图中，unlabeled data得到的pseudo-label的分布应该尽量集中，而不应该太分散。</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRJiR.md.png" alt="NXRJiR.md.png" style="zoom:50%;" /> 



<p>所以，在训练中，<strong>如何评估 $y^u$ 的分布的集中度？</strong></p>
<p>根据信息学，使用 $y^u$ 的entropy，即  $E\left(y^{u}\right)=-\sum_{m=1}^{5} y_{m}^{u} \ln \left(y_{m}^{u}\right)$ </p>
<p>(注：这里的 $y^u_m$ 应该是  $y^u=m$ 的概率)</p>
<p>当 $E(y^u)$ 越小，说明 $y^u$ 分布越集中，如下图。</p>
<img src="https://s1.ax1x.com/2020/07/03/NXR3dJ.md.png" alt="NXR3dJ.md.png" style="zoom:50%;" />

<hr>
<p>因此，在self-training中：</p>
<p>$L=\sum_{y^r} C(x^r,\hat{y}^r)+\lambda\sum_{x^u}E(y^u)$ </p>
<p>Loss function的前一项（cross entropy）minimize保证分类的正确性，后一项（entropy of  $y^u$ ) minimize保证 unlabeled data分布尽量集中，最大可能满足low-density separation的假设。</p>
<p>training：gradient decent.</p>
<p>因为这样的形式很像之前提到过的regularization(具体见<a href="/2020/04/21/tips-for-DL/" title="这篇文章的3.2">这篇文章的3.2</a>)，所以又叫entropy-based regularization.</p>
<h2 id="Outlook-Semi-supervised-SVM"><a href="#Outlook-Semi-supervised-SVM" class="headerlink" title="Outlook: Semi-supervised SVM"></a>Outlook: Semi-supervised SVM</h2><p>SVM也是解决semi-supervised learning的方法.</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRaQK.md.png" alt="NXRaQK.md.png" style="zoom:50%;" />

<p>上图中，在有unlabeled data的情况下，希望boundary 分的越开越好（largest margin）和有更小的error.</p>
<p>因此枚举unlabeled data所有可能的情况，但枚举在计算量上是巨大的，因此SVM（Support Vector Machines）可以实现枚举的目标，但不需要这么大的枚举量。</p>
<h1 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h1><p>Smoothness Assumption的思想可以用以下话归纳：</p>
<p>“You are known by the company you keep”</p>
<p>近朱者赤，近墨者黑。</p>
<p>蓬生麻中，不扶而直。白沙在涅，与之俱黑。</p>
<p>Assumption：“similar” $x$ has the same $\hat{y}$ .</p>
<p>【意思就是说：相近的 $x$ 有相同的label $\hat{y}$ .】</p>
<p><strong>More precise assumption：</strong></p>
<ul>
<li>x is not uniform</li>
<li>if $x^1$ and $x^2$ are close in a hign density region, $\hat{y}^1$ and $\hat{y}^2$ are the same.</li>
</ul>
<p>Smoothness Assumption假设更准确的表述是：</p>
<p> x不是均匀分布，如果 $x^1$ 和 $x^2$ 通过一个high density region的区域连在一起，且离得很近，则 $\hat{y}^1$ 和 $\hat{y}^2$ 相同。</p>
<p>如下图， $x^1$ 和 $x^2$ 通过high density region连接在一起，有相同的label，而 $x^2$ 和 $x^3$ 有不同的label.</p>
<img src="https://s1.ax1x.com/2020/07/03/NXR1Z4.md.png" alt="NXR1Z4.md.png" style="zoom:50%;" />

<hr>
<p>Smoothness Assumption通过观察大量unlabeled data，可以得到一些信息。</p>
<p>比如下图中的两张人的左脸和右脸图片，都是unlabeled，但如果给大量的过渡形态（左脸转向右脸）unlabeled data，可以得出这两张图片是相似的结论.</p>
<p><a href="https://imgchr.com/i/NXoQpj"><img src="https://s1.ax1x.com/2020/07/03/NXoQpj.md.png" alt="NXoQpj.md.png"></a> </p>
<p>Smoothness Assumption还可以用在文章分类中，比如分类天文学和旅游学的文章。</p>
<p>如下图， 文章 d1和d3有overlap word（重叠单词），所以d1和d3是同一类，同理 d4和d2是一类。</p>
<img src="https://s1.ax1x.com/2020/07/03/NXoutg.md.png" alt="NXoutg.md.png" style="zoom:50%;" />

<p>如果，下图中，d1和d3没有overlap word，就无法说明d1和d3是同一类。</p>
<img src="https://s1.ax1x.com/2020/07/03/NXoKhQ.md.png" alt="NXoKhQ.md.png" style="zoom:50%;" />

<p>但是，如果我们收集到足够多但unlabeled data，如下图，通过high density region的连接和传递，也可以得出d1和d3一类，d2和d4一类。</p>
<img src="https://s1.ax1x.com/2020/07/03/NXol1s.md.png" alt="NXol1s.md.png" style="zoom:80%;" />

<h2 id="Cluster-and-then-Label"><a href="#Cluster-and-then-Label" class="headerlink" title="Cluster and then Label"></a>Cluster and then Label</h2><p>在Smoothness Assumption假设下，直观的可以用cluster and then label，先用所有的data训练一个classifier。</p>
<p>直接聚类标记(比较难训练）。</p>
<h2 id="Graph-based-Approach"><a href="#Graph-based-Approach" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h2><p>另一种方法是利用图的结构（Graph structure）来得知 $x^1$ and $x^2$ are close in a high density region (connected by a high density path).</p>
<p>Represent the data points as a graph.</p>
<p>【把这些数据点看作一个图】</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRKMT.md.png" alt="NXRKMT.md.png" style="zoom:50%;" />

<p>建图有些时候是很直观的，比如网页中的超链接，论文中的引用。</p>
<p>但有的时候也需要自己建图。</p>
<p>注意：</p>
<p>如果是影像类，base on pixel，performance就不太好，一般会base on autoencoder，将feature抽象出来，效果更好。</p>
<h3 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h3><p>建图过程如下：</p>
<ol>
<li><p>Define the similarity $s(x^i, x^j)$ between $x^i$ and $x^j$ .</p>
<p>【定义data $x^i$ 和 $x^j$ 的相似度】</p>
</li>
<li><p>Add edge【定义数据点中加边（连通）的条件】</p>
<ul>
<li><p>K Nearest Neighbor【和该点最近的k个点相连接】</p>
<img src="https://s1.ax1x.com/2020/07/03/NXReGq.png" alt="NXReGq.png" style="zoom:50%;" />
</li>
<li><p>e-Neighborhood【与离该点距离小于等于e的点相连接】</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRZin.png" alt="NXRZin.png" style="zoom:50%;" />
</li>
</ul>
</li>
<li><p>Edge weight is proportional to $s(x^i, x^j)$ 【边点权重就是步骤1定义的连接两点的相似度】</p>
<p>Gaussian Radial Basis Function： $s\left(x^{i}, x^{j}\right)=\exp \left(-\gamma\left\|x^{i}-x^{j}\right\|^{2}\right)$ </p>
<p>一般采用如上公式（经验上取得较好的performance）。</p>
<p>因为利用指数化后（指数内是两点的Euclidean distance），函数下降的很快，只有当两点离的很近时，该相似度 $s(x^i,x^j)$  才大，其他时候都趋近于0.</p>
</li>
</ol>
<h3 id="Graph-based-Approach-1"><a href="#Graph-based-Approach-1" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h3><p>图建好后：</p>
<p>The labeled data influence their neighbors.</p>
<p>Propagate through the graph.</p>
<p>【label data 不仅会影响他们的邻居，还会一直传播下去】</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRmR0.md.png" alt="NXRmR0.md.png" style="zoom:50%;" />

<p>如果data points够多，图建的好，就会像下图这样：</p>
<img src="https://s1.ax1x.com/2020/07/03/NXREIs.png" alt="NXREIs.png" style="zoom:50%;" />

<p>但是，如果data较少，就可能出现下图这种label传不到unlabeled data的情况：</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRPsS.png" alt="NXRPsS.png" style="zoom:50%;" />

<h3 id="Smoothness-Definition"><a href="#Smoothness-Definition" class="headerlink" title="Smoothness Definition"></a>Smoothness Definition</h3><p>因为是基于Smoothness Assumption，所以最后训练出的模型应让得到的图尽可能满足smoothness的假设。</p>
<p><strong>注意：</strong> 这里的因果关系是，unlabeled data作为NN的输入，得到label $y$ ，该label $y$ 和labeled data的 label $\hat{y}$  一起得到的图是尽最大可能满足Smoothness Assumption的。</p>
<p>（<strong>而不是</strong>建好图，然后unlabeled data的label $y$ 是labeled data原有的 $\hat{y}$ 直接传播过来的，不然训练NN干嘛）</p>
<p>把unlabeled data作为NN的输入，得到label ，对labeled data和”unlabeled data” 建图。</p>
<p>为了在训练中使得最后的图尽可能满足假设，定义<strong>smoothness of the labels on the graph</strong>.</p>
$S=\frac{1}{2} \sum_{i,j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}$  

<p>（对于所有的labeled data 和 “unlabeled data”（作为NN输入后，有label））</p>
<p>按照上式计算，得到的Smoothness如下图所示：</p>
<p><a href="https://imgchr.com/i/NXRiqg"><img src="https://s1.ax1x.com/2020/07/03/NXRiqg.md.png" alt="NXRiqg.md.png"></a> </p>
<p><strong>Smaller means smoother.</strong> </p>
<p>【Smoothness $S$ 越小，表示图越满足这个假设】</p>
<hr>
<p>计算smoothness $S$ 有一种简便的方法：</p>
$S=\frac{1}{2} \sum_{i, j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}=y^{T} L y$  (这里的1/2只是为了计算方便)

<ul>
<li><p>$y$ : (R+U)-dim vector，是所有label data和”unlabeled data” 的label，所以是R+U维。</p>
<p>$y=\begin{bmatrix}…y^i…y^j…\end{bmatrix}^T$ </p>
</li>
<li><p>$L$ :(R+U) $\times$ (R+U) matrix，也叫Graph Laplacian（调和矩阵，拉普拉斯矩阵）</p>
<p>$L$ 的计算方法：$L=D-W$ </p>
<p>其中 $W$ 矩阵算是图的邻接矩阵（区别是无直接可达边的值是0）</p>
<p>$D$ 矩阵是一个对角矩阵，对角元素的值等于 $W$ 矩阵对应行的元素和</p>
<p>矩阵表示如下图所示：</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRkZQ.md.png" alt="NXRkZQ.md.png" style="zoom:50%;" />

</li>
</ul>
<p>（证明据说很枯燥，暂时略[2])</p>
<h3 id="Smoothness-Regularization"><a href="#Smoothness-Regularization" class="headerlink" title="Smoothness Regularization"></a>Smoothness Regularization</h3>$S=\frac{1}{2} \sum_{i, j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}=y^{T} L y$  

<p> $S$ 中的 $y$ 其实是和network parameters有关的（unlabeled data的label），所以把 $S$ 也放进损失函数中minimize，以求尽可能满足smoothness assumption.</p>
<p>以满足smoothness assumption的损失函数： $L=\sum_{x^r} C\left(y^{r}, \hat{y}^{r}\right)+\lambda S$ </p>
<p>损失函数的前部分使labeled data的输出更贴近其label，后部分 $\lambda S$ 作为regularization term，使得labeled data和unlabeled data尽可能满足smoothness assumption.</p>
<p>除了让NN的output满足smoothness的假设，还可以让NN的任何一层的输出满足smoothness assumption，或者让某层外接一层embedding layer，使其满足smoothness assumption，如下图：</p>
<img src="https://s1.ax1x.com/2020/07/03/NXRAaj.png" alt="NXRAaj.png" style="zoom:50%;" />

<h1 id="Better-Representation"><a href="#Better-Representation" class="headerlink" title="Better Representation"></a>Better Representation</h1><p>Better Presentation的思想就是：去芜存菁，化繁为简。</p>
<ul>
<li>Find the latent(潜在的) factors behind the observation.</li>
<li>The latent factors (usually simpler) are better representation.</li>
</ul>
<p>【找到所观察事物的潜在特征，即该事物的better representation】</p>
<p>该部分后续见这篇博客。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><p>挖坑：EM算法详解</p>
</li>
<li><p>挖坑：Graph Laplacian in smoothness.</p>
</li>
<li><p>Olivier Chapelle：Semi-Supervised Learning </p>
</li>
</ol>

        </div>
    
        <ul class="post-copyright">
        <li><strong>本文标题：</strong><a href="https://f1ed.github.io/2020/07/03/semi-supervised/">「机器学习-李宏毅」:Semi-supervised Learning</a></li>
        <li><strong>本文作者：</strong><a href="https://f1ed.github.io">f1ed</a></li>
        <li><strong>本文链接：</strong><a href="https://f1ed.github.io/2020/07/03/semi-supervised/">https://f1ed.github.io/2020/07/03/semi-supervised/</a></li>
        <li><strong>发布时间：</strong>2020-07-03</li>
        <li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！
        </li>
        </ul>
    
        
        <hr style="height:1px;margin:1rem 0"/>
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                <i class="fas fa-tags has-text-grey"></i>&nbsp;
                    <a class="has-link-grey -link" href="/tags/Machine-Learning/" rel="tag">Machine-Learning</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Semi-supervised/" rel="tag">Semi-supervised</a>,&nbsp;<a class="has-link-grey -link" href="/tags/open-classes/" rel="tag">open-classes</a>
                </div>
            </div>
        </div>
        
        
        
        <div class="social-share"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/qrcode_alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/qrcode_wechatpay.jpg" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/06/29/sort-preview/">
                <span class="level-item">「算法导论」:排序-总结</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: true,
        verify: false,
        app_id: 'U7ITnhsJjngmAcJUpFYdrq5m-gzGzoHsz',
        app_key: 'cSRtvM6PbCOJBTVBAUURyFfO',
        placeholder: 'xxxxxxxx'
    });
</script>

    </div>
</div>



<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="" src="/images/profile.png" alt="f1ed">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        f1ed
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        登高不傲，居低不怨。
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>成都</span>
                    </p>
                    
                </div>
            </div>
        </nav>
    <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
		<div>
                <a href="/archives/">
                    <p class="heading">
                        文章
                    </p>
                    <a href="/archives">
                        <p class="title has-text-weight-normal">
                            20
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <a href="/categories">
                        <p class="title has-text-weight-normal">
                            4
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <a href="/tags">
                        <p class="title has-text-weight-normal">
                            22
                        </p>
                    </a>
                </div>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/f1ed" target="_blank" rel="noopener">
                关注我</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/f1ed">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="RSS" href="/atom.xml">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
        </div>
        
    </div>
</div>

    
        

   <div class="card widget column-left is-sticky" id="toc">
       <div class="card-content" style="max-height:calc(100vh - 22px);overflow:scroll">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><ul class="menu-list"><li>
        <a class="is-flex" href="#Introduction">
        <span class="has-mr-6">1.1</span>
        <span>Introduction</span>
        </a></li><li>
        <a class="is-flex" href="#Why-Semi-supervised-learning-helps">
        <span class="has-mr-6">1.2</span>
        <span>Why Semi-supervised learning helps</span>
        </a></li></ul><li>
        <a class="is-flex" href="#Semi-supervised-Learning-for-Generative-Model">
        <span class="has-mr-6">2</span>
        <span>Semi-supervised Learning for Generative Model</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Supervised-Generative-Model">
        <span class="has-mr-6">2.1</span>
        <span>Supervised Generative Model</span>
        </a></li><li>
        <a class="is-flex" href="#Semi-supervised-Generative-Model">
        <span class="has-mr-6">2.2</span>
        <span>Semi-supervised Generative Model</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#EM">
        <span class="has-mr-6">2.2.1</span>
        <span>EM</span>
        </a></li><li>
        <a class="is-flex" href="#Why-EM">
        <span class="has-mr-6">2.2.2</span>
        <span>Why EM</span>
        </a></li></ul></li></ul></li><li>
        <a class="is-flex" href="#Low-density-Separation-Assumption">
        <span class="has-mr-6">3</span>
        <span>Low-density Separation Assumption</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Self-training">
        <span class="has-mr-6">3.1</span>
        <span>Self-training</span>
        </a></li><li>
        <a class="is-flex" href="#Hard-Label">
        <span class="has-mr-6">3.2</span>
        <span>Hard Label</span>
        </a></li><li>
        <a class="is-flex" href="#Entropy-based-Regularization">
        <span class="has-mr-6">3.3</span>
        <span>Entropy-based Regularization</span>
        </a></li><li>
        <a class="is-flex" href="#Outlook-Semi-supervised-SVM">
        <span class="has-mr-6">3.4</span>
        <span>Outlook: Semi-supervised SVM</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Smoothness-Assumption">
        <span class="has-mr-6">4</span>
        <span>Smoothness Assumption</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Cluster-and-then-Label">
        <span class="has-mr-6">4.1</span>
        <span>Cluster and then Label</span>
        </a></li><li>
        <a class="is-flex" href="#Graph-based-Approach">
        <span class="has-mr-6">4.2</span>
        <span>Graph-based Approach</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Graph-Construction">
        <span class="has-mr-6">4.2.1</span>
        <span>Graph Construction</span>
        </a></li><li>
        <a class="is-flex" href="#Graph-based-Approach-1">
        <span class="has-mr-6">4.2.2</span>
        <span>Graph-based Approach</span>
        </a></li><li>
        <a class="is-flex" href="#Smoothness-Definition">
        <span class="has-mr-6">4.2.3</span>
        <span>Smoothness Definition</span>
        </a></li><li>
        <a class="is-flex" href="#Smoothness-Regularization">
        <span class="has-mr-6">4.2.4</span>
        <span>Smoothness Regularization</span>
        </a></li></ul></li></ul></li><li>
        <a class="is-flex" href="#Better-Representation">
        <span class="has-mr-6">5</span>
        <span>Better Representation</span>
        </a></li><li>
        <a class="is-flex" href="#Reference">
        <span class="has-mr-6">6</span>
        <span>Reference</span>
        </a></li></ul>
            </div>
        </div>
    </div>


    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>


                

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」:Semi-supervised Learning" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2020 f1ed&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv"> 来访 <span id="busuanzi_value_site_uv"></span>人</span>
                <span id="busuanzi_container_site_pv">, 总访问 <span id="busuanzi_value_site_pv"></span>次</span>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                        <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="f1ed&#39;s GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'https://f1ed.github.io',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</body>
</html>
