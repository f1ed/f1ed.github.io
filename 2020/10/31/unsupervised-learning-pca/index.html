<!DOCTYPE html>
<html  lang="zh">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="baidu-site-verification" content="9ce18CEmjH" />
<meta name="google-site-verification" content="GSUThrU_AtZE-dgdz1QWWouv0L2teKqHWrZg7DfHbXo" />
<meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.1" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>「机器学习-李宏毅」:Unsupervised-PCA - fred&#39;s blog</title>


    <meta name="description" content="这篇文章详细讲解了无监督学习（Unsupervised learning）的PCA（主成分分析法）。 文章开篇从聚类（Clustering）引出Distributed Represention，其中粗略阐述了聚类中K-means和HAC（层次聚类）的思想。 文章的后半部分具体阐述了PCA的数学细节，PCA的去相关性性质，PCA的另一种解释角度（component的角度），PCA的不足等。">
<meta property="og:type" content="article">
<meta property="og:title" content="「机器学习-李宏毅」:Unsupervised-PCA">
<meta property="og:url" content="https://f7ed.com/2020/10/31/unsupervised-learning-pca/index.html">
<meta property="og:site_name" content="fred&#39;s blog">
<meta property="og:description" content="这篇文章详细讲解了无监督学习（Unsupervised learning）的PCA（主成分分析法）。 文章开篇从聚类（Clustering）引出Distributed Represention，其中粗略阐述了聚类中K-means和HAC（层次聚类）的思想。 文章的后半部分具体阐述了PCA的数学细节，PCA的去相关性性质，PCA的另一种解释角度（component的角度），PCA的不足等。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://f7ed.com/gallery/thumbnails/82937235_p0_master1200.jpg">
<meta property="article:published_time" content="2020-10-30T16:00:00.000Z">
<meta property="article:modified_time" content="2020-10-31T14:38:26.963Z">
<meta property="article:author" content="f1ed">
<meta property="article:tag" content="Machine-Learning">
<meta property="article:tag" content="open-classes">
<meta property="article:tag" content="Unsupervised">
<meta property="article:tag" content="PCA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://f7ed.com/gallery/thumbnails/82937235_p0_master1200.jpg">





<link rel="alternative" href="/atom.xml" title="「机器学习-李宏毅」:Unsupervised-PCA" type="application/atom+xml">



<link rel="icon" href="/images/heart.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171512660-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-171512660-1');
</script>

    
    <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?b1d53a77276f7e423bc7cf8fafd95b75";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body class="is-3-column">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」:Unsupervised-PCA" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/atom.xml">RSS</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    <a class="navbar-item" target="_blank" rel="noopener" title="My GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="/gallery/thumbnails/82937235_p0_master1200.jpg" alt="「机器学习-李宏毅」:Unsupervised-PCA">
        </span>
    </div>
    
    <div class="card-content article ">
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <i class="fas fa-bars"></i>「机器学习-李宏毅」:Unsupervised-PCA
            
        </h1>
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-10-30T16:00:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-10-31</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2020-10-31T14:38:26.963Z"><i class="far fa-calendar-check">&nbsp;</i>2020-10-31</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/">机器学习-李宏毅</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    <i class="far fa-clock"></i>&nbsp;
                    
                    
                    34 分钟 读完 (大约 5082 个字)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span>次访问
                </span>
                
            </div>
        </div>
        
        <div class="content">
            <p>这篇文章详细讲解了无监督学习（Unsupervised learning）的PCA（主成分分析法）。</p>
<p>文章开篇从聚类（Clustering）引出Distributed Represention，其中粗略阐述了聚类中K-means和HAC（层次聚类）的思想。</p>
<p>文章的后半部分具体阐述了PCA的数学细节，PCA的去相关性性质，PCA的另一种解释角度（component的角度），PCA的不足等。</p>
<a id="more"></a>

<h1 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h1><p>无监督学习分为两种：</p>
<ul>
<li><p>Dimension Reduction：化繁为简。</p>
<p>function 只有input，能将高维、复杂的输入，抽象为低维的输出。</p>
<p>如下图，能将3D的折叠图像，抽象为一个2D的表示（把他摊开）。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaoiAe.png" alt="BaodiAe.png" style="zoom:25%;" />
</li>
<li><p>Generation：无中生有。</p>
<p>function 只有output。</p>
<p>（后面的博客会提及）</p>
</li>
</ul>
<h2 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h2><p>此前，在<a href="/2020/07/03/semi-supervised/" title="semi-supervised learning">semi-supervised learning</a>的最后，提及过better presentation的思想，Dimension Reduction 其实就是这样的思想：去芜存菁，化繁为简。</p>
<p>比如，在MNIST中，一个数字的表示是28*28维度的向量（图如左），但大多28 *28维度的向量（图为右）都不是数字。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaopnK.png" alt="BaopnK.png" style="zoom:33%;" /> 

<p>因此，在表达下图一众“3”的图像中，根本不需要28*28维的向量表示，1-D即可表示一张图（图片的旋转角度）。28 * 28的图像表示就像左边中老者的头发，1-D的表示就像老者的头，是对头发运动轨迹一种更简单的表达。</p>
<img src="https://s1.ax1x.com/2020/10/31/Bao90O.png" alt="Bao90O.png" style="zoom:30%;" />

<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><p>在将Dimension Reduction之前，先将一种经典的无监督学习——clustering.</p>
<p>clustering也是一种降维的表达，将复杂的向量空间抽象为简单的类别，用某一个类别来表示该数据点。</p>
<p>这里主要讲述cluster的主要思想，算法细节可参考<a href="https://zhuanlan.zhihu.com/p/34168766">其他资料</a> 。（待补充）</p>
<h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><p>K-means的做法是：</p>
<ol>
<li><p>Clustering $X=\left\{x^{1}, \cdots, x^{n}, \cdots, x^{N}\right\}$ into K clusters.</p>
<p>把所有data分为K个类，K的确定是empirical的，需要自己确定</p>
</li>
<li><p>Initialize cluster center $c^i, i=1,2,…,K$ .(K random $x^n$ from $X$)</p>
<p>初始化K个类的中心数据点，建议从training set $X$ 中随机选K 个点作为初始点。</p>
<p>不建议直接在向量空间中随机初始化K个中心点，因为很可能随机的中心点不属于任何一个cluster。</p>
</li>
<li><p>Repeat：根据中心点标记所属类，再更新新的中心点，重复直收敛。</p>
<ol>
<li><p>For all $x^n$ in $X$ : 标记所属类。</p>

      $$
      b_{i}^{n}\left\{\begin{array}{ll}1 & x^{n} \text { is most "close" to } c^{i} \\ 0 & \text { Otherwise }\end{array}\right.
      $$
      
</li>
<li><p>Updating all $c^i$ :   $c^{i}=\sum_{x^{n}} b_{i}^{n} x^{n} / \sum_{x^{n}} b_{i}^{n}$  (计算该类中心点)</p>
</li>
</ol>
</li>
</ol>
<h3 id="HAC：Hierarchical-Agglomerative-Clustering-HAC"><a href="#HAC：Hierarchical-Agglomerative-Clustering-HAC" class="headerlink" title="HAC：Hierarchical Agglomerative Clustering(HAC)"></a>HAC：Hierarchical Agglomerative Clustering(HAC)</h3><p>另一种clustering的方法是层次聚类（Hierarchical Clustering），这里介绍Agglomerative（自下而上）的策略。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIzX6.png" alt="BaIzX6.png" style="zoom:33%;" />

<ol>
<li><p>Build a tree.</p>
<ol>
<li>如上图中，计算当前两两数据点（点或组合）的相似度（欧几里得距离或其他）。</li>
<li>选出最相近的两个合为一组（即连接在同一父子结点上，如最左边的两个）</li>
<li>重复1-2直至最后合为root。</li>
</ol>
<p>该树中，越早分支的点集合，说明越不像。</p>
</li>
<li><p>Pick a threshold.</p>
<p>选一个阈值，即从哪个地方开始划开，比如选上图中红色的线作为阈值，那么点集分为两个cluseter，蓝色、绿色同理。</p>
</li>
</ol>
<p>HAC和K-means相比，HAC不直接决定cluster的数目，而是通过决定threshold的值间接决定cluster的数目。</p>
<h2 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h2><p>Cluster：an object must belong to one cluster.</p>
<p>在做聚类时，一个数据点必须标注为某一具体类别。这往往会丢失很多信息，比如一个人可能是70%的外向，30%的内敛，如果做clustering，就将这个人直接归为外向，这样的表示过于粗糙。</p>
<p>因此仍用vector来表示这个人，如下图。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIv11.png" alt="BaIv11.png" style="zoom:33%;" /> 

<p>Distributed Representation（也叫Dimension Reduction）就是：一个高维的vector通过function，得到一个低维的vector。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIOh9.png" alt="BaIOh9.png" style="zoom:33%;" />

<p>Distributed的方法有常见的两种：</p>
<ul>
<li><p>Feature selection：</p>
<p>如下图数据点的分布，可以直接选择feature $x_2$ .</p>
<img src="https://s1.ax1x.com/2020/10/31/BaILtJ.png" alt="BaIdLtJ.png" style="zoom:33%;" /> 

<p>但这种方法往往只能处理2-D的情况，对于下图这种3-D情况往往不好做特征选择。</p>
<img src="https://s1.ax1x.com/2020/10/31/Ba70p9.png" alt="Ba70pd9.png" style="zoom:50%;" /> 
</li>
<li><p>Principle component analysis（PCA）</p>
<p>另一种方法就是著名的PCA，主成分分析法。</p>
<p>PCA中，这个function就是一个简单的linear function（$W$），通过 $z=Wx$ ，将高维的 $x$ 转化为低维的 $z$ .</p>
</li>
</ul>
<h1 id="PCA：Principle-Component-Analysis"><a href="#PCA：Principle-Component-Analysis" class="headerlink" title="PCA：Principle Component Analysis"></a>PCA：Principle Component Analysis</h1><p>PCA的参考资料见Bishop, Chapter12.</p>
<p>PCA就是要找 $z=Wx$ 中的 $W$ .</p>
<h2 id="Main-Idea"><a href="#Main-Idea" class="headerlink" title="Main Idea"></a>Main Idea</h2><h3 id="Reduce-1-D"><a href="#Reduce-1-D" class="headerlink" title="Reduce 1-D"></a>Reduce 1-D</h3><p>如果将dimension reduce to 1-D，那么可以得出 $z_1 = w^1\cdot x$ .</p>
<p>$w^1$ 是vector，$x$ 是vector，做内积。</p>
<p>如下图，内积即投影，将所有的点 $x$ 投影到 $w^1$ 方向上，然后得到对应的 $z_1$  值。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIjpR.png" alt="BaIjpR.png" style="zoom:33%;" /> 

<p>而对于得到的一系列 $z_1$ 值，我们希望 $z_1$ 的variance越大越好。</p>
<p>因为 $z_1$ 的分布越大，用 $z_1$ 来刻画数据，才能更好的区分数据点。</p>
<p>如下图，如果 $w^1$ 的方向是small variance的方向，那么这些点会集中在一起，而large variance方向，$z_1$ 能更好的刻画数据。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIH7F.png" alt="BadIH7F.png" style="zoom:33%;" />

<p>$z_1$ 的数学表达是： $ \operatorname{Var}\left(z_{1}\right)=\frac{1}{N} \sum_{z_{1}}\left(z_{1}-\overline{z_{1}}\right)^{2} \quad \left\|w^{1}\right\|_{2}=1$  (后文解释为什么要 $w^1$ 的长度为1)</p>
<h3 id="Reduce-2-D"><a href="#Reduce-2-D" class="headerlink" title="Reduce 2-D"></a>Reduce 2-D</h3><p>同理，如果将dimension reduce to 2-D .</p>
<p>$z=Wx$ 即</p>

$$
\left\{ \begin{array}{11}z_1=w^1\cdot x \\ z_2=w^2 \cdot x  \end{array} \right. ,\quad W=\begin{bmatrix}(w^1)^T \\ (w^2)^T \end{bmatrix}
$$


<ul>
<li><p>将所有点 $x$ 投影到 $w^1$ 方向，得到对应的 $z_1$ ，且让 $z_1$ 的分布尽可能的大：</p>

  $$
  \operatorname{Var}\left(z_{1}\right)=\frac{1}{N} \sum_{z_{1}}\left(z_{1}-\overline{z_{1}}\right)^{2} ,\quad \left\|w^{1}\right\|_{2}=1
  $$
  
</li>
<li><p>将所有点投影到 $w^2$ 方向，得到对应的 $z_2$ ，同样让 $z_2$ 的分布也尽可能大，再加一个约束条件，让 $w^2$ 和 $w^1$ 正交（后文会具体解释为什么）</p>

  $$
  \operatorname{Var}\left(z_{2}\right)=\frac{1}{N} \sum_{z_{2}}\left(z_{2}-\overline{z_{2}}\right)^{2} ,\quad \left\|w^{2}\right\|_{2}=1 ,\quad w^1\cdot w^2=0
  $$
  

<p>因此矩阵 $W$ 是Orthogonal matrix (正交矩阵)。</p>
</li>
</ul>
<h2 id="Detail-Warning-of-Math"><a href="#Detail-Warning-of-Math" class="headerlink" title="Detail[Warning of Math"></a><font color=#f00>Detail[Warning of Math</font></h2><p><strong>想跳过math部分的，可以直接看Conclusion。</strong> </p>
<p>1-D中：</p>
<p>Goal：find $w^1$ to  maximum $(w^1)^T S w^1$  s.t.$(w^1)^Tw^1=1$</p>
<p><strong>结论：$w^1$ 就是协方差矩阵 $S$ 最大特征值 $\lambda_1 $  对应的特征向量。 s.t.$(w^1)^Tw^1=1$</strong> </p>
<p>2-D中：</p>
<p>Goal：find $w^2$ to maximum $(w^2)^TSw^2 $  s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ </p>
<p><strong>结论：$w^2$ 就是协方差矩阵$S$ 第二大特征值 $\lambda_2 $   对应的特征向量。 s.t.$(w^2)^Tw^2=1$</strong>  </p>
<p>k-D中：</p>
<p><strong>结论：$w$ 就是协方差矩阵 $S$ 前 $k$ 大的特征值对应的特征向量。s.t. $W$ 是正交矩阵。</strong></p>
<h3 id="1-D"><a href="#1-D" class="headerlink" title="1-D"></a>1-D</h3><p><strong>Goal：Find $w^1$  to maximum the variance of $z_1$ .</strong> </p>
<ul>
<li> $\operatorname{Var}\left(z_{1}\right)=\frac{1}{N} \sum_{z_{1}}\left(z_{1}-\overline{z_{1}}\right)^{2}$  

<ul>
<li> $z_1=w^1\cdot x  ,\quad \overline{z_{1}}=\frac{1}{N} \sum_{z_{1}}=\frac{1}{N} \sum w^{1} \cdot x=w^{1} \cdot \frac{1}{N} \sum x=w^{1} \cdot \bar{x}$  
</li>
</ul>
</li>
<li>   $\operatorname{Var}\left(z_{1}\right)=\frac{1}{N} \sum_{z_{1}}\left(z_{1}-\overline{z_{1}}\right)^{2}=(w^1)^T\operatorname{Cov}(x)w^1$  

<ul>
<li>  $=\frac{1}{N} \sum_{x}\left(w^{1} \cdot x-w^{1} \cdot \bar{x}\right)^{2} $ 
</li>
<li>  $=\frac{1}{N} \sum\left(w^{1} \cdot(x-\bar{x})\right)^{2}$ 

<ul>
<li><p>$a,b$ 是vector：</p>
 $(a\cdot b)^2=(a^Tb)^2=a^Tba^Tb$ 
</li>
<li><p>$a^Tb$ 是scalar:</p>
 $(a\cdot b)^2  = (a^Tb)^2=a^Tba^Tb =a^Tb(a^Tb)^T=a^Tbb^Ta$ 
</li>
</ul>
</li>
<li> $=\frac{1}{N} \sum\left(w^{1}\right)^{T}(x-\bar{x})(x-\bar{x})^{T} w^{1}$ 
</li>
<li> $ = \left(w^{1}\right)^{T}\sum\frac{1}{N}(x-\bar{x})(x-\bar{x})^{T} \ w^{1}$ 
</li>
<li> $=(w^1)^T\operatorname{Cov}(x)w^1$ 
</li>
</ul>
</li>
<li><p>令 $S=\operatorname{Cov}(x)$ </p>
</li>
</ul>
<p>之前遗留的两个问题：</p>
<ol>
<li>$\left|w^1\right|_2=1$ ?</li>
<li>$w^1\cdot w^2=1$ ?</li>
</ol>
<p>现在来看第一个问题，为什么要 $\left|w^1\right|_2=1$ ？</p>
<p>现在的目标，变成了 maximum $(w^1)^T S w^1$ ，如果不限制 $\left|w^1\right|_2$ ，让 $\left|w^1\right|_2$ 无穷大，那么 $(w^1)^T S w^1$ 的值也会无穷大，问题无解了。</p>
<hr>
<p><strong>Goal：maximum $(w^1)^T S w^1$  s.t. $(w^1)^Tw^1=1$</strong></p>
<ul>
<li><p>Lagrange multiplier[挖坑] 求解多元变量在有限制条件下的驻点。</p>
<p>构造拉格朗日函数： $g\left(w^{1}\right)=\left(w^{1}\right)^{T} S w^{1}-\alpha\left(\left(w^{1}\right)^{T} w^{1}-1\right)$   ，$\alpha\neq 0$ 为拉格朗日乘数</p>
<ul>
<li>$\nabla_{w^1}g=0$ 的值为驻点（会单独写一篇博客来讲拉格朗日乘数）</li>
<li>$\frac{\partial g}{\partial \alpha}=0$ 为限制函数</li>
</ul>
</li>
<li><p>对矩阵微分：详情见<a href="https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors">wiki</a> </p>
<ul>
<li><p>scalar-by-vector(scalar对vector微分)</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIIXV.png" alt="BaIIddXV.png" style="zoom:67%;" />
</li>
<li><p>$S$ 是对称矩阵，不是 $w^1$ 的函数，结果用 $w^1$ 表达：$2Sw^1-2\alpha w^1=0$ </p>
</li>
</ul>
</li>
<li><p>maximum: $(w^1)^T S w^1=\alpha (w^1)^Tw^1=\alpha$ </p>
</li>
</ul>
<p>*<em>Goal：find $w^1$to maximum $\alpha$   *</em>    </p>
<ul>
<li><p>$\alpha$ 满足等式：$Sw^1=\alpha w^1$ </p>
</li>
<li><p>$\alpha$ 是 $S$ 的特征向量，$w^1$ 是 $S$ 对应于特征值 $\alpha$  的特征向量。</p>
<ul>
<li>关于特征值和特征向量的知识参考：参考下面线代知识</li>
</ul>
</li>
<li><p>$w^1$ is the eigenvector(特征向量) of the covarivance matrix S corresponding to the largest eigenvalue $\lambda_1$ . </p>
<p><strong>结论：$w^1$ 就是协方差矩阵最大特征值对应的特征向量。</strong> </p>
</li>
</ul>
<hr>
<blockquote>
<p><strong>梦回线代</strong>QWQ（自己线代学的太差啦 啊这！</p>
<ol>
<li><p>特征向量，特征值定义：</p>
<p>$A$ 是n阶方阵，如果存在数 $\lambda$ 和n维非零向量 $\alpha$ ，满足 $A\alpha=\lambda \alpha$ ,</p>
<p>则称 $\lambda$ 为方阵 $A$ 的一个特征值，$\alpha$ 为方阵 $A$ 对应于特征值 $\lambda$ 的一个特征向量。</p>
</li>
<li><p>求解特征向量和特征值：</p>
<p>$A\alpha -\lambda \alpha=(A-\lambda I)\alpha=0$ </p>
<p>齐次方程有非零解的充要条件是特征方程 $det(A-\lambda I)=0$ （行列式为0）</p>
<ul>
<li>根据特征方程先求解出 $\lambda$ 的所有值。</li>
<li>再根将 $\lambda$ 代入齐次方程，求解齐次方程的解 $\alpha$ ，即为对应 $\lambda$ 的特征向量。</li>
</ul>
</li>
</ol>
</blockquote>
<h3 id="2-D"><a href="#2-D" class="headerlink" title="2-D"></a>2-D</h3><p><strong>Goal：find $w^2$ to maximum $(w^2)^TSw^2 $  s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$</strong>  </p>
<ul>
<li>构造拉格朗日函数： $g\left(w^{2}\right)=\left(w^{2}\right)^{T} S w^{2}-\alpha\left(\left(w^{2}\right)^{T} w^{2}-1\right)-\beta\left(\left(w^{2}\right)^{T} w^{1}-0\right)$ </li>
<li>对 $w^2$ 求微分，所求点满足等式： $S w^{2}-\alpha w^{2}-\beta w^{1}=0$ <ul>
<li>左乘 $(w^1) ^T$： $(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta(w^1)^Tw^1=0$ </li>
<li>已有： $(w^1)^Tw^2=0, (w^1)^Tw^1=1$  </li>
<li>证明：$ (w^1)^TSw^2=0$ <ul>
<li>$\because (w^1)^TSw^2$ 是scalar</li>
<li> $\therefore (w^1)^TSw^2=((w^1)^TSw^2)^T=(w^2)^TS^Tw^1$  </li>
<li>$\because S^T=S$ (协方差矩阵是对称矩阵)</li>
<li>$\because Sw^1=\lambda_1 w^1$ </li>
<li>$\therefore (w^1)^TSw^2=(w^2)^TSw^1=\lambda_1(w^2)^Tw^1=0$  </li>
</ul>
</li>
<li> $(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta(w^1)^Tw^1=0-\alpha\cdot 0-\beta \cdot 1=0$ </li>
<li>$\therefore \beta=0$  </li>
</ul>
</li>
<li>$w^2$ 满足等式：$S w^{2}-\alpha w^{2}=0$ </li>
<li>和1-D的情况相同：find $w^2$ maximum $(w^2)^TSw^2$ <ul>
<li>$(w^2)^TSw^2=\alpha$ </li>
<li>$w^2$  is the eigenvector(特征向量) of the covarivance matrix S corresponding to the largest eigenvalue $\lambda_2$ .</li>
</ul>
</li>
<li>OVER!</li>
</ul>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>最后解决之前的Q2：$(w^1)^Tw^2=0$ ?</p>
<ul>
<li><p>先说明一下$S$ 的性质：</p>
<p>是对称矩阵，对应不同特征值对应的特征向量都是正交的。</p>
<p>（参考1，2）</p>
<p>也是半正定矩阵，其特征值都是非负的。</p>
<p>（参考4，5，6）</p>
</li>
<li><p>其次关于 $W$ 的性质 $ W=\begin{bmatrix}(w^1)^T \\ (w^2)^T \\ ...\end{bmatrix}$ ,易得 $W$ 是orthogonal matrix(正交矩阵)。</p>
</li>
<li><p>所以这是一个约束条件，能让PCA的最优化问题转化为求其特征值的问题。</p>
</li>
<li><p>（具体见下一小节：PCA-decorrelation）</p>
<p>其次 $z=Wx$ ，也因为 $W$ 的正交性质，让 $z$ 的各维度（特征）decorrelation，去掉相关性，降维后的特征相互独立，方便后面generative model的假设。</p>
</li>
</ul>
<blockquote>
<ol>
<li><p>$S=Cov(x)$ 为实对称矩阵。</p>
</li>
<li><p>实对称矩阵的性质：$A$ 是一个实对称矩阵，对于于 $A$ 的不同特征值的特征向量彼此正交。</p>
</li>
<li><p>正交矩阵的性质：$W^TW=WW^T=I$ </p>
</li>
<li><p>$Var(z)=(w^1)^T S w^1\geq 0$ ，方差一定大于等于0 。</p>
</li>
<li><p>半正定矩阵的定义：</p>
<p>实对称矩阵 $A$ ，对任意非零实向量 $X$ ，如果二次型 $f(X)=X^TAX\geq0$ ，</p>
<p>则有实对称矩阵 $A$ 是半正定矩阵。</p>
</li>
<li><p>半正定矩阵的性质：半正定矩阵的特征值都是非负的。</p>
</li>
</ol>
</blockquote>
<hr>
<p>1-D中：</p>
<p>Goal：find $w^1$ to  maximum $(w^1)^T S w^1$  s.t.$(w^1)^Tw^1=1$</p>
<p><strong>结论：$w^1$ 就是协方差矩阵 $S$ 最大特征值 $\lambda_1 $  对应的特征向量。 s.t.$(w^1)^Tw^1=1$</strong> </p>
<p>2-D中：</p>
<p>Goal：find $w^2$ to maximum $(w^2)^TSw^2 $  s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ </p>
<p><strong>结论：$w^2$ 就是协方差矩阵$S$ 第二大特征值 $\lambda_2 $   对应的特征向量。 s.t.$(w^2)^Tw^2=1$</strong>  </p>
<p>k-D中：</p>
<p><strong>结论：$w$ 就是协方差矩阵 $S$ 前 $k$ 大的特征值对应的特征向量。s.t. $W$ 是正交矩阵。</strong> </p>
<h2 id="PCA-decorrelation"><a href="#PCA-decorrelation" class="headerlink" title="PCA-decorrelation"></a>PCA-decorrelation</h2><p>$z=Wx$ </p>
<p>通过PCA找到的 $W$ ，$x$ 得到新的presentation $z$ ，如下图。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaITmT.png" alt="BaIdTmT.png" style="zoom:40%;" />

<p>可见，经过PCA后，original data变为decorrelated data，各维度（feature）是去相关性的，即各维度是独立的，方便generative model的假设（比如Gaussian distribution).</p>
<p> $z$ 是docorrelated，即 $Cov(z)=D$ 是diagonal matrix(对角矩阵)</p>
<p>证明：$Cov(z)=D$ is diagonal matrix</p>
<ul>
<li> $W=\begin{bmatrix}(w^1)^T \\ (w^2)^T \\ ...\end{bmatrix}$ ，$S=\operatorname{Cov}(x)$ 
</li>
<li> $\operatorname{Cov}(z)=\frac{1}{N} \sum(z-\bar{z})(z-\bar{z})^{T}=W S W^{T}$ </li>
<li> $=W S\left[\begin{array}{lll}w^{1} & \cdots & w^{K}\end{array}\right]=W\left[\begin{array}{lll}S{w}^{1} & \cdots & S w^{K}\end{array}\right]$ </li>
<li> $=W\left[\lambda_{1} w^{1} \quad \cdots \quad \lambda_{K} w^{K}\right]=\left[\lambda_{1} W w^{1} \quad \cdots \quad \lambda_{K} W w^{K}\right]$   ($\lambda$ is scalar) </li>
<li> $=\left[\begin{array}{lll}\lambda_{1} e_{1} & \cdots & \lambda_{K} e_{K}\end{array}\right]=D$ ($W$ is orthogonal matrix) 

</li>
</ul>
<h2 id="PCA-Another-Point-of-View"><a href="#PCA-Another-Point-of-View" class="headerlink" title="PCA-Another Point of View"></a>PCA-Another Point of View</h2><h3 id="Main-Idea-Component"><a href="#Main-Idea-Component" class="headerlink" title="Main Idea: Component"></a>Main Idea: Component</h3><p>PCA看作是一些basic component的组成，如下图，手写数字都是一些基本笔画组成的，记做  $\{u^1,u^2,u^3,...\}$ </p>
<img src="https://s1.ax1x.com/2020/10/31/BaI4lq.png" alt="BaId4lq.png" style="zoom:25%;" />

<p>因此，下图的”7”的组成为 $\{u^1,u^3,u^5\}$ </p>
<img src="https://s1.ax1x.com/2020/10/31/BaIhpn.png" alt="BadIhpn.png" style="zoom:25%;" />

<p>所以原28*28 vector $x$ 表示的图像能近似表示为：</p>

$$
x \approx c_{1} u^{1}+c_{2} u^{2}+\cdots+c_{K} u^{K}+\bar{x}
$$


<p>其中  $\{u^1,u^2,u^3,...\}$ 是compoment的vector表示， $\{c^1,c^2,c^3,...\}$ 是component的系数，$\bar{x}$ 是所有images的平均值。</p>
<p>因此 $\begin{bmatrix}c_1 \\c_2 \\... \\ c_k \end{bmatrix}$ 也能表示一个数字图像。</p>
<p>现在问题是找到这些component $\{u^1,u^2,u^3,...\}$ , 再得到 他的线形表出 $\begin{bmatrix}c_1 \\c_2 \\... \\ c_k \end{bmatrix}$ 就是我们想得到的better presentation.</p>
<h3 id="Detail"><a href="#Detail" class="headerlink" title="Detail"></a>Detail</h3><p>要满足：$x \approx c_{1} u^{1}+c_{2} u^{2}+\cdots+c_{K} u^{K}+\bar{x}$  </p>
<p>即，$x -\bar{x}\approx c_{1} u^{1}+c_{2} u^{2}+\cdots+c_{K} u^{K}$ ，等式两边的误差要尽量小。</p>
<p>问题变成：找 $\{u^1,u^2,u^3,...\}$ minimize the reconstruction error = $\|(x-\bar{x})-\hat{x}\|_2$ .</p>
<p>损失函数： $L=\min _{\left\{u^{1}, \ldots, u^{K}\right\}} \sum\left\|(x-\bar{x})-\left(\sum_{k=1}^{K} c_{k} u^{k}\right)\right\|_{2}$ </p>
<p>而求解PCA的过程就是在minimize损失函数 $L$ ，PCA中求解出的  $\{w^1,w^2,...,w^K\}$ 就是这里的component  $\{u^1,u^2,...,u^K\}$ .(Proof 见Bisho, Chapter 12.1.2)</p>
<p>*<em>Goal:  minimize the reconstruction error = $\|(x-\bar{x})-\hat{x}\|_2$ *</em>  </p>
<ul>
<li><p>$x -\bar{x}\approx c_{1} u^{1}+c_{2} u^{2}+\cdots+c_{K} u^{K}$ </p>
</li>
<li><p>每个sample:  $\left\{ \begin{matrix} x^{1}-\bar{x} \approx c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\cdots \\ x^{2}-\bar{x} \approx c_{1}^{2} u^{1}+c_{2}^{2} u^{2}+\cdots \\x^{3}-\bar{x} \approx c_{1}^{3} u^{1}+c_{2}^{3} u^{2}+\cdots \\ ...\end{matrix} \right.$  </p>
<ul>
<li><p>下图中 $X=x-\bar{x}$ 矩阵的第一列都和上面的 $x^1-\bar{x}$ 对应：</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIWfs.png" alt="BaIWfds.png" style="zoom:25%;" /> 
</li>
<li><p>而上面的 $c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\cdots$ 和下图的component矩阵乘系数矩阵的第一列对应：</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIRYj.png" alt="BaIRYj.png" style="zoom:25%;" /> 
</li>
</ul>
</li>
<li><p>因此，是要让下图矩阵的结果 minimize error：</p>
<img src="https://s1.ax1x.com/2020/10/31/Ba7dfJ.png" alt="Ba7dfJ.png" style="zoom:40%;" /> 
</li>
<li><p>如何求解: SVD矩阵分解-其实就是最大近似分解（挖坑）</p>
<p>SVD能将一个任意的矩阵，分解为下面三个矩阵的乘积。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIym8.png" alt="BaIym8.png" style="zoom:30%;" />

<p>$X = U\Sigma V$ </p>
<ul>
<li>$U,V$ 都是orthogonal matrix，$\Sigma$ 是diagonal matrix。</li>
<li>组成$U$ (M*K) 的K个列向量是 $XX^T$ 矩阵的前K大特征值对应的特征向量。</li>
<li>组成 $V$ (K*N)的K个行向量是 $X^TX$ 矩阵的前K大特征值对应的特征向量。</li>
<li>$XX^T$ 和 $X^TX$ 的特征值相同</li>
<li>$\Sigma$ 的对角值 $\sigma_i=\sqrt{\lambda_i}$ </li>
</ul>
</li>
<li><p>解：$U$ 矩阵作为 component矩阵， $\Sigma V$ 乘在一起作为系数矩阵。</p>
</li>
</ul>
<hr>
 $U=\{u^1,u^2,u^3,...\}$ 矩阵是$XX^T$ 的特征向量组成正交矩阵。

<p>而PCA的解 $W^T=\{w^1,w^2,...,w^K\}$ 也是特征向量组成的正交矩阵。</p>
<p><strong>所以和PCA的关系：$U$ 矩阵是 $XX^T=Cov(x)$ 的特征向量，所以$U$ 矩阵就是PCA的解。</strong></p>
<h3 id="PCA-NN：Autoencoder"><a href="#PCA-NN：Autoencoder" class="headerlink" title="PCA-NN：Autoencoder"></a>PCA-NN：Autoencoder</h3><p>上文说到求解PCA的解 $\{w^1,w^2,...,w^K\}$ 就是在最小化restruction error $x -\bar{x}\approx \sum_{k=1}^K c_kw^k$ .</p>
<p>两者的联系就是PCA的解 $\{w^1,w^2,...,w^K\}$ 就是component $\{u^1,u^2,u^3,...\}$ ,且PCA的表示是 $z$  对应这里的 $c_k$  (第k个image的表示）.</p>
<p>PCA视角： $z=c_k=(x-\bar{x})\cdot w^k$  </p>
<p>PCA looks like a neural network with one hidden layer(linear activation function)。</p>
<p>把PCA视角看作一个NN，如下图，其hidden layer的激活函数是一个简单的线性激活函数。</p>
<p><img src="https://s1.ax1x.com/2020/10/31/BaIBlt.png" alt="BaIdBlt.png" style="zoom:25%;" /><img src="https://s1.ax1x.com/2020/10/31/BaI0SI.png" alt="BaI0dSI.png" style="zoom:25%;" /></p>
<p>再看component视角： $\hat{x}=\sum_{k=1}^K c_kw^k\approx x-\bar{x}$ </p>
<p><img src="https://s1.ax1x.com/2020/10/31/BaID6P.png" alt="BaIdD6P.png" style="zoom:25%;" /><img src="https://s1.ax1x.com/2020/10/31/BaIJeO.png" alt="BaIdJeO.png" style="zoom:25%;" /></p>
<p>PCA就构成了下面的NN，hidden layer可以是deep，这就是autoencoder(后面的博客会再详细讲)。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIdfA.png" alt="BaIdfA.png" style="zoom:30%;" />

<p>用Gradient Descent对输入输出做minimize error，hidden layer的输出 $c$ 就是我们想要的编码（降维后的编码）。 </p>
<p>Q：用PCA求出的结果和用Gradient Descent训练NN的结果一样吗？</p>
<p>A：当然不一样，PCA的 $w$ 都是正交的，而NN的结果是gradient descent迭代出来的，并且该结果还会于初值有关。</p>
<p>Q：有了PCA，为什么还要用NN呢？</p>
<p>A：因为PCA只能处理linear的情况，对前文那种高维的非线形的无法处理，而NN可以是deep的，能较好处理非线形的情况。</p>
<h2 id="tips-how-many-components"><a href="#tips-how-many-components" class="headerlink" title="tips: how many components?"></a>tips: how many components?</h2><p>比如在对Pokemon进行PCA时，有六个features，如何确定principle component的数目？</p>
<p>往往在实际操作中，会对每个component计算一个ratio，如图中的公式：</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIUFH.png" alt="BaIUFH.png" style="zoom:25%;" /> 

<p>因为每一个component对应一个eigenvector，每个eigenvector对应一个eigenvalue，而这个eigenvalue的值代表了在这个component的维度的variance有多大，越大当然能更好的表示。</p>
<p>因此计算eigenvalue的ratio，来找出分布较大的component作为主成分。</p>
<h2 id="More-About-PCA"><a href="#More-About-PCA" class="headerlink" title="More About PCA"></a>More About PCA</h2><p>如果对MNIST做PCA分析，结果如下图，会发现下面eigen-digits这些并不像数字的某个组成部分：</p>
<img src="https://s1.ax1x.com/2020/10/31/Ba7ym6.png" alt="Ba7dym6.png" style="zoom:35%;" />

<p>同样，对face做PCA分析，结果下图：</p>
<img src="https://s1.ax1x.com/2020/10/31/Ba7BlR.png" alt="Ba7BlR.png" style="zoom:35%;" />

<p>为什么呢？</p>
<p>在MNIST中，一张image的表示如下图：</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIYwD.png" alt="BaIYwD.png" style="zoom:33%;" /> 

<p>其中，$\alpha$ 可以是任意实数，那么就有正有负，所以PCA的解包含了一些真正component的adding up and subtracting，所以MNIST的解不像这些数字的一部分。</p>
<p>如果想得到的解看起来像真正的component，可以规定图像只能是加，即 $\alpha$ 都是非负的。</p>
<ul>
<li>Non-negative matrix factorization(NMF)<ul>
<li>Forcing $\alpha$ be non-negative: additive combination</li>
<li>Forcing $w$ be non-negative: components more like “parts of digits”</li>
</ul>
</li>
</ul>
<h2 id="Weakness-of-PCA"><a href="#Weakness-of-PCA" class="headerlink" title="Weakness of PCA"></a>Weakness of PCA</h2><ol>
<li><p>PCA是unsupervised，因此可能不能区分本来是两个类别的东西。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaItTe.png" alt="BaItTe.png" style="zoom:33%;" /> 

<p>如图，PCA的结果可能是上图的维度方向，但如果引入labeled data，更好的表达应该按照下图LDA的维度方向。</p>
<ul>
<li>LDA (Linear Discriminant Analysis) 是一种supervised的分析方法。</li>
</ul>
</li>
<li><p>PCA是Linear的，前文已经提及过，除了可以用NN的方式也有很多其他的non-linear的解法。</p>
<img src="https://s1.ax1x.com/2020/10/31/BaIaYd.png" alt="BaIaYd.png" style="zoom:45%;" />

</li>
</ol>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><p>HAC的算法细节待补充完善：<a href="https://zhuanlan.zhihu.com/p/34168766">https://zhuanlan.zhihu.com/p/34168766</a></p>
</li>
<li><p>PCA: Bishop, Chapter12. </p>
</li>
<li><p>线代知识：特征值、特征向量、实对称矩阵等：</p>
</li>
<li><p>拉格朗日乘数：Bishop, Appendix E</p>
</li>
<li><p>矩阵微分：<a href="https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors">https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors</a></p>
</li>
<li><p>Proof-PCA的过程就是在minimize损失函数 $L$ :Bisho, Chapter 12.1.2</p>
</li>
<li><p>SVD：</p>
<p><a href="https://www.cnblogs.com/pinard/p/6251584.html">https://www.cnblogs.com/pinard/p/6251584.html</a> </p>
<p><a href="https://www.youtube.com/watch?v=rYz83XPxiZo">https://www.youtube.com/watch?v=rYz83XPxiZo</a></p>
</li>
<li><p>NMF：Non-negative matrix factorization</p>
</li>
<li><p>LDA：Linear Discriminant Analysis</p>
</li>
</ol>

        </div>
    
        <ul class="post-copyright">
        <li><strong>本文标题：</strong><a href="https://f7ed.com/2020/10/31/unsupervised-learning-pca/">「机器学习-李宏毅」:Unsupervised-PCA</a></li>
        <li><strong>本文作者：</strong><a href="https://f7ed.com">f1ed</a></li>
        <li><strong>本文链接：</strong><a href="https://f7ed.com/2020/10/31/unsupervised-learning-pca/">https://f7ed.com/2020/10/31/unsupervised-learning-pca/</a></li>
        <li><strong>发布时间：</strong>2020-10-31</li>
        <li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！
        </li>
        </ul>
    
        
        <hr style="height:1px;margin:1rem 0"/>
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                <i class="fas fa-tags has-text-grey"></i>&nbsp;
                    <a class="has-link-grey -link" href="/tags/Machine-Learning/" rel="tag">Machine-Learning</a>,&nbsp;<a class="has-link-grey -link" href="/tags/PCA/" rel="tag">PCA</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Unsupervised/" rel="tag">Unsupervised</a>,&nbsp;<a class="has-link-grey -link" href="/tags/open-classes/" rel="tag">open-classes</a>
                </div>
            </div>
        </div>
        
        
        
        <div class="social-share"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/qrcode_alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/qrcode_wechatpay.jpg" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2021/01/16/Marxism-in-China/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">「政治」:毛中特</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/10/21/pytorch-tensors/">
                <span class="level-item">「PyTorch」：2-Tensors Explained And Operations</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: true,
        verify: false,
        app_id: 'U7ITnhsJjngmAcJUpFYdrq5m-gzGzoHsz',
        app_key: 'cSRtvM6PbCOJBTVBAUURyFfO',
        placeholder: 'xxxxxxxx'
    });
</script>

    </div>
</div>



<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="" src="/images/profile.png" alt="f1ed">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        f1ed
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        登高不傲，居低不怨。
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>北京</span>
                    </p>
                    
                </div>
            </div>
        </nav>
    <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
		<div>
                <a href="/archives/">
                    <p class="heading">
                        文章
                    </p>
                    <a href="/archives">
                        <p class="title has-text-weight-normal">
                            36
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <a href="/categories">
                        <p class="title has-text-weight-normal">
                            10
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <a href="/tags">
                        <p class="title has-text-weight-normal">
                            54
                        </p>
                    </a>
                </div>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/f1ed" target="_blank" rel="noopener">
                关注我</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/f1ed">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="RSS" href="/atom.xml">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
        </div>
        
    </div>
</div>

    
        

   <div class="card widget column-left is-sticky" id="toc">
       <div class="card-content" style="max-height:calc(100vh - 22px);overflow:scroll">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Unsupervised-Learning">
        <span class="has-mr-6">1</span>
        <span>Unsupervised Learning</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Dimension-Reduction">
        <span class="has-mr-6">1.1</span>
        <span>Dimension Reduction</span>
        </a></li><li>
        <a class="is-flex" href="#Clustering">
        <span class="has-mr-6">1.2</span>
        <span>Clustering</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#K-means">
        <span class="has-mr-6">1.2.1</span>
        <span>K-means</span>
        </a></li><li>
        <a class="is-flex" href="#HAC：Hierarchical-Agglomerative-Clustering-HAC">
        <span class="has-mr-6">1.2.2</span>
        <span>HAC：Hierarchical Agglomerative Clustering(HAC)</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Distributed-Representation">
        <span class="has-mr-6">1.3</span>
        <span>Distributed Representation</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#PCA：Principle-Component-Analysis">
        <span class="has-mr-6">2</span>
        <span>PCA：Principle Component Analysis</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Main-Idea">
        <span class="has-mr-6">2.1</span>
        <span>Main Idea</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Reduce-1-D">
        <span class="has-mr-6">2.1.1</span>
        <span>Reduce 1-D</span>
        </a></li><li>
        <a class="is-flex" href="#Reduce-2-D">
        <span class="has-mr-6">2.1.2</span>
        <span>Reduce 2-D</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Detail-Warning-of-Math">
        <span class="has-mr-6">2.2</span>
        <span>Detail[Warning of Math</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#1-D">
        <span class="has-mr-6">2.2.1</span>
        <span>1-D</span>
        </a></li><li>
        <a class="is-flex" href="#2-D">
        <span class="has-mr-6">2.2.2</span>
        <span>2-D</span>
        </a></li><li>
        <a class="is-flex" href="#Conclusion">
        <span class="has-mr-6">2.2.3</span>
        <span>Conclusion</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#PCA-decorrelation">
        <span class="has-mr-6">2.3</span>
        <span>PCA-decorrelation</span>
        </a></li><li>
        <a class="is-flex" href="#PCA-Another-Point-of-View">
        <span class="has-mr-6">2.4</span>
        <span>PCA-Another Point of View</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Main-Idea-Component">
        <span class="has-mr-6">2.4.1</span>
        <span>Main Idea: Component</span>
        </a></li><li>
        <a class="is-flex" href="#Detail">
        <span class="has-mr-6">2.4.2</span>
        <span>Detail</span>
        </a></li><li>
        <a class="is-flex" href="#PCA-NN：Autoencoder">
        <span class="has-mr-6">2.4.3</span>
        <span>PCA-NN：Autoencoder</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#tips-how-many-components">
        <span class="has-mr-6">2.5</span>
        <span>tips: how many components?</span>
        </a></li><li>
        <a class="is-flex" href="#More-About-PCA">
        <span class="has-mr-6">2.6</span>
        <span>More About PCA</span>
        </a></li><li>
        <a class="is-flex" href="#Weakness-of-PCA">
        <span class="has-mr-6">2.7</span>
        <span>Weakness of PCA</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Reference">
        <span class="has-mr-6">3</span>
        <span>Reference</span>
        </a></li></ul>
            </div>
        </div>
    </div>


    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>


                

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」:Unsupervised-PCA" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2021 f1ed&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
		<br>
		<a href="http://www.beian.miit.gov.cn/" target="_blank">蜀ICP备2020028586号</a>
                
                <br>
                <span id="busuanzi_container_site_uv"> 来访 <span id="busuanzi_value_site_uv"></span>人</span>
                <span id="busuanzi_container_site_pv">, 总访问 <span id="busuanzi_value_site_pv"></span>次</span>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                        <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="f1ed&#39;s GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'https://f7ed.com',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</body>
</html>
