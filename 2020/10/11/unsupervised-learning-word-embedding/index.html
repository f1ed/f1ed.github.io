<!DOCTYPE html>
<html  lang="zh">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="baidu-site-verification" content="9ce18CEmjH" />
<meta name="google-site-verification" content="GSUThrU_AtZE-dgdz1QWWouv0L2teKqHWrZg7DfHbXo" />
<meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.1" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>「机器学习-李宏毅」:Unsupervised Learning：Word Embedding - fred&#39;s blog</title>


    <meta name="description" content="这篇文章主要是介绍一种无监督学习——Word Embedding（词嵌入）。 文章开篇介绍了word编码的1-of-N encoding方式和word class方式，但这两种方式得到的单词向量表示都不能很好表达单词的语义和单词之间的语义联系。 Word Embedding可以很好的解决这个问题。 Word Embedding有count based和prediction based两种方法。文章">
<meta property="og:type" content="article">
<meta property="og:title" content="「机器学习-李宏毅」:Unsupervised Learning：Word Embedding">
<meta property="og:url" content="https://f7ed.com/2020/10/11/unsupervised-learning-word-embedding/index.html">
<meta property="og:site_name" content="fred&#39;s blog">
<meta property="og:description" content="这篇文章主要是介绍一种无监督学习——Word Embedding（词嵌入）。 文章开篇介绍了word编码的1-of-N encoding方式和word class方式，但这两种方式得到的单词向量表示都不能很好表达单词的语义和单词之间的语义联系。 Word Embedding可以很好的解决这个问题。 Word Embedding有count based和prediction based两种方法。文章">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://f7ed.com/gallery/thumbnails/79545396_p0_master1200.jpg">
<meta property="article:published_time" content="2020-10-10T16:00:00.000Z">
<meta property="article:modified_time" content="2020-10-11T15:33:36.817Z">
<meta property="article:author" content="f1ed">
<meta property="article:tag" content="Machine-Learning">
<meta property="article:tag" content="open-classes">
<meta property="article:tag" content="Unsupervised-learning">
<meta property="article:tag" content="Word Embedding">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://f7ed.com/gallery/thumbnails/79545396_p0_master1200.jpg">





<link rel="alternative" href="/atom.xml" title="「机器学习-李宏毅」:Unsupervised Learning：Word Embedding" type="application/atom+xml">



<link rel="icon" href="/images/heart.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171512660-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-171512660-1');
</script>

    
    <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?b1d53a77276f7e423bc7cf8fafd95b75";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body class="is-3-column">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」:Unsupervised Learning：Word Embedding" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/atom.xml">RSS</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    <a class="navbar-item" target="_blank" rel="noopener" title="My GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="/gallery/thumbnails/79545396_p0_master1200.jpg" alt="「机器学习-李宏毅」:Unsupervised Learning：Word Embedding">
        </span>
    </div>
    
    <div class="card-content article ">
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <i class="fas fa-bars"></i>「机器学习-李宏毅」:Unsupervised Learning：Word Embedding
            
        </h1>
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-10-10T16:00:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-10-11</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2020-10-11T15:33:36.817Z"><i class="far fa-calendar-check">&nbsp;</i>2020-10-11</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/">机器学习-李宏毅</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    <i class="far fa-clock"></i>&nbsp;
                    
                    
                    13 分钟 读完 (大约 1923 个字)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span>次访问
                </span>
                
            </div>
        </div>
        
        <div class="content">
            <p>这篇文章主要是介绍一种无监督学习——Word Embedding（词嵌入）。</p>
<p>文章开篇介绍了word编码的1-of-N encoding方式和word class方式，但这两种方式得到的单词向量表示都不能很好表达单词的语义和单词之间的语义联系。</p>
<p>Word Embedding可以很好的解决这个问题。</p>
<p>Word Embedding有count based和prediction based两种方法。文章主要介绍了prediction based的方法，包括如何predict the word vector? 为什么这样的模型works？介绍了prediction based的变体；详细阐述了该模型中sharing parameters的做法和其必要性。</p>
<p>文章最后简单列举了word embedding的相关应用，包括multi-lingual embedding, multi-domain embedding, document embedding 等。</p>
<a id="more"></a>

<h1 id="Word-to-Vector"><a href="#Word-to-Vector" class="headerlink" title="Word to Vector"></a>Word to Vector</h1><p>如何把word转换为vector?</p>
<h2 id="1-of-N-Encoding"><a href="#1-of-N-Encoding" class="headerlink" title="1-of-N Encoding"></a>1-of-N Encoding</h2><p>第一种方法是1-of-N Encoding：</p>
<p>Vector的维度是单词总数，每一维度都代表一个单词。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLLGT.png" alt="0gLLGT.png" style="zoom:25%;" /> 

<p>1-of-N Encoding的方法简单，但这种向量的表示方式not imformative，即向量表示不能体现单词之间的语义关系。</p>
<h2 id="Word-Class"><a href="#Word-Class" class="headerlink" title="Word Class"></a>Word Class</h2><p>对1-of-N Encoding方式改进，Word Class采用聚类cluster的方式，根据类别训练一个分类器。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLqiV.png" alt="0gLdqiV.png" style="zoom:33%;" /> 

<p>但这种人为分类的方式，信息是会部分丢失的，即光做clustering是不够的，会丢失单词的部分信息。</p>
<h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>第三种方式是Word Embedding。（词嵌入）</p>
<p>Word Embedding: Machine learns the meaning of words from reading a lot of documents without supervision.</p>
<p>Word Embedding，机器通过阅读大量文章学习单词的含义，用vector的形式表示单词的语义。训练时只需要给机器大量文章，不需要label，因此是无监督学习。</p>
<h1 id="Word-Embedding-1"><a href="#Word-Embedding-1" class="headerlink" title="Word Embedding"></a>Word Embedding</h1><p>如何做Word Embedding呢？</p>
<h2 id="auto-encoder？"><a href="#auto-encoder？" class="headerlink" title="auto-encoder？"></a>auto-encoder？</h2><p>能否用auto-encoder的方式来做词嵌入呢？</p>
<p>即用1-of-N encoding的方式对单词编码，作为训练的输入和输出。</p>
<p>word2vec时，把model中的某一hidden layer的输出作为该单词的向量表示。</p>
<p>这种方式是不可以的，不可以用auto-encoder。因为auto-encoder不能学到informative的信息，即用auto-encoder表示的向量不能表达word的语义。</p>
<h2 id="Exploit-the-Context"><a href="#Exploit-the-Context" class="headerlink" title="Exploit the Context"></a>Exploit the Context</h2><p>A word can be understood by its context.</p>
<p>所以Word Embedding可以利用上下文来学习word的语义。</p>
<p>如何利用单词的上下文来学习呢？</p>
<ul>
<li><p>Count based</p>
<p>如果两个单词 $w_i$ 和 $w_j$ 在文章中经常同时出现，那么 $V(w_i)$ ( $w_i$ 的向量表示)和 $V(w_j)$ 的向量表示会很close.</p>
<p>E.g. Glove Vector: <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></p>
<p>GloVe的表示法有两个亮点：</p>
<ol>
<li><p>Nearest neighbors：vectors之间的欧几里得距离（或者余弦相似度）能较好表示words之间的语义相似度。</p>
</li>
<li><p>Linear substructures：用GloVe方法表示的vectors有有趣的线性子结构。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLfxg.png" alt="0gLfxg.png" style="zoom:%;" />  
</li>
</ol>
</li>
<li><p>Prediction based</p>
<p>使用预测的方式来表示。</p>
</li>
</ul>
<h2 id="Prediction-based"><a href="#Prediction-based" class="headerlink" title="Prediction based"></a>Prediction based</h2><h3 id="How-to-predict？"><a href="#How-to-predict？" class="headerlink" title="How to predict？"></a>How to predict？</h3><p>prediction based的方法是用前一个单词来预测当前单词。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLHI0.png" alt="0gLHI0.png" style="zoom:25%;" /> 



<img src="https://s1.ax1x.com/2020/10/11/0gLORU.png" alt="0gLORU.png" style="zoom:33%;" /> 

<p>训练时： $w_{i-1}$ 的1-of-N encoding编码作为输入，$w_i$ 的1-of-N encoding的编码作为输出。</p>
<p>NN如上图，$w_{i-1}$ 的1-of-N encoding编码作为输入，输出的vector表示下一个单词是 $w_i$ 的概率。</p>
<p>word2vec : $w_{i-1}$ 的1-of-N encoding编码作为NN的输入，$w_i$ 的向量表示为第一个hidden layer的neurons的输入 $z$ 。</p>
<h3 id="Why-it-works"><a href="#Why-it-works" class="headerlink" title="Why it works?"></a>Why it works?</h3><p>直觉的解释他为什么能work。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gL7aq.png" alt="0gdL7aq.png" style="zoom:33%;" /> 

<p>如上图，蔡英文 宣誓就职 和 马英九 宣誓就职，虽然 $w_{i-1}$ 不同，但NN的输出中，“宣誓就职”的概率应该最大。</p>
<p>即hidden layers必须把不同的 $w_{i-1}$ project到相同的space，要求hidden layer的input是相近的，NN的输出才是相近的。</p>
<h3 id="Prediction-based-：Various-Architecture"><a href="#Prediction-based-：Various-Architecture" class="headerlink" title="Prediction-based ：Various Architecture"></a>Prediction-based ：Various Architecture</h3><p>因为一个单词的下一个单词范围非常大，所以使用前一个单词预测当前单词的方法，performance是较差的。</p>
<p>因此常常会使用多个单词来预测下一个单词，NN的输入是多个单词连接在一起组成的向量，一般NN的输入至少为10个单词，word embedding的performance较好。</p>
<p>除了使用多个单词的方法，prediction-based的方法还用两种变体结构。</p>
<ul>
<li><p>Continuous bag of word (CBOW) model: predicting the word given its context.</p>
<p>使用单词的前后文（前一个单词和后一个单词）来预测当前单词。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gL5rj.png" alt="0gL5drj.png" style="zoom:25%;" /> 
</li>
<li><p>Skip-gram: predicting the context given a word.</p>
<p>使用中间单词来预测单词的前一个单词和后一个单词。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLIqs.png" alt="0gLdIqs.png" style="zoom:25%;" />  

</li>
</ul>
<h3 id="Sharing-Parameters"><a href="#Sharing-Parameters" class="headerlink" title="Sharing Parameters"></a>Sharing Parameters</h3><p>使用多个单词作为NN的输入，提高了word embedding的performance，但也大幅增加了模型训练的参数数量。</p>
<p>使用sharing parameters（共享参数）能大量减少模型的参数数量。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gOfFx.png" alt="0gOfFix.png" style="zoom:33%;" /> 

<p>如上图，输入单词连接到neurons的权重应该是相同的。</p>
<p>除了能减少参数，sharing parameters也是必要的。否则，如果NN的输入的单词顺序交换，那么得到的单词向量是不同的。</p>
<p><strong>How to train sharing parameters?</strong> </p>
<p>假设两个单词相同维度连接到neuron的weight是 $w_i,w_j$ ，在训练中，如何让 $w_i=w_j$ ?</p>
<ol>
<li><p>Given the same initialization.(相同的初始化)</p>
</li>
<li><p>原来的参数更新：<br>$$<br>w_i \longleftarrow w_i - \frac{\partial C}{\partial w_i} \<br>w_j \longleftarrow w_j - \frac{\partial C}{\partial w_j}<br>$$<br>虽然有相同的初始化，但在Backpropagation求偏微分时，$\frac{\partial C}{\partial w_i}$ 和 $\frac{\partial C}{\partial w_j}$ 不一样，那么参数 $w_i$ 和 $w_j$ 更新一次后就不同了。</p>
<p>在训练sharing parameters的参数更新：<br>$$<br>w_i \longleftarrow w_i - \frac{\partial C}{\partial w_i} -\frac{\partial C}{\partial w_j}\<br>w_j \longleftarrow w_j - \frac{\partial C}{\partial w_j}-\frac{\partial C}{\partial w_i}<br>$$<br>这样更新后，$w_i$ 和 $w_j$ 仍保持一致。如果有多个单词，亦然。</p>
</li>
</ol>
<hr>
<p><strong>Word2Vec</strong> </p>
<p>在word2vec时，根据sharing parameters的性质，计算单词的向量表示时，可以简化运算。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLWRS.png" alt="0gLWRS.png" style="zoom:33%;" /> 

<p>如上图，用前文单词 $x_{i-1},x_{i-2}$  表示单词 $x_i$ 的向量表示 $z=W_1x_{i-2}+W_2x_{i-1}=W(x_{i-2}+x_{i-1})$ .</p>
<p>其中  $x_{i-1},x_{i-2}$ 的维度是|V|，$x_i$ 的向量表示 $z$ 的维度是 |Z|，$W_1=W_2=W$ 的维度为|Z|*|V|。</p>
<h1 id="Advantages-of-Word-Embedding"><a href="#Advantages-of-Word-Embedding" class="headerlink" title="Advantages of Word Embedding"></a>Advantages of Word Embedding</h1><p>Word Embedding能得到一些有趣的特性。</p>
<ul>
<li><p>向量之间有趣的线性子结构</p>
<img src="https://s1.ax1x.com/2020/10/11/0gL2Pf.png" alt="0gL2Pf.png" style="zoom:40%;" /> 
</li>
<li><p>相近的向量有相近的语义</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLTZn.png" alt="0gLTZn.png" style="zoom:40%;" /> 
</li>
<li><p>向量之间表示的语义特性</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLRG8.png" alt="0gLRG8.png" style="zoom:35%;" />  

</li>
</ul>
<h2 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h2><h3 id="Multi-lingual-Embedding：实现翻译"><a href="#Multi-lingual-Embedding：实现翻译" class="headerlink" title="Multi-lingual Embedding：实现翻译"></a>Multi-lingual Embedding：实现翻译</h3><img src="https://s1.ax1x.com/2020/10/11/0gORT1.png" alt="0gORdT1.png" style="zoom:40%;" />  

<p>不同语言之间分开训练，训练出的不同语言所对应词汇的向量表示肯定不同，再将对应词汇的向量project到同一点，即实现了翻译。</p>
<h3 id="Multi-domain-Embedding"><a href="#Multi-domain-Embedding" class="headerlink" title="Multi-domain Embedding"></a>Multi-domain Embedding</h3><p>还可以做影像嵌入。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLcIP.png" alt="0gLcIP.png" style="zoom:25%;" /> 

<h3 id="Document-Embedding：将文件表示为一个向量"><a href="#Document-Embedding：将文件表示为一个向量" class="headerlink" title="Document Embedding：将文件表示为一个向量"></a>Document Embedding：将文件表示为一个向量</h3><ul>
<li><p>Bag of Word:</p>
<p>用Bag-of-word的方式编码文件，再实现semantic embedding。得到的文件表示向量可以表示文件的语义主题。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLyVI.png" alt="0gLyVI.png" style="zoom:33%;" /> 
</li>
<li><p>Beyond Bag of Word:</p>
<p>句子中单词的顺序也很大程度影响句子的语义。</p>
<p>因此，下图的两句话有相同的bag-of-word，但表达的含义完全相反。</p>
<img src="https://s1.ax1x.com/2020/10/11/0gLrqA.png" alt="0gLrqA.png" style="zoom:25%;" /> 

<p>关于beyond bag of word的相关工作参考reference 2.</p>
</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><p><a href="https://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a> </p>
</li>
<li><p>beyond bag of word:</p>
<img src="https://s1.ax1x.com/2020/10/11/0gL4MQ.png" alt="0gL4MQ.png" style="zoom:33%;"  />    





</li>
</ol>

        </div>
    
        <ul class="post-copyright">
        <li><strong>本文标题：</strong><a href="https://f7ed.com/2020/10/11/unsupervised-learning-word-embedding/">「机器学习-李宏毅」:Unsupervised Learning：Word Embedding</a></li>
        <li><strong>本文作者：</strong><a href="https://f7ed.com">f1ed</a></li>
        <li><strong>本文链接：</strong><a href="https://f7ed.com/2020/10/11/unsupervised-learning-word-embedding/">https://f7ed.com/2020/10/11/unsupervised-learning-word-embedding/</a></li>
        <li><strong>发布时间：</strong>2020-10-11</li>
        <li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！
        </li>
        </ul>
    
        
        <hr style="height:1px;margin:1rem 0"/>
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                <i class="fas fa-tags has-text-grey"></i>&nbsp;
                    <a class="has-link-grey -link" href="/tags/Machine-Learning/" rel="tag">Machine-Learning</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Unsupervised-learning/" rel="tag">Unsupervised-learning</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Word-Embedding/" rel="tag">Word Embedding</a>,&nbsp;<a class="has-link-grey -link" href="/tags/open-classes/" rel="tag">open-classes</a>
                </div>
            </div>
        </div>
        
        
        
        <div class="social-share"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/qrcode_alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/qrcode_wechatpay.jpg" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/10/21/Tensors/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">「PyTorch」：Tensors Explained And Operations</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/10/10/Leetcode-math/">
                <span class="level-item">「LeetCode」：Math</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: true,
        verify: false,
        app_id: 'U7ITnhsJjngmAcJUpFYdrq5m-gzGzoHsz',
        app_key: 'cSRtvM6PbCOJBTVBAUURyFfO',
        placeholder: 'xxxxxxxx'
    });
</script>

    </div>
</div>



<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="" src="/images/profile.png" alt="f1ed">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        f1ed
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        登高不傲，居低不怨。
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>成都</span>
                    </p>
                    
                </div>
            </div>
        </nav>
    <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
		<div>
                <a href="/archives/">
                    <p class="heading">
                        文章
                    </p>
                    <a href="/archives">
                        <p class="title has-text-weight-normal">
                            29
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <a href="/categories">
                        <p class="title has-text-weight-normal">
                            7
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <a href="/tags">
                        <p class="title has-text-weight-normal">
                            42
                        </p>
                    </a>
                </div>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/f1ed" target="_blank" rel="noopener">
                关注我</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/f1ed">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="RSS" href="/atom.xml">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
        </div>
        
    </div>
</div>

    
        

   <div class="card widget column-left is-sticky" id="toc">
       <div class="card-content" style="max-height:calc(100vh - 22px);overflow:scroll">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Word-to-Vector">
        <span class="has-mr-6">1</span>
        <span>Word to Vector</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#1-of-N-Encoding">
        <span class="has-mr-6">1.1</span>
        <span>1-of-N Encoding</span>
        </a></li><li>
        <a class="is-flex" href="#Word-Class">
        <span class="has-mr-6">1.2</span>
        <span>Word Class</span>
        </a></li><li>
        <a class="is-flex" href="#Word-Embedding">
        <span class="has-mr-6">1.3</span>
        <span>Word Embedding</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Word-Embedding-1">
        <span class="has-mr-6">2</span>
        <span>Word Embedding</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#auto-encoder？">
        <span class="has-mr-6">2.1</span>
        <span>auto-encoder？</span>
        </a></li><li>
        <a class="is-flex" href="#Exploit-the-Context">
        <span class="has-mr-6">2.2</span>
        <span>Exploit the Context</span>
        </a></li><li>
        <a class="is-flex" href="#Prediction-based">
        <span class="has-mr-6">2.3</span>
        <span>Prediction based</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#How-to-predict？">
        <span class="has-mr-6">2.3.1</span>
        <span>How to predict？</span>
        </a></li><li>
        <a class="is-flex" href="#Why-it-works">
        <span class="has-mr-6">2.3.2</span>
        <span>Why it works?</span>
        </a></li><li>
        <a class="is-flex" href="#Prediction-based-：Various-Architecture">
        <span class="has-mr-6">2.3.3</span>
        <span>Prediction-based ：Various Architecture</span>
        </a></li><li>
        <a class="is-flex" href="#Sharing-Parameters">
        <span class="has-mr-6">2.3.4</span>
        <span>Sharing Parameters</span>
        </a></li></ul></li></ul></li><li>
        <a class="is-flex" href="#Advantages-of-Word-Embedding">
        <span class="has-mr-6">3</span>
        <span>Advantages of Word Embedding</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#其他应用">
        <span class="has-mr-6">3.1</span>
        <span>其他应用</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Multi-lingual-Embedding：实现翻译">
        <span class="has-mr-6">3.1.1</span>
        <span>Multi-lingual Embedding：实现翻译</span>
        </a></li><li>
        <a class="is-flex" href="#Multi-domain-Embedding">
        <span class="has-mr-6">3.1.2</span>
        <span>Multi-domain Embedding</span>
        </a></li><li>
        <a class="is-flex" href="#Document-Embedding：将文件表示为一个向量">
        <span class="has-mr-6">3.1.3</span>
        <span>Document Embedding：将文件表示为一个向量</span>
        </a></li></ul></li></ul></li><li>
        <a class="is-flex" href="#Reference">
        <span class="has-mr-6">4</span>
        <span>Reference</span>
        </a></li></ul>
            </div>
        </div>
    </div>


    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>


                

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」:Unsupervised Learning：Word Embedding" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2020 f1ed&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
		<br>
		<a href="http://www.beian.miit.gov.cn/" target="_blank">蜀ICP备2020028586号</a>
                
                <br>
                <span id="busuanzi_container_site_uv"> 来访 <span id="busuanzi_value_site_uv"></span>人</span>
                <span id="busuanzi_container_site_pv">, 总访问 <span id="busuanzi_value_site_pv"></span>次</span>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                        <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="f1ed&#39;s GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'https://f7ed.com',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</body>
</html>
