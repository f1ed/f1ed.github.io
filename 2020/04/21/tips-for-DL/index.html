<!DOCTYPE html>
<html  lang="zh">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="baidu-site-verification" content="9ce18CEmjH" />
<meta name="google-site-verification" content="GSUThrU_AtZE-dgdz1QWWouv0L2teKqHWrZg7DfHbXo" />
<meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.1" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>「机器学习-李宏毅」：Tips for Deep Learning - fred&#39;s blog</title>


    <meta name="description" content="这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。tips从Training和Testing两个方面展开。在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有">
<meta property="og:type" content="article">
<meta property="og:title" content="「机器学习-李宏毅」：Tips for Deep Learning">
<meta property="og:url" content="https://f1ed.github.io/2020/04/21/tips-for-DL/index.html">
<meta property="og:site_name" content="fred&#39;s blog">
<meta property="og:description" content="这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。tips从Training和Testing两个方面展开。在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://f1ed.github.io/gallery/thumbnails/contest_pc_header_303.png">
<meta property="article:published_time" content="2020-04-20T16:00:00.000Z">
<meta property="article:modified_time" content="2020-07-03T08:45:39.300Z">
<meta property="article:author" content="f1ed">
<meta property="article:tag" content="Machine-Learning">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="open-classes">
<meta property="article:tag" content="DNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://f1ed.github.io/gallery/thumbnails/contest_pc_header_303.png">





<link rel="alternative" href="/atom.xml" title="「机器学习-李宏毅」：Tips for Deep Learning" type="application/atom+xml">



<link rel="icon" href="/images/heart.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171512660-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-171512660-1');
</script>

    
    <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?b1d53a77276f7e423bc7cf8fafd95b75";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body class="is-3-column">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」：Tips for Deep Learning" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/atom.xml">RSS</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    <a class="navbar-item" target="_blank" rel="noopener" title="My GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="/gallery/thumbnails/contest_pc_header_303.png" alt="「机器学习-李宏毅」：Tips for Deep Learning">
        </span>
    </div>
    
    <div class="card-content article ">
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <i class="fas fa-bars"></i>「机器学习-李宏毅」：Tips for Deep Learning
            
        </h1>
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-04-20T16:00:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-04-21</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2020-07-03T08:45:39.300Z"><i class="far fa-calendar-check">&nbsp;</i>2020-07-03</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/">机器学习-李宏毅</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    <i class="far fa-clock"></i>&nbsp;
                    
                    
                    36 分钟 读完 (大约 5358 个字)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span>次访问
                </span>
                
            </div>
        </div>
        
        <div class="content">
            <p>这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。<br>tips从Training和Testing两个方面展开。<br>在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有RMSprop、Momentum、Adam等）。<br>当在Training中得到好的performance，但在testing中perform bad时，即遇到了overfitting，又该怎么处理呢？文章后半部分详尽介绍了EarlyStopping、Regularization和Dropout三个solution。</p>
<a id="more"></a>

<h1 id="Recipe-of-Deep-Learning"><a href="#Recipe-of-Deep-Learning" class="headerlink" title="Recipe of Deep Learning"></a>Recipe of Deep Learning</h1><p>Deep Learning 的三个步骤：</p>
<p><a href="https://imgchr.com/i/JGW8js"><img src="https://s1.ax1x.com/2020/04/21/JGW8js.md.png" alt="JGW8js.md.png"></a> </p>
<p>如果在Training Data中没有得到好的结果，需要重新训练Neural Network。</p>
<p>如果在Training Data中得到好的结果，在Testing Data（这里的Testing Data是指有Label的Data，比如Kaggle的Public Data或者是从Training Data中划分出的Development Data）没有得到的好的结果，说明Overfitting了，需要重新设计Neural Network的结构。</p>
<h2 id="Do-not-always-blame-Overfitting"><a href="#Do-not-always-blame-Overfitting" class="headerlink" title="Do not always blame Overfitting"></a>Do not always blame Overfitting</h2><p><a href="https://imgchr.com/i/JGW3cj"><img src="https://s1.ax1x.com/2020/04/21/JGW3cj.md.png" alt="JGW3cj.md.png"></a> </p>
<p>如果在Testing Data中，看到上图，20-layer的error小，56-layer的error大，56-layer一定overfitting了。</p>
<p>No!!!不要总把原因归咎于Overfitting。</p>
<p><a href="https://imgchr.com/i/JGW13Q"><img src="https://s1.ax1x.com/2020/04/21/JGW13Q.md.png" alt="JGW13Q.md.png"></a> </p>
<p>再看Testing Data error之前，先看看Training Data的error。上图中，56-layer的DNN在Training Data的error本来就比20-layer的大，说明56-layer的DNN根本没有train好。</p>
<p>所以56-layer的DNN在Testing Data上的error大，原因不是overfitting，而是模型根本没有train好。</p>
<p><strong>注：</strong> Overfitting是在Training Data上error小，但在Testing Data上的error大。</p>
<p>因此，对于在Training Data上得到不好的结果和在Training Data上得到好的结果但在Testing Data上得到不好的结果这两种情况，需要不同的解决方法。</p>
<h1 id="Bad-Results-on-Training-Data"><a href="#Bad-Results-on-Training-Data" class="headerlink" title="Bad Results on Training Data"></a>Bad Results on Training Data</h1><p>在不重新设计DNN结构时，如果在Training Data中得到Bad Results，一般有两种方法来改进结果：</p>
<ul>
<li>New activation function【neuron换新的激活函数】</li>
<li>Adaptive Learning Rate</li>
</ul>
<h2 id="New-activation-function"><a href="#New-activation-function" class="headerlink" title="New activation function"></a>New activation function</h2><h3 id="Vanishing-Gradient-Problem"><a href="#Vanishing-Gradient-Problem" class="headerlink" title="Vanishing Gradient Problem"></a>Vanishing Gradient Problem</h3><p><a href="https://imgchr.com/i/JGWl9g"><img src="https://s1.ax1x.com/2020/04/21/JGWl9g.md.png" alt="JGWl9g.md.png"></a> </p>
<p>上图表示，在手写数字辨识中，Deeper layers并不能有好的performance。</p>
<p>为什么会这样呢？</p>
<p>因为出现了Vanishing Gradient Problem，即gradient随着deeper layer逐渐消失的问题。</p>
<p><a href="https://imgchr.com/i/JGWM4S"><img src="https://s1.ax1x.com/2020/04/21/JGWM4S.md.png" alt="JGWM4S.md.png"></a> </p>
<p>上图中，假设neuron的activation function是sigmod函数。</p>
<p>靠近Input layer层的参数的变化对Loss的影响很小，所以对Loss function做微分，gradient很小，参数更新慢。</p>
<p>而靠近Output layer层的参数的编号对Loss的影响更大，所以对Loss function做微分，gradient很大，参数更新快。</p>
<p>因为靠近Output Layer层的参数更新快，所以很快converge（收敛、趋于稳定）；但靠近Input Layer层的参数更新慢，几乎还处在random（随机）的状态。</p>
<p>当靠近Output Layer层的参数趋于稳定时，由于靠近Output Layer层的参数对Loss影响大，所以观察到的Loss的值也趋于稳定，于是，你就把training停掉了。</p>
<p>但是，靠近Input层的参数几乎处在random状态，所以拿模型用在Testing Data上，发现结果几乎是随机的。</p>
<hr>
<p>怎么直观理解靠近Input Layer的参数的gradient小呢？</p>
<p>用微分的直观含义来表示gradient $\partial{l}/\partial{w}$ : </p>
<p><strong>当 $w$ 增加 $\Delta{w}$ 时，如果 $l$ 的变化 $\Delta{l}$ 变化大，说明 $\partial{l}/\partial{w}$ 大，否则 $\partial{l}/\partial{w}$ 小。</strong></p>
<p><a href="https://imgchr.com/i/JGWKN8"><img src="https://s1.ax1x.com/2020/04/21/JGWKN8.md.png" alt="JGWKN8.md.png"></a> </p>
<p>我们在DNN中使用的activation function是sigmod函数，sigmod函数会把值压到0和1之间。</p>
<p>因此，上图中，其他值不变，只有连接 $x_N$ 的参数 $w$ 增加 $\Delta w$ 时，输入通过neuron的sigmod函数，函数的输出增加的 $\Delta$ 会变小，随着Deeper Layer，neuron的输出的 $\Delta$ 会越变越小，趋至0。</p>
<p>最后DNN输出的变化对 loss的影响小，即 $\Delta{l}$ 趋至0，即参数的gradient  $\partial{l}/\partial{w}$ 趋至0。（即 Vanishing Gradient）</p>
<h3 id="ReLu-：Rectified-Linear-Unit"><a href="#ReLu-：Rectified-Linear-Unit" class="headerlink" title="ReLu ：Rectified Linear Unit"></a>ReLu ：Rectified Linear Unit</h3><p>为了防止发生Vanishing Gradient Problem，在DNN中选择使用新的activation function。</p>
<p>ReLu长下面这个样子：</p>
<p><a href="https://imgchr.com/i/JGWuAf"><img src="https://s1.ax1x.com/2020/04/21/JGWuAf.md.png" alt="JGWuAf.md.png"></a> </p>
<p>z: input</p>
<p>a: output</p>
<p>当 $z\leq0$ 时， $a=0$ ；当 $z &gt;0$ 时， $a=z$ 。</p>
<p><u>Reason :</u> </p>
<ol>
<li>Fast to compute</li>
<li>Biological reason【有生物上的原因】</li>
<li>Infinite sigmod with different biases. 【是无穷个 有不同bias的sigmod函数 的叠加】</li>
<li><strong>Vanishing gradient problem</strong> 【最重要的是没有vanishing gradient problem】</li>
</ol>
<hr>
<p>为什么ReLu没有vanishing gradient problem</p>
<p><a href="https://imgchr.com/i/JGWmHP"><img src="https://s1.ax1x.com/2020/04/21/JGWmHP.md.png" alt="JGWmHP.md.png"></a> </p>
<p>上图DNN中，ReLu在输入是负数时，输出是0。因此这些输出是0的neuron可以去掉。</p>
<p>就变成了下图这个A Thinner linear network。由于ReLu函数的性质，靠近Input Layer的参数不会有smaller gradient。</p>
<p><a href="https://imgchr.com/i/JGWeBt"><img src="https://s1.ax1x.com/2020/04/21/JGWeBt.md.png" alt="JGWeBt.md.png"></a> </p>
<p>这里有一个Q&amp;A: </p>
<p>Q1: function变成linear的，会不会DNN就变弱了？</p>
<p>： 当neuron的operation region不变的话，DNN的确是linear的，但是当neuron的operation region改变后，就是unlinear的。</p>
<p>：即，当input的变化小，operation region不变（即输入不会从大于0变成小于0，小于0变成大于0这种），model还是linear的；但当input的变化大时，很多neuron的operation region都变化了，model其实就是unlinear的。</p>
<p>Q2: ReLu 怎么微分？</p>
<p>：ReLu在0点不可微，那就随便指定为0这样（台湾腔QAQ）。</p>
<h3 id="ReLu-variant"><a href="#ReLu-variant" class="headerlink" title="ReLu - variant"></a>ReLu - variant</h3><p>当 $z\leq 0$ 时，输出为0，就不能更新参数了。于是就有下图变体：</p>
<p><a href="https://imgchr.com/i/JGWZnI"><img src="https://s1.ax1x.com/2020/04/21/JGWZnI.md.png" alt="JGWZnI.md.png"></a> </p>
<p>当 $z\leq0$ 时，gradient都为0.01，为什么不能是其他值。于是就有下图变体：其中 $\alpha$ 也是一个需要学习的参数</p>
<h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>Maxout，如下图，在设计neural network时，会给每一层的neuron分组，成为一个新的neuron。</p>
<p><a href="https://imgchr.com/i/JGWk1H"><img src="https://s1.ax1x.com/2020/04/21/JGWk1H.md.png" alt="JGWk1H.md.png"></a> </p>
<p>Maxout也是一个Learnable activation function。</p>
<p><strong>ReLu是Maxout学出来的一个特例。</strong></p>
<p><a href="https://imgchr.com/i/JGWAcd"><img src="https://s1.ax1x.com/2020/04/21/JGWAcd.md.png" alt="JGWAcd.md.png"></a> </p>
<p>上图中，左图是ReLu。 ReLu的输入 $z = wx+b$ ，输出 $a$ 如上图的绿色的线。</p>
<p>右图是Maxout。Maxout的输入 $z_1 =wx+b,z_2=0$ ，那么输出取max，输出 $a$ 如上图中绿色的线，和左图的ReLu相同。</p>
<hr>
<p><strong>Maxout is more than ReLu。</strong> </p>
<p>当参数更新时，Maxout的函数图像如下图：</p>
<p><a href="https://imgchr.com/i/JGWPhD"><img src="https://s1.ax1x.com/2020/04/21/JGWPhD.md.png" alt="JGWPhD.md.png"></a> </p>
<p>DNN中的参数是learnable的，所以Maxout也是一个learnable的activation function。</p>
<p><u>Reason ：</u></p>
<ul>
<li><p>Learnable activation function [Ian J. Goodfellow, ICML’13]</p>
<ul>
<li><p>Activation function in maxout network can be any piecewise linear convex function.</p>
<p>在maxout神经网络中的激活函数可以是任意的分段凸函数。</p>
</li>
<li><p>How many pieces depending on how many elements in a group.</p>
<p>分段函数分几段取决于一组中有多少个元素。</p>
<p><a href="https://imgchr.com/i/JGWCtO"><img src="https://s1.ax1x.com/2020/04/21/JGWCtO.md.png" alt="JGWCtO.md.png"></a> </p>
</li>
</ul>
</li>
</ul>
<h3 id="Maxout-how-to-train"><a href="#Maxout-how-to-train" class="headerlink" title="Maxout : how to train"></a>Maxout : how to train</h3><p>Given a training data x, we know which z would be the max.</p>
<p>【当给出每笔training data时，我们能知道Maxout neuron中哪一个最大】</p>
<p><a href="https://imgchr.com/i/JGW9AK"><img src="https://s1.ax1x.com/2020/04/21/JGW9AK.md.png" alt="JGW9AK.md.png"></a> </p>
<p>如上图，在这笔training data x中，我们只train this thin and linear network 的参数，即max z相连的参数。</p>
<p>每笔不同的training data x，会得到不同的thin and linear network，最后，会train到每一个参数。</p>
<h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><h3 id="Review-Adagrad"><a href="#Review-Adagrad" class="headerlink" title="Review Adagrad"></a>Review Adagrad</h3><p>在这篇文章： <a href="/2020/03/01/Gradient/" title="Gradient">Gradient</a> 第一小节讲到一种adaptive learning rate的gradient 算法：Adagrad 算法。在那篇文章中，我们得出的结论是 the best step $\propto$  |First dertivative| / Second derivative.</p>
<p><a href="https://imgchr.com/i/JGWF9e"><img src="https://s1.ax1x.com/2020/04/21/JGWF9e.md.png" alt="JGWF9e.md.png"></a>  </p>
<p>在上图中，两个方向，因为蓝色方向的二阶微分更小，所以蓝色方向应该有更大的learning rate。</p>
<p>因此，在Adagrad中，我们用一阶微分来估量二阶微分的大小：</p>

$$
w^{t+1}  \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}
$$


<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>但是，在训练NN时，Error Surface（Total Loss对参数的变化）的图像可能会更复杂，如下图：</p>
<p><a href="https://imgchr.com/i/JGWS76"><img src="https://s1.ax1x.com/2020/04/21/JGWS76.md.png" alt="JGWS76.md.png"></a> </p>
<p>因为函数图像过于复杂，可能在同一方向的不同位置，也需要有不同的learning rate。</p>
<p>RMSProp是Adagrad的进阶版。</p>
<p><strong>RMSProp过程：</strong> </p>
<ol>
<li> $w^{1} \leftarrow w^{0}-\frac{\eta}{\sigma^{0}} g^{0} \quad \sigma^{0}=g^{0}$ </li>
<li> $w^{2} \leftarrow w^{1}-\frac{\eta}{\sigma^{1}} g^{1} \quad \sigma^{1}=\sqrt{\alpha (\sigma^{0})^2+(1-\alpha)(g^1)^2}$ </li>
<li> $w^{3} \leftarrow w^{2}-\frac{\eta}{\sigma^{2}} g^{2} \quad \sigma^{2}=\sqrt{\alpha (\sigma^{1})^2+(1-\alpha)(g^2)^2}$ </li>
<li><p>…</p>
</li>
<li> $w^{t+1} \leftarrow w^{t}-\frac{\eta}{\sigma^{t}} g^{t} \quad \sigma^{t}=\sqrt{\alpha (\sigma^{t-1})^2+(1-\alpha)(g^t)^2}$ 

<p>$\sigma^t$ 也是在算gradients的 root mean squar。</p>
</li>
</ol>
<p>但是在RMSProp中，加入了参数 $\alpha$ (需要手动调节大小的参数)，可以给当前算出来的gradient $g^t$ 更大的权重，即更相信现在gradient的方向，不那么相信以前gradient的方向。</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Momentum，则是引用物理中的惯性。</p>
<p><a href="https://imgchr.com/i/JGRxn1"><img src="https://s1.ax1x.com/2020/04/21/JGRxn1.md.png" alt="JGRxn1.md.png"></a> </p>
<p>上图中，当小球到达local minima时，会因为惯性继续往前更新，则有可能到达minima的位置。</p>
<p>这里的Momentum，就代指上一次前进（参数更新）的方向。</p>
<p><strong>Vanilla Gradient Descent</strong> </p>
<p>如果将Gradient的步骤画出图来，就是下图这样：</p>
<p><a href="https://imgchr.com/i/JGRXc9"><img src="https://s1.ax1x.com/2020/04/21/JGRXc9.md.png" alt="JGRXc9.md.png"></a> </p>
<p>过程：</p>
<ol>
<li><p>Start at position $\theta^0$</p>
</li>
<li><p>Compute gradietn at $\theta^0$</p>
<p>Move to  $\theta^1=\theta^0-\eta\nabla{L(\theta^0)}$ </p>
</li>
<li><p>Compute gradietn at $\theta^1$ </p>
<p>Move to  $\theta^2=\theta^1-\eta\nabla{L(\theta^1)}$ </p>
</li>
<li><p>… …Stop until $\nabla{L(\theta^t)}\approx0$</p>
</li>
</ol>
<hr>
<p><strong>Momentum</strong> </p>
<p>在Momentum中，参数更新方向是当前Gradient方向和Momentum方向（上一次更新方向）的叠加。</p>
<p><a href="https://imgchr.com/i/JGRjXR"><img src="https://s1.ax1x.com/2020/04/21/JGRjXR.md.png" alt="JGRjXR.md.png"></a> </p>
<p>Movement方向：上一次更新方向 - 当前gradient方向。</p>
<p>过程：</p>
<ol>
<li><p>Start at position $\theta^0$</p>
<p>Movement: $v^0=0$ </p>
</li>
<li><p>Compute gradient at $\theta^0$ </p>
<p>Movement  $v^1=\lambda v^0-\eta\nabla{L(\theta^0)}$  </p>
<p>Move to  $\theta^1=\theta^0+v^1$ </p>
</li>
<li><p>Compute gradient at $\theta^1$  </p>
<p>Movement  $v^2=\lambda v^1-\eta\nabla{L(\theta^1)}$ </p>
<p>Move to $\theta^2=\theta^1+v^2$  </p>
</li>
<li><p>… …Stop until $\nabla{L(\theta^t)}\approx0$ </p>
</li>
</ol>
<p>和Vanilla Gradient Descent比较，$v^i$ 其实是过去gradient( $\nabla{L(\theta^0)}$ 、$\nabla{L(\theta^1)}$ 、… 、 $\nabla{L(\theta^{i-1})}$  )的加权和。</p>
<ul>
<li>迭代过程：<ul>
<li>$v^0=0$ </li>
<li> $v^1=-\eta\nabla{L(\theta^0)}$ </li>
<li> $v^2=-\lambda\eta\nabla{L(\theta^0)}-\eta\nabla{L(\theta^1)}$ </li>
<li>…</li>
</ul>
</li>
</ul>
<hr>
<p>再用那个小球的例子来直觉的解释Momentum：</p>
<p><a href="https://imgchr.com/i/JGRO1J"><img src="https://s1.ax1x.com/2020/04/21/JGRO1J.md.png" alt="JGRO1J.md.png"></a> </p>
<p>当小球在local minima时，gradient为0，但是Momentum（即上次移动方向）是继续往前，于是小球可以继续向前更新。</p>
<h3 id="Adam-RMSProp-Momentum"><a href="#Adam-RMSProp-Momentum" class="headerlink" title="Adam = RMSProp + Momentum"></a>Adam = RMSProp + Momentum</h3><p><a href="https://imgchr.com/i/JGRLp4"><img src="https://s1.ax1x.com/2020/04/21/JGRLp4.md.png" alt="JGRLp4.md.png"></a> </p>
<p>Algorithm：Adam, our proposed algorithm for stochastic optimization. </p>
<p>【Adam，是为了优化stochastic gradient】（至于什么是stochastic gradient，建议戳)</p>
<p>$g_t^2$ indicates the elementwise square $g_t\odot g_t$ .</p>
<p>【$g_t^2$ 是gradient $g_t$ 向量和 $g_t$ 的元素乘】</p>
<p>Good default settings for the tested machine learning problems are $\alpha=0.001$ , $\beta_1=0.9$ , $\beta_2=0.999$ and $\epsilon=10^{-8}$ . All operations on vectors are element-wise. With $\beta_1^t$ and $\beta_2^t$ we denote $\beta_1$ and $\beta_2$ to the power t.</p>
<p>【参数说明：算法默认的参数设置是 $\alpha=0.001$ , $\beta_1=0.9$ , $\beta_2=0.999$ ， $\epsilon=10^{-8}$ 。算法中所有vector之间的操作都是对元素操作。 $\beta_1^t$ 和 $\beta_2^t$ 是 $\beta_1$ 和 $\beta_2$ 的 $t$ 次幂】</p>
<p><strong>Adam Pseudo Code：</strong> </p>
<ol start="0">
<li><p><strong>Require</strong>：$\alpha$ : Stepsize 【步长/learning rate $\eta$ 】</p>
<p><strong>Require</strong>：$\beta_1,\beta_2\in\left[0,1\right)$ : Exponential decay rates for the moment estimates.</p>
<p><strong>Require</strong>：$f(\theta)$ : Stochastic objective function with parameters $\theta$ .【参数 $\theta$ 的损失函数】</p>
<p><strong>Require</strong>: $\theta_0$ ：Initial parameter vector 【初值】</p>
</li>
<li><p>$m_0\longleftarrow 0$ (Initial 1st moment vector) 【 $m$ 是Momentum算法中的更新参数后的方向 $v$ 】</p>
<p>$v_0\longleftarrow 0$ (Initial 2nd moment vector) 【 $v$ 是RMSprop算法中gradient的root mean square $\sigma$ 】</p>
<p>$t\longleftarrow 0$ (Initial timestep) 【更新次数】</p>
</li>
<li><p><strong>while</strong> $\theta_t$ not concerged <strong>do</strong> 【当 $\theta$ 趋于稳定，即 $\nabla{f(\theta)}\approx0$ 时】</p>
<ol>
<li><p>$t\longleftarrow t+1$ </p>
</li>
<li> $g_t\longleftarrow \nabla{f_t(\theta_{t-1})}$  (Get gradients w.r.t. stochastic objective at timestep t)

<p>【算第t次时 $\theta$ 的gradient】</p>
</li>
<li> $m_{t} \leftarrow \beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t}$   (Update biased first momen t estimate)

<p>【用Momentum算更新方向】</p>
</li>
<li> $v_{t} \leftarrow \beta_{2} \cdot v_{t-1}+\left(1-\beta_{2}\right) \cdot g_{t}^{2}$  (Update biased second raw moment estimate)

<p>【RMSprop估测最佳步长（ 和$v$ 负相关） 】</p>
</li>
<li> $\widehat{m}_{t} \leftarrow m_{t} /\left(1-\beta_{1}^{t}\right)$ （Comppute bbi. as-corrected first momen t estima te)

<p>【算出来的值有bias，论文中有具体解释为什么有。当更新次数增加时， $1-\beta_1^t$ 也趋近于1】</p>
</li>
<li> $\widehat{v}_{t} \leftarrow v_{t} /\left(1-\beta_{2}^{t}\right)$  (Compute bias-corrected second raw momen t estimate)

<p>【和上同理】</p>
</li>
<li> $\theta_{t} \leftarrow \theta_{t-1}-\alpha \cdot \widehat{m}_{t} /(\sqrt{\widehat{v}_{t}}+\epsilon)$ （Update parameters）

<p>【 $\widehat{m}<em>t$ 相当于是更准确的gradient的方向，$\sqrt{\widehat{v}</em>{t}}+\epsilon$ 是为了估测最好的步长，调节learning rate】</p>
</li>
</ol>
</li>
</ol>
<h3 id="Gradient-Descent-Limitation？"><a href="#Gradient-Descent-Limitation？" class="headerlink" title="Gradient Descent Limitation？"></a>Gradient Descent Limitation？</h3><p>在<a href="/2020/03/01/Gradient/" title="Gradient">Gradient</a>这篇文章中，讲到过Gradient有一些问题不能处理：</p>
<ul>
<li>Stuck at local minima</li>
<li>Stuck at saddle point</li>
<li>Very slow at the plateau</li>
</ul>
<p><a href="https://imgchr.com/i/JG4l9O"><img src="https://s1.ax1x.com/2020/04/21/JG4l9O.md.png" alt="JG4l9O.md.png"></a> </p>
<p>（李老师说的，不是我说的QAQ）：但是Andrew（吴恩达）在2017年说过，不用太担心这个问题。为什么呢？</p>
<p>如果要stuck at local minima，前提是每一维度都是local minima。</p>
<p>如果在一个维度遇到local minima的概率是p，当NN很复杂时，有很多参数时，比如1000，那么遇到local minima的概率是 $p^{1000}$ ，趋近于0了，几乎不会发生。</p>
<p>：所以不用太担心Gradient Descent的局限性。</p>
<h1 id="Bad-Results-on-Testing-Data"><a href="#Bad-Results-on-Testing-Data" class="headerlink" title="Bad Results on Testing Data"></a>Bad Results on Testing Data</h1><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p>在更新参数时，可能会出现这样曲线图：</p>
<p><a href="https://imgchr.com/i/JGRbhF"><img src="https://s1.ax1x.com/2020/04/21/JGRbhF.md.png" alt="JGRbhF.md.png"></a> </p>
<p>图中，Total Loss在training set中逐渐减小，但在validation set中逐渐增大。</p>
<p>而我们真正关心的其实是validation set的Loss。</p>
<p>所以想让参数停在validation set中loss最低时。</p>
<p>Keras能够实现EarlyStopping功能[1]：click <a href="https://keras.io/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore">here</a> </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line">early_stopping = EarlyStopping(monitor=<span class="string">'val_loss'</span>, patience=<span class="number">2</span>)</span><br><span class="line">model.fit(x, y, validation_split=<span class="number">0.2</span>, callbacks=[early_stopping])</span><br></pre></td></tr></table></figure>

<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Regularization：Find a set of weight not only minimizing original cost but also close to zero.</p>
<p>构造一个新的loss function，除了最小化原来的loss function，还能使得参数趋紧0，使得function更平滑。</p>
<p>function的曲线更平滑，当输入有轻微扰动，不会太影响输出的结果。</p>
<h3 id="L2-norm-regularization"><a href="#L2-norm-regularization" class="headerlink" title="L2 norm regularization"></a>L2 norm regularization</h3><p><strong>New loss function:</strong> </p>

$$
\begin{equation}
\begin{aligned}
\mathrm{L}^{\prime}(\theta)&=L(\theta)+\lambda \frac{1}{2}\|\theta\|_{2}
\\ \theta &={w_1,w_2,...}
\\ \|\theta\|_2&=(w1)^2+(w_2)^2+...

\end{aligned}
\end{equation}
$$


<p>其中用第二范式 $\lambda\frac{1}{2}|\theta|_2$ 作为regularization term。做regularization是为了使函数更平滑，所以一般不考虑bias)</p>
<p><strong>New gradient:</strong> </p>

$$
\frac{\partial \mathrm{L}^{\prime}}{\partial w}=\frac{\partial \mathrm{L}}{\partial w}+\lambda w
$$

<p><strong>New update:</strong> </p>

$$
\begin{equation}
\begin{aligned}
w^{t+1} &\longrightarrow w^{t}-\eta \frac{\partial \mathrm{L}^{\prime}}{\partial w}
\\ &=w^{t}-\eta\left(\frac{\partial \mathrm{L}}{\partial w}+\lambda w^{t}\right)
\\ &=(1-\eta \lambda) w^{t}-\eta \frac{\partial \mathrm{L}}{\partial w}
\end{aligned}
\end{equation}
$$

<p>在更新参数时，先乘一个 $(1-\eta\lambda)$ ，再更新。</p>
<p>weight decay（权值衰减）：由于 $\eta,\lambda$ 都是很小的值，所以 $w^t$ 每次都会先乘一个小于1的数，即逐渐趋于0，实现regularization。但是，因为更新中还有gradient部分，所以不会等于0。</p>
<h3 id="L1-norm-regularization"><a href="#L1-norm-regularization" class="headerlink" title="L1 norm regularization"></a>L1 norm regularization</h3><p>Regularization除了用第二范式，还可以用其他的，比如第一范式 $|\theta|_1=|w_1|+|w_2|+…$ </p>
<p><strong>New loss function:</strong> </p>

$$
\begin{equation}\begin{aligned}\mathrm{L}^{\prime}(\theta)&=L(\theta)+\lambda \frac{1}{2}\|\theta\|_1\\ \theta &={w_1,w_2,...}
\\ \|\theta\|_1&=|w_1|+|w_2|+...\end{aligned}\end{equation}
$$


<p>用sgn()符号函数来表示绝对值的求导。</p>
<blockquote>
<p>符号函数：Sgn(number)</p>
<p>如果number 大于0，返回1；等于0，返回0；小于0，返回-1。</p>
</blockquote>
<p><strong>New gradient:</strong> </p>

$$
\frac{\partial \mathrm{L}^{\prime}}{\partial w}=\frac{\partial \mathrm{L}}{\partial w}+\lambda \text{sgn}(w)
$$

<p><strong>New update:</strong> </p>

$$
\begin{equation}
\begin{aligned}
w^{t+1} &\longrightarrow w^{t}-\eta \frac{\partial \mathrm{L}^{\prime}}{\partial w}
\\ &=w^{t}-\eta\left(\frac{\partial \mathrm{L}}{\partial w}+\lambda \text{sgn}(w^t)\right)
\\ &=w^{t}-\eta \frac{\partial \mathrm{L}}{\partial w}-\eta \lambda \operatorname{sgn}\left(w^{t}\right)
\end{aligned}
\end{equation}
$$


<p>在用第一范式做regularization时，每次 $w^t$ 都要减一个值 $\eta\lambda\text{sgn}(w^t)$ ，和用第二范式做regularization比较，后者每次都要乘一个小于1的值，即使是乘0.99，w下降也很快。</p>
<blockquote>
<p>Weight decay（权值衰减）的生物意义：</p>
<p>Our brain prunes（修剪） out the useless link between neurons.</p>
<p><a href="https://imgchr.com/i/JGRHtU"><img src="https://s1.ax1x.com/2020/04/21/JGRHtU.md.png" alt="JGRHtU.md.png"></a> </p>
</blockquote>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Wiki: <strong>Dropout</strong>是Google提出的一种正则化技术，用以在人工神经网络中对抗过拟合。Dropout有效的原因，是它能够避免在训练数据上产生复杂的相互适应。Dropout这个术语代指在神经网络中丢弃部分神经元（包括隐藏神经元和可见神经元）。在训练阶段，dropout使得每次只有部分网络结构得到更新，因而是一种高效的神经网络模型平均化的方法。[2]</p>
<p>这里讲Dropout怎么做。</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p><a href="https://imgchr.com/i/JG4M4K"><img src="https://s1.ax1x.com/2020/04/21/JG4M4K.md.png" alt="JG4M4K.md.png"></a> </p>
<ul>
<li><p>Each time before updating the parameters:</p>
<ul>
<li><p>Each neuron has p% to dropout. Using the new thin network for training.</p>
<p>【如上图，每个neuron有p的概率被dropout。于是NN就变成了下图thinner的NN】</p>
<p><a href="https://imgchr.com/i/JGR5mq"><img src="https://s1.ax1x.com/2020/04/21/JGR5mq.md.png" alt="JGR5mq.md.png"></a> </p>
</li>
<li><p>For each mini-batch, we resample the dropout neurons.</p>
<p>【每次mini-batch，都要重新dropout，更新NN的结构】</p>
</li>
</ul>
</li>
</ul>
<h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p><strong>Testing中不做dropout</strong> </p>
<ul>
<li><p>If the dropout rate at training is p%, all the weights times 1-p%.</p>
<p>【如果在training中 dropout rate是 p%，在testing是，每个参数都乘 （1-p%)】</p>
<p>【比如dropout rate 是0.5。如果train出来的w是 1，那么testing中 w=0.5】</p>
</li>
</ul>
<h3 id="Why-dropout-in-training：Intuitive-Reason"><a href="#Why-dropout-in-training：Intuitive-Reason" class="headerlink" title="Why dropout in training：Intuitive Reason"></a>Why dropout in training：Intuitive Reason</h3><ol>
<li><p>这是一个比较有趣的比喻：</p>
<p><a href="https://imgchr.com/i/JGRI00"><img src="https://s1.ax1x.com/2020/04/21/JGRI00.md.png" alt="JGRI00.md.png"></a> </p>
</li>
<li><p>这也是一个有趣的比喻hhh:</p>
<p><a href="https://imgchr.com/i/JGRo7V"><img src="https://s1.ax1x.com/2020/04/21/JGRo7V.md.png" alt="JGRo7V.md.png"></a> </p>
<p>即，团队合作的时候，如果每个人都认为队友在带我，那每个人都可能划水。</p>
<p>但是，（training中）如果你知道你的队友在划水，那你可能会做的更好。</p>
<p>但是，（testing中）发现每个人都有更好地做，都没有划水，那么结果就会很好。</p>
<p>（hhhh，李老师每次讲Intuitive Reason的时候，都觉得好有道理hhh，科学的直觉orz给我也整一个）</p>
</li>
</ol>
<h3 id="Why-multiply-1-p-in-testing-Intuitive-reason"><a href="#Why-multiply-1-p-in-testing-Intuitive-reason" class="headerlink" title="Why multiply (1-p%) in testing: Intuitive reason"></a>Why multiply (1-p%) in testing: Intuitive reason</h3><p>为什么在testing中 weights要乘（1-p%)?</p>
<p>用一个具体的例子来直观说明：</p>
<p><a href="https://imgchr.com/i/JGRf6s"><img src="https://s1.ax1x.com/2020/04/21/JGRf6s.md.png" alt="JGRf6s.md.png"></a> </p>
<p>上图中，如果dropout rate=0.5，假设只训练一次， $w_2,w_4$ 相连的neuron都被dropout。</p>
<p>在testing中，因为不对neurondropout，所以如果不改变weight，计算出的结果 $z’\approx 2z$ 。</p>
<p>因此将所有weight简单地和(1-p%) 相乘，能尽量保证计算出的结果 $z’\approx z$ 。</p>
<h3 id="Dropout-is-a-kind-of-ensemble"><a href="#Dropout-is-a-kind-of-ensemble" class="headerlink" title="Dropout is a kind of ensemble"></a>Dropout is a kind of ensemble</h3><p>Ensemble(合奏)，如下图，将testing data丢给train好的NN来估计，最后的估计值取所有NN输出的平均，如下图：</p>
<p><a href="https://imgchr.com/i/JGRhXn"><img src="https://s1.ax1x.com/2020/04/21/JGRhXn.md.png" alt="JGRhXn.md.png"></a> </p>
<p>为什么说dropout is a kind of ensemble?</p>
<p><a href="https://imgchr.com/i/JGRRpQ"><img src="https://s1.ax1x.com/2020/04/21/JGRRpQ.md.png" alt="JGRRpQ.md.png"></a> </p>
<ul>
<li><p>Using one mini-batch to train one network</p>
<p>【dropout相当于每次用一个mini-batch来训练一个network】</p>
</li>
<li><p>Some parameters in the network are shared</p>
<p>【有些参数可能会在很多个mini-batch都被train到】</p>
</li>
</ul>
<p>由于每个神经元有 p%的概率被dropout，因此理论上，如果有M个neuron，可能会训练 $2^M$ 个network。</p>
<p>但是在Ensemble中，将每个network存下来，testing的时候输出取平均，这样的过程太复杂了，结果也不一定会很好。</p>
<p>所以在testing中，no dropout，对原始network中的每个参数乘 (1-p%)，用这样简单的操作来达到ensemble的目的。</p>
<p><a href="https://imgchr.com/i/JGRWlj"><img src="https://s1.ax1x.com/2020/04/21/JGRWlj.md.png" alt="JGRWlj.md.png"></a> </p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li>Keras: <a href="https://keras.io/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore">how can i interrupt training when the validation loss isn’t decresing anymore.</a> </li>
<li>Dropout-wiki：<a href="https://zh.wikipedia.org/wiki/Dropout">https://zh.wikipedia.org/wiki/Dropout</a></li>
</ol>

        </div>
    
        <ul class="post-copyright">
        <li><strong>本文标题：</strong><a href="https://f1ed.github.io/2020/04/21/tips-for-DL/">「机器学习-李宏毅」：Tips for Deep Learning</a></li>
        <li><strong>本文作者：</strong><a href="https://f1ed.github.io">f1ed</a></li>
        <li><strong>本文链接：</strong><a href="https://f1ed.github.io/2020/04/21/tips-for-DL/">https://f1ed.github.io/2020/04/21/tips-for-DL/</a></li>
        <li><strong>发布时间：</strong>2020-04-21</li>
        <li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！
        </li>
        </ul>
    
        
        <hr style="height:1px;margin:1rem 0"/>
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                <i class="fas fa-tags has-text-grey"></i>&nbsp;
                    <a class="has-link-grey -link" href="/tags/DNN/" rel="tag">DNN</a>,&nbsp;<a class="has-link-grey -link" href="/tags/DeepLearning/" rel="tag">DeepLearning</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Machine-Learning/" rel="tag">Machine-Learning</a>,&nbsp;<a class="has-link-grey -link" href="/tags/open-classes/" rel="tag">open-classes</a>
                </div>
            </div>
        </div>
        
        
        
        <div class="social-share"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/qrcode_alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/qrcode_wechatpay.jpg" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/04/25/CNN/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">「机器学习-李宏毅」：Convolution Neural Network（CNN）</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/04/18/Backpropagation/">
                <span class="level-item">「机器学习-李宏毅」：Backpropagation</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: true,
        verify: false,
        app_id: 'U7ITnhsJjngmAcJUpFYdrq5m-gzGzoHsz',
        app_key: 'cSRtvM6PbCOJBTVBAUURyFfO',
        placeholder: 'xxxxxxxx'
    });
</script>

    </div>
</div>



<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="" src="/images/profile.png" alt="f1ed">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        f1ed
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        登高不傲，居低不怨。
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>成都</span>
                    </p>
                    
                </div>
            </div>
        </nav>
    <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
		<div>
                <a href="/archives/">
                    <p class="heading">
                        文章
                    </p>
                    <a href="/archives">
                        <p class="title has-text-weight-normal">
                            20
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <a href="/categories">
                        <p class="title has-text-weight-normal">
                            4
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <a href="/tags">
                        <p class="title has-text-weight-normal">
                            22
                        </p>
                    </a>
                </div>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/f1ed" target="_blank" rel="noopener">
                关注我</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/f1ed">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="RSS" href="/atom.xml">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
        </div>
        
    </div>
</div>

    
        

   <div class="card widget column-left is-sticky" id="toc">
       <div class="card-content" style="max-height:calc(100vh - 22px);overflow:scroll">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Recipe-of-Deep-Learning">
        <span class="has-mr-6">1</span>
        <span>Recipe of Deep Learning</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Do-not-always-blame-Overfitting">
        <span class="has-mr-6">1.1</span>
        <span>Do not always blame Overfitting</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Bad-Results-on-Training-Data">
        <span class="has-mr-6">2</span>
        <span>Bad Results on Training Data</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#New-activation-function">
        <span class="has-mr-6">2.1</span>
        <span>New activation function</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Vanishing-Gradient-Problem">
        <span class="has-mr-6">2.1.1</span>
        <span>Vanishing Gradient Problem</span>
        </a></li><li>
        <a class="is-flex" href="#ReLu-：Rectified-Linear-Unit">
        <span class="has-mr-6">2.1.2</span>
        <span>ReLu ：Rectified Linear Unit</span>
        </a></li><li>
        <a class="is-flex" href="#ReLu-variant">
        <span class="has-mr-6">2.1.3</span>
        <span>ReLu - variant</span>
        </a></li><li>
        <a class="is-flex" href="#Maxout">
        <span class="has-mr-6">2.1.4</span>
        <span>Maxout</span>
        </a></li><li>
        <a class="is-flex" href="#Maxout-how-to-train">
        <span class="has-mr-6">2.1.5</span>
        <span>Maxout : how to train</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Adaptive-Learning-Rate">
        <span class="has-mr-6">2.2</span>
        <span>Adaptive Learning Rate</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Review-Adagrad">
        <span class="has-mr-6">2.2.1</span>
        <span>Review Adagrad</span>
        </a></li><li>
        <a class="is-flex" href="#RMSProp">
        <span class="has-mr-6">2.2.2</span>
        <span>RMSProp</span>
        </a></li><li>
        <a class="is-flex" href="#Momentum">
        <span class="has-mr-6">2.2.3</span>
        <span>Momentum</span>
        </a></li><li>
        <a class="is-flex" href="#Adam-RMSProp-Momentum">
        <span class="has-mr-6">2.2.4</span>
        <span>Adam = RMSProp + Momentum</span>
        </a></li><li>
        <a class="is-flex" href="#Gradient-Descent-Limitation？">
        <span class="has-mr-6">2.2.5</span>
        <span>Gradient Descent Limitation？</span>
        </a></li></ul></li></ul></li><li>
        <a class="is-flex" href="#Bad-Results-on-Testing-Data">
        <span class="has-mr-6">3</span>
        <span>Bad Results on Testing Data</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Early-Stopping">
        <span class="has-mr-6">3.1</span>
        <span>Early Stopping</span>
        </a></li><li>
        <a class="is-flex" href="#Regularization">
        <span class="has-mr-6">3.2</span>
        <span>Regularization</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#L2-norm-regularization">
        <span class="has-mr-6">3.2.1</span>
        <span>L2 norm regularization</span>
        </a></li><li>
        <a class="is-flex" href="#L1-norm-regularization">
        <span class="has-mr-6">3.2.2</span>
        <span>L1 norm regularization</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Dropout">
        <span class="has-mr-6">3.3</span>
        <span>Dropout</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Training">
        <span class="has-mr-6">3.3.1</span>
        <span>Training</span>
        </a></li><li>
        <a class="is-flex" href="#Testing">
        <span class="has-mr-6">3.3.2</span>
        <span>Testing</span>
        </a></li><li>
        <a class="is-flex" href="#Why-dropout-in-training：Intuitive-Reason">
        <span class="has-mr-6">3.3.3</span>
        <span>Why dropout in training：Intuitive Reason</span>
        </a></li><li>
        <a class="is-flex" href="#Why-multiply-1-p-in-testing-Intuitive-reason">
        <span class="has-mr-6">3.3.4</span>
        <span>Why multiply (1-p%) in testing: Intuitive reason</span>
        </a></li><li>
        <a class="is-flex" href="#Dropout-is-a-kind-of-ensemble">
        <span class="has-mr-6">3.3.5</span>
        <span>Dropout is a kind of ensemble</span>
        </a></li></ul></li></ul></li><li>
        <a class="is-flex" href="#Reference">
        <span class="has-mr-6">4</span>
        <span>Reference</span>
        </a></li></ul>
            </div>
        </div>
    </div>


    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>


                

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」：Tips for Deep Learning" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2020 f1ed&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv"> 来访 <span id="busuanzi_value_site_uv"></span>人</span>
                <span id="busuanzi_container_site_pv">, 总访问 <span id="busuanzi_value_site_pv"></span>次</span>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                        <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="f1ed&#39;s GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'https://f1ed.github.io',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</body>
</html>
