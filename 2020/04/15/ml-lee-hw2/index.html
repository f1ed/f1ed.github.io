<!DOCTYPE html>
<html  lang="zh">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta name="baidu-site-verification" content="9ce18CEmjH" />
<meta name="google-site-verification" content="GSUThrU_AtZE-dgdz1QWWouv0L2teKqHWrZg7DfHbXo" />
<meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.1" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>「机器学习-李宏毅」：HW2-Binary Income Predicting - fred&#39;s blog</title>


    <meta name="description" content="这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。包括对数据集的处理，训练模型，可视化，预测等。有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub">
<meta property="og:type" content="article">
<meta property="og:title" content="「机器学习-李宏毅」：HW2-Binary Income Predicting">
<meta property="og:url" content="https://f7ed.com/2020/04/15/ml-lee-hw2/index.html">
<meta property="og:site_name" content="fred&#39;s blog">
<meta property="og:description" content="这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。包括对数据集的处理，训练模型，可视化，预测等。有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://f7ed.com/gallery/thumbnails/80504355_p0_master1200.jpg">
<meta property="article:published_time" content="2020-04-14T16:00:00.000Z">
<meta property="article:modified_time" content="2020-07-03T08:42:48.419Z">
<meta property="article:author" content="f1ed">
<meta property="article:tag" content="Machine-Learning">
<meta property="article:tag" content="Classification">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://f7ed.com/gallery/thumbnails/80504355_p0_master1200.jpg">





<link rel="alternative" href="/atom.xml" title="「机器学习-李宏毅」：HW2-Binary Income Predicting" type="application/atom+xml">



<link rel="icon" href="/images/heart.svg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-171512660-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-171512660-1');
</script>

    
    <script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?b1d53a77276f7e423bc7cf8fafd95b75";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body class="is-3-column">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」：HW2-Binary Income Predicting" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">主页</a>
                
                <a class="navbar-item"
                href="/archives">归档</a>
                
                <a class="navbar-item"
                href="/categories">分类</a>
                
                <a class="navbar-item"
                href="/tags">标签</a>
                
                <a class="navbar-item"
                href="/atom.xml">RSS</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    <a class="navbar-item" target="_blank" rel="noopener" title="My GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="搜索" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-image">
        <span  class="image is-7by1">
            <img class="thumbnail" src="/gallery/thumbnails/80504355_p0_master1200.jpg" alt="「机器学习-李宏毅」：HW2-Binary Income Predicting">
        </span>
    </div>
    
    <div class="card-content article ">
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <i class="fas fa-bars"></i>「机器学习-李宏毅」：HW2-Binary Income Predicting
            
        </h1>
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-04-14T16:00:00.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-04-15</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2020-07-03T08:42:48.419Z"><i class="far fa-calendar-check">&nbsp;</i>2020-07-03</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/">机器学习-李宏毅</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    <i class="far fa-clock"></i>&nbsp;
                    
                    
                    34 分钟 读完 (大约 5066 个字)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span>次访问
                </span>
                
            </div>
        </div>
        
        <div class="content">
            <p>这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。<br>包括对数据集的处理，训练模型，可视化，预测等。<br>有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的<a href="https://github.com/f1ed/ML-HW2">GitHub</a></p>
<a id="more"></a>
<h1 id="Task-introduction-and-Dataset"><a href="#Task-introduction-and-Dataset" class="headerlink" title="Task introduction and Dataset"></a>Task introduction and Dataset</h1><p> Kaggle competition: <a href="https://www.kaggle.com/c/ml2020spring-hw2">link</a> </p>
<p><strong>Task: Binary Classification</strong></p>
<p>Predict whether the income of an individual exceeds $50000 or not ?</p>
<p>*<em>Dataset: *</em> Census-Income (KDD) Dataset</p>
<p>(Remove unnecessary attributes and balance the ratio between positively and negatively labeled data)</p>
<h1 id="Feature-Format"><a href="#Feature-Format" class="headerlink" title="Feature Format"></a>Feature Format</h1><ul>
<li><p>train.csv, test_no_label.csv【都是没有处理过的数据，可作为数据参考和优化参考】</p>
<ul>
<li><p>text-based raw data</p>
</li>
<li><p>unnecessary attributes removed, positive/negative ratio balanced.</p>
</li>
</ul>
</li>
<li><p>X_train, Y_train, X_test【已经处理过的数据，可以直接使用】</p>
<ul>
<li><p>discrete features in train.csv =&gt; one-hot encoding in X_train (education, martial state…)</p>
</li>
<li><p>continuous features in train.csv =&gt; remain the same in X_train (age, capital losses…).</p>
</li>
<li><p>X_train, X_test : each row contains one 510-dim feature represents a sample.</p>
</li>
<li><p>Y_train: label = 0 means “&lt;= 50K” 、 label = 1 means “ &gt;50K ”</p>
</li>
</ul>
</li>
</ul>
<p>注：数据集超大，用notepad查看比较舒服；调试时，也可以先调试小一点的数据集。</p>
<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><p>Logistic Regression 原理部分见<a href="/2020/04/01/Classification2/" title="这篇博客">这篇博客</a>。</p>
<h2 id="Prepare-data"><a href="#Prepare-data" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>本文直接使用X_train Y_train X_test 已经处理好的数据集。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_Train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./logistic_output/output_logistic.csv'</span></span><br><span class="line">fpath = <span class="string">'./logistic_output/logistic'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br></pre></td></tr></table></figure>

<p>统计一下数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In logistic model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of Training set: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of development set: &#123;&#125;\n'</span>.format(dev_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n'</span>.format(data_dim))</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">In</span> logistic model:</span><br><span class="line">Size of Training set: <span class="number">48830</span></span><br><span class="line">Size of development set: <span class="number">5426</span></span><br><span class="line">Size of test set: <span class="number">27622</span></span><br><span class="line">Dimension of <span class="keyword">data</span>: <span class="number">510</span></span><br></pre></td></tr></table></figure>

<h3 id="normalize"><a href="#normalize" class="headerlink" title="normalize"></a>normalize</h3><p>normalize data.</p>
<p>对于train data，计算出每个feature的mean和std，保存下来用来normalize test data。</p>
<p>代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br></pre></td></tr></table></figure>

<h3 id="Development-set-split"><a href="#Development-set-split" class="headerlink" title="Development set split"></a>Development set split</h3><p>在logistic regression中使用的gradient，没有closed-form解，所以在train set中划出一部分作为development set 优化参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_dev_split</span><span class="params">(X, Y, dev_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br></pre></td></tr></table></figure>



<h2 id="Useful-function"><a href="#Useful-function" class="headerlink" title="Useful function"></a>Useful function</h2><h3 id="shuffle-X-Y"><a href="#shuffle-X-Y" class="headerlink" title="_shuffle(X, Y)"></a>_shuffle(X, Y)</h3><p>本文使用mini-batch gradient。</p>
<p>所以在每次epoch时，以相同顺序同时打乱X_train,Y_train数组，再mini-batch。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br></pre></td></tr></table></figure>



<h3 id="sigmod-z"><a href="#sigmod-z" class="headerlink" title="_sigmod(z)"></a>_sigmod(z)</h3><p>计算 $\frac{1}{1+e^{-z}}$ ，注意：防止溢出，给函数返回值规定上界和下界。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br></pre></td></tr></table></figure>

<h3 id="f-X-w-b"><a href="#f-X-w-b" class="headerlink" title="_f(X, w, b)"></a>_f(X, w, b)</h3><p>是sigmod函数的输入，linear part。</p>
<ul>
<li>输入：<ul>
<li>X：shape = [size, data_dimension]</li>
<li>w：weight vector, shape = [data_dimension, 1]</li>
<li>b: bias, scalar</li>
</ul>
</li>
<li>输出：<ul>
<li>属于Class 1的概率（Label=0，即收入小于$50k的概率）</li>
</ul>
</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br></pre></td></tr></table></figure>

<h3 id="predict-X-w-b"><a href="#predict-X-w-b" class="headerlink" title="_predict(X, w, b)"></a>_predict(X, w, b)</h3><p>预测Label=0？（0或者1，不是概率）</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br></pre></td></tr></table></figure>

<h3 id="accuracy-Y-pred-Y-label"><a href="#accuracy-Y-pred-Y-label" class="headerlink" title="_accuracy(Y_pred, Y_label)"></a>_accuracy(Y_pred, Y_label)</h3><p>计算预测出的结果（0或者1）和真实结果的正确率。</p>
<p>这里使用 $1-\overline{error}$ 来表示正确率。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>

<h3 id="cross-entropy-loss-y-pred-Y-label"><a href="#cross-entropy-loss-y-pred-Y-label" class="headerlink" title="_cross_entropy_loss(y_pred, Y_label)"></a>_cross_entropy_loss(y_pred, Y_label)</h3><p>计算预测出的概率（是sigmod的函数输出）和真实结果的交叉熵。</p>
<p>计算公式为： $\sum_n {C(y_{pred},Y_{label})}=-\sum[Y_{label}\ln{y_{pred}}+(1-Y_{label})\ln(1-{y_{pred}})]$ </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<h3 id="gradient-X-Y-label-w-b"><a href="#gradient-X-Y-label-w-b" class="headerlink" title="_gradient(X, Y_label, w, b)"></a>_gradient(X, Y_label, w, b)</h3><p>和Regression的最小二乘一样。（严谨的说，最多一个系数不同）</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">def</span> <span class="string">_gradient(X, Y_label, w, b):</span></span><br><span class="line"><span class="comment">    # This function calculates the gradient of cross entropy</span></span><br><span class="line"><span class="comment">    # X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    <span class="attr">y_pred</span> = <span class="string">_f(X, w, b)</span></span><br><span class="line">    <span class="attr">pred_error</span> = <span class="string">Y_label - y_pred</span></span><br><span class="line">    <span class="attr">w_grad</span> = <span class="string">- np.dot(X.T, pred_error)</span></span><br><span class="line">    <span class="attr">b_grad</span> = <span class="string">- np.sum(pred_error)</span></span><br><span class="line">    <span class="attr">return</span> <span class="string">w_grad, float(b_grad)</span></span><br></pre></td></tr></table></figure>

<h2 id="Training-Adagrad"><a href="#Training-Adagrad" class="headerlink" title="Training (Adagrad)"></a>Training (Adagrad)</h2><p>初始化一些参数。</p>
<p><strong>这里特别注意</strong> :</p>
<p>由于adagrad的参数更新是 $w \longleftarrow w-\eta \frac{gradient}{ \sqrt{gradsum}}$ .</p>
<p><strong>防止除0</strong>，初始化gradsum的值为一个较小值。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.float(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.float(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br></pre></td></tr></table></figure>

<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Aagrad具体原理见<a href="/2020/03/01/Gradient/" title="这篇博客">这篇博客</a>的1.2节。</p>
<p>迭代更新时，每次epoch计算一次loss和accuracy，以便可视化更新过程，调整参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> range(int(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br></pre></td></tr></table></figure>

<h3 id="Loss-amp-accuracy"><a href="#Loss-amp-accuracy" class="headerlink" title="Loss &amp; accuracy"></a>Loss &amp; accuracy</h3><p>输出最后一次迭代的loss和accuracy。</p>
<p>结果如下：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Training loss: <span class="number">0.2933570286596322</span></span><br><span class="line">Training accuracy: <span class="number">0.8839238173254147</span></span><br><span class="line">Development loss: <span class="number">0.31029505347634456</span></span><br><span class="line">Development accuracy: <span class="number">0.8336166253549906</span></span><br></pre></td></tr></table></figure>

<p>画出loss 和 accuracy的更新过程：</p>
<p>loss：</p>
<p><a href="https://imgchr.com/i/JPCjx0"><img src="https://s1.ax1x.com/2020/04/15/JPCjx0.png" alt="JPCjx0.png"></a> </p>
<p>accuracy：</p>
<p><a href="https://imgchr.com/i/JPCxMV"><img src="https://s1.ax1x.com/2020/04/15/JPCxMV.png" alt="JPCxMV.png"></a> </p>
<p>由于Feature数量较大，将权重影响最大的feature输出看看：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Other Rel &lt;<span class="number">18</span> spouse of subfamily RP: [<span class="number">7.11323764</span>]</span><br><span class="line"> Grandchild &lt;<span class="number">18</span> ever marr not <span class="keyword">in</span> subfamily: [<span class="number">6.8321061</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> ever marr RP of subfamily: [<span class="number">6.77322397</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> ever marr RP of subfamily: [<span class="number">6.76688406</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> never married RP of subfamily: [<span class="number">6.37488958</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> spouse of subfamily RP: [<span class="number">5.97717831</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.53932651</span>]</span><br><span class="line"> Grandchild <span class="number">18</span>+ spouse of subfamily RP: [<span class="number">5.42948497</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.41543809</span>]</span><br><span class="line"> Mexico: [<span class="number">4.79920763</span>]</span><br></pre></td></tr></table></figure>

<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>完整数据集、代码等，欢迎光临小透明<a href="https://github.com/f1ed/ML-HW2">GitHub</a> </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################</span></span><br><span class="line"><span class="comment"># Data:2020-04-05</span></span><br><span class="line"><span class="comment"># Author: Fred Lau</span></span><br><span class="line"><span class="comment"># ML-Lee: HW2 : Binary Classification</span></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################</span></span><br><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_Train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./logistic_output/output_logistic.csv'</span></span><br><span class="line">fpath = <span class="string">'./logistic_output/logistic'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_dev_split</span><span class="params">(X, Y, dev_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In logistic model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of Training set: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of development set: &#123;&#125;\n'</span>.format(dev_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################</span></span><br><span class="line"><span class="comment"># useful function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to calculate probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - (<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This is the logistic function, parameterized by w and b</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [batch_size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       predict probability of each row of X being positively labeled, shape = [batch_size, 1]</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This fucntion returns a truth value prediction for each row of X by logistic regression</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradient</span><span class="params">(X, Y_label, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the gradient of cross entropy</span></span><br><span class="line">    <span class="comment"># X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    y_pred = _f(X, w, b)</span><br><span class="line">    pred_error = Y_label - y_pred</span><br><span class="line">    w_grad = - np.dot(X.T, pred_error)</span><br><span class="line">    b_grad = - np.sum(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, float(b_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#######################################</span></span><br><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.float(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.float(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> range(int(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'Training loss: &#123;&#125;\n'</span>.format(train_loss[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Training accuracy: &#123;&#125;\n'</span>.format(train_acc[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Development loss: &#123;&#125;\n'</span>.format(dev_loss[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Development accuracy: &#123;&#125;\n'</span>.format(dev_acc[<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###################</span></span><br><span class="line"><span class="comment"># Plotting Loss and accuracy curve</span></span><br><span class="line"><span class="comment"># Loss curve</span></span><br><span class="line">plt.plot(train_loss, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(dev_loss, label=<span class="string">'dev'</span>)</span><br><span class="line">plt.title(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'./logistic_output/loss.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(train_acc, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(dev_acc, label=<span class="string">'dev'</span>)</span><br><span class="line">plt.title(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'./logistic_output/acc.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id, label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> id, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;, &#123;&#125;\n'</span>.format(id, label[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line"><span class="comment"># Output the weights and bias</span></span><br><span class="line">ind = (np.argsort(np.abs(w), axis=<span class="number">0</span>)[::<span class="number">-1</span>]).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>, <span class="number">0</span>: <span class="number">10</span>]:</span><br><span class="line">       f.write(<span class="string">'&#123;&#125;: &#123;&#125;\n'</span>.format(content[i], w[i]))</span><br></pre></td></tr></table></figure>

<h1 id="Generative-Model"><a href="#Generative-Model" class="headerlink" title="Generative Model"></a>Generative Model</h1><p>Generative Model 原理部分见 <a href="/2020/03/21/Classification1/" title="这篇博客">这篇博客</a></p>
<h2 id="Prepare-data-1"><a href="#Prepare-data-1" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>这部分和Logistic regression一样。</p>
<p>只是，因为generative model有closed-form solution，不需要划分development set。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./generative_output/output_&#123;&#125;.csv'</span></span><br><span class="line">fpath = <span class="string">'./generative_output/generative'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In generative model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of training data: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n\n'</span>.format(data_dim))</span><br></pre></td></tr></table></figure>

<h2 id="Useful-functions"><a href="#Useful-functions" class="headerlink" title="Useful functions"></a>Useful functions</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>

<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="公式再推导"><a href="#公式再推导" class="headerlink" title="公式再推导"></a>公式再推导</h3><p>计算公式： </p>

$$
\begin{equation}\begin{aligned}P\left(C_{1} | x\right)&=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}\\&=\frac{1}{1+\frac{P\left(x | C_{2}\right) P\left(C_{2}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)}}\\&=\frac{1}{1+\exp (-z)} =\sigma(z)\qquad(z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}\end{aligned}\end{equation}
$$


<p>计算z的过程：</p>
<ol>
<li>首先计算Prior Probability。</li>
<li>假设模型是Gaussian的，算出 $\mu_1,\mu_2 ,\Sigma$  的closed-form solution 。</li>
<li>根据 $\mu_1,\mu_2,\Sigma$ 计算出 $w,b$ 。</li>
</ol>
<hr>
<ol>
<li><p><strong>计算Prior Probability。</strong> </p>
<p>程序中用list comprehension处理较简单。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>计算 $\mu_1,\mu_2 ,\Sigma$ （Gaussian）</p>
<p>$\mu_0=\frac{1}{C0} \sum_{n=1}^{C0} x^{n} $  (Label=0)</p>
<p>$\mu_1=\frac{1}{C1} \sum_{n=1}^{C1} x^{n} $  (Label=0)</p>
<p>$\Sigma_0=\frac{1}{C0} \sum_{n=1}^{C0}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$  (<strong>注意</strong> ：这里的 $x^n,\mu$ 都是行向量，注意转置的位置）</p>
<p>$\Sigma_1=\frac{1}{C1} \sum_{n=1}^{C1}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$ </p>
<p>$\Sigma=(C0 \times\Sigma_0+C1\times\Sigma_1)/(C0+C1)$   (shared covariance) </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="3">
<li><p>计算 $w,b$ </p>
<p>在 <a href="/2020/03/21/Classification1/" title="这篇博客">这篇博客</a>中的第2小节中的公式推导中， $x^n,\mu$ 都是列向量，公式如下：</p>

   $$
   z=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} x-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}
   $$
   
  $w^T=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} \qquad b=-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}$ 

<hr>
<p><strong>但是</strong> ，一般我们在处理的数据集，$x^n,\mu$ 都是行向量。推导过程相同，公式如下：</p>
<p><font color=#f00> <strong>（主要注意转置和矩阵乘积顺序）</strong> </font></p>

   $$
   z=x\cdot \Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  -\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}
   $$
   
  $w=\Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  \qquad b=-\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}$ 

</li>
</ol>
<hr>
<p><font color=#f00>但是，协方差矩阵的逆怎么求呢？ </font> </p>
<p>numpy中有直接求逆矩阵的方法(np.linalg.inv)，但当该矩阵是nearly singular，是奇异矩阵时，就会报错。</p>
<p>而我们的协方差矩阵（510*510）很大，很难保证他不是奇异矩阵。</p>
<p>于是，有一个 <del>牛逼</del> 强大的数学方法，叫SVD(singular value decomposition, 奇异值分解) 。</p>
<p>原理步骤我……还没有完全搞清楚QAQ（先挖个坑）[1]</p>
<p>利用SVD，可以将任何一个矩阵（即使是奇异矩阵），分界成 $A=u s v^T$ 的形式：其中u,v都是标准正交矩阵，s是对角矩阵。（numpy.linalg.svd方法实现了SVD）</p>
<p><font color=#f00>可以利用SVD求矩阵的伪逆 </font> </p>
<ul>
<li>$A=u s v^T$<ul>
<li>u,v是标准正交矩阵，其逆矩阵等于其转置矩阵</li>
<li>s是对角矩阵，其”逆矩阵“<strong>（注意s矩阵的对角也可能有0元素）</strong> 将非0元素取倒数即可。</li>
</ul>
</li>
<li>$A^{-1}=v s^{-1} u$</li>
</ul>
<p>计算 $w,b$ 的代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>



<h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><p> accuracy结果：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">0.8756450899439694</span></span><br></pre></td></tr></table></figure>

<p>也将权重较大的feature输出看看：</p>
<figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">age: [-<span class="number">0.51867291</span>]</span><br><span class="line"> Masters degree(MA MS MEng MEd MSW MBA): [-<span class="number">0.49912643</span>]</span><br><span class="line"> Spouse of householder: [<span class="number">0.49786805</span>]</span><br><span class="line">weeks worked <span class="keyword">in</span> year: [-<span class="number">0.44710924</span>]</span><br><span class="line"> Spouse of householder: [-<span class="number">0.43305697</span>]</span><br><span class="line">capital gains: [-<span class="number">0.42608727</span>]</span><br><span class="line">dividends from stocks: [-<span class="number">0.41994666</span>]</span><br><span class="line"> Doctorate degree(PhD EdD): [-<span class="number">0.39310961</span>]</span><br><span class="line">num persons worked <span class="keyword">for</span> employer: [-<span class="number">0.37345994</span>]</span><br><span class="line"> Prof school degree (MD DDS DVM LLB JD): [-<span class="number">0.35594107</span>]</span><br></pre></td></tr></table></figure>



<h2 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h2><p>具体数据集和代码，欢迎光临小透明<a href="https://github.com/f1ed/ML-HW2">GitHub</a> </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment">##############################################</span></span><br><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./generative_output/output_&#123;&#125;.csv'</span></span><br><span class="line">fpath = <span class="string">'./generative_output/generative'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In generative model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of training data: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n\n'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment">########################</span></span><br><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"><span class="comment">#######################</span></span><br><span class="line"><span class="comment"># Generative Model: closed-form solution, can be computed directly</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute accuracy on training set</span></span><br><span class="line">Y_train_pred = <span class="number">1</span> - _predict(X_train, w, b)</span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'\nTraining accuracy: &#123;&#125;\n'</span>.format(_accuracy(Y_train_pred, Y_train)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = <span class="number">1</span> - _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath.format(<span class="string">'generative'</span>), <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id, label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;, &#123;&#125;\n'</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output the most significant weight</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">ind = np.argsort(np.abs(np.concatenate(w)))[::<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>)<span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>:<span class="number">10</span>]:</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;: &#123;&#125;\n'</span>.format(content[i], w[i]))</span><br></pre></td></tr></table></figure>



<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li>SVD原理，待补充</li>
</ol>

        </div>
    
        <ul class="post-copyright">
        <li><strong>本文标题：</strong><a href="https://f7ed.com/2020/04/15/ml-lee-hw2/">「机器学习-李宏毅」：HW2-Binary Income Predicting</a></li>
        <li><strong>本文作者：</strong><a href="https://f7ed.com">f1ed</a></li>
        <li><strong>本文链接：</strong><a href="https://f7ed.com/2020/04/15/ml-lee-hw2/">https://f7ed.com/2020/04/15/ml-lee-hw2/</a></li>
        <li><strong>发布时间：</strong>2020-04-15</li>
        <li><strong>版权声明：</strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！
        </li>
        </ul>
    
        
        <hr style="height:1px;margin:1rem 0"/>
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                <i class="fas fa-tags has-text-grey"></i>&nbsp;
                    <a class="has-link-grey -link" href="/tags/Classification/" rel="tag">Classification</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Machine-Learning/" rel="tag">Machine-Learning</a>
                </div>
            </div>
        </div>
        
        
        
        <div class="social-share"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>支付宝</span>
    <div class="qrcode"><img src="/images/qrcode_alipay.jpg" alt="支付宝"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>微信</span>
    <div class="qrcode"><img src="/images/qrcode_wechatpay.jpg" alt="微信"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/04/18/DL-introdunction/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">「机器学习-李宏毅」：Deep Learning-Introduction</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/04/06/ml-lee-hw1/">
                <span class="level-item">「机器学习-李宏毅」:HW1-Predict PM2.5</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">评论</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: true,
        verify: false,
        app_id: 'U7ITnhsJjngmAcJUpFYdrq5m-gzGzoHsz',
        app_key: 'cSRtvM6PbCOJBTVBAUURyFfO',
        placeholder: 'xxxxxxxx'
    });
</script>

    </div>
</div>



<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="" src="/images/profile.png" alt="f1ed">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        f1ed
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        登高不傲，居低不怨。
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>成都</span>
                    </p>
                    
                </div>
            </div>
        </nav>
    <nav class="level menu-list is-mobile" style="margin-bottom:1rem">
            <div class="level-item has-text-centered is-marginless">
		<div>
                <a href="/archives/">
                    <p class="heading">
                        文章
                    </p>
                    <a href="/archives">
                        <p class="title has-text-weight-normal">
                            21
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        分类
                    </p>
                    <a href="/categories">
                        <p class="title has-text-weight-normal">
                            5
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        标签
                    </p>
                    <a href="/tags">
                        <p class="title has-text-weight-normal">
                            26
                        </p>
                    </a>
                </div>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/f1ed" target="_blank" rel="noopener">
                关注我</a>
        </div>
        
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="Github" href="https://github.com/f1ed">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank" rel="noopener"
                title="RSS" href="/atom.xml">
                
                <i class="fas fa-rss"></i>
                
            </a>
            
        </div>
        
    </div>
</div>

    
        

   <div class="card widget column-left is-sticky" id="toc">
       <div class="card-content" style="max-height:calc(100vh - 22px);overflow:scroll">
            <div class="menu">
                <h3 class="menu-label">
                    目录
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Task-introduction-and-Dataset">
        <span class="has-mr-6">1</span>
        <span>Task introduction and Dataset</span>
        </a></li><li>
        <a class="is-flex" href="#Feature-Format">
        <span class="has-mr-6">2</span>
        <span>Feature Format</span>
        </a></li><li>
        <a class="is-flex" href="#Logistic-Regression">
        <span class="has-mr-6">3</span>
        <span>Logistic Regression</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Prepare-data">
        <span class="has-mr-6">3.1</span>
        <span>Prepare data</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#normalize">
        <span class="has-mr-6">3.1.1</span>
        <span>normalize</span>
        </a></li><li>
        <a class="is-flex" href="#Development-set-split">
        <span class="has-mr-6">3.1.2</span>
        <span>Development set split</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Useful-function">
        <span class="has-mr-6">3.2</span>
        <span>Useful function</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#shuffle-X-Y">
        <span class="has-mr-6">3.2.1</span>
        <span>_shuffle(X, Y)</span>
        </a></li><li>
        <a class="is-flex" href="#sigmod-z">
        <span class="has-mr-6">3.2.2</span>
        <span>_sigmod(z)</span>
        </a></li><li>
        <a class="is-flex" href="#f-X-w-b">
        <span class="has-mr-6">3.2.3</span>
        <span>_f(X, w, b)</span>
        </a></li><li>
        <a class="is-flex" href="#predict-X-w-b">
        <span class="has-mr-6">3.2.4</span>
        <span>_predict(X, w, b)</span>
        </a></li><li>
        <a class="is-flex" href="#accuracy-Y-pred-Y-label">
        <span class="has-mr-6">3.2.5</span>
        <span>_accuracy(Y_pred, Y_label)</span>
        </a></li><li>
        <a class="is-flex" href="#cross-entropy-loss-y-pred-Y-label">
        <span class="has-mr-6">3.2.6</span>
        <span>_cross_entropy_loss(y_pred, Y_label)</span>
        </a></li><li>
        <a class="is-flex" href="#gradient-X-Y-label-w-b">
        <span class="has-mr-6">3.2.7</span>
        <span>_gradient(X, Y_label, w, b)</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Training-Adagrad">
        <span class="has-mr-6">3.3</span>
        <span>Training (Adagrad)</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Adagrad">
        <span class="has-mr-6">3.3.1</span>
        <span>Adagrad</span>
        </a></li><li>
        <a class="is-flex" href="#Loss-amp-accuracy">
        <span class="has-mr-6">3.3.2</span>
        <span>Loss & accuracy</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Code">
        <span class="has-mr-6">3.4</span>
        <span>Code</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Generative-Model">
        <span class="has-mr-6">4</span>
        <span>Generative Model</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Prepare-data-1">
        <span class="has-mr-6">4.1</span>
        <span>Prepare data</span>
        </a></li><li>
        <a class="is-flex" href="#Useful-functions">
        <span class="has-mr-6">4.2</span>
        <span>Useful functions</span>
        </a></li><li>
        <a class="is-flex" href="#Training">
        <span class="has-mr-6">4.3</span>
        <span>Training</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#公式再推导">
        <span class="has-mr-6">4.3.1</span>
        <span>公式再推导</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Accuracy">
        <span class="has-mr-6">4.4</span>
        <span>Accuracy</span>
        </a></li><li>
        <a class="is-flex" href="#Code-1">
        <span class="has-mr-6">4.5</span>
        <span>Code</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Reference">
        <span class="has-mr-6">5</span>
        <span>Reference</span>
        </a></li></ul>
            </div>
        </div>
    </div>


    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>


                

            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/f1ed_logo.png" alt="「机器学习-李宏毅」：HW2-Binary Income Predicting" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2020 f1ed&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv"> 来访 <span id="busuanzi_value_site_uv"></span>人</span>
                <span id="busuanzi_container_site_pv">, 总访问 <span id="busuanzi_value_site_pv"></span>次</span>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                        <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="f1ed&#39;s GitHub" href="https://github.com/f1ed">
                        
                        <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("zh-CN");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'https://f7ed.com',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="回到顶端" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="想要查找什么..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</body>
</html>
