{"pages":[],"posts":[{"title":"Adagrag-demo","text":"实现这篇文章中前面两个tips。 实现了tip1 Adagrad + tip2 Stochastic Gradient Descent demo代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869################## 2020/03/06 ## Adagrad demo ##################import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model# datax_data = [[338.], [333.], [328.], [207.], [226.], [25.], [179.], [60.], [208.], [606.]]y_data = [640., 633., 619., 393., 428., 27., 193., 66., 226., 1591.]# coordinatex = np.arange(-200, -100, 1)y = np.arange(-5, 5, 0.1)Z = np.zeros((len(y), len(x)))# cal the Loss of every point(function)for i in range(len(x)): for j in range(len(y)): b = x[i] w = y[j] for k in range(len(x_data)): Z[j][i] += (y_data[k] - b - w * x_data[k][0])**2# initialb = -120w = -4lr = 1 # learning rateiteration = 100000# record the iterationb_his = [b]w_his = [w]# Adagradb_grad_sum2 = 0.0w_grad_sum2 = 0.0for i in range(iteration): for k in range(len(x_data)): b_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-1) w_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-x_data[k][0]) b_grad_sum2 += b_grad**2 w_grad_sum2 += w_grad**2 b = b - lr / np.sqrt(b_grad_sum2) * b_grad w = w - lr / np.sqrt(w_grad_sum2) * w_grad b_his.append(b) w_his.append(w)# sklearn linear modelreg = linear_model.LinearRegression()print(reg.fit(x_data, y_data))print(reg.coef_[0])print(reg.intercept_)# display the figureplt.contourf(x, y, Z, 50, alpha=0.5, cmap=plt.get_cmap('jet'))plt.plot(reg.intercept_, reg.coef_, 'x', ms=13, lw=1.5, color='orange')plt.plot(b_his, w_his, 'o-', ms=3, lw=1.5, color='black')plt.xlim(-200, -100)plt.ylim(-5, 5)plt.xlabel('$b$', fontsize=16)plt.ylabel('$w$', fontsize=16)# plt.show()plt.savefig(\"Loss.png\") Loss 迭代图画出的图片很直观","link":"/2020/03/09/Adagrad-demo/"},{"title":"「Cryptography-Dan」:Stream Cipher 1","text":"Stream Cipher的第一部分：介绍了One Time Pad和Stream Cipher中的PRG。其中OTP部分叙述了什么是Perfect Secrecy？为什么OTP很难在实践中应用？Stream Cipher部分中，本文主要阐述了什么是PRG？Stream Cipher的另一种安全的定义（依靠PRG的unpredictable)。本文后半部分，详细阐述了一种weak PRG——线性同余生成器，它是如何工作的？它为什么不安全？如何attack it elegantly? The One Time PadSymmetric Ciphers: difinitionDef :a cipher difined over $\\mathcal{(K,M,C)}$ is a paire of “efiicient “ algorithms $(E,D)$ where $$ E :\\mathcal{K \\times M \\longrightarrow \\mathcal{C}} \\quad ,\\quad D:\\mathcal{K\\times\\mathcal{C}\\longrightarrow\\mathcal{M}} \\\\ s.t. \\quad \\forall m\\in \\mathcal{M},k\\in \\mathcal{K}:D(k,E(k,m))=m $$ $\\mathcal{(K,M,C)}$ 分别是密钥空间、明文空间、密文空间。 对称加密其实是定义在$\\mathcal{(K,M,C)}$ 的两个有效算法 $(E,D)$ ，这两个算法满足consistence equation(一致性方程)：$D(k,E(k,m))=m$ 。 一些说明： $E$ is ofen randomized. 即加密算法E总是随机生成一些bits，用来加密明文。 $D$ is always deterministic. 即当确定密钥和明文时，解密算法的输出总是唯一的。 “efficient” 的含义 对于理论派：efficient表示 in polynomial time（多项式时间） 对于实践派：efficient表示 in a certain time One Time Pad(OTP)Definition of OTPThe one time pad(OTP) 又叫一次一密。 用对称加密的定义来表示OTP： $\\mathcal{M=C=}{0,1}^n\\quad \\mathcal{K}={0,1}^n$ $E：\\quad c = E(k,m)=k\\oplus m \\quad$ $D:\\quad m = D(k,c)=k\\oplus c$ 明文空间和密文空间相同，密钥空间也是n位01串集合。 而且，在OTP中，密钥key的长度和明文message长度一样长。 加密过程如上图所示。 证明其一致性方程 Indeed ： $D(k,E(k,m))=D(k,k\\oplus m)=k\\oplus (k\\oplus m)=0\\oplus m=m$ 但是OTP加密安全吗？ 如果已知明文(m)和他的OTP密文(c)，可以算出用来加密m的OTP key吗？ ：当然，根据异或的性质，key $k=m\\oplus c$ 所以什么是安全呢？ Information Theoretic Security根据Shannon 1949发表的论文，Shannon’s basic idea: CT(Ciphertext) should reveal no “info” about PT(Plaintext)，即密文不应该泄露明文的任何信息。 Perfect Security Def:A cipher $(E,D)$ over $\\mathcal{(K,M,C)}$ has perfect security if $\\forall m_0,m_1 \\in \\mathcal{M}\\ (|m_0|=|m_1|) \\quad \\text{and} \\quad \\forall c\\in \\mathcal{C} $$$Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$$ $k \\overset{R}\\longleftarrow \\mathcal{K}$ 的意思是 $k$ 是 从$\\mathcal{K}$ 中随机取的，即随机变量 $k$ 的取值是均匀分布。 对任意 $m_0,m_1$ （并且message长度相同），那么在密钥空间任意取 $k$ , $k$ 将 $m_0,m_1$ 加密为相同密文的概率相同。 对attacker来说 ：攻击者截取一段密文c，那么c是由 $m_0,m_1$ 加密而来的概率是相同的，即攻击者也不知道明文到底是 $m_0$ 还是 $m_1$ （因为概率相同）。 $\\Rightarrow$ Given CT can’t tell if msg is $m_0 \\ \\text{or}\\ m_1 $ (for all $m_i$ ) . 【攻击者不能区分明文到底是 $m_?$ 】 $\\Rightarrow$ most powerful adv.(adversary) learns nothing about PT from CT. 【不管攻击者多聪明，都不能从密文中得到密文的信息】 $\\Rightarrow$ no CT only attack!! (but other attackers possible). 【惟密文攻击对OTP无效】 OTP has perfect secrecyLemma : OTP has perfect secrecy. 用上一小节的perfect securecy的定义来证明这个引理。 Proof： 要证明： $Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$ 表达式： $\\forall m, c: \\quad \\operatorname{Pr}_{k}[E(k,m)=c]=\\frac{\\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c}{|\\mathcal{K}|}$ 对于任意m,c, $\\operatorname{Pr}_{k}[E(k,m)=c]$ 等于能将m加密为c的密钥个数除以密钥空间的大小。 $\\because |\\mathcal{K}|$ 是相同的，所以即证 ： $\\{ \\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c \\}=\\text{const}$ 对于任意 m,c，能将m加密为c的OTP key只有一个： $k=m\\oplus c$ $\\therefore$ OTP has perfect secrecy. key-len $\\geq$ msg-len Perfect Secrecy的性质带来了一个bad news。 Thm: perfect secrecy $\\Rightarrow$ $|\\mathcal{K}|\\geq|\\mathcal{M}|$ 如果一个cipher满足perfect secrecy,那么其密钥的长度必须大于等于明文长度。这也是perfect secrecy的必要条件。 所以OTP是perfect secrecy的最优情况，$|\\mathcal{K}|=|\\mathcal{M}|$ ，密钥长度等于明文长度。 为什么说是一个bad news呢？ 如果Alice用OTP给Bob发一段msg，在她发之前，她需要先发一个和msg等长的key，这个key只有Alice和Bob知道。 所以如果Alice有能保密传输key的方法，那她何不直接用这个方法传输msg呢？ 所以OTP : hard to use in practice! (long key-len) 因此，我们需要key-len短的cipher。 Pseudorandom Generators（伪随机数生成器）Stream Ciphers: making OTP practicalStream Ciphers（流密码）的思想就是：用PRG（pseudorandom Generators） key 代替 “random” key。 PRG其实就是一个function G：${ 0,1 }^s\\longrightarrow { 0,1 }^n \\quad, n&gt;&gt;s$ 。 通过函数将较小的seed space映射到大得多的output space。 注意： function G is eff. computable by a deterministic algorithm. 函数G是确定的，随机的只有s，s也是G的输入。 PRG的输出应该是 “look random”（下文会提到的PRG必须是unpredictable） Stream Ciphers的过程如上图所示：通过PRG，将长度较短的k映射为足够长的G(k)，G(k)异或m得到密文。 有两个问题？ 第一，Stream Cipher安全吗？为什么安全？ 第二，Stream Cipher have perfect secrecy? 现在，只能回答第二个问题。 ：流密码没有perfect secrecy。因为它不满足key-len $\\geq$ msg-len，流密码的密钥长度远小于明文长度。 流密码没有perfect secrecy，所以我们还需要引入另一种安全，这种安全和PRG有关。 PRG must be unpredictablePRG如果predictable，流密码安全吗？ Suppose predictable假设PRG是可预测的，即： $ \\exists:\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1,...,n} $ 已知G(k)输出的前i bis，存在一种算法，能计算G(k)的后面剩余的bits。 攻击如上图所示： 如果attacker has prior knowledge：已知一段密文前缀的对应明文（m斜线字段）（比如在SMTP协议中，报文的开头总是”from”） attacker将该密文字段与已知明文字段异或，得到G(k)的前缀。 因为PRG是可预测的，所以可以通过G(k)的前缀计算出G(k)的剩下部分。 得到的G(K)就可以recover m。 即使，G(k)只能预测后一位，即 $\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1}$ ，也不安全，当预测出下一位时，又得到了新的前缀，最终得到完整的G(k)。 所以当PRG可预测时，流密码就不安全了。 所以用Stream Cipher时，PRG必须unpredictable! Predictable: difinitionPredictable Def : $ \\exists $ \"eff\" alg. A and $\\exists$ $0\\leq i\\leq n-1$ ， s.t. $Pr_{k \\overset{R}\\leftarrow \\mathcal{K} } {[A(G(k)|_{1,2,...,i})=G(k)|_{i+1}]}>1/2 +\\epsilon$ for non-negligible $\\epsilon$ (e.g. $\\epsilon=1/2^{30}$) 可预测：即存在算法，通过G(k)的前i位可以计算出第i+1位的概率大于1/2 + $\\epsilon$ (不可忽略的值) Unpredictable Def : 即predictable的反面， $\\forall i$ : no “eff.” adv. can predict bit(i+1) for “non-neg” $\\epsilon$ . Q：假设 $\\mathrm{G}: \\mathrm{K} \\rightarrow{0,1}^{\\mathrm{n}} $ ，满足XOR(G(k))=1，G可预测吗？ W：G可预测，存在i = n-1,因为当已知前n-1位,可以预测第n位。 Weak PRGsLinear Congruential Generators一种应该永远不在安全应用中使用PRG——LCG（linear congruential generators）(线性同余随机生成器)。 虽然他们在应用中使用很快，而且其输出还有良好的统计性质（比如0的个数和1的个数基本相等等），但他们应该never be used for cryptographic。 因为在实践中，给出LCG输出的一些连续序列，很容易计算出输出的剩余序列。 Basic LCGDefinitionBasic LCG has four public system parameters: an integer q, two constants a,b $\\in { 0,…,q-1}$ , and a positive integer $w\\leq q$ . The constant a is taken to be relatively prime to q. 【有四个公开参数：整数q，两个q剩余系下的常数a,b，（a与q互素）一个小于等于q的正整数w。】 We use $\\mathcal{S}_q$ and $\\mathcal{R}$ to denote the sets: $\\mathcal{S}_{q}:=\\{0, \\ldots, q-1\\} ; \\quad \\mathcal{R}:=\\{0, \\ldots,\\lfloor(q-1) / w\\rfloor\\}$ Now, the generators $G_{\\mathrm{lcg}}: \\mathcal{S}_{q} \\rightarrow \\mathcal{R} \\times \\mathcal{S}_{q}$ with seed $s\\in\\mathcal{S}_{q}$ defined as follows: $G_{\\operatorname{lcg}}(s):=(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ 【LCG的输出是一对数，$(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ 】 当 $w=2^t$ 时，$\\lfloor s / w\\rfloor$ simpky erases the t lease significant bits of s【向右平移t位】。 Insecure当已知 $s^{\\prime}:=a s+b \\bmod q$ ，即可直接求出s，也就求出了所谓的随机数 $\\lfloor s/w\\rfloor$ . Variant: Blum-Micali constructionDefinition 如上图所示，变体的LCG是一个迭代，输出不包括 $s_i$ ，把 $r_1,…,r_n$ 作为一次迭代的输出。 不同的应用系统使用不同的 $q,a,b,w$ 参数，在Java 8 Development Kit（JDKv8）中，$q=2^{48}$ , $w=2^{22}$ ,constant $a=\\text{0x5DEECE66D}$ , $b=\\text{0x0B}$ 。 所以在JDKv8中, LCG的输出其实是 $s_i$（48bits） 的前48-22=26 bits 。 显然JDKv8中的参数大小应用在安全系统中，还是太不安全了。 how to attack in JDKv8 在迭代的第一次输出中，LCG就 reveal 26bits of the seed s。 对于s剩下的后22个bits，attacker can easily recover them by exhausitive search(穷举)： 对于每个可能的取值，attacker都能得到一个候选seed $\\hat{s}$ 用 $\\hat{s}$ 来验证我们所直接得到的LCG的输出。 如果 $\\hat{s}$ 验证失败，则到第三步继续穷举。直至验证成功。 当穷举至正确的s时，就可以直接预测LCG的剩余输出。 在现代处理器中，穷举 $2^{22}$ (4 million) 只需要1秒。所以LCG的参数较小时，是很容易attack。 当 $q=2^{512}$ 时，这种穷举的攻击方法就失效了。但是有一种对于LCG的著名攻击方法[1]，即使每次迭代，LCG只输出较少的bits，也能从这些较少的但连续的输出序列中预测出整个LCG输出序列。 Cryptanalysis ：elegant attackWarning of MathSupposeSuppose : q is large (e.g. $q=2^{512}$ ), and $G_{lcg}^{(n)}$ outputs about half the bits of the state s per iteration. 【q很大， $G_{lcg}^{(n)}$ 每次输出s的一半左右的bits】 More precisely, suppose: $w&lt;\\sqrt{q}/c$ for fixed c（e.g. $c=32$ ） 【保证输出s前一半左右bits的这个条件】 Suppose the attacker is given two consecutive outputs of the gnerator $r_i,r_{i+1}\\in \\mathcal{R}$ . 【已知两个连续输出 $r_i,r_{i+1}\\in \\mathcal{R}$ 】 Attacker Knows $r_{i}=\\left\\lfloor s_{i} / w\\right\\rfloor \\quad \\text { and } \\quad r_{i+1}=\\left\\lfloor s_{i+1} / w\\right\\rfloor=\\left\\lfloor\\left(a s_{i}+b \\bmod q\\right) / w\\right\\rfloor$ 【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i$ 】 $r_{i} \\cdot w+e_{0}=s \\quad \\text { and } \\quad r_{i+1} \\cdot w+e_{1}=a s+b+q x \\qquad (0\\leq e_0,e_1&lt;w&lt;\\sqrt{q}/c)$ 【 去掉floor符号和mod：$e_0,e_1$ 是 $s_i,s_{i+1}$ 除 $w$ 的余数】 【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i,e_0,e_1,x$ 】 re-arranging: put $x$ and $s$ on the left $s=r_{i} \\cdot w+e_{0} \\quad \\text { and } \\quad a s+q x=r_{i+1} w-b+e_{1}$ 【把未知参数s，x放在等式左边，方便写成矩阵形式】 $s \\cdot\\left(\\begin{array}{l}1 \\ a\\end{array}\\right)+x \\cdot\\left(\\begin{array}{l}0 \\ q\\end{array}\\right)=\\boldsymbol{g}+\\boldsymbol{e} \\quad \\text { where } \\quad \\boldsymbol{g}:=\\left(\\begin{array}{c}r_{i} w \\ r_{i+1} w-b\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{e}:=\\left(\\begin{array}{c}e_{0} \\ e_{1}\\end{array}\\right)$ 【已知： $\\boldsymbol{g},a,q$ ，未知：$\\boldsymbol{e},s,x$ 】 to break the generator it suffices to find the vector $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}$ . 【令 $u\\in {Z}^2$ , $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}=s \\cdot(1, a)^{\\mathrm{T}}+x \\cdot(0, q)^{\\mathrm{T}}$ 】 【如果我们求出了 $\\boldsymbol{u}$ ，那可以用线性代数的知识解出 $s$ 和 $x$ ,再用 $s$ 来预测PRG的剩下输出】 konws $\\boldsymbol{g}$ , knows $\\boldsymbol{e}$ is shorter, and $|\\boldsymbol{e} |_{\\infty}$ is at most $\\sqrt{q}/c$ , knows that $\\boldsymbol{u}$ is “close” to $\\boldsymbol{g}$ . 【e向量很小，$|\\boldsymbol{e} |_{\\infty}$ 上界是$\\sqrt{q}/c$ ，u离g很近】 Taxicab norm or Manhattan(1-norm) ${\\|}A{\\|}_1=\\max \\{ \\sum|a_{i1}|,\\sum|a_{i2}|,...,\\sum|a_{in}| \\}$ （列和范数，A矩阵每一列元素绝对值之和的最大值） Euclidean norm(2-norm) $\\|\\mathbf{x}\\|=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}\\right)^{1 / 2}$ $\\infty$-范数 $\\|A\\|_{\\infty}=\\max \\{ \\sum|a_{1j}|,\\sum|a_{2j}|,...,\\sum|a_{mj}| \\}$ （行和范数，A矩阵每一行元素绝对值之和的最大值） attack can figure the lattice with attacking LCG. the lattice is generated by the vectors $(1,5)^T$ and $(0,29)^T$ , the attacker has a vector $\\boldsymbol{g}=(9,7)^T$ and wishes to find the closest lattice vector $\\boldsymbol{u}$ . 【上图是 $(1,5)^T$ 和 $(0,29)^T$ 两个向量生成的的格点，希望能从以上格点找到离已知 $\\boldsymbol{g}$ 向量最近的格点】 $\\mathcal{L}_a$ :由 $(1, a)^{\\mathrm{T}},(0, q)^{\\mathrm{T}}$ 作为基向量生成的点集合。 The problem is a special case of a general problem call the closest vector problem: given a lattice $\\mathcal{L}$ and a vector $\\boldsymbol{g}$ ,find a vector in $\\mathcal{L}$ that is closest to $\\mathcal{g}$ . There is an efficient polynomial time algorithm for this problem.[2] 【问题归结于 closest vector problem问题，在已知栅格点集合中找离某一向量最近的点，此问题已有多项式时间算法】 step 8 aboveLemma : * For at least $(1-16/c^2)\\cdot q $ of the a in $\\mathcal{S}_q$ , the lattice $\\mathcal{L}_a\\sub Z_2$ has the following property: for every $\\boldsymbol{g} \\in Z^2$ there is at most one vector $\\boldsymbol{u}\\in \\mathcal{L}_a$ such that $\\|\\boldsymbol{g}-\\boldsymbol{u}\\|_{\\infty}","link":"/2020/03/15/StreamCipher1/"},{"title":"「Cryptography-Dan」：Introduction","text":"本系列是学习Dan Boneh教授的Online Cryptography Course。 这是Dan教授的第一讲：对密码学的一些Introduction。 What is cryptography?Crypto core：安全通信 Secret key establishment (密钥的建立)： Alice 和 Bob 会得到一个shared secret key，而且Alice 知道她是在和Bob通信，Bob也知道他是在和Alice通信。而attacker不能从通信中获取key。 Secure communicati （安全通信）： 在通信中，Alice、Bob用k将信息加密，保证了通信的confidentiality（机密性）；同时attacker也无法篡改通信的信息，保证了通信的integrity（完整性）。 Crypto can do much more密码学除了能保证安全通信，密码学还能做很多其他的事。 Digital signature &amp; Anonymous Digital signatures（数字签名）： 现实中，人们对不同的文档进行签名，虽然是不同的文档，但是签名的字是相同的。 如果这应用在网络的文档签名中，这将是很危险的。攻击者只需要将签名进行复制、粘贴，就可以将你的签名签在你并不想签的文档中。 数字签名的主要思想：数字签名其实是代签内容的函数值，所以如果攻击者只是复制数字签名（原签名的函数值），那么攻击者得到的数字签名也是无效的（函数值不同）。 在后面的课程系列中会详细讲这部分的内容。[1] Anonymous communication（匿名通信）： 匿名通信的实现，有Mix network （wiki详细介绍）协议，这是一种路由协议，通过使用混合的代理服务器链来实现难以追踪的通信。 通过这些代理的不断加密解密可以实现： Bob不知道与之通信的是Alice。 代理也不知道是Alice和Bob在通信。 双向通信：虽然Bob不知与之通信的是Alice，但也能respond。 Anonymous digital cash（匿名数字现金）： 现实中，我们可以去超市花掉一元钱，而超市不知道我是谁。 在网络中，如果Alice想去网上商店花掉数字现金一元钱，网上商店可以不知道是谁花掉的这一元钱吗？ 这就是匿名数字现金需要解决的问题： 可以在匿名的情况下花掉数字现金吗？ 如果可以，当Alice将这一元钱复制多次（数字现金都是数据串），得到了三元钱，再去把它花掉，由于匿名的原因，没人知道是谁花掉的这三元钱，商店找不到责任人。 这是匿名数字现金需要解决的第二个问题： 如何防止 double spending情况的发生？ 可以用这样的机制去实现匿名数字现金：当Alice花费这一块 once时，系统保证Alice的匿名性；但当Alice花费这一块 more than once ,系统立即揭露Alice的全部信息。 Protocols在介绍什么是Protocols之前，先介绍两种应用场景。 Elections 有5个人要进行投票选举0和1号候选人，但是需要保证：每个人除了知道自己的投票结果，互相不知道其他人的投票情况。在这种情况下怎么知道最后的winner是谁吗？ 如上图，可以引入一个第三方——election center，第三方验证每一个人只能投一次，最后统计票数决策出最后的winner。 Private auctions 介绍一种拍卖机制，Vickery auction：对一个拍卖品，每个投标者在不知道其他人投标价格的情况下进行投标，最后的acution winner： highest bidder &amp; pays 2nd highers bid。即是标价最高者得标，但他只需要付第二高的标价。 所以public知道的信息只有：中标者和第二高投标者的标价。 需要实现这种机制，也可以引入一个第三方——auction center。 但是引入第三方真的安全吗？安全第三方也不安全。 再看上面那个Election的例子，如果把上面四个人的投票情况作为输入，第三方的任务其实是输出一个函数 $f(x_1,x_2,x_3,x_4)$ 而不公开其他信息。 因为安全第三方也许并不安全，所以如果去掉第三方，上面四个人遵从某种协议，相互通信，最后能否得出这个 $f(x_1,x_2,x_3,x_4)$ 这个结果函数，而不透露其投票信息？ 答案是 “Yes”。 有一个惊人的定理：任何能通过第三方做到的事，也能不通过第三方做到。 Thm: anythong that can done with trusted auth. can also be done without. 怎么做到？答案是 Secure multi-party computation（安全多方计算）。 挖坑博文：姚氏百万富翁问题[2] Crypto magic Privately outsourcing computation (安全外包计算) Alice想要在Google服务器查询信息，为了不让别人知道她查询的是什么，她把search query进行加密。 Google服务器接收到加密的查询请求，虽然Google不知道她实际想查询什么信息，但是服务器能根据E[query]返回E[results]。 最后Alice将收到的E[results]解密，得到真正的results。 这就是安全外包计算的简单过程：Encryption、Search、Decryption。 Zero knowl（proof of knowledge) (零知识证明)： Alice 知道p、q(两个1000位的质数)相乘等于N。 Bob只知道N的值，不知道具体的p、q值。 Alice 给 Bob说她能够分解数N，但她不用告诉Bob N的具体因子是什么，只需要证明我能分解N，证明这是我的知识。 最后Bob知道Alice能够分解N，但他不知道怎么分解（也就是不知道N的因子到底是什么）。 A rigorous science在密码学的研究中，通常是这样的步骤： Precisely specify threat model. 准确描述其威胁模型或为达到的目的。比如签名的目的：unforgeable（不可伪造）。 Propose a construction. Prove that breaking construction under threat mode will solve an underlying hard problem. 证明攻击者攻击这个系统必须解决一个很难的问题（大整数分解问题之类的NP问题）。 这样也就证明了这个系统是安全的。 HistorySubstitution cipher（替换）what is it 替换密码很好理解，如上图的这种替换表（key）。 比较historic的替换密码——Caesar Cipher（凯撒密码），凯撒密码是一种替换规则：向后移三位，因此也可以说凯撒密码没有key。 the size of key space用$\\mathcal{K}$ （花体的K）来表示密钥空间。 英语字母的替换密码，易得密钥空间的大小是 $|\\mathcal{K}|=26!\\approx2^{88}$ （即26个字母的全排列）。 这是一个就现在而言也就比较perfect的密钥空间。 但替换密码也很容易被破解。 how to break it问：英语文本中最commom的字母是什么？ 答：“E” 在英语文本（大量）中，每个字母出现的频率并不是均匀分布，我们可以利用一些最common的字母和字母组合来破解替换密码。 Use frequency of English letters. Dan教授统计了标准文献中字母频率： “e”: 12.7% , “t”: 9.1% , “a” : 8.1%. 统计密文中（大量）出现频率最高、次高、第三高的字母，他们的明文也就是e、t、a。 Use frequency of pairs of letters (diagrams).（二合字母） 频率出现较高的二合字母：”he”, “an”, “in” , “th” 也能将h, n,i等破解出。 trigrams（继续使用三合字母） ……直至全部破解 因此substitution cipher是CT only attack！（惟密文攻击：仅凭密文就可以还原出原文） Vigener cipherEncryption 加密过程如上图所示： 密钥是 “CRYPTO”, 长度为6，将密钥重复书写直至覆盖整个明文长度。 将密钥的字母和对应的明文相加模26，得到密文。 Decryption解密只需要将密文减去密钥字母，再模26即可。 How to break it破解方法和替换密码类似，思想也是使用字母频率来破解。 这里分两种情况讨论： 第一种：已知密钥长度 破解过程： 将密文按照密钥长度分组，按照图中的话，6个一组。 统计每组的的第一个位置的字母出现频率。 假设密文中第一个位置最common的是”H” 密钥的第一个字母是：”H”-“E”=”C” 统计剩下位置的字母频率，直至完全破解密钥。 第二种：未知密钥长度 未知密钥长度，只需要依次假设密钥长度是1、2、3…，再按照第一种情况破解，直至破解情况合理。 Rotor MachinesRotor: 轴轮。 所以这种密码的加密核心是：输入明文字母，轴轮旋转一定角度，映射为另一字母。 single rotor 早期的是单轴轮，rotor machine的密钥其实是图右中间那个圆圆的可以旋转的柱子。 图左是变动的密钥映射表。 变动过程： 第一次输入A，密文是K。 轴轮旋转一个字母位：看图中E，从最下到最上（一个圈，只相隔一位）。 所以第二次再输入A，密文是E。 …… Most famous ：the Enigma Enigma machine是二战时期纳粹德国使用的加密机器，因此完全破解了Enigma是盟军提前胜利的关键。 左图中可以看出Enigma机器中是有4个轴轮，每个轴轮都有自己的旋转字母位大小，因此密钥空间大小是 $|\\mathcal{K}|=26^4\\approx2^{18}$ (在plugboard中，实际是 $2^{36}$)。 密钥空间很小，放在现在很容易被暴力破解。 plugboard 允许操作员重新配置可变接线，实现两个字母的交换。plugboard比额外的rotor提供了更多的加密强度。 对于Enigma machine的更多的具体介绍可以戳Enigma machine 的wiki链接。 Data Encryption StandardDES：#keys = $2^{56}$ ,block siez = 64bits，一次可以加密8个字母。 Today：AES（2001）、Salsa20（2008）…… 这里只是简单介绍。 Discrete Probability这个segment比较简单，概率论基本完全cover了，这里只讲一些重点。 Randomized algorithms随机算法有两种，一种是Deterministic algorithm（也就是伪随机），另一种是Randomized algorithm。 Deterministic algorithm $ y\\longleftarrow A(m)$ ，这是一个确定的函数，输入映射到唯一输出。 Randomized algorithm $y\\longleftarrow A(m ; r) \\quad \\text { where } r \\stackrel{R}{\\longleftarrow}{0,1}^{n}$ output： $y \\stackrel{R}{\\longleftarrow} A(m)$ ，y is a random variable. $ r \\stackrel{R}\\longleftarrow { 0,1 }^n $ :意思是r是n位01序列中的任意一个取值。R，random。变量r服从在 ${0,1}^n$ 取值的均匀分布。 由于随机变量r，对于给定m，$A(m;r)$ 是 ${0,1}^n$ 中的一个子集。 所以，对m的加密结果y，也是一个的随机变量，而且，y在 $A(m,r)$ 也是服从均匀分布。 因此，由于r的影响，对于给定m，加密结果不会映射到同一个值。（如上图所示） XORXOR有两种理解：（ $x \\oplus y $ ） 一种是：x,y的bit位相比较，相同则为0，相异为1. 另一种是：x,y的bit位相加 mod2. 异或在密码学中被频繁使用，主要是因为异或有一个重要的性质。 异或的重要性质：有两个在 ${0,1}^n$ （n位01串）取值的随机变量X、Y。X、Y相互独立，X服从任意某种分布，随机变量Y服从均匀分布。那么 $Z=Y\\oplus X$ ，Z在 ${0,1}^n$ 取值，且Z服从均分分布。 Thm: Y a rand. var. over ${0,1}^n$ , X an index. uniform var. on ${0,1}^n$ ​ Then Z := Y $\\oplus$ X is uniform var. on ${0,1}^n$ . Proof： 当n=1 画出联合分布 Pr[ Z=0 ]=Pr[ (x,y)=(0,0)] + Pr[(x,y)=(1,1)]=1/2 每一bit位都服从均匀分布，可以容易得出 Z是服从难过均匀分布。 The birthday paradox（生日悖论）更具体的分析见 Birthday problem 。 问题前提：一个人的生日在365天的任意一天是均匀分布的（实际当然不是，貌似更多集中在9月）。 根据信鸽理论（有N个鸽子，M个隔间，如果N&gt;M，那么一定有一个隔间有两只鸽子），所以367个人中，以100%的概率有两个人的生日相同。但是，当只有70个人时，就有99.9%的概率，其中两人生日相同；当只有23人，这个概率可以达到50%。 其实这并不是一个悖论，只是直觉误导，理性和感性认识的矛盾。当只有一个人，概率为0，当有367人时，为100%，所以我们直觉认为，这是线性增长的，其实不然。 概率论知识： 设事件A：23个人中，有两个人生日相等。 $P\\left(A^{\\prime}\\right)=\\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\frac{362}{365} \\times \\cdots \\times \\frac{343}{365}$ $P\\left(A^{\\prime}\\right)=\\left(\\frac{1}{365}\\right)^{23} \\times(365 \\times 364 \\times 363 \\times \\cdots \\times 343)$ $P\\left(A^{\\prime}\\right) \\approx 0.492703$ $P(A) \\approx 1-0.492703=0.507297 \\quad(50.7297 \\%)$ 推广到一般情况，n个人(n","link":"/2020/03/04/Dan-Intro/"},{"title":"「机器学习-李宏毅」：Gradient","text":"总结「李宏毅老师-机器学习」的Gradient，主要从以下三个方面展开：调节learning rate；加快训练速度；对数据进行Feature Scaling。 阅读完三个tips，具体实现可demo Tip 1: Tuning your learning rates carefullyVisualize 损失函数随着参数变化的函数图 左图是Loss Function的函数图，红色是最好的Step，当Step过小（蓝色），会花费很多时间，当Step过大（绿色、黄色），会发现Loss越来越大，找不到最低点。 所以在Training中，尽可能的visualize loss值的变化。 但是当参数大于等于三个时， $loss function$的函数图就不能visualize了。 因此，在右图中，visualize Loss随着参数更新的变化，横轴即迭代次数，当图像呈现蓝色（small）时，就可以把learning rate 调大一些。 Adaptive Learning Rates(Adagrad)但是手动调节 $\\eta$是低效的，我们更希望能自动地调节。 直观上的原则是： $\\eta$ 的大小应该随着迭代次数的增加而变小。 最开始，初始点离minima很远，那step应该大一些，所以learning rate也应该大一些。 随着迭代次数的增加，离minima越来越近，就应该减小 learning rate。 E.g. 1/t decay： $\\eta^t=\\eta/ \\sqrt{t+1}$ 不同参数的 $\\eta$应该不同（cannot be one-size-fits-all)。 AdagradAdagrad 的主要思想是：Divide the learning rate of each parameter by the root mean squear of its previous derivatives.(通过除这个参数的 计算出的所有导数 的均方根) root mean squar : $ \\sqrt{\\frac{1}{n}(x_1^2+x_2^2+...+x_n^2)} $ Vanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $w^{t+1} \\leftarrow w^{t}-\\frac{\\eta^{t}}{\\sigma^{t}} g^{t} $ $\\eta^t$：第t次迭代的leaning rate $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}}$ $g^{t}=\\frac{\\partial L\\left(\\theta^{t}\\right)}{\\partial w} $ $\\sigma^t$：root mean squar of previous derivatives of w $\\tau^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}} $ 对比上面两种Adaptive Gradient，Adagrade的优势是learning rate 是和parameter dependent（参数相关的）。 Adagrad步骤简化步骤： 简化公式： $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ ( $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}} $ , $ \\sigma^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}$ ,约掉共同项即可) Adagrad Contradiction? ——Adagrad原理解释Vanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 在Vanilla Gradient descent中， $g^t$越大，也就是当前梯度大，也就有更大的step。 而在Adagrad中，当 $g^t$越大，有更大的step,而当 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 越大，反而有更小step。 Contradiction？ 「Intuitive Reason（直观上解释）」 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 是为了造成反差的效果。 类比一下，如果一个一直很凶的人，突然温柔了一些，你会觉得他特别温柔。所以同样是 $0.1$,第一行中，你会觉得特别大，第二行中，你会觉得特别小。 因此 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 这一项的存在就能体现 $g^t$的变化有多surprise。 「数学一些的解释」1. Larger Gradient,larger steps?在前面我们都深信不疑这一点，但这样的描述真的是正确的吗？ 在这张图中，只有一个参数，认为当该点的导数越大，离minima越远，这样看来，Larger Gradient,larger steps是正确的。 在上图中的 $x_0$点，该点迭代更新的best step 应该正比于 $|x_0+\\frac{b}{2a}|$ ，即 $\\frac{|2,a, x_0+b|}{2a}$。 而 $\\frac{|2,a, x_0+b|}{2a}$的分子也就是该点的一阶导数的绝对值。 上图中，有 $w_1,w_2$两个参数。 横着用蓝色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较a、b两点，a点导数大，离minima远。 竖着用绿色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较c、d两点，c点导数大，离minima远。 但是，如果比较a、c两点呢？ a点对 $w_1$ 的偏导数和c点对 $w_2$的偏导数比较？ 比较出来，c点点偏导数更大，离minima更远吗？ 再看左图的图像，横着的弧度更平滑，竖着的弧度更尖一些，直观上看应该c点离minima更近一些。 所以Larger Gradient,larger steps点比较方法不能（cross parameters)跨参数比较。 所以最好的step $\\propto$ 一阶导数（Do not cross parameters)。 2.** Second Derivative** 前面讨论best step $\\frac{|2,a, x_0+b|}{2a}$的分子是该点一阶导数，那么其分母呢？ 当对一阶导数再求导时，可以发现其二阶导数就是best step的分母。 得出结论：the best step $\\propto$ |First dertivative| / Second derivative。 因此，再来看两个参数的情况，比较a点和c点，a点的一阶导数更小，二阶导数也更小；c点点一阶导数更大，二阶导数也更大。 所以如果要比较a、c两点，谁离minima更远，应该比较其一阶导数的绝对值除以其二阶导数的大小。 回到 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 上一部分得出的结论是：the best step $\\propto$ |First dertivative| / Second derivative。 所以我们的learning rate 也应该和 |First dertivative| / Second derivative相关。 $g^t$也就是一阶导数，但为什么 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 能代表二阶导数呢？ 上图中，蓝色的函数图有更小的二阶导数，绿色的函数图有更大的二阶导数。 在复杂函数中，求二阶导数是一个很复杂的计算。 所以我们想用一阶导数来反映二阶导数的大小。 在一阶导数的函数图中，认为一阶导数值更小的，二阶导数也更小，但是取一个点显然是片面的，所以考虑取多个点。 也就是用 $ \\sqrt{\\text{(first derivative)}^2}$ 来代表best step中的二阶导数。 总结一下Adagrad的为了找寻最好的learning rate，从找寻best step下手，用简单的二次函数为例，得出 best step $\\propto$ |First dertivative| / Second derivative。 但是复杂函数的二阶导数是难计算的，因此考虑用多个点的一阶导数来反映其二阶导数。 得出 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 。 直观来解释公式中的一阶导数的root mean square，即来为该次迭代的一阶导数造成反差效果。 其他文献中的Adaptive Gradient理应都是为了调节learning rate使之有best step。(待补充的其他Gradient)[1] Tip 2:Stochastic Gradient DescentStochastic Gradient Descent在linear model中，我们这样计算Loss function： $L=\\sum_{n}\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ 每求一次Loss function，L都对所有training examples的 $\\text{error}^2$求和，因此每一次的loss function的计算，都是一重循环。 在Stochastic Gradient Descent中，每一次求loss function，只取一个example $x^n$，减少一重循环，无疑更快。 Stochastic Gradient Descent Pick an example $x^n$ $L=\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ 上图中，传统的Gradient Descent看完一次所有的examples，离minima还很远；而Stochastic Gradient Descent ，看完一次，已经离minima较近了。 Tip 3:Feature ScalingWhat is Feature Scaling 如上图所示，希望能让不同的feature能有相同的scale（定义域/规模） Why Feature Scaling假设model都是 $y = b+ w_1 x_1 +w_2 x_2$。 上图中，左边 $x_2$的规模更大，可以认为 $x_1$ 对loss 的影响更小， $ x_2$对loss的影响更大。 即当 $w_1,w_2$轻微扰动时，同时加上相同的 $\\Delta w$时，$x_2$ 使 $y$的取值更大，那么对loss 的影响也更大。 如图中下方的函数图 $w_1$方向的L更平滑， $w_2$ 方向更陡峭些，Gradient descent的步骤如图所示。 但当对 $x_2$进行feature scaling后，图像会更像正圆，Gradient descent使，参数更新向着圆心走，更新会更有效率。 How Feature Scaling概率论知识：标准化。 概率论： 随机变量 $X$ 的期望和方差均存在，且 $ D(X)>0$,令 $X^*=\\frac{X-E(X)}{\\sqrt{D(X)}}$ 那么 $E(X^*)=0,D(X)=1 $ , $ X^* $ 称为X的标准化随机变量。 对所有向量的每一维度，进行标准化处理： $x_{i}^{r} \\leftarrow \\frac{x_{i}^{r}-m_{i}}{\\sigma_{i}} $ （ $m_i$是该维度变量的均值， $\\sigma_i$ 是该维度变量的方差） 标准化后，每一个feature的期望都是0，方差都是1。 Gradient Descent Theory(公式推导)当用Gradient Descent解决 $\\theta^*=\\arg \\min_\\theta L(\\theta)$时，我们希望每次更新 $\\theta $ 都能得到 $L(\\theta^0)&gt;L(\\theta^1)&gt;L(\\theta^2)&gt;…$ 这样的理论结果，但是不总能得到这样的结果。 上图中，我们虽然不能一下知道minima的方向，但是我们希望：当给一个点 $\\theta^0$ 时，我们能很容易的知道他附近（极小的附近）的最小的loss 是哪个方向。 所以怎么做呢？ Tylor Series微积分知识：Taylor Series（泰勒公式）。 Tylor Series:函数 $h(x)$ 在 $x_0$ 无限可导，那么 $\\begin{aligned} \\mathrm{h}(\\mathrm{x}) &=\\sum_{k=0}^{\\infty} \\frac{\\mathrm{h}^{(k)}\\left(x_{0}\\right)}{k !}\\left(x-x_{0}\\right)^{k} \\\\ &=h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right)+\\frac{h^{\\prime \\prime}\\left(x_{0}\\right)}{2 !}\\left(x-x_{0}\\right)^{2}+\\ldots \\end{aligned}$ 当 x 无限接近 $x_0$ 时，忽略后面无穷小的高次项， $h(x) \\approx h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right) $ 上图中，用 $\\pi/4$ 处的一阶泰勒展示来表达 $\\sin(x)$ ,图像是直线，和 $\\sin(x)$ 图像相差很大，但当 x无限接近 $\\pi/4$ 是，函数值估算很好。 Multivariable Taylor Series $h(x, y)=h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right) +\\text{something raleted to} (x-x_x^0)^2 \\text{and} (y-y_0)^2+…$ 当 $(x,y)$ 接近 $(x_0,y_0)$ 时， $h(x,y)$ 用 $(x_0,y_0)$ 处的一阶泰勒展开式估计。 $ h(x, y) \\approx h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right)$ Back to Formal Derivation 当图中的红色圆圈足够小时，红色圆圈中的loss 值就可以用 $(a,b)$ 处的一阶泰勒展开式来表示。 $ \\mathrm{L}(\\theta) \\approx \\mathrm{L}(a, b)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}\\left(\\theta_{1}-a\\right)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}}\\left(\\theta_{2}-b\\right) $ $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$ ,d 足够小。 用 $s=L(a,b)$ , $ u=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}, v=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} $ 表示。 最后问题变成： $L(\\theta)\\approx s+u(\\theta_1-a)+v(\\theta_2-b)$ 找 $(\\theta_1,\\theta_2)$，且满足 $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$，使 $L(\\theta)$ 最小。 变成了一个简单的最优化问题。 令 $\\Delta \\theta_1=\\theta_1-a$ , $\\Delta\\theta_2=\\theta_2-b$ 问题简化为： $\\text{min}:u \\Delta \\theta_1+v\\Delta\\theta_2$ $\\text{subject to}:{\\Delta\\theta_1}^2+{\\Delta\\theta_2}^2\\leq d^2$ 画出图，就是初中数学了。更新的方向应该是 $(u,v)$ 向量反向的方向。 所以： $\\left[\\begin{array}{l} \\Delta \\theta_{1} \\\\ \\Delta \\theta_{2} \\end{array}\\right]=-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $\\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $ \\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}} \\\\ \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} \\end{array}\\right] $ Limitation of Gradient Descent Gradient Descent 可能会卡在local minima或者saddle point（鞍点：一个方向是极大值，一个方向是极小值，导数为0） 实践中，我们往往会在导数无穷接近0的时候停下来（&lt; 1e-7)，Gradient Descent 可能会停在plateau(高原；增长后的稳定) Reference[1] 待补充的其他Gradient","link":"/2020/03/01/Gradient/"},{"title":"「机器学习-李宏毅」：Regression","text":"在YouTube上看台大李宏毅老师的课，看完Regression讲座的感受就是： 好想去抓Pokemon！！！ 这篇文章将总结李宏毅老师Regression的讲座，并尝试实现其demo。 Regression（回归）DefineRegression：是找到一个$function$，进行预测。对输入的feature，输出一个$Scalar$(数值，标量)。 Example Application Look for a $function$ Stock Market Forecast（股票预测） $input$：过去的股价信息 $output$：明天的股价平均值（$Scalar$) Self-Driving Car(自动驾驶) $input$：路况信息 $output$：方向盘角度（$Scalar$) Recommendation（推荐系统） $input$：使用者A、商品B $output$：使用者A购买商品B的可能性 可见，$input$都是一些特征信息，$output$都是一个标量数值，这就是Regression。 Regression Case: Pokenmon 看完这节课，感想：好想去抓宝可梦QAQ 预测一个pokemon进化后的CP（Combat Power，战斗力）值。 为什么要预测呐？ 如果进化后的CP值高，就进化他，不然就把他当糖果，因为宝可梦很难抓的。（？没玩过，我也不懂o r z） 上图妙蛙种子的信息(可能的$input$)： $x_{cp}$：CP值 $x_s$:物种 $x_{hp}$:生命值 $x_w$:重量 $x_h$:高度 output：进化后的CP值。 $x_{cp}$：用下标表示一个object的component。 $x^1$：用上标表示一个完整的object。 Step 1: 找一个Model（function set）Model ：$y = b + w \\cdot x_{cp}$ 假设用上式作为我们的Model，那么这些函数： $ \\begin{aligned} &\\mathrm{f}_{1}: \\mathrm{y}=10.0+9.0 \\cdot \\mathrm{x}_{\\mathrm{cp}}\\\\ &f_{2}: y=9.8+9.2 \\cdot x_{c p}\\\\ &f_{3}: y=-0.8-1.2 \\cdot x_{c p} \\end{aligned} $ 等都属于这个集合，但是显然像$f_3$这种函数是bad，CP值不可能是负数。bad functions 很多，所以在下面的步骤，会说明如何判别一个函数的好坏，自动的选出最好的那个 $function$。 把Model 1一般化，得到线代中的 Linear Model：$y = b+\\sum w_ix_i$ $x_i$：x的feature $b$：bias,偏置值 $w_i$：weight，权重 Step 2: 判别Goodness of Function(Training Data)Training Data假定使用Model ：$y = b + w \\cdot x_{cp}$ Training Data：十只宝可梦，用向量的形式表示。 使用Training data来judge the goodness of function.。 Loss Function(损失函数)概率论：做线性回归，一般使用最小二乘法。一般回归，大多使用极大似然估计。 Loss function $L$ ：$L(f)=L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ 其中的 $\\hat{y}^n-(b+w\\cdot x_{cp}^n)$是Estimation error(估测误差) Loss Function的意义：它的 $input$是一个 $function$，它的 $output$体现了how bad it is,这个函数有多糟/好。 Figure the Result 上图横纵坐标是函数 $L$的参数 $w 、b$，图中的每一个point都是一个 $function $。 color：体现函数的输出，越红越大，说明选择的函数越bad。 所以我们要选择紫色区域结果最小的函数。 而这个得到best function的过程是可以通过无数次迭代实现的。（重复的迭代当时是交给计算机做了） Step 3:迭代找出Best Function$L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ 找到Best Function: $f^{*}=\\arg \\min _{f} L(f)$ 也就是找到参数 $w^{*},b^{*}=\\arg \\min_{w,b} L(w,b)=\\arg \\min_{w,b}\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ arg ：argument,变元 arg min：使之最小的变元 arg max：使之最大的变元 据悉，线性回归的参数可以用线性代数的知识，解出closed-form solution（解析解），我先挖个坑QAQ，以后来填这块知识。[1] 在机器学习中，只要$L$函数可微分， 即可用Gradient Descent（梯度下降）的方法来求解。 Gradient Decent（梯度下降）和概率论中的梯度下降估计参数的原理相同，只是计算机不能直接解出方程的解，所以计算机的方法是迭代。 考虑一个参数w*$w^*=\\arg \\min_w L(w)$ 步骤： 随机选取一个初始值 $w^0$ 计算 $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp; &nbsp; $\\begin{equation} w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{0}} \\end{equation}$ 计算 $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp;&nbsp; $\\begin{equation} w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{1}} \\end{equation}$ …until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$ 上图迭代过程的几点说明 $\\begin{equation}\\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}}\\end{equation}$的正负 如果是negative，也就是该点切线斜率是负的，那应该Increse w，以找到最低点。 Negative $\\rightarrow$ Increase w Positive $\\rightarrow$ Decrease w $-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{i}}$：步长 $\\eta$：learning rate（学习速度），事先设好的值。 $-$(负号)：如果 $\\begin{equation}\\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}}\\end{equation}$是负的，应该增加w。 Local optimal：局部最优和全局最优 如果是以上图像，则得到的w不是全局最优。 但线性回归的损失函数是凸函数，存在一个全局最优，没有局部最优。 考虑多个参数 $w^{*},b^{*}$ 微积分知识：gradient（梯度，向量)： $\\nabla L=\\left[\\begin{array}{l}\\frac{\\partial L}{\\partial w} \\\\frac{\\partial L}{\\partial b}\\end{array}\\right]$ 考虑多个参数和考虑一个参数思路相同，每次迭代，迭代两个参数。 $w^{*}, b^{*}=\\arg \\min _{w, b} L(w, b)$ 步骤： 随机选取初值 $w^0,b^0$ 计算 $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ &nbsp; &nbsp; $w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0}} \\quad b^{1} \\leftarrow b^{0}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ 计算 $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ &nbsp; &nbsp; &nbsp; $w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1}} \\quad b^{2} \\leftarrow b^{1}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ …until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$, $ \\frac{{\\rm d}L}{{\\rm d}b}|_{b=b^n}=0$ 上图，坐标为 $L(w,b)$函数的参数，Color代表 $L$的大小，越紫值越小。 每一个点都是一个 $function$，沿着梯度方向（图中法线方向）迭代，找到全局最优点。 再次说明：线性回归中，损失函数是convex（凸函数），没有局部最优解。 $\\frac{\\partial L}{\\partial w}$和 $\\frac{\\partial L}{\\partial b}$的公式推导$L(w, b)=\\sum_{n=1}^{10}\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)^{2}$ 微积分的知识，显然。 数学真香。———我自己 $\\frac{\\partial L}{\\partial w}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)（-x_{cp}^n)$ $\\frac{\\partial L}{\\partial b}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)(-1)$ 实际结果分析Training Data Training Data的Error=31.9，但我们真正关心的是Testing Data的error。 Testing Data 是new Data：另外的Pokemon！。 Testing DataModel 1： $y = b+w\\cdot x_{cp}$ error = 35,比Training Data error更大。 Model 2：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2$ Testing error=18.4，比Model 1 好。 Model 3：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3$ Testing error=18.1，比Model 2好。 Model 4:$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4$ Testing error =28.8,比Model3更差。 Model 5：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4+w_5\\cdot (x_{cp})^5$ Testing error = 232.1,爆炸了一样的差。 Overfiting（过拟合了）从上面5个Model中可以得出，越复杂的函数模型，在Testing data上不一定能得到更好的结果。（过拟合使Training data 的误差越来越小） 所以在选择Model时，需要选择合适的Model。 对模型进行改进如果收集更多的Training Data，可以发现他好像不是一个Linear Model。 Back to step 1:Redesigh the Model从上面那张图，感觉他不是一个Linear Model,而是需要if 是伊布，模型是…，if 是…,可见是和物种有关系。 （很抱歉，我只认识右上角时伊布，QAQ，我也说不出名字） 但用 $\\delta$(微积分学的狄拉克函数)表示条件语句，可以发现，他仍然是一个线性模型。 $\\delta(x_s= \\text{Pidgey)}\\left\\{\\begin{array}{ll}=1 & \\text { If } x_{s}=\\text { Pidgey } \\\\ =0 & \\text { otherwise }\\end{array}\\right.$ $y = b_1\\cdot \\delta_1+w_1\\cdot \\delta_1+b2\\cdot \\delta_2+w_2\\cdot \\delta_2+…$是一个linear model。 拟合出来，Training Data 和Testing Data的error都蛮小的。 如果想让拟合误差更小，还可以考虑其他的feature，重量、高度、HP等。 但同样的，如果函数过于复杂，也会出现Overfitting的情况。 Back to Step 2:Regularization（正则化）对于Linear Model :$y = b+\\sum w_i x_i$ 为什么要正则化？我们希望得到的函数是较平滑的，这样测试时，函数的输出对输入的noise不sensitive，即输入x的细微扰动，并不太会影响输出的结果。 所以当参数越接近0，函数越平滑。因此在原本的loss function后加入 $\\lambda \\sum(w_i)^2$项（ $\\lambda$需手调），可以保证函数较平滑。 正则化： $L = \\sum_n(\\hat{y}^n-(b+\\sum w_i x_i))^2 + \\lambda\\sum(w_i)^2$ $\\lambda $大小的选择 可以得出结论： $\\lambda $越大，Training Error变大了。 当 $\\lambda$更大，损失函数更考虑w参数的取值，更关心函数的平滑程度，而更少的关心拟合的error。 $\\lambda $越大，Testing Error变小了，当 $\\lambda$过大时，又变大。 $\\lambda $较小时，$\\lambda $增大，函数更平滑，能良好适应数据的扰动。 $\\lambda $较大时，函数过于平滑，宛如直线，这显然不能准确预测。 因此，在调节$\\lambda $大小时，也要适当选择。 正则化的一个注意点在regularization中，我们只考虑了w参数，没有考虑bias偏置值参数。 因为正则化是寻找较平滑拟合，而偏置参数只是让函数平移，与平滑无关。 Again：Regularization不考虑bias Fllowing Gradient descent[2] Overfitting and regularization[3] Validation[4] 由于博主也是在学习阶段，学习后，会po上下面内容的链接。 希望能在学习、写博客的过程中，锻炼自己的表达能力，尽量让文风言简意赅又科学严谨。 写博客是为了记录与分享，感谢指正。 Reference[1] “周志华西瓜书p55,待补充” [2] [3] [4]","link":"/2020/02/29/Regression/"},{"title":"「Python」：Module & Method","text":"长期记录帖：关于遇到过的那些Python 的Packets &amp; Module &amp; Method &amp; Attribute。中英记录。 Pythonprint print(*objects, sep=’ ‘, end=’\\n’, file=sys.stdout) numpynumpy.zeros numpy.zeros(shape, dtype=float, order=’C’) Return a new array of given shape and type, filled with zeros. parameters: shape: int or truple of ints. e.g.,(2,3) or 2 dtype: data-type, optional.(Defaults is numpy.float64) oder: optional Returns: out ndarray numpy.arange numpy.arange([start, ]stop, [step, ]dtype=None) Return evenly spaced values within a given interval. parameters: start: number, optional. (Defaults is 0) stop :number. [start,stop) step :number, optional(Defaults is 1) dtype : Returns :ndarray differ with built-in range function numpy.arange returnan ndarray rathan than a list. numpy.arange’s step can be float. numpy.meshgrid numpy.meshgrid(x, y) 生成用x向量为行，y向量为列的矩阵（坐标系） 返回 X矩阵和Y矩阵 X矩阵：网格上所有点的x值 Y矩阵：网格上所有点的y值 e.g., X, Y = np.meshgrid(x, y) 【X,Y 都是网格点坐标矩阵】 numpy.genfromtext numpy.genfromtxt (fname, delimiter=None) Load data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments characters are discarded. Parameters: fname :file, str, list of str, generator. dtype :dtype, optional. delimiter :str, int, or sequence, optional. (default = whitespace) The strin used to separate values. Python的列表读取处理数据很慢，numpy.genfromtext就很棒。 numpy.isnan numpy.isnan(x) Test element-wise for NaN(Not a number) and return result as a boolean array. Parameters: x :array_like Returns: y:ndarray or bool. True where x is NaN, false otherwise. numpy.empty nmpy.empty(shape, dtype=float, order=’C’) Return a new arry of given shape and type, without initializing entries. Parameters: shape :int or tuple of int dtype :data-type,optional Default is numpy.float64. Returns: out: ndarray numpy.reshape numpy.reshape(a, newshape, order=’C’) Gives a new shape to an array without changing its data.【改变张量的shape，不改变张量的数据】 Parameters: a : array-like newshape : int or tuple of ints One shape dimension can be -1. The value is inferred from the length of the array and remaning dimensions. Returns: reshaped_array:ndarray numpy.mean numpy.mean(a, axis=None) Compute the arithmetic mean along the specifiied axis.(the average of the array elements) Parameters: a : array_like axis ：None or int or tuple of ints, optional Axis or axes along which the means are computed. axis=0 ：沿行的垂直往下（列） axis=1 ：沿列的方向水平向右（行） numpy.std numpy.std(a, axis=None,) Compute the standard deviation along the specified axis.【标准差】 Parameters: a :array_like axis :Axis or axes along which the means are computed. numpy.shape attribute Tuple of array dimensions. numpy.concatenate numpy.concatenate((a1, a2, …), axis=0) Join a sequence of arrays along an existing axis. Parameters: a1, a2, … :sequence of array_like The arrays must have the same shape, excepting in the dimension corresponding to axis.【除了axia方向，其他维度的shape要相同】 If axis is None, arrays are flattened before use.【值为None，就先将向量变成一维的】 Default=0 numpy.ndarray.astype method numpy.ndarray.astype(dtype) Copy of the array cast to a specified type.【强制转换数据类型】 Parameters: dtype : str or dtype numpy.ones numpy.ones(shape, dtype=None) Return a new array of given shape and type, filled with ones. Parameters: shape : int or sequence of ints. dtype : data-type, optional numpy ndarray 运算 [[1]]*3 = [[1],[1],[1]] A * B 元素相乘 numpy.dot(A, B) 矩阵相乘 numpy.dot numpy.dot(a,b) numpy.power numpy.power(x1, x2) First array elements raised to powers from second array. Parameters: x1 :array_like . The bases. x2 :array_like The exponents. numpy.sum numpy.sum(a, axis=None, dtype=None) Sum of arrays elements over a given axis. Parameters: a :array_like Elements to sum. axis :None or int or tuple of ints, optional Axis or axes along which a sum is perfomed. The default, None, will sum all of the elementsof the input array. numpy.transpose numpy.transpose(a, axes=None) Permute the dimensions of the array.【tensor的维度换位】 Parameters: a : array_like axes : list of ints, optinal Default, reverse the dimensions. Otherwise permute the axes according to the values given. Returns : ndarray 张量a的shape是(10,2,15), numpy.transport(2,0,1)的shape就是(15,10,2) 对于一维：行向量变成列向量 对于二维：矩阵的转置 numpy.save numpy.save(file, arr) Save an array to a binary file in Numpy .npy format. Parameters: file :file, str, or pathlib arr :array_like Array data to be saved. sklearnskelearn.linear_model.LinearRegression class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None) Ordinary least squares Linear Regress(最小二乘法回归运算) Parameters: fit_intercept :bool, optional, defalut True【True：需要bias的截距项】 normalize :bool, optional, default False 【True：对样本做feature scaling】 Attributes： coef_ :array of shape(n_features) 【权重】 intercept_ :bias Methods： fit(self,X,y[,sample_weight]) :Fit linear model e.g. , LinearRegression().fit(x_data, y_data) matplotlib.pyplotmatplotlib.pyplot.contourf contour and contourf draw contour lines and filled contours, respectively.【一个画等高线，一个填充等高线/轮廓】 contour([X, Y, ] Z, [levels], **kwargs) Parameters X, Y: The coordinates of the values in Z. A and Y must both be 2-D with the sanme shape as Z(e.g. created via numpy.meshgrid), or they must both be 1-D such that len(X) == M is the number of columns in Z and len(Y) = N is the number of rows in Z. 【X，Y要么是由像numpy.mershgrid(x, y) 生成的网格点坐标矩阵，要么X，Y是（基）向量，X向量是x轴的，对应到Z矩阵，是Z矩阵的列数，Y向量同理】 Z ：array-like(N,M) levels : int or array-like, optional. Determines the number and positions of contour lines / religions.【划分多少块登高区域】 alpha :float, optional. Between 0(transparent) and 1(opaque).【透明度】 cmap :str or Colormap, optional. e.g., pyplot.contourf(x, y, Z, 50, alpha=0.5, cmap=pyplot.get_cmap(‘jet’))【‘jet’是常用的那种红橙黄绿青蓝紫】 matplotlib.pyplot.plot plot([x], y, [fmt], , data=None, *kwargs) The coordinates of the points or line nodes are given by x, y. Parameters: x, y :array-like or scalar. fmt :str, optional. A format string. e.g., ‘.’, point marker. ‘-‘, solid line style. ‘–’,dashed line style. ‘b’, blue. ms/markersize : float lw/linewidth :float color : matplotlib.pyplot.xlim xlim(args, *kwargs) Get or set the x limits of the current axes. e.g. left, right = xlim() :get xlim(left, right) :set matplotlib.pyplot.show show(args, *kwargs) display a figure. syssys.argv[] python a.py data.csv sys.argv = [‘a.py’, ‘data.csv’]","link":"/2020/03/07/python/"},{"title":"「机器学习-李宏毅」：Error","text":"这篇文章叙述了进行regression时，where dose the error come from?这篇文章除了解释了error为什么来自bias和variance，还给出了当error产生时应该怎么办？如何让模型在实践应用中也能表现地和测试时几乎一样的好？ Error在中的2.4节，我们比较了不同的Model。下图为不同Model下，testing data error的变化。 可以发现，随着模型越来越复杂，testing data的error变小一些后，爆炸增大。 越复杂的模型在testing data上不一定能得到好的performance。 所以，where dose the error come from? ：bias and variance Bias and Variance of Estimator用打靶作比，如果你的准心，没有对准靶心，那打出的很多发子弹的中心应该离靶心有一段距离，这就是bias。 但把准心对准靶心，你也不一定能打中靶心，可能会有风速等一系列原因，让子弹落在靶心周围，这就是variance。 上图中，可以直观体现出bias 和 variance的影响。 概率论中 ： 一个通过样本值得到了估计量，有三个评判准则：无偏性、有效性和相和性。 这里的无偏性的偏也就是bias。 概率论中定义：设 $\\hat{\\theta}(X_1,X_2,…,X_n)$ 是未知参数 $\\theta$ 的估计量，若 $E(\\hat{\\theta})=\\theta$ ，则称 $\\hat{\\theta}$ 是 $\\theta$ 的无偏估计。 变量 $x$ ，假设他的期望是 $\\mu$ ，他的方差是 $\\sigma^2$. 对于样本： $x^1,x^2,…,x^N$ ，估计他的期望和方差。 概率论的知识： $m=\\frac{1}{N} \\sum_{n} x^{n} \\quad s^{2}=\\frac{1}{N} \\sum_{n}\\left(x^{n}-m\\right)^{2}$ $E(m)=\\mu$ ，所以用 $m$ 是 $\\mu$ 的无偏估计。(unbiased) 但是 $E\\left[s^{2}\\right]=\\frac{N-1}{N} \\sigma^{2} \\quad \\neq \\sigma^{2}$ ，所以这样的估计是有偏差的。(biased) 因此统计学中用样本估算总体方差都进行了修正。 而在机器学习中，Bias和Variance通常与模型相关。 上图中，假设黑色的线是 true function，红色的线是训练得到的函数，蓝色的线是，训练函数的平均函数。 可见，随着函数模型越来越复杂，bias在变小，但variance也在增大。 右下角图中，红色的线接近铺满了，variance已经很大了，模型过拟合了。 对机器学习中模型对bias影响的直观解释 左图的model简单，右图的model复杂。 简单的model，包含的函数集较小，可能集合圈根本没有包括target（true function），因此在这个model下，无论怎么训练，得到的函数都有 large bias。 而右图中，因为函数非常复杂，所以大概率包含了target，因此训练出的函数可能variacne很大，但有 small bias。 what to do with large bias/variance 上图中，红色的线表示bias的误差，绿色的线表示variance的误差，蓝色的线表示观测的误差。 当模型过于简单时：来自bias的误差会较大，来自vaiance的误差较小，也就是 Large Bias Small Variance 当模型过雨复杂时：来自bias的误差会较小，来自variance的误差会很大，也就是 Small Bias Large Variance 2 case : Underfitting ：If your model cannot even fit the training examples, then you have large bias. Overfitting : If you can fit the traning data, but large error on testing data , then you probably have large variance. With Large BiasFor bias, redesign your model. Add more features as input. A more complex model. 考虑更多的feature；使用稍微复杂些的模型。 With Large Variance More data Regularization (在这篇2.5.2文章中有叙述什么是regularization) Model Selection There is usually a trade-off beween bias and variance. Select a model that balances two kinds of error to minimize total error. 选择模型需要在bias和variance中平衡，尽量使得总error最小。 What you should NOT do: 以上，描述的是这样的一个情形：在traning data中，得到了三个自认不错的模型，kaggle的公开的testing data测试，分别得到三个模型的error，认为第三个模型最好！ 但是，当把kaggle用private的testing data 进行测试时，error肯定是大于0.5的，最好的model也不一定是第三个。 同理，当把我们训练出的model拿来实际应用时，可能会发现情况很糟，并且，这个model可能选的是测试中最好的，但在应用中并不是最好的。 Cross Validation什么是Cross Validation(交叉验证)？ 在机器学习中，就是下图过程： 把Traning Set 分成两个部分：Training Set和Validation Set。 在Training Set部分选出模型。 用Validation Set来判断哪个模型好：计算模型在Validate Set的error。 再用模型预测Testing Set(public)，得到的error一定是比Validation Set中大的。 Not recommend : Not用public testing data的误差结果去调整你的模型。 这样会让模型在public的performance比private的好。 但模型在private testing data的performance才是我们真正关注的。 那么当模型预测private testing set时（投入应用时），能尽最大可能的保证模型和在预测public testing data相近。 N-fold Cross ValidationN-fold Cross Validation（N-折交叉验证）的过程如下： 把Training Set 分为3（3-fold）份，每一次拿其中一份当Validation Set，另外两份当作Training Set。 每一次用Train Set来训练。得到了三个Model。 要判断哪一个Model好？ 每一个Model都计算出不同Validation Set的error。 得到一个Average Error。 最后选这个average error最小的model。 最后应用在public traning set，来评估模型应用在private training set的performance。","link":"/2020/03/15/error/"},{"title":"「Cryptography-Dan」:Stream Cipher 2","text":"作为Stream Cipher的第二篇文章。第一部分分析了基于Stream Cipher的两种攻击：第一种是Two time pad,第二种是对与其完整性的攻击，即流密码是可被篡改的。第二部分具体说明了一些使用流密码加密的例子。包括分析基于软件的RC4流密码、基于硬件的CSS流密码和现代的安全流密码:eStream中的Salsa20。 Attack on OTP and stream ciphersAttack1: two time pad is insecureNever use strame cipher key more than once!! why insecure使用相同的PRG(k)加密不同明文时：$$C_1 \\leftarrow m_1 \\oplus \\text{PRG(k)}\\C_2 \\leftarrow m_2 \\oplus \\text{PRG(k)}$$Eavesdropper（窃听者）截获这两段密文 $C_1\\ C_2$ ，对密文进行疑惑操作，可得： $C_1 \\oplus C_2\\rightarrow m_1\\oplus m_2$ 。 在传输中，英语字母是用ASCII编码后再传输，所以这样的编码会带来很多redundancy（冗余），即根据 $m_1\\oplus m_2$ ，可以得到 $m_1\\ m_2$ 。 因此，当一个密钥会被使用多次时，就不应该直接用stream cipher，后面的章节会介绍multi-use ciphers。 Examples: Project Venona(1941-1946)我们已经知道：加密应该用OTP，即一次性密钥。 但是，当时是通过人工掷骰子并记录得到密钥，工作费时费力。因此不得不用生成的密钥加密多条消息。 最后仅凭截获密文，就破译了3000多条消息。 Examples: MS-PPTP(Windows NT)微软在Windows NT的PPTP协议（point to point transfer protocol）中使用的流密码是：RC4。 在这个协议中允许一个端系统向另一个端系统发送加密后的信息。 过程如下： 在一次对话连接中：主机想发送$m_1\\ m_1\\ m_3$ 三条消息进行查询，服务器想发送 $s_1\\ s_1\\ s_3$ 三条消息进行响应。 主机和服务器hava a shared key:k。 知道密钥不能加密多次，于是主机将三条消息进行concatenation（联结）： $m_1||m_2||m_3$ 。 主机用k作为密钥，得到G(k)，进行加密 $[m_1||m_2||m_3]\\oplus\\text{G(k)}$ 。 同样，服务器也将响应消息进行联结： $s_1||s_2||s_3$ 。 服务器也用k作为密钥，得到相同的G(k)，对响应消息进行加密 $[s_1||s_2||s_3]\\oplus\\text{G(k)}$ 。 因此，在一次对话中，主机和服务器都使用了相同的 G(k)进行加密，也就是 two time pad。 如何改进主机和服务器have a shared pair of key, 即主机和服务器都使用不同的key进行加密。 Examples: 802.11b WEPHow it worksWEP(Wired Equivalent Privacy)，有效等效加密，是一种用于IEEE 802.11b的一种安全算法。这个算法设计的很糟糕，现已被WPA所淘汰。 WEP用于Wi-Fi通信，是他的的加密层。 WEP的算法过程如下： 主机和路由器进行通信： 主机和路由 have a shared key。 主机想要发送一段消息，包括明文m和其校验码CRC(m)。 PRG’s seed： IV||k, k is a long term key，IV is a counter. Length of IV: 24 bits. IV的作用：每一次传送数据包时，用IV来改变每次的密钥。 用(IV||k作为密钥，得到PRG(IV||k),使用流密码进行加密传输。 主机直接发送IV和密文。 路由器用收到的IV和k连接，用PRG(IV||k)，对密文解密。 Problems of IV IV 导致的问题1: two time pad Length of IV: 24 bits Related IV after $2^{24}$ (16M frames) 【当发送16百万的帧后，PRG又会重复】 On some 802.11 cards: IV rests to 0 after power cycle. 【在有些单片机上，重启后IV会变成0：每次重启都会使用PRG(0||k)加密】 IV 导致的问题2: related keys 在PRG中，key for frame is related。(IV||k)，k是104 bits, IV 是24 bits，所以key的后104位总是相同的，不同密钥之间的差异也很小。 对RC4 PRG的攻击： Fluhrer, Mantin, and Shamir在2001年:只需要监听 $10^6$ 帧即可破译密钥[1]。 Recent attacks：只需要监听4000帧，即可破译WEP网络中的密钥。 所以，密钥关联是灾难性的。 Avoid related keys！ A better construction对于WEP，一种更好的做法是：将多个要发送的帧联结起来，得到 $m_1||m_2||…||m_n$ 长流，再用PRG对这个长流加密。 如上图所示，k扩展后，被分成很多段，用第一段加密第一帧，第二段加密第二帧……。 这样，加密每一帧的密钥都是一个伪随机密钥。(no relationship, and looks like random)。 当然，更好的解决方法是使用更强的加密算法（WPA2）。 Examples: disk encryption另一个例子是硬盘加密，在这个例子中，你会发现：使用流密码对硬盘加密不是一个好的举措。 如果使用流密码： Alice 想要给Bob写一封邮件，如上图所示。 邮件经过硬盘加密（流密码）后，存入内存块。 Alice 想要对存在这个硬盘中的邮件进行修改。 Alice只把Bob改成了Eve，其他部分都没有变，如上图所示。 保存后，邮件再次经过硬盘加密（流密码）后，存入内存块。 Attacker：他得到了硬盘上最初的密文和修改后的密文。 通过分析，他发现两段密文只有前小部分不同。（用相同的流密码密钥加密，修改后，密文很容易看出变化） 虽然Attacker不知道Alice是怎么修改的，但是他知道了Alice修改的具体位置。 $\\Rightarrow$ attacker得到了他不应该知道的信息，即修改的具体位置。 在硬盘加密中，对于文本内容的修改前后，也使用了相同的密钥段加密不同的消息，即two time pad。 因此在硬盘加密中，不推荐使用流密码。 Two time pad: SummaryNever use stream cipher key more than once!! Network traffic: negotiate new key for every session. Disk encryption: typically do not use a stream cipher. Attack2: no integrity(OTP is malleable)OPTP和Stream Cipher一样，不提供完整性的保证，只提供机密性的保证。 如上图所示： 如果attacke截获：密文 $m\\oplus k$ 。 并用sub-permutation key（子置换密钥）来对密文进行修改，得到新的密文：$(m\\oplus k)\\oplus p$ 新的密文最后解密得到的明文是 $m\\oplus p$ 。 所以对于有修改密文能力的攻击者来说，攻击者很容易修改密文，并且修改后的密文，对原本解密后的明文也有很大的影响。 具体攻击如下： Bob想要发送一封邮件：消息开头是From: Bob，使用OTP加密后，发送密文。 Attacker：截获了这段密文 假设：attacker知道这封邮件是来自Bob。 attacker想修改这封密文邮件，使得它来自others。 于是它用一个子置换密钥对原密文的特定位置（即Bob密文位置）进行操作，得到新的密文：From： Eve。 这个子置换密钥是什么呢？ 如上图所示，Bob的ASCII码是 42 6F 62，Eve的ASCII码是 45 76 65。 那么Bob $\\oplus$ Eve的ASCII码是 07 19 07。 因此这个子置换密钥是07 19 07。 最后收件人进行解密，得到的是明文：From：Eve。 对attacker来说，虽然他不能创建来自Eve的密文，但是他可以通过修改原本的密文，达到相同的目的。 Conclusion: Modifications to ciphertext are undertected and have predictable impact on plaintext. Real-world Stream CiphersOld example(SW): RC4RC4流密码，是Ron RivestRC4在1987年设计的。曾用于secure Web traffic(in the SSL/TLS protocol) 和wireless traffic (in the 802.11b WEP protocol). It is designed to operate on 8-bit processors with little internal memory. At the heart of the RC4 cipher is a PRG, called the RC4 PRG. The PRG maintains an internal state consisting of an array S of 256 bytes plus two additional bytes i,j used as pointers into S. 【RC4 cipher的核心是一个PRG，called the RC4 PRG。这个PRG的内部状态是一个256字节的数组S和两个指向S数组的指针】 RC4 stream cipher generator setup algorithms: 对S数组进行初始化，0-255都只出现一次入下图所示： 伪代码 stream generator: Once tha array S is initialized, the PRG generates pseudo-random output one byte at atime, using the following stream generator: The procedure runs for as long as necessary. Again, the index i runs linearly through the array while the index j jumps around. Security of RC4具体参见「A Graduate Course in Applied Cryptography」的p76-78 挖坑，有空填坑 cryptanalysis of RC4[2] Weakness of RC4 Bias in initial output: Pr[2^nd^ byte=0]=2/256. 如果PRG是随机的，其概率应该是1/256。 而Pr[2^nd^ byte=0]=2/256的结果是：消息的第二个字节不被加密的概率比正常情况多一倍。（0异或不变） 统计的真实情况是，不止第二个字节，前面很多字节的概率很不都均匀。 因此，如果要使用RC4 PRG，从其输出的257个字节开始使用。 Prob. of (0,0) is 1/256^2^ +1/256^3^ . 如果PRG是随机的，00串出现的概率应该是(1/256)^2^ . Related key attackes. 在上小节中「Examples: 802.11b WEP」，WEP使用的RC4流密码，related key对安全通信也是灾难性的。 Old example(HW): CSS (badly broken)The Content Scrambling System (CSS) is a system used for protecting movies on DVD disks. It uses a stream cipher, called the CSS stream cipher, to encrypt movie contents. CSS was designed in the 1980’s when exportable encryption was restricted to 40-bits keys. As a result, CSS encrypts movies using a 40-bits key. 【1980的美国出口法，限制出口的加密算法只能是40位，于是CSS的密钥是40位】[amazing.jpg] While ciphers using 40-bit keys are woefully insecure, we show that the CSS stream cipher is particularly weak and can be broken in far less time than an exhaustive search over all 2^40^ keys. 【虽然40位的密钥本来就不够安全，但是我们能用远小于穷举时间的方法破解CSS】 因为博主也是第一次学，很多东西也不了解。 所以概述性语言，我用教科书的原文记录，附注一些中文。望海涵～ Linear feedback shift register(LFSR)CSS 流密码是由两个LFSR（线性反馈移位寄存器）组成的，除了CSS，还有很多硬件系统是基于LFSR进行加密操作，但无一例外，都被破解了。 DVD encryption (CSS)：2 LFSRs GSM encryption (A5/,2): 3 LFSRs 【全球通信系统】 Bluetooth(E0): 4LFSRs LFSR can be implemented in hardware with few transistors. And a result, stream ciphers built from LFSR are attractive for low-cost consumer electronics such as DVD players, cell phones, and Bluetooth devices. 【LFSR在硬件上运行很快，也很省电，所以虽然基于LFSR的算法都被破解了，但是改硬件有点困难，所以还是有很多系统在使用】 上图是一个8位LFSR{4,3,2,0}。 LFSR是由一组寄存器组成，LFSR每个周期输出一位（即0位）。 有一些位（如上图的4，3，2，0）称为tap positions(出头)，通过这些位计算出feedback bit(反馈位)。 反馈位和寄存器组的未输出位组成新的寄存器组。 伪代码如下： 所以基于LFSR的PGR的seed是寄存器组的初始状态。 how CSS worksCSS的seed=5 bytes=40 bits。 CSS由两个LFSR组成，如下图所示，一个17-bit LFSR，一个25-bit LFSR。 seed of LFSR: 17-bit LFSR: 1||first 2 bytes ，即17位。 25-bit LFSR: 1||last 3 bytes，即25位。 CSS过程： 两个LFSR分别运行8轮，输出8 bits。 两个8bits 相加mod 256（还有加上前一块的进位）即是CSS一轮的输出：one byte at a time. Cryptanalysis of CSS (2^16 time attack)当已知CSS PRG的输出时，我们通过穷举（2^40^ time）破解得到CSS的seed。 但还有一种更快的破解算法，只需要最多2^16^ 的尝试。 破解过程如下： 影片文件一般是MPEG文件，假设我们已知明文MPEG文件的前20个字节。（已知明文的prefix） CSS是流密码，可以通过已知prefix还原出CSS的prefix，即CSS PRG的前20个字节的输出。 For all possible initial settings of 17-bit LFSR do: run 17-it LFSR to get 20 bytes of output. 【先让17-bit输出20个字节】 subtract from CSS prefix $\\Rightarrow$ candidate 20 bytes output of 25-bit LFSR. 【通过还原的CSS PRG的输出，得到25-bit输出的前20个字节】 If consistent with 25-bit LFSR, found correct initial settings of both!! 【具体是如何判别这20个字节是否是一个25-bit LFSR的输出呢？】 假设前三个字节是y1, y2, y3. 那么25-bit LFSR的initial :{1, y1 , y2, y3},其中y都是8 bits. 用这个初始状态生成20个字节，如果和得到的20个字节相同，则正确，否则再次枚举。 当得到了两个LFSR的seed, 就可以得到CSS PRG的全部输出，即可破解。 Modern stream ciphers: eStreammain idea eStream PRG ： $\\{0,1\\}^s\\times R \\rightarrow \\{0,1\\}^n$ (n&gt;&gt;s) seed: ${0,1}^s$ nonce: a non-repeating value for a given key.【就对于确定的seed,nonce绝不重复】 Encryption: $\\text{E(k, m; r)}=\\text{m}\\oplus \\text{PRG(k; r)} $ The pair (k,r) is never used more than once. 【PRG的输入是(k,r), 确保OTP】 eStram: Salsa 20(SW+HW)Salsa20/12 and Salsa20/20 are fast stream ciphers designed by Dan Bernstein in 2005. Salsa 20/12 is one of four Profile 1 stream cipher selected for the eStream portfolio of stream ciphers. eStream is a project that identifies fast and secure stream ciphers that are appropriate for practicle use. Variants of Salsa20/12 and Salsa20/20, called ChaCha12 and ChaCha20 respectively, were proposed by Bernstein in 2008. These stream ciphers have been incorporated into several widely deployed protocols such as TLS and SSH. Salsa20 PRG: $\\{0,1\\}^{128 \\text { or } 256} \\times\\{0,1\\}^{64} \\longrightarrow\\{0,1\\}^{n}$ (max n = 2^73^ bits) Salsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ 通过计数器，使得输出联结，可以输出as long as you want pseudorandom segment. 算法过程如上图所示：Salsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ 32 bytes的{k,r,i}通过扩展得到64 bytes的{ $\\tau_0,k,\\tau_1,r,i,\\tau_2,k,\\tau_3$ }. k :16 bytes的seed. i: 8 bytes的index，计数器。 r: 8 bytes的nonce. $\\tau_{0,1,2,3}$ 都是4 bytes的常数，Salsa20算法说明书上公开确定的值。 64 bytes 通过h函数映射，十轮。 h : invertible function designed to be fast on x86(SEE2). 在x86上有SEE2指令可以直接运行h函数，所以很快。 h是逆函数（也是公开的函数），输出可以通过逆运算得到其输入。 h是一个一一映射的map，每一个64bytes的输入都有唯一对应的64 bytes的输出。 将第十轮H函数的输出和最开始的输入做加法运算，word by word(32位)，即每4 bytes相加。 为什么要有这一步？ h是可逆运算，如果直接将函数的输出作为PRG的输出，那可以通过h的逆运算得到原64 bytes，也就得到了(k; r). Is Salsa20 secure(unpredictable)前文我们通过unpredictable来定义PRG的安全（下一篇文章会给出安全PRG更好的定义），所以Salsa20 安全吗？是否是不可预测的？ Unknown：no known provably secure PRGs. 【不能严格证明是一个安全PRG（后文会继续讲解什么是安全的PRG），如果严格证明了，也就证明了P=NP】 In reality： no known attacks bertter than exhaustive search. 【现实中还没有比穷举更快的算法】 Performance通过下图的比较，如果在系统中需要使用流密码，建议使用eStream。 speed ：该算法每秒加密多少MB的数据。 Reference S. Fluhrer, I. Mantin, and A. Shamir. Weaknesses in the key scheduling algorithm of RC4. In proceedings of selected areas of cryptography (SAC), pages 1-24, 2001. 「A Graduate Course in Applied Cryptography」p76-78:挖坑 待补充","link":"/2020/03/19/StreamCipher2/"},{"title":"「机器学习-李宏毅」:Classification-Generative Model","text":"Classification 有Generative Model和Discriminative Model。这篇文章主要讲述了用生成模型来做分类的原理及过程。 What is Classification?分类是什么呢？分类可以应用到哪些场景呢？ Credit Scoring【贷款评估】 Input: income, savings, profession, age, past financial history …… Output: accept or refuse Medical Diagnosis【医疗诊断】 Input: current symptoms, age, gender, past medical history …… Output: which kind of diseases Handwritten character recognition【手写数字辨别】 Input： Output：金 Face recognition 【人脸识别】 Input: image of a face output: person Classification：Example Application【图】 如上图，Pokemon又来啦！ Pokemon有很多属性，比如皮卡丘是电属性，杰尼龟是水属性之类。 关于Pokemon的Classification：Predict the “type” of Pokemon based on the information Input：Information of Pokemon (数值化） Output：the type Training Data: ID在前400的Pokemon Testing Data: ID在400后的Pokemon Classification as Regression?1. 简化问题，只考虑二分类：Class 1 ， Class2。 如果把分类问题当作回归问题，把类别数值化。 在Training中： Class 1 means the target is 1; Class 2 means the target is -1. 在Testing中：如果Regression的函数值接近1，说明是class 1；如果函数值接近-1，说明是class 2. Regression：输入信息只考虑两个特征。 Model：$y=w_1x_1+w_2x_2+b$ 当Training data的分布如上图所示时，得到的（最优函数）分界线感觉很合理。 但当Training data在右下角也有分布时（如右图），训练中为了减少error，训练得到的分界线会变成紫色的那一条。 所以，如果用Regression来做Classification：Penalize to the examples that are “too correct” .[1] 训练中会因为惩罚一些“过于正确”（即和我们假定的target离太远）的example，得到的最优函数反而have bad performance. 2. 此外，如果用Regression来考虑多分类。 Multiple class: Class 1 means the target is 1; Class 2 means the target is 2; Class 3 means the target is 3…… 如果用上面这种假设，可以认为Class 3和Class 2 的关系更近，和Class 1的关系更远一些。但实际中，可能这些类别have no relation。 Classification: Ideal Alternatives在上面，我们假设二元分类每一个类别都有一个target，结果不尽人意。 如上图所示，将模型改为以上形式，也可以解决分类问题。（挖坑）[2] Generative Model(生层模型)Estimate the Probabilities用概率的知识来考虑分类这个问题，如下图所示，有两个两个类别，C1和C2。 在Testing中，如果任给一个x，属于C1的概率是（贝叶斯公式） $$ P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} $$ 所以在Training中知道这些： $P(C_1),P(x|C_1),P(C_2),P(x|C_2)$ P(C1)和P(C2)很容易得知。 而P(x|C1)和P(x|C2)的概率应该如何得知呢？ 如果能假设：类别是C1中的变量x服从某种分布，如高斯分布等，即可以得到任意P(x|C1)的值。 所以Generative Model：是对examples假设一个分布模型，在training中调节分布模型的参数，使得examples出现的概率最大。（极大似然的思想） Prior Probabilities（先验概率）先只考虑Water和Normal两个类别。 先验概率：即通过过去资料分析得到的概率。 在Pokemon的例子中，Training Data是ID&lt;400的水属性和一般属性的Pokemon信息。 Training Data：79 Water，61 Normal。 得到的先验概率 P(C1)=79/(79+61)=0.56, P(C2)=61/(79+61)=0.44。 Probability from Class先只考虑Defense和SP Defense这两个feature。 如果不考虑生成分布模型，在testing中直接计算P(x|Water)的概率，如下图右下角的那只龟龟，在training data中没有出现过，那值为0吗？显然不对。 假设：上图中water type的examples是从Gaussian distribution（高斯分布）中取样出来的。 因此在training中通过training data得到最优的Gaussian distribution的参数，计算样本中没有出现过的P(x|Water)也就迎刃而解了。 Gaussian Distribution多维的高斯分布（高斯分布就是正态分布啦）的联合概率密度： $$ f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\} $$ D: 维数 $\\mu$ : mean $\\Sigma$ :covariance matrix(协方差矩阵) 协方差： $ cov(X,Y)=E[[X-E(X)][Y-E(Y)]]=E(XY)-E(X)E(Y)$ 具体协方差性质，查阅概率论课本吧。 x: vector,n维随机变量 高斯分布的性质只和 $\\mu$ 和 $\\Sigma$ 有关。 $\\Sigma$ 一定时，$\\mu$ 不同，如下图： $\\mu$ 一定， $\\Sigma$ 不同时，如下图： Maximum Likelihood（极大似然）样本分布如下图所示，假设这些样本是从Gaussian distribution中取样，那如何在训练中得到高斯分布的 $\\mu$ 和 $\\Sigma$ 呢？ 极大似然估计。 考虑Water，有79个样本，估计函数 $L(\\mu, \\Sigma)=f_{\\mu, \\Sigma}\\left(x^{1}\\right) f_{\\mu, \\Sigma}\\left(x^{2}\\right) f_{\\mu, \\Sigma}\\left(x^{3}\\right) \\ldots \\ldots f_{\\mu, \\Sigma}\\left(x^{79}\\right)$ 极大似然估计，即找到 $f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\}$ 中的 $\\mu$ 和$\\Sigma$ 使得估计函数最大（使得取出这些样本的概率最大化）。 $\\mu^{*}, \\Sigma^{*}=\\arg \\max _{\\mu, \\Sigma} L(\\mu, \\Sigma)$ 求导计算（过于复杂，但也不是不能做是吧） 背公式[3] $\\mu^{*}=\\frac{1}{79} \\sum_{n=1}^{79} x^{n} \\qquad \\Sigma^{*}=\\frac{1}{79} \\sum_{n=1}^{79}\\left(x^{n}-\\mu^{*}\\right)\\left(x^{n}-\\mu^{*}\\right)^{T}$ 得到Water和Normal的高斯分布，如下图: Do Classification: different $\\Sigma$TestingTesting： $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ P(x|C1)由训练得出的Water的高斯分布计算出，P(x|C2)由Normal的高斯分布计算出。（如下图，过于难打） 如果P(C1|x)&gt;0.5，说明x 属于Water(Class 1)。 Results如果只考虑两个feature（Defense和SP Defense），下图是testing data的样本图，蓝色属于Water，红色属于Normal。 用训练得出的模型，Testing Data: 47% accuracy。（结果如下图） 如果考虑全部features(7个)，重新训练出的模型，结果：Testing Data：54% accuracy。（结果如下图） 结果并不好。参数过多，模型过于复杂，有些过拟合了。 Modifying Model：same $\\Sigma$模型中的参数有两个的Gaussian Distribution中的 $\\mu^$ 和 $\\Sigma^$ ，其中协方差矩阵的大小等于feature的平方，所以让不同的class share 同一个 $\\Sigma$ ，以此来减少参数，简化模型。 极大似然估计的估计函数： $$ L\\left(\\mu^{1}, \\mu^{2}, \\Sigma\\right)=f_{\\mu^{1}, \\Sigma}\\left(x^{1}\\right) f_{\\mu^{1}, \\Sigma}\\left(x^{2}\\right) \\cdots f_{\\mu^{1}, \\Sigma}\\left(x^{79}\\right)\\times f_{\\mu^{2}, \\Sigma}\\left(x^{80}\\right) f_{\\mu^{2}, \\Sigma}\\left(x^{81}\\right) \\cdots f_{\\mu^{2}, \\Sigma}\\left(x^{140}\\right) $$ 公式推导:[3] $\\mu$ 的公式不变。 $\\Sigma=\\frac{79}{140} \\Sigma^{1}+\\frac{61}{140} \\Sigma^{2}$ ,即是原 $\\Sigma^1\\ \\Sigma^2$的加权平均。 Results当只考虑两个features，用同样的协方差参数，结果如下图： 可以发现，用了同样的协方差矩阵参数后，边界变成了线性的，所以这也是一个线性模型。 再考虑7个features，用同样的协方差矩阵参数，模型也是线性模型，但由于在高维空间，人无法直接画出其boundary，这也是机器学习的魅力所在，能解决一些人无法解决的问题。 结果：从之前的54% accuracy增加到 73% accurancy. 结果明显变好了。 SummaryThree Steps： Function Set（Model）： Goodness of a function: The mean µ and convariance $\\Sigma$ that maximizing the likelihood(the probability of generating data) Find the best function:easy(公式) Appendix为什么要选择Gaussian DistributionYou can always use the distribution you like. 可以选择你喜欢的任意分布，t分布，开方分布等。 （老师说：如果我选择其他分布，你也会问这个问题，哈哈哈） Naive Bayes ClassifierIf you assume all the dimensions are independent, then you are using Naive Bayes Classifier. 如果假设features之间互相独立， $P\\left(x | C_{1}\\right)=P\\left(x_{1} | C_{1}\\right) P\\left(x_{2} | C_{1}\\right) \\quad \\ldots \\ldots \\quad P\\left(x_{k} | C_{1}\\right) $ 。 xi是x第i维度的feature。 对于每一个 P(xi|C1)，可以假设其服从一维高斯分布。如果是binary features（即feature取值只有两个），也可以假设它服从Bernoulli distribution(贝努利分布)。 Posterior Probability（后验概率）Posterior Probability后验概率，即使用贝叶斯公式，已知结果，寻找最优可能导致它发生的原因。 对 $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ 进行处理。 得到： $$ \\begin{equation} \\begin{aligned} P\\left(C_{1} | x\\right)&=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\\\ &=\\frac{1}{1+\\frac{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}}\\\\&=\\frac{1}{1+\\exp (-z)} =\\sigma(z) \\qquad(z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} \\end{aligned} \\end{equation} $$ Worning of Math $z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}=\\ln \\frac{P\\left(x | C_{1}\\right)}{P\\left(x | C_{2}\\right)}+\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}$ $\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}=\\frac{\\frac{N_{1}}{N_{1}+N_{2}}}{\\frac{N_{2}}{N_{1}+N_{2}}}=\\frac{N_{1}}{N_{2}}$ $P\\left(x | C_{1}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma 1|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)\\right\\}$ $P\\left(x | C_{2}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{\\left|\\Sigma^{2}\\right| 1 / 2} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right\\}$ $\\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2}\\left[\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)-\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right]$ $\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)=x^{T}\\left(\\Sigma^{1}\\right)^{-1} x-2\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1}$ $\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)=x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-2\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}$ $\\begin{aligned} z=& \\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2} x^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1} \\\\ &+\\frac{1}{2} x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} \\end{aligned}$ 简化模型后， $\\Sigma^1=\\Sigma^2=\\Sigma$ : $$ z=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} $$ 令 $w^T=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} \\qquad b=-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}}$ 当简化模型后，z是线性的，这也是为什么在之前的结果中边界是线性的原因。 最后模型变成这样： $P\\left(C_{1} | x\\right)=\\sigma(w \\cdot x+b)$ . 在生成模型中，我们先估计出 $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ 的值，也就得到了 $w\\ b$ 的值。 那，我们能不能跳过 $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ ，直接估计 $w\\ b$ 呢？ 在下一篇博客[4]中会继续Classification。 Reference Classification as Regression: Bishop, P186. 挖坑：Classification：Perceptron，SVM. Maximum likelihood solution：Bishop chapter4.2.2","link":"/2020/03/20/Classification1/"}],"tags":[{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"Cryptography","slug":"Cryptography","link":"/tags/Cryptography/"},{"name":"公开课","slug":"公开课","link":"/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/"},{"name":"StreamCipher","slug":"StreamCipher","link":"/tags/StreamCipher/"},{"name":"Gradient","slug":"Gradient","link":"/tags/Gradient/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Cryptography-Dan","slug":"Cryptography-Dan","link":"/categories/Cryptography-Dan/"},{"name":"机器学习-李宏毅","slug":"机器学习-李宏毅","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"}]}