{"pages":[],"posts":[{"title":"Adagrag-demo","text":"实现这篇文章中前面两个tips。 实现了tip1 Adagrad + tip2 Stochastic Gradient Descent demo代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869################## 2020/03/06 ## Adagrad demo ##################import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model# datax_data = [[338.], [333.], [328.], [207.], [226.], [25.], [179.], [60.], [208.], [606.]]y_data = [640., 633., 619., 393., 428., 27., 193., 66., 226., 1591.]# coordinatex = np.arange(-200, -100, 1)y = np.arange(-5, 5, 0.1)Z = np.zeros((len(y), len(x)))# cal the Loss of every point(function)for i in range(len(x)): for j in range(len(y)): b = x[i] w = y[j] for k in range(len(x_data)): Z[j][i] += (y_data[k] - b - w * x_data[k][0])**2# initialb = -120w = -4lr = 1 # learning rateiteration = 100000# record the iterationb_his = [b]w_his = [w]# Adagradb_grad_sum2 = 0.0w_grad_sum2 = 0.0for i in range(iteration): for k in range(len(x_data)): b_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-1) w_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-x_data[k][0]) b_grad_sum2 += b_grad**2 w_grad_sum2 += w_grad**2 b = b - lr / np.sqrt(b_grad_sum2) * b_grad w = w - lr / np.sqrt(w_grad_sum2) * w_grad b_his.append(b) w_his.append(w)# sklearn linear modelreg = linear_model.LinearRegression()print(reg.fit(x_data, y_data))print(reg.coef_[0])print(reg.intercept_)# display the figureplt.contourf(x, y, Z, 50, alpha=0.5, cmap=plt.get_cmap('jet'))plt.plot(reg.intercept_, reg.coef_, 'x', ms=13, lw=1.5, color='orange')plt.plot(b_his, w_his, 'o-', ms=3, lw=1.5, color='black')plt.xlim(-200, -100)plt.ylim(-5, 5)plt.xlabel('$b$', fontsize=16)plt.ylabel('$w$', fontsize=16)# plt.show()plt.savefig(\"Loss.png\") Loss 迭代图画出的图片很直观","link":"/2020/03/09/Adagrad-demo/"},{"title":"「机器学习-李宏毅」：Backpropagation","text":"这篇文章中，讲解了Deep Learning中使用的一种高效Gradient Descent的算法：BackPropagation。BackPropagation通过正向传播和反向传播两个阶段，最后能一起算出损失函数对每一个参数的gradient。 Gradient Descent在Neural Network中，参数的更新也是通过Gradient Descent。 但是当Neural Network层数很深，结构很复杂的时候，会有millions of parapmeters。 Backpropagation：To compute the gradient efficiently. Chain RuleBP中需要用到的数学知识：微积分中的链式法则。 Backpropagation 在NN中，定义损失函数 $L(\\theta)=\\sum_{n=1}^{N} C^{n}(\\theta)$ （$\\theta$ 代指NN中所有的weight 和bias） 对某一参数的gradient为 $\\frac{\\partial L(\\theta)}{\\partial w}=\\sum_{n=1}^{N} \\frac{\\partial C^{n}(\\theta)}{\\partial w}$ 在上图NN中，我们先只研究红框部分，即是以下结构： z：每个activation function的输入。 根据链式法则， $\\frac{\\partial C}{\\partial w}= \\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}$ . 要计算每个参数的 $\\frac{\\partial C}{\\partial w}$ ，分为两部分。 Forward pass: compute $\\frac{\\partial z}{\\partial w} $ for all parameters. Backward pass: compute $\\frac{\\partial C}{\\partial z} $ for all activation function inputs z. BP：Forward passCompute $\\frac{\\partial z}{\\partial w} $ for all parameters. 还是只看上图这一部分，可以轻易得出： $\\partial{z}/\\partial{w_1}=x_1\\qquad \\partial{z}/\\partial{w_2}=x_2$ 得到结论： $\\frac{\\partial z}{\\partial w} $ 等于 the value of the input connected by the weight. 【$\\frac{\\partial z}{\\partial w} $ 等于 连接w的输入的值】 那么，如何计算出NN中全部的 $\\frac{\\partial z}{\\partial w} $ ？ ：Forward pass. 用当前参数（w,b) 从hidden layer的第一层开始，计算出第一层的输出，即第二层的输入。 依次相前计算，计算出每一层的输出，即下一层的输入，即输入所连接权重的 $\\frac{\\partial z}{\\partial w}$ 。 BP：Backward passCompute $\\frac{\\partial C}{\\partial z} $ for all activation function inputs z. z：activation function的 input a：activation function的 output 这里的activation function 是 sigmod函数 $a=\\sigma(z)=\\frac{1}{1+e^{-z}}$ 要求 $\\frac{\\partial C}{\\partial z}$ ， 再根据链式法则： $\\frac{\\partial C}{\\partial z}=\\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a}$ 求 $\\frac{\\partial{a}}{\\partial{z}}$ : $\\frac{\\partial{a}}{\\partial{z}}=\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$ （是其他activation function 也能轻易求出） 求 $\\frac{\\partial C}{\\partial a}$ ：根据链式法则： $\\frac{\\partial C}{\\partial a}=\\frac{\\partial z^{\\prime}}{\\partial a} \\frac{\\partial C}{\\partial z^{\\prime}}+\\frac{\\partial z^{\\prime \\prime}}{\\partial a} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}$ $\\frac{\\partial z^{\\prime}}{\\partial a} =w_3$ ， $\\frac{\\partial z^{\\prime\\prime}}{\\partial a} =w_4$ $\\frac{\\partial C}{\\partial z^{\\prime}}$ 和 $\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$ ？假设，已经通过某种方法算出这个值。 $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ 这个式子，可以画成一个反向传播的NN，见下图。 $\\frac{\\partial C}{\\partial z^{\\prime}},\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$ 是这个neuron的输入， $w_3,w_4$ 仍然是 neuron的 weight（无bias）。 $\\sigma’(z)$ 是一个常数，因为在forward pass中每一个activation的输入已经被算出来了。 和forward pass中的NN的区别是，forward 中是一个activation function，输入z作用于这个函数； 而在 backward pass中，这更像一个放缩器，将他的输入变小，即乘上一个 $\\sigma’(z)$ 。 问题还是如何计算 $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ ？ 分为两种情况讨论， $z’,z’’$ 是否为输出层的输入？ Output Layer： z’,z’’：activation function的输入。 y1,y2：actiavtion function（也是NN）的输出。 C：NN输出和target的cross entropy。 根据链式法则： $\\frac{\\partial C}{\\partial z^{\\prime}}=\\frac{\\partial y_{1}}{\\partial z^{\\prime}} \\frac{\\partial C}{\\partial y_{1}} \\quad \\frac{\\partial C}{\\partial z^{\\prime \\prime}}=\\frac{\\partial y_{2}}{\\partial z^{\\prime \\prime}} \\frac{\\partial C}{\\partial y_{2}}$ 所以，已知activation function（simod或者其他），可以轻易求出 $\\frac{\\partial y_{1}}{\\partial z^{\\prime}}(=\\sigma'(z'))$ 和 $\\frac{\\partial y_{2}}{\\partial z^{\\prime\\prime}}(=\\sigma''(z''))$ 。 所以，已知损失函数，也可以轻易求出 $\\frac{\\partial C}{\\partial y_1}$ 和 $\\frac{\\partial C}{\\partial y_2}$ 。（ $C\\left(y, \\hat{y}\\right)=-\\left[\\hat{y} \\ln y+\\left(1-\\hat{y}\\right) \\ln \\left(1-y\\right)\\right]$ ) 所以，可以直接求出 $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ 。 Not Output Layer: 上图中，如果我们要计算 $\\frac{\\partial C}{\\partial z’}$ ，必须要已知下一层的 $\\frac{\\partial C}{\\partial z_a}$ ，然后一直递归下去，直到到达最后的输出层，也就是上面一种情况，可以直接计算出，再递归回来，计算当前层的 $\\frac{\\partial C}{\\partial z’}$ 。 但是，这样计算每个参数的 $\\frac{\\partial{C}}{\\partial{z}}$ 都要一直递归到输出层，效率显然太低了。 计算方法如上图： 当我们已知输出层的 $\\frac{\\partial{C}}{\\partial{z'}},\\frac{\\partial{C}}{\\partial{z''}}$ 时，再通过上面的步骤3（且的确算出了 $\\frac{\\partial{C}}{\\partial{z'}},\\frac{\\partial{C}}{\\partial{z''}}$ ），画成反向的NN，计算$\\frac{\\partial{C}}{\\partial{z}}$. 再依次反向传播计算出每一个neuron的输出z （也是正向传播neuron的输入）的 $\\frac{\\partial{C}}{\\partial{z}}$ . Backforward pass 的做法： 先计算出输出层的 $\\frac{\\partial{C}}{\\partial{z}}$ （也就是上图的 $\\frac{\\partial{C}}{\\partial{z_5}}$ 和 $\\frac{\\partial{C}}{\\partial{z_6}}$ ） 用反向传播的NN，向后依次计算出每一层每一个neuron的 $\\frac{\\partial{C}}{\\partial{z}}$ 。 Summary 公式： $\\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}=\\frac{\\partial C}{\\partial w}$ 在正向传播NN中，z是neuron的activation function的输入。 在反向传播NN中，z是neuron的放缩器的输出。 通过Forward Pass计算出正向传播NN的每一个neuron的 $\\frac{\\partial z}{\\partial w}$ ，等于该层neuron的输入。 通过Backward Pass计算出反向传播NN的每一个neuron的 $\\frac{\\partial C}{\\partial z}$ 。 然后，通过相乘，计算出每个参数的 $\\frac{\\partial C}{\\partial w}$。 Reference","link":"/2020/04/18/Backpropagation/"},{"title":"「机器学习-李宏毅」：Convolution Neural Network（CNN）","text":"这篇文章中首先介绍了为什么要用CNN做图像识别，或者说图像识别问题的特点是什么？文章中也详细介绍了CNN的具体架构，主要包括Convolution、Max Pooling、Flatten。文章最后简要介绍了CNN在诸多领域的应用。 Why CNN for Image?图片本质都是pixels。 在做图像识别时，本质是对图片中的某些特征像素（properities)识别。 So Why CNN for image? Some patterns are much smaller than the whole image. A neuron does not have to see the whole image to discover the pattern. Connecting to small region with less parameters. 【很多特征图案的大小远小于整张图片的大小，因此一个neuron不需要为了识别某个pattern而看完整张图片。并且，如果只识别某个小的region，会减少大量参数的数目。】 如下图，用一个neuron识别红框中的beak，即能大概率认为图片中有bird。 The same patterns appear in different regions. They can use the same set of parameters. 【同样的pattern可能出现在图片的不同位置。pattern几乎相同，因此可以用同一组参数。】 如下图，两个neuron识别两个不同位置的beak。被识别的beak几乎无差别，因此neuron的参数可以是相同的。 Subsampling the pixels will not change the object. 【一张图片是由许多pixel组成的，如下图，如果去掉图片的所有奇数行偶数列的pixel，图片内容几乎无差别。并且，Subsample pixels，即减少了输入的size，也可以减少NN的参数数量。】 The whole CNNCNN的架构如下图。 一张图片经过多次Convolution、Max Pooling得到新的image，再将新的image Flatten（拉直）得到一组提取好的features，将这组features放入前馈神经网络。 Convolution满足图片识别的： Property 1 : Some patterns are much smaller than the whole image. Property 2 : The same patterns appear in different regions. Max Pooling满足图片识别的： Property 3 : Subsamplingthe pixels will not change the object. CNN-Convolution一张简单的黑白图片如下图，0为白色，1为黑色。 如果图片是彩色的，即用RGB三原色来表示，用三个matrix分别表示R、G、B的值，如下图： 下文中，以黑白图举例。 Property 1设计Filer matrix满足Property 1，如下图： 上图中，filter的大小是3*3，可以检测到小区域的某个pattern。 每个filter的参数都是NN中的参数，需要learned。 如果是彩色图片，filter应该是3张3*3matrix组成的，分别代表R、G、B的filter。 Property 2为了满足Property 2，filter可以在图片中移动。设置stride，即每次filter移动的步长。 filter与覆盖图片的位置做内积，需要走完整张图片，最后得到一张feature map。 下图为stride=1的convolution结果： Convolution layer（卷积层）有几个filter，就会得到几张feature maps。 Convolution v.s. Fully ConnectedFully Connected: 如果用全连接的方式做图片识别，图片的每一个pixel都要和第一层的所有neurons连接，需要大量参数。 如下图： Convolution: 而在Convolution中，把feature map中的每一个值作为neuron的输出，因此图片中只有部分pixels会和第一层的第一个neuron连接，而不是全部pixels。 对于一个3*3的filter，一个neuron的连接如下： filter中的值是连接参数，则每一个neuron只需要与3*3个input连接，与全连接相比减少了大量参数。 shared weights filter在图中移动时，filter的参数不变，即第二个neuron的连接参数和第一个neuron的连接参数是相同的，连接图如下： 通过filter实现了shared weights（参数共享），更大幅度减少了参数数量。 CNN-Max PoolingMax Pooling：将convolution layer的neuron作为输入，neuron的activation function其实就是Maxout（Maxout介绍见 的介绍）。 将convolution layer得到的feature map做Max pooling（池化），即取下图中每个框中的最大值。 如下图，6*6的image经过Convolution layer 和 Max Pooling layer后，得到了new but smaller image，新的image的由两层channel组成，每层channel都是2 * 2的image。 一个image每经过一次Convolution layer 和 Max Pooling layer，都会得到a new image。 This new image is smaller than the origin image. And the number of channel (of the new image) is the number of filters. 举个例子： Convolution layer有25个filters，再经过Max Pooling，得到的新的image有25 个channel。 再重复一次Convolution 和Max Pooling，新的Convolution layer也有25个filters，再经过Max Pooling，得到的新的image有多少个channel呢？ 答案是25个channel。 注意 ：在第二次Convolution中，image有depth，depth=25。因此在convolution中，filter其实是一个cubic，也有depth，depth=image-depth=25，再做内积。 因此，新的image的channel数是等于filter数的。 FlattenFlatten很好理解，将最后得到的新的image 拉直（Flatten）为一个vector。 拉直后的vector是一组提取好的features，作为 前馈神经网络的输入。 zero padding如何让卷积后的图像不变小？ 答案就是zero padding，在原图的padding填0，再做卷积。 zero-padding后如下图： 卷积后，图像大小不变： What dose CNN learn为什么CNN能够学习pattern，最终达到识别图像的目的？ Filter在下图CNN过程中，我们先分析能从Convolution layer的filter能够学到什么？ 每个filter本质上是一组shared weights 的neuron。 因此，定义这组filter的激活程度，即： Degree of the activation of the k-th filter: $a^k=\\sum_{i=1}^{11}\\sum_{j=1}^{11}a_{ij}^{k}$ . 目标是找到使k-th filter激活程度最大的输入image，即 $x^{*}=\\arg \\max _{x} a^{k}$ ，(method :gradient descent). 部分结果如下图： (每一张图都代表一个让filter激活程度最大的 $x$) 上图中，找到使filter激活程度最大的image，即上图中每个filter可以检测一定的条纹，只有当图像中有该条纹，filter（一组neuron）的激活程度（即输出）才能达到最大。 Neuron（Hidden layer）这里的neuron指前馈神经网络中的neuron，如下图的 $a_j$ : 目标：找到使neuron的输出最大的输入image，即： $x^{*}=\\arg \\max _{x} a^{j}$ . 部分结果如下： （每一张图代表一个neuron) 在上图中，感觉输入像一个什么东西吧emmmm。 但和filter学到的相比，neuron学到的不仅是图中的小小的pattern（比如条纹、鸟喙等），neuron学的是看整张图像什么。 Output（Output layer）再用同样的方法，看看输出层的neuron学到了什么，如下图的 $y_i$ ： 在手写数字辨识中 $y_i$ 是数字为 $i$ 的概率，因此目标是：找到一个使输出是数字 $i$ 概率最大的输入image，即： $x^{*}=\\arg \\max _{x} y^{i}$ . 结果如下图： 结果和我们期望相差甚远，根本不能辨别以上图片是某个数字。 这其实也是DNN的一个特点: Deep Neural Networks are Easily Fooled [1]，即NN学到的东西往往和人类学到的东西是不一样的。 CNN所以CNN到底学到了什么？ 上文中，output 学到的都是一团密密麻麻杂乱的像素点，根本不像数字。 但是，再考虑手写数字image的特点：图片中应该有少量模式，大片空白部分。 因此目标改进为： $x^{*}=\\arg \\max _{x}\\left(y^{i}+\\sum_{i, j}\\left|x_{i j}\\right|\\right)$ $\\sum_{i, j}\\left|x_{i j}\\right|$ 就像是regularization的限制。 结果如下： （注：图中白色为墨水，黑色为空白） ApplicationDeep DreamCNN exaggerates what it sees. CNN可以夸大图片中他所看到的东西。 比如： 可以把下图 变成下图（emmmm看着有点难受） 附上生成deep dream image的网站[2] . Deep Style[3]Given a photo, make its style like famous paintings. 上图中，用一个CNN学习图中的content，用另一个CNN学习风格图中的style。 再用一个CNN使得输入的图像content像原图，风格像另一张图。 Playing GoCNN 还可以用在下围棋中，如下图，输入是19 * 19的围棋局势（matrix/image），通过CNN，学出下一步应该走哪？ Why CNN playing Go?下围棋满足以下两个property： Some patterns are much smaller than the whole image. （围棋新手，博主只下赢过几次hhh) 如果白棋棋手，看到上图的pattern，上图的白子只有一口气了，被堵住就会被吃掉，那白棋棋手大概率会救那个白子，下在白棋的下方。 Alpha Go uese 5 * 5 for first layer. The same patterns appear in different regions. 但如何解释CNN的另一结构——Max Pooling？ 因为围棋的棋谱matrix不像image的pixel，subsample后，围棋的棋谱就和原棋谱完全不像了。 Alpha Go的论文中：Alpha Go并没有用Max Pooling。 所以，可以根据要训练的东西调整CNN模型。 Speech可以用CNN学习Spectrogram ，即识别出这一时段说的是什么话。 TextCNN还可以用在文本的情感分析中，对句子中每个word embedding后，通过CNN，学习sentence表达的是negative 还是positive还是neutral的情绪。 More（挖坑…生命很漫长，学无止境QAQ） The methods of visualization in these slides： https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html More about visualization： http://cs231n.github.io/understanding-cnn/ Very cool CNN visualization toolkit http://yosinski.com/deepvis http://scs.ryerson.ca/~aharley/vis/conv/ The 9 Deep Learning Papers You Need To Know About https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html How to let machine draw an image PixelRNN https://arxiv.org/abs/1601.06759 Variation Autoencoder (VAE) https://arxiv.org/abs/1312.6114 Generative Adversarial Network (GAN) http://arxiv.org/abs/1406.2661 Reference Deep Neural Networks are Easily Fooled： https://www.youtube.com/watch?v=M2IebCN9Ht4 deep dream generator: http://deepdreamgenerator.com/ A Neural Algorithm of Artistic Style: https://arxiv.org/abs/1508.06576","link":"/2020/04/25/CNN/"},{"title":"「机器学习-李宏毅」:Classification-Logistic Regression","text":"在上篇文章中，讲解了怎么用Generative Model做分类问题。这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。文章主要有两部分：第一部分讲解了Logistic Regression的三个步骤。第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。 Logistic RegressionStep1: Function Set在文章末尾，我们得出 $P_{w, b}\\left(C_{1} | x\\right)=\\sigma(w\\cdot x+b)$ 的形式，想跳过找 $\\mu_1,\\mu_2,\\Sigma$ 的过程，直接找 $w,b$ 。 因此Function Set: $f_{w, b}(x)=P_{w, b}\\left(C_{1} | x\\right)$ 。值大于0.5，则属于C1类，否则属于C2类。 Step2: Goodness of a Function使用极大似然的思想（在前一篇机率模型/生成模型中有讲） 估计函数是 ：$L(w, b)=f_{w, b}\\left(x^{1}\\right) f_{w, b}\\left(x^{2}\\right)\\left(1-f_{w, b}\\left(x^{3}\\right)\\right) \\cdots f_{w, b}\\left(x^{N}\\right)$ 目标： $ w^{*}, b^{*}=\\arg \\max _{w, b} L(w, b)$ 由于在之前的Regression中，我们都是找极小值点，为了方便处理，将估计函数转换为如下形式的损失函数： $$ \\begin{equation} \\begin{aligned} -\\ln L(w, b)&=-(\\ln f_{w, b}\\left(x^{1}\\right)+\\ln f_{w, b}\\left(x^{2}\\right)+\\ln \\left(1-f_{w, b}\\left(x^{3}\\right)\\right) \\cdots ) \\\\ Loss&=\\sum_{n}-\\left[\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)\\right] \\end{aligned} \\end{equation} $$ 目标 ： $w^{*}, b^{*}=\\arg \\min _{w, b} L(w, b)$ Cross entropy（交叉熵） 关于熵、交叉熵、相对熵（KL散度）的理解：强烈安利 上式中的 $\\left[\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)\\right]$ 其实是两个Bernoulli distribution的交叉熵。 交叉熵是什么？ 简单来说，交叉熵是评估两个distribution 有多接近。所以当这两个Bernoulli 分布的交叉熵为0时，表明这两个分布一模一样。 对于 $\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)$ ： Distribution p: p(x = 1) = $\\hat{y}^n$ ; p( x = 0 ) = 1 - $\\hat{y}^n$ Distribution q: q(x = 1 ) = $f(x^n)$ ; q(x = 0 ) = 1 - $f(x^n)$ 交叉熵 $H(p,q)=-\\Sigma_xp(x)\\ln(q(x))$ p是真实的分布，q是预测的分布。 因此，这个损失函数的表达式其实也是输出分布和target分布的交叉熵，即： $L(f)=\\sum_{n} C\\left(f\\left(x^{n}\\right), \\hat{y}^{n}\\right)$ （ $C\\left(f\\left(x^{n}\\right), \\hat{y}^{n}\\right)=-\\left[\\hat{y}^{n} \\ln f\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f\\left(x^{n}\\right)\\right)\\right]$ ） 和Linear Regression不同，为什么Logistic Regression不用square error，而要使用cross entropy。 在1.4小节会给出解释。 Step3: Find the best function在第三步，同样使用Gradient来寻找最优函数。 推导过程： $\\left.\\frac{-\\ln L(w, b)}{\\partial w_{i}}=\\sum_{n}-\\left[\\hat{y}^{n} \\frac{\\ln f_{w, b}\\left(x^{n}\\right)}{\\partial w_{i}}+\\left(1-\\hat{y}^{n}\\right) \\frac{\\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right.}{\\partial w_{i}}\\right)\\right]$ $\\frac{\\partial \\ln f_{w, b}(x)}{\\partial w_{i}}=\\frac{\\operatorname{\\partial\\ln} f_{w, b}(x)}{\\partial z} \\frac{\\partial z}{\\partial w_{i}}$ $\\frac{\\partial \\ln \\sigma(z)}{\\partial z}=\\frac{1}{\\sigma(z)} \\frac{\\partial \\sigma(z)}{\\partial z}=\\frac{1}{\\sigma(z)} \\sigma(z)(1-\\sigma(z))$ $\\frac{\\partial z}{\\partial w_{i}}=x_{i}$ $\\frac{\\partial \\ln \\left(1-f_{w, b}(x)\\right)}{\\partial w_{i}}=\\frac{\\operatorname{\\partial\\ln}\\left(1-f_{w, b}(x)\\right)}{\\partial z} \\frac{\\partial z}{\\partial w_{i}}$ $\\frac{\\partial \\ln (1-\\sigma(z))}{\\partial z}=-\\frac{1}{1-\\sigma(z)} \\frac{\\partial \\sigma(z)}{\\partial z}=-\\frac{1}{1-\\partial(z)} \\sigma(z)(1-\\sigma(z))$ $\\frac{\\partial z}{\\partial w_{i}}=x_{i}$ $\\frac{\\partial \\sigma(z)}{\\partial z}=\\sigma(z)\\cdot(1-\\sigma(z))$ 注：$f_{w, b}(x)=\\sigma(z)$ ; $z=w \\cdot x+b=\\sum_{i} w_{i} x_{i}+b$ $$ \\begin{equation} \\begin{aligned} \\frac{-\\ln L(w, b)}{\\partial w_{i}}&=\\sum_{n}-\\left[\\hat{y}^{n}\\left(1-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n}-\\left(1-\\hat{y}^{n}\\right) f_{w, b}\\left(x^{n}\\right) x_{i}^{n}\\right] \\\\&=\\sum_{n}-\\left(\\hat{y}^{n}-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n} \\end{aligned} \\end{equation} $$ 因此Logistic Regression的损失函数的导数和Linear Regression的一样。 迭代更新： $w_{i} \\leftarrow w_{i}-\\eta \\sum_{n}-\\left(\\hat{y}^{n}-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n}$ 与Linear Regression 的对比如图所示。 If : Logistic + Square Error前面一小节我们提到，在Logistic Regression中使用cross entropy判别一个函数的好坏,那为什么不使用square error来judge the goodness？ 如果使用 Square Error的方法，步骤如下： 来看Step 3: 损失函数的导数是 $2\\left(f_{w, b}(x)-\\hat{y}\\right) f_{w, b}(x)\\left(1-f_{w, b}(x)\\right) x_{i}$ 考虑 $\\hat{y}^n=1$ （即我们的target是1）： 如果 $f_{w,b}(x^n)=1$ , 即预测值接近 target, 算出来的 $\\partial{L}/\\partial{w_i}=0$ 是期望的。 如果 $f_{w,b}(x^n)=0$ , 即预测值原理 target, 算出来的 $\\partial{L}/\\partial{w_i}=0$ 是不期望的。 同理，当考虑 $\\hat{y}^n=0$ 情况时，也是如此。 更直观的看： 上图中，画出了两种损失函数的平面，中心的最低点是我们的target。 但在Square Error中，远离target的蓝色点，也处在很平坦的位置，其导数小，参数的更新会很慢。 因此在Cross Entropy中，离target越远，其导数更大，更新更快。 所以Cross Entropy的效果比Square Error更快，效果更好。 Discriminative V.S. Generative这篇文章中的Logistic Regression是Discriminative Model。 上篇文章中Classification是Generative Model。 有什么区别呢？ 上图中，Generative Model做了假设（脑补），假设它是 Gaussian Distribution，假设它是Bernoulli Distribution。然后去找这些分布的参数，在求出 $w,b$。 而在Discriminative Model中，没有做任何假设，直接找 $w,b$ 参数。 所以，这两种Model经过training找出来的参数一样吗？ 答案是不一样的。 The same model(function set), but different function is selected by the same training data. 在上篇Pokemon的例子中，比较两种方法的结果差异。 可见，在Pokemon的例子总，Discriminative的效果比Generative的效果好一些。 但是Generative Model就不好吗？ Benefit of generative model With the assumption of probability distribution, less training data is needed. 【训练生成模型所需数据更少】 With the assumption of probability distribution, more robust to the noise. 【生成模型对noise data更兼容】 Priors and class-dependent probabilities can be estimated from different sources. 【生成模型中的 先验概率Priors 和 基于类别的分布概率不同】 比如，做语音辨识系统，整个系统是generative的。 因为Prior（某一句话的概率）并不需要从data中知道，可以直接在网络上爬虫统计。 而class-dependent probabilities（这段语音是这句话的概率）需要data进行训练才能得知。 Multi-class classificationsoftmax 假设有三个类别：C1、C2、C3 。模型已经得到，参数分别是 w、b。 对于输入x, 判断x属于哪一个类别。 通过每个类别的 w、b求出 $z^i=w^i\\cdot x+b_i$ Softmax的步骤： exponential：每个z值得到 $=e^z$ . sum：将指数化后的值加起来$=\\Sigma_{j=1}^3e^{z_j}$ output: 每个类别的输出 $y_i=e^{z_1}/\\Sigma_{j=1}^3e^{z_j}$ ，即x属于类别i的概率。 求出的 $1&gt;y_i&gt;0$ 且 $\\Sigma_iy_i=1$ 。 通过Softmax，得到 $y_i=P(C_i|x)$ 。 Steps（手写笔记，略倾斜，原来不切一切还不知道自己歪的这么厉害 泪） Step 1: Step 2: Step 3: 使用Stochastic Gradient（即每个样本更新一次）的话： data: [x, $\\hat{y}$ ] , $\\hat{y}_i=1$ 更新 $w^j$ : $j=i$ : $w^j \\leftarrow w^j-\\eta\\cdot (y_i-1)\\cdot x$ $j\\neq i$ : $w^j \\leftarrow w^j-\\eta\\cdot y_i\\cdot x$ (下次一定，笔记写直一点！) 更为规范的推导见[1] Limitation of Logistic Regression 对于如上情况，Logistic Regression并不能进行分类，因为他的boundary 应该是线性的。 Feature Transforming如果对feature做转换后，就可以用Logistic Regression处理。 重定义feature， $x_1’$ :定义为到[0,0]的距离， $x_2’$ :定义为到[1,1]的距离。 于是图变成下图，即可用Logistic Regression进行分类。 但这样的做法，就不像人工智能了，因为Feature Transformation需要人来设计，而且较难设计。 Cascading logistic regression models另一种做法是，将logistic regression连接起来。 上图中，左边部分的两个logistic regression就相当于在做Feature Transformation，右边部分相当于在做Classification。 而通过这种形式，将多个model连接起来，也就是大热的Neural Network。 Reference Multi-class Classification推导：Bishop，P209-210 关于Entropy, Cross Entropy, KL-Divergence的理解：强烈安利：https://www.youtube.com/watch?v=ErfnhcEV1O8","link":"/2020/04/01/Classification2/"},{"title":"「机器学习-李宏毅」:Classification-Generative Model","text":"Classification 有Generative Model和Discriminative Model。这篇文章主要讲述了用生成模型来做分类的原理及过程。 What is Classification?分类是什么呢？分类可以应用到哪些场景呢？ Credit Scoring【贷款评估】 Input: income, savings, profession, age, past financial history …… Output: accept or refuse Medical Diagnosis【医疗诊断】 Input: current symptoms, age, gender, past medical history …… Output: which kind of diseases Handwritten character recognition【手写数字辨别】 Input： Output：金 Face recognition 【人脸识别】 Input: image of a face output: person Classification：Example Application【图】 如上图，Pokemon又来啦！ Pokemon有很多属性，比如皮卡丘是电属性，杰尼龟是水属性之类。 关于Pokemon的Classification：Predict the “type” of Pokemon based on the information Input：Information of Pokemon (数值化） Output：the type Training Data: ID在前400的Pokemon Testing Data: ID在400后的Pokemon Classification as Regression?1. 简化问题，只考虑二分类：Class 1 ， Class2。 如果把分类问题当作回归问题，把类别数值化。 在Training中： Class 1 means the target is 1; Class 2 means the target is -1. 在Testing中：如果Regression的函数值接近1，说明是class 1；如果函数值接近-1，说明是class 2. Regression：输入信息只考虑两个特征。 Model：$y=w_1x_1+w_2x_2+b$ 当Training data的分布如上图所示时，得到的（最优函数）分界线感觉很合理。 但当Training data在右下角也有分布时（如右图），训练中为了减少error，训练得到的分界线会变成紫色的那一条。 所以，如果用Regression来做Classification：Penalize to the examples that are “too correct” .[1] 训练中会因为惩罚一些“过于正确”（即和我们假定的target离太远）的example，得到的最优函数反而have bad performance. 2. 此外，如果用Regression来考虑多分类。 Multiple class: Class 1 means the target is 1; Class 2 means the target is 2; Class 3 means the target is 3…… 如果用上面这种假设，可以认为Class 3和Class 2 的关系更近，和Class 1的关系更远一些。但实际中，可能这些类别have no relation。 Classification: Ideal Alternatives在上面，我们假设二元分类每一个类别都有一个target，结果不尽人意。 如上图所示，将模型改为以上形式，也可以解决分类问题。（挖坑）[2] Generative Model(生层模型)Estimate the Probabilities用概率的知识来考虑分类这个问题，如下图所示，有两个两个类别，C1和C2。 在Testing中，如果任给一个x，属于C1的概率是（贝叶斯公式） $$ P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} $$ 所以在Training中知道这些： $P(C_1),P(x|C_1),P(C_2),P(x|C_2)$ P(C1)和P(C2)很容易得知。 而P(x|C1)和P(x|C2)的概率应该如何得知呢？ 如果能假设：类别是C1中的变量x服从某种分布，如高斯分布等，即可以得到任意P(x|C1)的值。 所以Generative Model：是对examples假设一个分布模型，在training中调节分布模型的参数，使得examples出现的概率最大。（极大似然的思想） Prior Probabilities（先验概率）先只考虑Water和Normal两个类别。 先验概率：即通过过去资料分析得到的概率。 在Pokemon的例子中，Training Data是ID&lt;400的水属性和一般属性的Pokemon信息。 Training Data：79 Water，61 Normal。 得到的先验概率 P(C1)=79/(79+61)=0.56, P(C2)=61/(79+61)=0.44。 Probability from Class先只考虑Defense和SP Defense这两个feature。 如果不考虑生成分布模型，在testing中直接计算P(x|Water)的概率，如下图右下角的那只龟龟，在training data中没有出现过，那值为0吗？显然不对。 假设：上图中water type的examples是从Gaussian distribution（高斯分布）中取样出来的。 因此在training中通过training data得到最优的Gaussian distribution的参数，计算样本中没有出现过的P(x|Water)也就迎刃而解了。 Gaussian Distribution多维的高斯分布（高斯分布就是正态分布啦）的联合概率密度： $$ f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\} $$ D: 维数 $\\mu$ : mean $\\Sigma$ :covariance matrix(协方差矩阵) 协方差： $ cov(X,Y)=E[[X-E(X)][Y-E(Y)]]=E(XY)-E(X)E(Y)$ 具体协方差性质，查阅概率论课本吧。 x: vector,n维随机变量 高斯分布的性质只和 $\\mu$ 和 $\\Sigma$ 有关。 $\\Sigma$ 一定时，$\\mu$ 不同，如下图： $\\mu$ 一定， $\\Sigma$ 不同时，如下图： Maximum Likelihood（极大似然）样本分布如下图所示，假设这些样本是从Gaussian distribution中取样，那如何在训练中得到高斯分布的 $\\mu$ 和 $\\Sigma$ 呢？ 极大似然估计。 考虑Water，有79个样本，估计函数 $L(\\mu, \\Sigma)=f_{\\mu, \\Sigma}\\left(x^{1}\\right) f_{\\mu, \\Sigma}\\left(x^{2}\\right) f_{\\mu, \\Sigma}\\left(x^{3}\\right) \\ldots \\ldots f_{\\mu, \\Sigma}\\left(x^{79}\\right)$ 极大似然估计，即找到 $f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\}$ 中的 $\\mu$ 和$\\Sigma$ 使得估计函数最大（使得取出这些样本的概率最大化）。 $\\mu^{*}, \\Sigma^{*}=\\arg \\max _{\\mu, \\Sigma} L(\\mu, \\Sigma)$ 求导计算（过于复杂，但也不是不能做是吧） 背公式[3] $\\mu^{*}=\\frac{1}{79} \\sum_{n=1}^{79} x^{n} \\qquad \\Sigma^{*}=\\frac{1}{79} \\sum_{n=1}^{79}\\left(x^{n}-\\mu^{*}\\right)\\left(x^{n}-\\mu^{*}\\right)^{T}$ 得到Water和Normal的高斯分布，如下图: Do Classification: different $\\Sigma$TestingTesting： $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ P(x|C1)由训练得出的Water的高斯分布计算出，P(x|C2)由Normal的高斯分布计算出。（如下图，过于难打） 如果P(C1|x)&gt;0.5，说明x 属于Water(Class 1)。 Results如果只考虑两个feature（Defense和SP Defense），下图是testing data的样本图，蓝色属于Water，红色属于Normal。 用训练得出的模型，Testing Data: 47% accuracy。（结果如下图） 如果考虑全部features(7个)，重新训练出的模型，结果：Testing Data：54% accuracy。（结果如下图） 结果并不好。参数过多，模型过于复杂，有些过拟合了。 Modifying Model：same $\\Sigma$模型中的参数有两个的Gaussian Distribution中的 $\\mu^$ 和 $\\Sigma^$ ，其中协方差矩阵的大小等于feature的平方，所以让不同的class share 同一个 $\\Sigma$ ，以此来减少参数，简化模型。 极大似然估计的估计函数： $$ L\\left(\\mu^{1}, \\mu^{2}, \\Sigma\\right)=f_{\\mu^{1}, \\Sigma}\\left(x^{1}\\right) f_{\\mu^{1}, \\Sigma}\\left(x^{2}\\right) \\cdots f_{\\mu^{1}, \\Sigma}\\left(x^{79}\\right)\\times f_{\\mu^{2}, \\Sigma}\\left(x^{80}\\right) f_{\\mu^{2}, \\Sigma}\\left(x^{81}\\right) \\cdots f_{\\mu^{2}, \\Sigma}\\left(x^{140}\\right) $$ 公式推导:[3] $\\mu$ 的公式不变。 $\\Sigma=\\frac{79}{140} \\Sigma^{1}+\\frac{61}{140} \\Sigma^{2}$ ,即是原 $\\Sigma^1\\ \\Sigma^2$的加权平均。 Results当只考虑两个features，用同样的协方差参数，结果如下图： 可以发现，用了同样的协方差矩阵参数后，边界变成了线性的，所以这也是一个线性模型。 再考虑7个features，用同样的协方差矩阵参数，模型也是线性模型，但由于在高维空间，人无法直接画出其boundary，这也是机器学习的魅力所在，能解决一些人无法解决的问题。 结果：从之前的54% accuracy增加到 73% accurancy. 结果明显变好了。 SummaryThree Steps： Function Set（Model）： Goodness of a function: The mean µ and convariance $\\Sigma$ that maximizing the likelihood(the probability of generating data) Find the best function:easy(公式) Appendix为什么要选择Gaussian DistributionYou can always use the distribution you like. 可以选择你喜欢的任意分布，t分布，开方分布等。 （老师说：如果我选择其他分布，你也会问这个问题，哈哈哈） Naive Bayes ClassifierIf you assume all the dimensions are independent, then you are using Naive Bayes Classifier. 如果假设features之间互相独立， $P\\left(x | C_{1}\\right)=P\\left(x_{1} | C_{1}\\right) P\\left(x_{2} | C_{1}\\right) \\quad \\ldots \\ldots \\quad P\\left(x_{k} | C_{1}\\right) $ 。 xi是x第i维度的feature。 对于每一个 P(xi|C1)，可以假设其服从一维高斯分布。如果是binary features（即feature取值只有两个），也可以假设它服从Bernoulli distribution(贝努利分布)。 Posterior Probability（后验概率）Posterior Probability后验概率，即使用贝叶斯公式，已知结果，寻找最优可能导致它发生的原因。 对 $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ 进行处理。 得到： $$ \\begin{equation} \\begin{aligned} P\\left(C_{1} | x\\right)&=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\\\ &=\\frac{1}{1+\\frac{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}}\\\\&=\\frac{1}{1+\\exp (-z)} =\\sigma(z) \\qquad(z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} \\end{aligned} \\end{equation} $$ Worning of Math $z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}=\\ln \\frac{P\\left(x | C_{1}\\right)}{P\\left(x | C_{2}\\right)}+\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}$ $\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}=\\frac{\\frac{N_{1}}{N_{1}+N_{2}}}{\\frac{N_{2}}{N_{1}+N_{2}}}=\\frac{N_{1}}{N_{2}}$ $P\\left(x | C_{1}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma 1|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)\\right\\}$ $P\\left(x | C_{2}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{\\left|\\Sigma^{2}\\right| 1 / 2} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right\\}$ $\\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2}\\left[\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)-\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right]$ $\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)=x^{T}\\left(\\Sigma^{1}\\right)^{-1} x-2\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1}$ $\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)=x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-2\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}$ $\\begin{aligned} z=& \\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2} x^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1} \\\\ &+\\frac{1}{2} x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} \\end{aligned}$ 简化模型后， $\\Sigma^1=\\Sigma^2=\\Sigma$ : $$ z=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} $$ 令 $w^T=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} \\qquad b=-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}}$ 当简化模型后，z是线性的，这也是为什么在之前的结果中边界是线性的原因。 最后模型变成这样： $P\\left(C_{1} | x\\right)=\\sigma(w \\cdot x+b)$ . 在生成模型中，我们先估计出 $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ 的值，也就得到了 $w\\ b$ 的值。 那，我们能不能跳过 $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ ，直接估计 $w\\ b$ 呢？ 在下一篇博客[4]中会继续Classification。 Reference Classification as Regression: Bishop, P186. 挖坑：Classification：Perceptron，SVM. Maximum likelihood solution：Bishop chapter4.2.2","link":"/2020/03/21/Classification1/"},{"title":"「机器学习-李宏毅」：Deep Learning-Introduction","text":"这篇文章中，介绍了Deep Learning的一般步骤。 Up and downs of Deep Learning 1958: Perceptron (linear model) 1969: Perceptron has limitation 1980s: Multi-layer perceptron ​ Do not have significant difference from DNN today 1986: Backpropagation ​ Usually more than 3 hidden layers is not helpful 1989: 1 hidden layer is “good enough”, why deep? 2006: RBM initialization (breakthrough) 2009: GPU 2011: Start to be popular in speech recognition【语音辨识】 2012: win ILSVRC image competition 【图像识别】 Step 1: Neural Network在将Regression 和 Classification时，Step 1 是确定一个function set。 在Deep Learning中，也是相同的，只是这里的function set就是一个neural network的结构。 上图中，一个Neuron就是如上图所示的一个unit，neuron之间不同的连接方式构成不同的Neural Network。 Fully Connect Feedforward Network 这是一个Fully Connect Feedforward Network【全连接反馈网络】，其中每个neuron的activation function都是一个sigmod函数。 为什么说neural network其实就是一个function呢？上面两张图中，输入是一个vector，输出也是一个vector，可以用下面函数来表示。 $$ f\\left(\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]\\right)=\\left[\\begin{array}{c}0.62 \\\\ 0.83\\end{array}\\right] f\\left(\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right]\\right)=\\left[\\begin{array}{l}0.51 \\\\ 0.85\\end{array}\\right] $$ 上图为全连接网络的一般形式，第一层是Input Layer，最后一层是Output Layer，中间的其他层称为Hidden Layer。 而Deep Learning中的Deep的含义就是Many hidden layers的意思。 Matrix Operation 上图的全连接网络中，第一个hidden layer的输出可以写成矩阵和向量的形式： $$ \\sigma\\left(\\left[\\begin{array}{cc}1 & -2 \\\\ -1 & 1\\end{array}\\right]\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]+\\left[\\begin{array}{c}1 \\\\ 0\\end{array}\\right]\\right)=\\left[\\begin{array}{c}0.98 \\\\ 0.12\\end{array}\\right] $$ 更为一般的公式，用W表示权重，b代表bias，a表示hidden layer的输出。输出vector y可以写成 $y = f(x)$ 的形式，即： $y= f(x)=$ 转换为矩阵运算的形式，就可以使用并行计算的硬件技术（GPU）来加速矩阵运算，这也是为什么用GPU来训练Neural Network 更快的原因。 Output Layer在 Logistic Regression中第4节讲到Logistic Regression有局限，消除局限的一种方法是Feature Transformation。 但是Feature Transformation需要人工设计，不太“机器学习”。 在下图全连接图中，把Output Layer换成一个Multi-class Classifier（SoftMax），而其中Hidden Layers的作用就是Feature extractor，从feature x提取出新的feature，也就是 output layer的输入。 这样就不需要人工设计Feature Transformation/Feature engineering，可以让机器自己学习：如何将原来的feature转换为更好分类的feature。 Handwriting Digit Recognition 在手写数字辨别中，输出是一个16*16的image（256维的vector），输出是一个10维的vector，每一维表示是该image是某个数字的概率。 在手写数字辨别中，需要设计neural network的结构来提取输入的256维feature。 Step 2: Goodness of function之前我们已经使用过的最小二乘法和交叉熵作为损失函数。 一般在Neural Network中，使用output vector 和target vector的交叉熵作为Loss。 Step 3: Pick the best function在NN中，也使用Gradient Descent。 但是，Deep Neural Network中，参数太多了，计算结构也很复杂。 Backpropagation：an efficient way to compute $\\partial{L}/\\partial{w}$ in neural network. Backpropagation本质也是Gradient Descent，只是一种更高效进行Gradient Descent的算法。 在很多 toolkit（TensorFlow，PyTorch ，Caffe等）中都实现了Backpropgation。 Backpropagation部分，见下一篇博客。 Reference","link":"/2020/04/18/DL-introdunction/"},{"title":"「Cryptography-Dan」：Introduction","text":"本系列是学习Dan Boneh教授的Online Cryptography Course。 这是Dan教授的第一讲：对密码学的一些Introduction。 What is cryptography?Crypto core：安全通信 Secret key establishment (密钥的建立)： Alice 和 Bob 会得到一个shared secret key，而且Alice 知道她是在和Bob通信，Bob也知道他是在和Alice通信。而attacker不能从通信中获取key。 Secure communicati （安全通信）： 在通信中，Alice、Bob用k将信息加密，保证了通信的confidentiality（机密性）；同时attacker也无法篡改通信的信息，保证了通信的integrity（完整性）。 Crypto can do much more密码学除了能保证安全通信，密码学还能做很多其他的事。 Digital signature &amp; Anonymous Digital signatures（数字签名）： 现实中，人们对不同的文档进行签名，虽然是不同的文档，但是签名的字是相同的。 如果这应用在网络的文档签名中，这将是很危险的。攻击者只需要将签名进行复制、粘贴，就可以将你的签名签在你并不想签的文档中。 数字签名的主要思想：数字签名其实是代签内容的函数值，所以如果攻击者只是复制数字签名（原签名的函数值），那么攻击者得到的数字签名也是无效的（函数值不同）。 在后面的课程系列中会详细讲这部分的内容。[1] Anonymous communication（匿名通信）： 匿名通信的实现，有Mix network （wiki详细介绍）协议，这是一种路由协议，通过使用混合的代理服务器链来实现难以追踪的通信。 通过这些代理的不断加密解密可以实现： Bob不知道与之通信的是Alice。 代理也不知道是Alice和Bob在通信。 双向通信：虽然Bob不知与之通信的是Alice，但也能respond。 Anonymous digital cash（匿名数字现金）： 现实中，我们可以去超市花掉一元钱，而超市不知道我是谁。 在网络中，如果Alice想去网上商店花掉数字现金一元钱，网上商店可以不知道是谁花掉的这一元钱吗？ 这就是匿名数字现金需要解决的问题： 可以在匿名的情况下花掉数字现金吗？ 如果可以，当Alice将这一元钱复制多次（数字现金都是数据串），得到了三元钱，再去把它花掉，由于匿名的原因，没人知道是谁花掉的这三元钱，商店找不到责任人。 这是匿名数字现金需要解决的第二个问题： 如何防止 double spending情况的发生？ 可以用这样的机制去实现匿名数字现金：当Alice花费这一块 once时，系统保证Alice的匿名性；但当Alice花费这一块 more than once ,系统立即揭露Alice的全部信息。 Protocols在介绍什么是Protocols之前，先介绍两种应用场景。 Elections 有5个人要进行投票选举0和1号候选人，但是需要保证：每个人除了知道自己的投票结果，互相不知道其他人的投票情况。在这种情况下怎么知道最后的winner是谁吗？ 如上图，可以引入一个第三方——election center，第三方验证每一个人只能投一次，最后统计票数决策出最后的winner。 Private auctions 介绍一种拍卖机制，Vickery auction：对一个拍卖品，每个投标者在不知道其他人投标价格的情况下进行投标，最后的acution winner： highest bidder &amp; pays 2nd highers bid。即是标价最高者得标，但他只需要付第二高的标价。 所以public知道的信息只有：中标者和第二高投标者的标价。 需要实现这种机制，也可以引入一个第三方——auction center。 但是引入第三方真的安全吗？安全第三方也不安全。 再看上面那个Election的例子，如果把上面四个人的投票情况作为输入，第三方的任务其实是输出一个函数 $f(x_1,x_2,x_3,x_4)$ 而不公开其他信息。 因为安全第三方也许并不安全，所以如果去掉第三方，上面四个人遵从某种协议，相互通信，最后能否得出这个 $f(x_1,x_2,x_3,x_4)$ 这个结果函数，而不透露其投票信息？ 答案是 “Yes”。 有一个惊人的定理：任何能通过第三方做到的事，也能不通过第三方做到。 Thm: anythong that can done with trusted auth. can also be done without. 怎么做到？答案是 Secure multi-party computation（安全多方计算）。 挖坑博文：姚氏百万富翁问题[2] Crypto magic Privately outsourcing computation (安全外包计算) Alice想要在Google服务器查询信息，为了不让别人知道她查询的是什么，她把search query进行加密。 Google服务器接收到加密的查询请求，虽然Google不知道她实际想查询什么信息，但是服务器能根据E[query]返回E[results]。 最后Alice将收到的E[results]解密，得到真正的results。 这就是安全外包计算的简单过程：Encryption、Search、Decryption。 Zero knowl（proof of knowledge) (零知识证明)： Alice 知道p、q(两个1000位的质数)相乘等于N。 Bob只知道N的值，不知道具体的p、q值。 Alice 给 Bob说她能够分解数N，但她不用告诉Bob N的具体因子是什么，只需要证明我能分解N，证明这是我的知识。 最后Bob知道Alice能够分解N，但他不知道怎么分解（也就是不知道N的因子到底是什么）。 A rigorous science在密码学的研究中，通常是这样的步骤： Precisely specify threat model. 准确描述其威胁模型或为达到的目的。比如签名的目的：unforgeable（不可伪造）。 Propose a construction. Prove that breaking construction under threat mode will solve an underlying hard problem. 证明攻击者攻击这个系统必须解决一个很难的问题（大整数分解问题之类的NP问题）。 这样也就证明了这个系统是安全的。 HistorySubstitution cipher（替换）what is it 替换密码很好理解，如上图的这种替换表（key）。 比较historic的替换密码——Caesar Cipher（凯撒密码），凯撒密码是一种替换规则：向后移三位，因此也可以说凯撒密码没有key。 the size of key space用$\\mathcal{K}$ （花体的K）来表示密钥空间。 英语字母的替换密码，易得密钥空间的大小是 $|\\mathcal{K}|=26!\\approx2^{88}$ （即26个字母的全排列）。 这是一个就现在而言也就比较perfect的密钥空间。 但替换密码也很容易被破解。 how to break it问：英语文本中最commom的字母是什么？ 答：“E” 在英语文本（大量）中，每个字母出现的频率并不是均匀分布，我们可以利用一些最common的字母和字母组合来破解替换密码。 Use frequency of English letters. Dan教授统计了标准文献中字母频率： “e”: 12.7% , “t”: 9.1% , “a” : 8.1%. 统计密文中（大量）出现频率最高、次高、第三高的字母，他们的明文也就是e、t、a。 Use frequency of pairs of letters (diagrams).（二合字母） 频率出现较高的二合字母：”he”, “an”, “in” , “th” 也能将h, n,i等破解出。 trigrams（继续使用三合字母） ……直至全部破解 因此substitution cipher是CT only attack！（惟密文攻击：仅凭密文就可以还原出原文） Vigener cipherEncryption 加密过程如上图所示： 密钥是 “CRYPTO”, 长度为6，将密钥重复书写直至覆盖整个明文长度。 将密钥的字母和对应的明文相加模26，得到密文。 Decryption解密只需要将密文减去密钥字母，再模26即可。 How to break it破解方法和替换密码类似，思想也是使用字母频率来破解。 这里分两种情况讨论： 第一种：已知密钥长度 破解过程： 将密文按照密钥长度分组，按照图中的话，6个一组。 统计每组的的第一个位置的字母出现频率。 假设密文中第一个位置最common的是”H” 密钥的第一个字母是：”H”-“E”=”C” 统计剩下位置的字母频率，直至完全破解密钥。 第二种：未知密钥长度 未知密钥长度，只需要依次假设密钥长度是1、2、3…，再按照第一种情况破解，直至破解情况合理。 Rotor MachinesRotor: 轴轮。 所以这种密码的加密核心是：输入明文字母，轴轮旋转一定角度，映射为另一字母。 single rotor 早期的是单轴轮，rotor machine的密钥其实是图右中间那个圆圆的可以旋转的柱子。 图左是变动的密钥映射表。 变动过程： 第一次输入A，密文是K。 轴轮旋转一个字母位：看图中E，从最下到最上（一个圈，只相隔一位）。 所以第二次再输入A，密文是E。 …… Most famous ：the Enigma Enigma machine是二战时期纳粹德国使用的加密机器，因此完全破解了Enigma是盟军提前胜利的关键。 左图中可以看出Enigma机器中是有4个轴轮，每个轴轮都有自己的旋转字母位大小，因此密钥空间大小是 $|\\mathcal{K}|=26^4\\approx2^{18}$ (在plugboard中，实际是 $2^{36}$)。 密钥空间很小，放在现在很容易被暴力破解。 plugboard 允许操作员重新配置可变接线，实现两个字母的交换。plugboard比额外的rotor提供了更多的加密强度。 对于Enigma machine的更多的具体介绍可以戳Enigma machine 的wiki链接。 Data Encryption StandardDES：#keys = $2^{56}$ ,block siez = 64bits，一次可以加密8个字母。 Today：AES（2001）、Salsa20（2008）…… 这里只是简单介绍。 Discrete Probability这个segment比较简单，概率论基本完全cover了，这里只讲一些重点。 Randomized algorithms随机算法有两种，一种是Deterministic algorithm（也就是伪随机），另一种是Randomized algorithm。 Deterministic algorithm $ y\\longleftarrow A(m)$ ，这是一个确定的函数，输入映射到唯一输出。 Randomized algorithm $y\\longleftarrow A(m ; r) \\quad \\text { where } r \\stackrel{R}{\\longleftarrow}{0,1}^{n}$ output： $y \\stackrel{R}{\\longleftarrow} A(m)$ ，y is a random variable. $ r \\stackrel{R}\\longleftarrow {0,1}^n $ :意思是r是n位01序列中的任意一个取值。R，random。变量r服从在 ${0,1}^n$ 取值的均匀分布。 由于随机变量r，对于给定m，$A(m;r)$ 是 ${0,1}^n$ 中的一个子集。 所以，对m的加密结果y，也是一个的随机变量，而且，y在 $A(m,r)$ 也是服从均匀分布。 因此，由于r的影响，对于给定m，加密结果不会映射到同一个值。（如上图所示） XORXOR有两种理解：（ $x \\oplus y $ ） 一种是：x,y的bit位相比较，相同则为0，相异为1. 另一种是：x,y的bit位相加 mod2. 异或在密码学中被频繁使用，主要是因为异或有一个重要的性质。 异或的重要性质：有两个在 ${0,1}^n$ （n位01串）取值的随机变量X、Y。X、Y相互独立，X服从任意某种分布，随机变量Y服从均匀分布。那么 $Z=Y\\oplus X$ ，Z在 ${0,1}^n$ 取值，且Z服从均分分布。 Thm: Y a rand. var. over ${0,1}^n$ , X an index. uniform var. on ${0,1}^n$ ​ Then Z := Y $\\oplus$ X is uniform var. on ${0,1}^n$ . Proof： 当n=1 画出联合分布 Pr[ Z=0 ]=Pr[ (x,y)=(0,0)] + Pr[(x,y)=(1,1)]=1/2 每一bit位都服从均匀分布，可以容易得出 Z是服从难过均匀分布。 The birthday paradox（生日悖论）更具体的分析见 Birthday problem 。 问题前提：一个人的生日在365天的任意一天是均匀分布的（实际当然不是，貌似更多集中在9月）。 根据信鸽理论（有N个鸽子，M个隔间，如果N&gt;M，那么一定有一个隔间有两只鸽子），所以367个人中，以100%的概率有两个人的生日相同。但是，当只有70个人时，就有99.9%的概率，其中两人生日相同；当只有23人，这个概率可以达到50%。 其实这并不是一个悖论，只是直觉误导，理性和感性认识的矛盾。当只有一个人，概率为0，当有367人时，为100%，所以我们直觉认为，这是线性增长的，其实不然。 概率论知识： 设事件A：23个人中，有两个人生日相等。 $P\\left(A^{\\prime}\\right)=\\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\frac{362}{365} \\times \\cdots \\times \\frac{343}{365}$ $P\\left(A^{\\prime}\\right)=\\left(\\frac{1}{365}\\right)^{23} \\times(365 \\times 364 \\times 363 \\times \\cdots \\times 343)$ $P\\left(A^{\\prime}\\right) \\approx 0.492703$ $P(A) \\approx 1-0.492703=0.507297 \\quad(50.7297 \\%)$ 推广到一般情况，n个人(n","link":"/2020/03/04/Dan-introduction/"},{"title":"「Tools」：Docker","text":"本篇文章主要分四个部分，首先介绍了Docker是什么：为什么会有Docker技术的出现；虚拟化技术和容器虚拟化技术的区别；Docker的基本组成；Docker的运行为什么会比虚拟机快。 第二个部分主要介绍了Docker的常用命令，包括镜像命令和容器命令，文中还从底层的角度分析Docker镜像。 第三个部分介绍了Docker中的容器数据卷，和如何挂载数据卷。 最后一个部分，简单介绍了Dockerfile文件。 Docker简介Docker 是什么开发和运维之间的环境和配置问题：在我的机器上可以正常工作。 把代码/配置/系统/数据等全部打包成镜像，运维工程师带环境安装软件。 Docker基于Go语言实现的云开源项目，Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，做到一次封装，处处运行。 Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用运行在 Docker 容器上面，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作。 Docker解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体分布的容器虚拟化技术。 能干嘛？之前的虚拟化技术虚拟机是带环境安装的解决方案，可以在一种操作系统中运行另一种操作系统。 虚拟机用软件实现了硬件、内核、操作系统及应用程序，对底层来说，虚拟机就是一个普通文件。 虚拟机的缺点缺点： 资源占用多 冗余步骤 启动慢 容器虚拟化技术Linux容器（Linux Containers,LXC)，对进程隔离，将软件运行所需的资源打包到一个隔离的痛其中。 Linux容器不是模拟一个完整的操作系统，而是将软件工作所需的库资源和设置等资源打包到一个隔离的容器中，因此Linux容器变得高效且轻量，并且能保证部署在任何环境中的软件都能始终如一地运行。在 宿主机上，Linux容器就是一个运行的进程，所以Linux容器是对进程进行隔离。 再看Docker的图标，上面的集装箱就是一个一个容器，鲸鱼就是宿主机的硬件、内核。 比较： 传统虚拟机技术虚拟一套硬件，在其上运行一个完整的操作系统，再运行所需的应用进程。 容器内的应用直接运行于宿主的内核，容器内没有硬件虚拟，容器更轻便。 容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响。 所以，可以认为容器是一个轻量的Linux。 开发/运维（DevOps)DevOps, Develop and Operations, 可以利用Docker实现开发自运维。 更快速的应用交付和部署。 更便捷的升级和扩缩容器。 更简单的系统运维。 更高效的计算资源利用。 Docker的基本组成Docker的三要素： 镜像(image)：只读的模版，类比Java中的类。镜像可以用来创造Docker容器。 容器(container)：镜像的实例，独立运行的一个或一组实例。可以把容器看作一个简易版的Linux环境。 仓库(repository)：保存镜像的场所。 Docker本身是一个容器运行载体或管理引擎。 把应用程序和配置打包成为一个可交付的运行环境，打包好的运行环境就是一个image镜像文件，只有通过这个镜像文件才能生成Docker容器。image文件可以看作是容器的模版。Docker根据image文件生成容器的实例。 Docker运行原理Docker是一个C/S结构的系统。 Docker守护进程运行在宿主机上，客户通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。 为什么比虚拟机快 Docker有比虚拟机更少的抽象层，不需要实现硬件资源虚拟化，运行在docker容器中的程序直接使用的都是实际物理机的硬件资源。 Docker使用宿主机上的内核，新建容器时，不需要和虚拟机一样重新加载一个操作系统内核。因此新建一个dock er容器只需要几秒钟。 Docker镜像加速可以登陆阿里云获得专属镜像加速器链接，配置本机Docker拉取镜像仓库的链接，将拉取镜像的链接从DockerHub换成阿里云的仓库，下载更快捷。 具体按照系统自行Google。 Docker常用命令docker version docker info docker –help 帮助命令 镜像命令 列出本地images docker images repo 参数 -a :包括中间映像层 -q : 只显示镜像id –digests :显示摘要信息 –no-trunc :显示完整信息 从Docker Hub查询镜像名 docker search [OPTIONS] image_name –no-trunc -s n：收藏数不小于n的镜像 –automated 下载/拉取镜像 docker pull 镜像名[:TAG] 默认:latest 删除镜像 docker rmi 镜像唯一名字/镜像ID -f :强制删除运行中的镜像文件 删除单个： docker rmi -f 镜像ID 删除多个 docker rmi -f 镜像名1:TAG 镜像名2:TAG 删除全部： docker rmi -f $(docker images -qa) 容器命令容器是一个建议的Linux。 启动容器： docker run [OPTIONS] IMAGE [COMMAND] [ARG...] --name 容器名 :为容器指定一个名字 -d ：后台运行容器，返回 -i : 以交互模式运行容器，通常与-t 一同使用 -t :为容器重新分配一个伪输入终端，通常与-i 一同使用。 -p :主机端口和容器端口 -p ip:hostPort:containerPort -p ip::containerPort -p hostPort:containerPort -p containerPort -P :随机分配端口 列出当前运行所有容器： docker ps -a : 列出当前所有正在运行的容器和历史上运行过的容器 -l :显示最近创建的容器 -n :显示最近创建的num个容器 docker ps -n 3 -q :静默模式，只显示容器编号 --no-trunc : 不截断输出 退出/停止容器 容器停止退出 exit 容器不停止退出 Ctrl + P + Q 启动容器 docker start 容器名/容器ID 重启容器 docker restart 容器名/容器ID 重启成功后返回容器名/容器ID 停止容器 docker stop 容器名/容器ID 强制停止容器 docker kill 容器名/容器ID 删除已停止的容器 docker rm 镜像ID 一次删除多个容器 docker rm -f $(docker ps -a -q) docker ps -a -q | xargs docker rm （管道传递参数） 启动守护式容器 docker run -d 镜像名/镜像ID docker run -d -p 主机端口:容器内端口 容器ID 如果使用 docker ps -a 查看，会发现容器已经退出 Docker容器后台运行，就必须要有一个前台进程与之交互 如果容器后台运行，如果不是一直挂起的命令，他就会自动退出。 所以最佳的解决方式是将运行的进程以前台进程运行。 查看容器日志 docker logs -f -t --tail 容器ID -t：显示加入时间戳 -f ：持续显示最新的日志 --tail ：显示最后多少条 显示容器内运行的进程 docker top 容器ID 查看容器内部的细节 docker inspect 容器ID 进入正在运行的容器并以命令行与之交互 直接进入容器启动命令的终端 docker attach 容器ID 在容器中打开新的终端，并且可以启动新的进程。 docker exec -it 容器ID bashShell docker exec -it 容器ID /bin/bash 和docker attach 容器ID 相同。 把容器内文件拷贝文件到主机上 docker cp 容器ID:容器内的路径 目录主机路径 docker cp 130b1f6708dd:/x.txt /Users Docker镜像image： 镜像是轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，包含运行某个软件所需的所有内容，包括代码、库、环境变量、配置文件等。 UnionFSUnionFS（联合文件系统）是一种分层、轻量高性能的文件系统，支持对文件系统的修改作为一次提交来一层层的叠加，同时将不同目录挂载到同一个虚拟文件系统下。 Union文件系统时Docker镜像的基础。 镜像通过分层来进行继承，基于基础镜像可以制作各种具体的应用镜像。 特点：一次加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，最终的文件系统包含所有底层的文件和目录。 Docker镜像的加载Docker镜像实际是由一层一层的文件系统组成。 bootfs(boot file system)包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。 Docker镜像的最底层就是bootfs，这一层和典型的Linux/Unix系统是一样的，包含bootloader和kernel。 当boot加载完成后，整个kernel就在内存中了，此时内存的使用权已由bootfs转交给kernel，此时系统也会卸载bootfs。 rootfs（root file system)，在bootfs之上，包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Linux，Centos等。 平常安装等虚拟机的CentOS都是几个G，为什么docker版的centos只有几百兆？ 对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库，因为底层直接使用宿主机的kernel，自己只需要提供rootfs就行了。 因此，对于不同的Linux发行版，bootfs基本一致，rootfs会有差别，因此不同的发行版可以共用bootfs。 分层的镜像在docker image下载、删除时，可以发现是一层一层的。 分层的镜像的一个最大的好处是共享资源。 如果有多个镜像都是从相同的base镜像build而来，那宿主机中只需在磁盘上保存一份base镜像，同时内存中也只需要加载一份base镜像，就可以为所有的容器服务了。 镜像commit操作Docker镜像都是只读的，但当镜像实例化，启动容器时，一个新的可写层被加载到镜像的顶部，这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。 docker commit提交容器层副本使之成为一个新的镜像。 docker commit -m &quot;message&quot; -a &quot;author&quot; 容器ID 命名空间/新建镜像名[:TAGS] 容器数据卷Docker理念： 将代码和运行的环境打包形成容器，运行伴随着容器，但希望运行中的数据是持久化的，希望容器之间是共享数据的。 如果不通过docker commit生成新的镜像，使得数据作为镜像的一部分保存下来，那么容器删除后，数据也没有了，为了保存数据，使用容器数据卷。 如果不使用commit 生成新的镜像，Docker容器产生的数据将随着容器的删除而一起删除，为了保存数据，我们使用卷。 卷卷就是目录或者文件，存在于一个或多个容器中，由docker挂载到容器，但不属于UnionFS（联合文件系统），因此能绕过UnionFS，提供一些用于持续存储或共享数据的特性。 卷的设计目的就是为了数据持久化，完全独立于容器的生存周期，因此Docker不会在容器删除的时候删除其挂载的数据卷。 数据卷的特点： 数据卷可以在容器之间共享或重用数据。 卷中的更改直接在所有共享该卷容器中生效。 数据卷中的更改不会包含在镜像的更新中。 数据卷的生命周期一直持续到没有容器使用它为止。 数据卷挂载直接命令添加 数据挂载(-v value) docker run -it -v /宿主机目录:/容器内目录 镜像名 查看挂载是否成功 docker inspect 镜像名 宿主机和容器之间实现数据共享，在容器停止退出后，修改宿主机数据，数据完全同步。 带权限的数据挂载，加:ro (readonly) docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 此时容器中对数据卷只读。 当挂载主机目录事，Docker访问出现cannot open directory .: Permission denied 解决办法：在挂砸目录后加参数 --privileged=true DockerFile添加在DockerFile中可以使用VOLUME 指令给镜像添加一个或多个数据卷。 注意： Docker出于可移植性和分享的考虑，指令中只有容器内的地址，因为宿主主机目录依赖于特定的主机。 Dockerfile文件构建 1234FROM centosVOLUME [\"/dataVolumeContainer1\", \"/dataVolumeContainer2\", \"/dataVolumeContainer3\"]CMD echo \"finished,-----success\"CMD /bin/bash 以上docker文件类似于一下命令挂载 1docker run -it -v /host1:/dataVolumeContainer1 -v/host1:/dataVolumeContainer2 -v /host3:/dataVolumeContainer3 centos /bin/bash build构建镜像（-f file) docker build -f DockerFile文件路径 -t 命名空间/镜像名 镜像生成路径 docker build -f ./Dockerfile -t fred/centos . 数据卷容器数据容器卷： 命名的容器挂载数据卷，其他容器通过挂载这个父容器实现数据共享，挂载数据卷的容器称为数据卷容器。 容器之间可以传递配置信息，数据卷的生命周期一直持续到没有容器使用它为止。 挂载数据卷到父容器（命名为dc01 ）上：命令添加/Dockerfile添加 容器继承父容器的数据卷(--volumes-from ) docker run -it --name 子容器名 --volumes-from 父容器名 生成子容器的镜像名 e.g: docker run -it --name dc02 --volumes-from dc01 fred/centos dc01已经挂载数据卷，此时dc02继承它，那么dc01挂载的数据卷，dc02也实现了共享。 DockerfileDockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。 构建容器卷的步骤： 编写Dockerfile文件 docker build构建 docker run启动容器 Centos的Dockerfile文件 123456789101112131415FROM scratchADD centos-7.8.2003-x86_64-docker.tar.xz /LABEL \\ org.label-schema.schema-version=\"1.0\" \\ org.label-schema.name=\"CentOS Base Image\" \\ org.label-schema.vendor=\"CentOS\" \\ org.label-schema.license=\"GPLv2\" \\ org.label-schema.build-date=\"20200504\" \\ org.opencontainers.image.title=\"CentOS Base Image\" \\ org.opencontainers.image.vendor=\"CentOS\" \\ org.opencontainers.image.licenses=\"GPL-2.0-only\" \\ org.opencontainers.image.created=\"2020-05-04 00:00:00+01:00\"CMD [\"/bin/bash\"] Dockerfile构建过程基础规则： 保留字指令必须大写，且后面必须至少一个参数。 指令顺序执行。 注释符号：# 每条指令都会创建一个新的镜像层，并对该镜像进行提交。 执行流程： 从基础镜像运行一个容器 执行一条指令后并对容器进行修改 执行类似docker commit操作提交一个新的镜像层 docker再基于刚提交的镜像运行一个容器 直到文件所有指令执行完成 辨析Dockerfile，Docker镜像，Docker容器： Dockerfile、Docker镜像与Docker容器从软件应用的角度分别代表软件的三个不同阶段： Dockerfile是软件的原材料，是面向开发的。 Dockerfile定义了进程需要的一切东西。Dockerfile设计的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程等等。 Docker镜像是软件的交付品，是交付标准。 在用Dockerfile定义一个文件之后，docker build会产生一个Docker镜像，运行 Docker镜像时，才真正开始提供服务。 Docker容器则可以认为是软件的运行态，涉及部署和运维。 Docker容器是直接提供服务的。 Dockfile体系结构 FROM 基础镜像 MAINTAINER 镜像维护者的姓名和邮箱地址 RUN 容器构建时需要运行的命令 EXPOSE 当前容器对外暴露的端口号 WORKDIR 指定在创建容器后，终端默认登陆进来的工作目录 ENV 构建容器中的设置环境变量 ADD 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包 COPY 拷贝文件和目录到镜像中 COPY src dest COPY [&quot;src&quot;, &quot;dest&quot;] VOLUME 容器数据卷 用于数据保存和持久化工作 CMD 指定一个容器启动时运行的命令 shell 格式：CMD &lt;命令&gt; exec格式：CMD[“可执行文件”, “arg1”, “arg2”,…] 参数列表格式：CMD [“arg1”, “arg2”,…] 在指定来ENTRYPOINT指令后，用⌘指定具体的参数。 只有最后一个CMD生效，CMD会被docker run之后的参数替换 ENTRYPOINT 指定一个容器启动时运行的命令 会在docker run后面追加参数 ONBUILD 当构建一个被继承的Dockerfile时，父镜像在被子镜像继承后父镜像的ONBUILD被触发","link":"/2020/07/21/Docker/"},{"title":"「Tools」:Git and GitHub","text":"这篇文章具体讲述了Git工具的基本本地库操作和与远程库交互的基本操作，包括使用GitHub进行团队外的协作开发。 GitGit简介Git历史：1991 Linus本人手动合并代码 2002 商业软件，授予Linux社区免费使用版本控制 2005 Linus自己用C语言开发了一个分布式版本控制系统：Git Talk is cheap, show me the code. 2008 Github上线 Git的优势： 大部分操作在本地完成，不需要联网 完整性保证：每次提交进行哈希 尽可能添加数据而不是删除/修改数据，版本都在 分支操作快捷流畅，以快照的形式 与Linux命令全面兼容 Git的结构 Git和代码托管中心代码托管中心的任务：维护远程库 局域网环境：搭建GitLab作为代码托管中心 外网环境：可以用GitHub和码云作为代码托管中心 本地库和远程库的交互团队内： 团队外： fork：复制一份属于自己的远程库 开发新的内容后向库的拥有者 pull request拉取请求，原拥有者可以审核，审核通过后执行merge操作合并到自己的远程库的分支上。 Git命令行基本操作本地库初始化 初始化本地库 git init .git文件存放的是本地库相关的子目录和文件，不要删除和随意修改。 本地库设置签名 形式： 用户名： Email： 作用：区分不同开发人员的身份 注：这里设置的签名与远程代码托管中心没有关系。 命令： 项目级别：设置签名仅在本地库起效（如果既有项目级别和用户级别的签名，按照项目级别为准） 设置用户名命令：git config user.name *** 设置用户邮箱： git config user.email *****@outlook.com 该信息保存在.git/config文件中。 用户级别：设置签名在当前操作系统的用户范围 设置用户名命令：git config --global user.name *** 设置用户邮箱命令：git config --global user.email **** 该消息保存在系统文件~/.gitconfig文件 查看状态 查看工作区、暂存区状态。 git status 暂存区操作：添加/修改/提交/删除 添加/修改：将工作区的文件添加到暂存区（/或更新暂存区的文件）。 git add [filename] 删除：将文件从暂存区删除 git rm --cached [filename] 提交：将暂存区的文件提交到本地库。（输入提交信息） git commit -m &quot;commit message&quot; [filename] 修改后的提交：提交修改后的文件至本地库（已在暂存区有旧版本），同时更新暂存区和本地库。 git commit -m &quot;message&quot; [filename] 本地库版本信息查看HEAD: 指针，表示当前版本的位置。 显示版本： 完整的版本信息记录（包括完整版本哈希值、作者、提交时间） git log （空格向下翻页；b 向上翻页； q退出显示） 一行只显示一个版本，简洁版。 git log --pretty=oneline 一行也只显示一个版本，终极简洁版，哈希值也只显示前面的一部分（当作该版本的局部索引）。 git log --oneline HEAD@{i}：i表示HEAD指针移动到该版本需要后退的步数。 git reflog 版本前进/后退本质是HEAD指针的移动。 基于索引操作：版本可以后退和前进。(索引就是reflog形式下的局部哈希值) git reset --hard [局部索引值] 使用^ ： 版本只能往后退 。（基于reflog形式下的步数） git reset --hard HEAD^^ (后退两步) 使用~n ：版本往后退n步。 git reset --hard HEAD~3 版本前进/后退reset命令的参数对比： --soft 仅仅在本地库移动HEAD指针。 如下图，显得暂存区和工作区版本比本地库前进了一步。 --mixed 在本地库移动HEAD指针 并重置暂存区，暂存区和本地库一致。 如下图，显得工作区版本比本地库和暂存区版本前进了一步。 --hard 在本地库移动HEAD指针 重置暂存区 重置工作区 删除文件后找回前提：删除前，文件存在的状态提交到了本地库。 操作： 删除的操作已经提交到本地库 删除操作： 123rm a.txtgit add a.txtgit commit -m \"delete a.txt\" a.txt git reset --hard [历史版本指针位置] 删除操作未提交到本地库 删除操作： 12345//工作区删除rm a.txt//缓存区也删除rm a.txtgit add a.txt git reset --hard HEAD 比较文件差异 工作区文件和暂存区文件比较 git diff [filename] 工作区文件和本地库文件比较，指针可以使用HEAD^ git diff [指针] [filename] 可以不加文件名，即比较全部文件。 分支管理分支分支：版本控制过程中，使用多条线同时推进多个任务。 master: 主版本分支/部署到服务器运行的分支。 feature_ ：开发其他功能的分支。 hot_fix: 热修复，bug修复分支。 分支的好处： 并行：同时并行推进多个功能开发。 独立：各个分支在开发过程中，如果有个分支开发失败，不会影响其他分支。 分支操作：创建/查看/切换/合并 创建分支 git branch [branch name] 查看分支 git branch -v 切换分支 git checkout [branch name] 合并分支 切换到接受修改的分支（如master） git checkout [合并到的主分支] 执行merge合并操作 git merge [有修改的分支] 解决合并分支后产生的冲突冲突的表现，显示到有冲突的文件： 冲突解决： 删除文件中的特殊符号 协商再编辑文件 添加新文件 git add [filename] 提交（注意：此时的提交不能带文件名） git commit -m &quot;message&quot; Git 基本原理哈希算法特点： 得到的加密密文长度相同。 算法确定，输入确定后，输出一定确定。 输入数据发生一点点变化，输出的变化会很大。 Git底层采用SHA-1算法。 哈希算法保证了Git的数据完整性。 Git保存版本的机制集中式版本控制工具（如SVN）：保存的信息是每个基本文件和每个文件随时间逐步累积的差异。 Git是分布式的版本控制工具。 Git把数据看作是文件系统的快照（可以理解为当前内存版本的文件的索引），每次提交更新时Git对当前内存的全部文件制作一个快照并保存这个快照的索引。如果文件没有修改，Git不会重新存储该文件，只是保留一个连接指向之前存储的文件。 Git的提交对象： 上图中，每个文件都有一个哈希值/索引，提交时新建一个树结点，其中包含指向每个文件的指针/索引，提交的对象包括该树结点的指针/哈希值。 Git版本对象链条： 所以： Git 分支的创建：等于新建一个指向版本的指针。 Git分支的切换：改变HEAD指针所指的指针。 Git分支版本的移动：分支指针的移动。 GitHub基本交互创建/查看远程库地址别名在GitHub创建远程库后 在本地添加远程库地址别名 git remote add [别名] [https/ssh 地址] git remote add orgin https://... 查看当前所有远程库地址别名 git remote -v 本地库内容推送到远程库前提：本地库已添加远程库地址别名。 在本地将本地库推送到远程分支 git push [别名] [分支名] git push origin master 将远程库克隆到本地库 git clone https/ssh_address 效果：完整把远程库下载到本地；添加origin作为远程库地址别名；初始化本地库（含有.git文件） 团队内协作团队成员邀请项目创建者在项目”Setting”-“Callaborators”里邀请成员。 拉取：同步本地库 在本地pull操作同步本地库与远程库相同。 fetch：查看远程库分支，可以切换至远程库分支，查看远程库分支的文件具体内容，决定是否合并。 git fetch [远程库地址别名] [远程分支名] 切换至远程库分支 git checkout orgin/master merge：（切换至本地库master分支），合并远程库分支。 git merge [远程库地址别名]/[远程分支名] pull = fetch + merge git pull [远程库地址别名] [远程分支名] 注：如果是简单的修改，可以直接pull拉取，如果不确定远程库修改内容，可以先fetch后再合并分支。 本地拉取与远程库冲突 冲突发生原因：不是基于GitHub远程库的最新版进行修改，就不能push，在修改之前必须pull。 pull拉取下来后如果进入冲突状态，就按照“分支冲突解决办法” 跨团队协作 fork操作：复制一份远程库。 团队外的人，在项目节目点fork，即可fork一份远程库，该远程库的来源是创建该库的开发者，而fork出的远程库的所有者是执行fork操作的人。 clone操作：下载到本地库。 push操作：本地修改，推送至远程库。 pull request 请求：在远程库（代码托管中心GitHub）执行pull request请求，请求合并该修改到原远程库。 （原远程库所有者）审核操作：确认是否合并。 SSH登陆 在当前用户的根目录，生产.ssh密钥目录 ssh-keygen -t rsa -C email@address 将.ssh/id_rsa.pub 文件的内容复制到GitHub新建ssh密钥的窗口下。 创建ssh远程地址别名 git remote add origin ssh_address Git仓库和SSH-key关联 ssh-add &quot;id_rsa address Git工作流待补充[1] Gitlab服务器搭建待补充[2] Reference Git工作流待补充 Gitlab服务器搭建","link":"/2020/07/18/Git-and-GitHub/"},{"title":"「LeetCode」：String","text":"LeetCode String 专题记录。 9月毕。 「我祝福你有时有坏运气，你会意识到概率和运气在人生中扮演的角色，并理解你的成功并不完全是你应得的，其他人的失败也并不完全是他们应得的。」 「不想要刚好错过的悔恨，那就要有完全碾压的实力。」 String28-Implement strStr()28-Implement strStr() Problem: 返回第一个字串出现的下标 Solution： Python就暴力匹配。 12345678class Solution: def strStr(self, haystack: str, needle: str) -&gt; int: n1 = len(haystack) n2 = len(needle) for i in range(0, n1-n2+1): if haystack[i:i+n2] == needle: return i return -1 14-Longest Common Prefix14-Longest Common Prefix Problem: 返回串的公共最长前缀。 Solution： 暴力匹配长度就好。 12345678910111213141516from typing import Listclass Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: LCP = 0 n = len(strs) if n == 0: return \"\" while True: for i in range(0, n): if LCP &lt; len(strs[i]) and strs[i][LCP] == strs[0][LCP]: continue else: return strs[0][0: LCP] LCP += 1 58-Length of Last Word58-Length of Last Word Problem: 单词串由字母和空格组成，返回最后一个单词的长度。 Solution： 注意串最后的空格。 12345678910111213class Solution: def lengthOfLastWord(self, s: str) -&gt; int: n = len(s) length = 0 end = n-1 while end-1 &gt;= 0 and s[end] == ' ': end -= 1 for i in range(end, -1, -1): if s[i] == ' ': return length else: length += 1 return length 58-First Unique Character in a StringProblem: 找第一个没有重复出现的字符下标。 Solution： 暴力。 12345678910class Solution: def firstUniqChar(self, s: str) -&gt; int: a_ascii = ord('a') cnt = [0]*(26+5) for i in s: cnt[ord(i)-a_ascii] += 1 for idx in s: if cnt[ord(i)-a_ascii] == 1: return s.index(i) return -1 寻找子串开始索引： str.find(substr, beg=0, end=len(string)) substr: 字串 beg: 开始索引 end: 结束索引，默认字符串长度。 如果字符串不包含子串，则返回-1 str.index(str, beg=0, end=len(string)) 和find差不多，如果不包含子串会抛出异常。 383-Ransom Note383-Ransom Note Problem: 给两个字符串，判断串1的字符能否由串2的字符组成。 Solution： 字典计数。 12345678910111213141516class Solution: def canConstruct(self, ransomNote: str, magazine: str) -&gt; bool: ransomDir = {} magazineDir = {} for ch in ransomNote: ransomDir.setdefault(ch, 0) ransomDir[ch] += 1 for ch in magazine: magazineDir.setdefault(ch, 0) magazineDir[ch] += 1 for (k, v) in ransomDir.items(): if k not in magazineDir: return False if ransomDir[k] &gt; magazineDir[k]: return False return True 初始化字典的值：dic.setdefault(ch, 0) 344-Reverse String344-Reverse String Problem: in-place 反转字符串 with O(1) 的额外空间。 Solution： 前后两个指针交换。 123456789101112131415from typing import Listclass Solution: def reverseString(self, s: List[str]) -&gt; None: \"\"\" Do not return anything, modify s in-place instead. \"\"\" n = len(s) l = 0 r = n-1 while l &lt; r: s[l], s[r] = s[r], s[l] l += 1 r -= 1 151-Reverse Words in a String151-Reverse Words in a String Problem: 反转字符串word by word.(结果中单词间只能有一个空格) Solution： 把单词存入列表，再输出。 1234567891011121314151617class Solution: def reverseWords(self, s: str) -&gt; str: n = len(s) i = 0 li = [] while i &lt; n: while i &lt; n and s[i] == ' ': i += 1 l = i while i &lt; n and s[i] != ' ': i += 1 r = i if r != l: li.append(s[l:r]) ans = ' '.join(li[-1::-1]) return ans Python连接字符串总结 加号连接：'a' + 'b' 逗号连接，只能用于print打印: print(a, b) 直接连接: print('a' 'b') 使用 % 格式化字符串：'%s %s' % ('hello', 'world') format 格式化字符串：'{}{}'.format('hello', 'world') join 内置方法：用字符来连接一个序列，数组或列表等：'-'.join(['aa', 'bb', 'cc']) f-string 方法：aa, bb = 'hello', 'world' , f'{aa} {bb}' * 操作符：字符串乘法。 反转列表：[-1: : -1] 186-Reverse Words in a String II186-Reverse Words in a String II Problem: 反转单词in-places. Solution: 两次反转，第一次整体反转，第二次再单词反转。 （不额外开个数组来逐个赋值AC不了，不知道为啥q w q) 123456789101112131415161718class Solution: def reverseWords(self, s: List[str]) -&gt; None: \"\"\" Do not return anything, modify s in-place instead. \"\"\" temp = s[-1::-1] n = len(temp) i = 0 while i &lt; n: l = i while i &lt; n and temp[i] != ' ': i += 1 r = i temp[l:r] = list(reversed(temp[l:r])) i += 1 for index in range(n): s[index] = temp[index] Python 反转列表的方法： list(reversed(a)) , reversed(a)返回的是迭代器，转换成list。 a[::-1] Python 字符串(str)和列表(list)互相转换： str 转换为 list list() 转换为单个字符列表 str.split() 或者str.split(' ') 空格分割转换 1234567891011121314str1 = \"123\"list1 = list(str1)print list1# ['1', '2', '3']str2 = \"123 sjhid dhi\"list2 = str2.split() #or list2 = str2.split(\" \")print list2# ['123', 'sjhid', 'dhi']str3 = \"www.google.com\"list3 = str3.split(\".\")print list3# ['www', 'google', 'com'] list转换为str: &quot;&quot;.join(list) 无空格连接 &quot;.&quot;.join(list) 345-Reverse Vowels of a String345-Reverse Vowels of a String Problem: 反转字符串中的元音字母。 Solution： 元音字母，包括大写元音字母和小写元音字母。 1234567891011121314151617181920class Solution: def reverseVowels(self, s: str) -&gt; str: n = len(s) s = list(s) dic = {'a': 1, 'e': 1, 'i': 1, 'o': 1, 'u': 1} rev = [0] * n for index in range(n): if s[index] in dic or s[index].lower() in dic: rev[index] = 1 l = 0 r = n - 1 while l &lt; r: while l &lt; r and rev[l] == 0: l += 1 while l &lt; r and rev[r] == 0: r -= 1 s[l], s[r] = s[r], s[l] l += 1 r -= 1 return ''.join(s) Python大小写转换： 所有字符转换为大写：str.upper() 所有字符转换为小写：str.lower() 第一个字母转换为大写字母，其余小写：str.capitalize() 把每个单词的第一个字母转换为大写，其余小写。 12345678910str = \"www.runoob.com\"print(str.upper()) # 把所有字符中的小写字母转换成大写字母print(str.lower()) # 把所有字符中的大写字母转换成小写字母print(str.capitalize()) # 把第一个字母转化为大写字母，其余小写print(str.title()) # 把每个单词的第一个字母转化为大写，其余小写# WWW.RUNOOB.COM# www.runoob.com# Www.runoob.com# Www.Runoob.Com Python中string是不可变对象，不能通过下标的方式（如str[0]='a' )改变字符串。 205-Isomorphic Strings205-Isomorphic Strings Problem: 判断是否同构字符串。 Solution： 字符到字符的映射，必须是单射。 12345678910111213141516class Solution: def isIsomorphic(self, s: str, t: str) -&gt; bool: n = len(s) dic = {} vSet = set() # satisfy single map for idx in range(n): ch = s[idx] # single map if ch not in dic and t[idx] not in vSet: dic[ch] = t[idx] vSet.add(t[idx]) continue if ch in dic and dic[ch] == t[idx]: continue return False return True Python 集合的操作： 创建空集合：set() 创建有初值的集合：SET = {v0, v1, v2} 或者SET = set(v0) 判断元素是否在集合中：x in SET 集合运算： a-b :属于a集合不属于b集合 a|b :属于a集合或属于b集合 a&amp;b :集合a和集合b都包含的元素 a^b : 不同时包含于集合a和集合b的元素 集合中添加元素：s.add(x) 集合中添加元素，且参数可以是列表、元组、字典(是每个元素都添加进去）等：s.update(x) 移除元素：s.remove(x) ，如果元素不存在，则会发生错误。 移除元素：s.discard(x) ，如果元素不存在，不会发生错误。 随机删除集合中的一个元素：s.pop() （原理：对集合无序排序，然后删除无序排列集合的第一个） 计算集合元素的个数：len(s) 清空集合s.clear() List Comprehension &amp;&amp; Set Comprehension &amp;&amp; Dictionary Comprehension 这个相当于数学中的 $S={2\\cdot x\\mid x\\in \\left[0,9\\right)}$ 的表达。 List Comprehension 如果用数学中的这个表达来看下面的式子，就很显而易见了。 1arr = [i for i in range(10)] 再看看加了其他限制的例子 12345678# filter the elementsarr1 = [x for x in arr if x % 2==0]# add more conditionsarr2 = [x**2 for x in arr if x &gt;= 3 and x % 2]# use nested for loopsarr3 = [(x, y) for x in range(3) for y in range(4)] 使用List Comprehension不仅优美，而且效率也会很高。 Set Comprehension 同样的 1s = {x for x in range(100) if x%2 != 0 and x%3 != 0} Dictionary Comprehension Syntax：{expression(variable): expression(variable) for variable, variable in input_set [predicate][, …]} 1234567891011121314# [(set_k), (set_v)]&gt;&gt;&gt; {k: v for k, v in [(1, 2), (3, 4)]}{1: 2, 3: 4}&gt;&gt;&gt; {n: n for n in range(2)}{0: 0, 1: 1}&gt;&gt;&gt; {chr(n): n for n in (65, 66, 66)}{'A': 65, 'B': 66}# ((k1, v1), (k2, v2))&gt;&gt;&gt; {k: v for k, v in (('I', 1), ('II', 2))}{'I': 1, 'II': 2}&gt;&gt;&gt; {k: v for k, v in (('a', 0), ('b', 1)) if v == 1}{'b': 1} 68-Text Justification68-Text Justification Problem: 文本对齐，总结下来有以下几点要求。 如果不是最后一行，且该行不止一个单词，则要求左右对齐。 左右对齐：尽可能让单词间的空格均匀分布，如果不能均匀分布，则单词左边的空格应该比右边的空格多。 贪心的思想：应该尽可能的多放单词。 如果是最后一行，或者该行只有一个单词，则要求左对齐。 Solution： 分两种情况处理，判断是左对齐，还是右对齐。 左对齐：该行有x个单词 前x-1个单词的后面都应该只有一个空格。 最后一个单词后面就应该补齐所有空格。 左右对齐：该行有x个单词，有x-1个空格间隙。 计算得到该行的空格数w，则如果能均匀分配，则每个间隙应该有aver = w // (x-1) 个空格。 但也许不会均匀分配，因此，可能会多出m个空格（m &lt; x-1 ) 即前m个单词，单词的后面应该有（aver+1)个空格，后面的(x-1) - m个单词应该有aver个空格。 最后一个单词的后面没有空格。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from typing import Listclass Solution: def __init__(self): self.words = None self.maxWidth = None self.sum = None def leftJustify(self, l: int, r: int) -&gt; str: wordsNum = r - l + 1 lengthNum = self.sum[r] if l == 0 else self.sum[r] - self.sum[l - 1] spaceNum = self.maxWidth - lengthNum temp = \"\" for i in range(l, r): temp += self.words[i] + \" \" temp += self.words[r] + \" \"*(spaceNum - (wordsNum - 1)) return temp def leftRightJustify(self, l: int, r: int) -&gt; str: wordsNum = r - l + 1 lengthNum = self.sum[r] if l == 0 else self.sum[r] - self.sum[l - 1] spaceNum = self.maxWidth - lengthNum temp = \"\" averSpace = spaceNum // (wordsNum - 1) moreSpace = spaceNum - averSpace*(wordsNum - 1) for i in range(moreSpace): temp += self.words[l+i] + \" \" * (averSpace + 1) for i in range(l+moreSpace, r): temp += self.words[i] + \" \" * averSpace temp += self.words[r] return temp def fullJustify(self, words: List[str], maxWidth: int) -&gt; List[str]: self.words = words self.maxWidth = maxWidth n = len(words) sum = [0]*n sum[0] = len(words[0]) # sum prefix length of words for i in range(1, n): sum[i] = sum[i - 1] + len(words[i]) self.sum = sum l = 0 ans = [] while l &lt; n: r = l lengthNum = len(self.words[l]) while r+1 &lt; n and lengthNum + len(self.words[r+1]) + 1 &lt;= maxWidth: lengthNum += len(self.words[r+1]) + 1 # space r += 1 # only one word or the last line if r - l + 1 == 1 or r == n - 1: ans.append(self.leftJustify(l, r)) else: ans.append(self.leftRightJustify(l, r)) l = r + 1 return ans Python的三元运算符： #如果条件为真，返回真 否则返回假condition_is_true if condition else condition_is_false 12is_fat = Truestate = \"fat\" if is_fat else \"not fat\" Python的整除是：\\\\ ，实数除是：\\ Reference Python中字符串的连接方法总结： https://segmentfault.com/a/1190000015475309","link":"/2020/09/29/LeetCode_String/"},{"title":"「机器学习-李宏毅」：Gradient","text":"总结「李宏毅老师-机器学习」的Gradient，主要从以下三个方面展开：调节learning rate；加快训练速度；对数据进行Feature Scaling。 阅读完三个tips，具体实现可demo Tip 1: Tuning your learning rates carefullyVisualize 损失函数随着参数变化的函数图 左图是Loss Function的函数图，红色是最好的Step，当Step过小（蓝色），会花费很多时间，当Step过大（绿色、黄色），会发现Loss越来越大，找不到最低点。 所以在Training中，尽可能的visualize loss值的变化。 但是当参数大于等于三个时， $loss function$的函数图就不能visualize了。 因此，在右图中，visualize Loss随着参数更新的变化，横轴即迭代次数，当图像呈现蓝色（small）时，就可以把learning rate 调大一些。 Adaptive Learning Rates(Adagrad)但是手动调节 $\\eta$是低效的，我们更希望能自动地调节。 直观上的原则是： $\\eta$ 的大小应该随着迭代次数的增加而变小。 最开始，初始点离minima很远，那step应该大一些，所以learning rate也应该大一些。 随着迭代次数的增加，离minima越来越近，就应该减小 learning rate。 E.g. 1/t decay： $\\eta^t=\\eta/ \\sqrt{t+1}$ 不同参数的 $\\eta$应该不同（cannot be one-size-fits-all)。 AdagradAdagrad 的主要思想是：Divide the learning rate of each parameter by the root mean squear of its previous derivatives.(通过除这个参数的 计算出的所有导数 的均方根) root mean squar : $ \\sqrt{\\frac{1}{n}(x_1^2+x_2^2+...+x_n^2)} $ Vanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $w^{t+1} \\leftarrow w^{t}-\\frac{\\eta^{t}}{\\sigma^{t}} g^{t} $ $\\eta^t$：第t次迭代的leaning rate $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}}$ $g^{t}=\\frac{\\partial L\\left(\\theta^{t}\\right)}{\\partial w} $ $\\sigma^t$：root mean squar of previous derivatives of w $\\tau^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}} $ 对比上面两种Adaptive Gradient，Adagrade的优势是learning rate 是和parameter dependent（参数相关的）。 Adagrad步骤简化步骤： 简化公式： $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ ( $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}} $ , $ \\sigma^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}$ ,约掉共同项即可) Adagrad Contradiction? ——Adagrad原理解释Vanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 在Vanilla Gradient descent中， $g^t$越大，也就是当前梯度大，也就有更大的step。 而在Adagrad中，当 $g^t$越大，有更大的step,而当 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 越大，反而有更小step。 Contradiction？ 「Intuitive Reason（直观上解释）」 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 是为了造成反差的效果。 类比一下，如果一个一直很凶的人，突然温柔了一些，你会觉得他特别温柔。所以同样是 $0.1$,第一行中，你会觉得特别大，第二行中，你会觉得特别小。 因此 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 这一项的存在就能体现 $g^t$的变化有多surprise。 「数学一些的解释」1. Larger Gradient,larger steps?在前面我们都深信不疑这一点，但这样的描述真的是正确的吗？ 在这张图中，只有一个参数，认为当该点的导数越大，离minima越远，这样看来，Larger Gradient,larger steps是正确的。 在上图中的 $x_0$点，该点迭代更新的best step 应该正比于 $|x_0+\\frac{b}{2a}|$ ，即 $\\frac{|2,a, x_0+b|}{2a}$。 而 $\\frac{|2,a, x_0+b|}{2a}$的分子也就是该点的一阶导数的绝对值。 上图中，有 $w_1,w_2$两个参数。 横着用蓝色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较a、b两点，a点导数大，离minima远。 竖着用绿色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较c、d两点，c点导数大，离minima远。 但是，如果比较a、c两点呢？ a点对 $w_1$ 的偏导数和c点对 $w_2$的偏导数比较？ 比较出来，c点点偏导数更大，离minima更远吗？ 再看左图的图像，横着的弧度更平滑，竖着的弧度更尖一些，直观上看应该c点离minima更近一些。 所以Larger Gradient,larger steps点比较方法不能（cross parameters)跨参数比较。 所以最好的step $\\propto$ 一阶导数（Do not cross parameters)。 2.** Second Derivative** 前面讨论best step $\\frac{|2,a, x_0+b|}{2a}$的分子是该点一阶导数，那么其分母呢？ 当对一阶导数再求导时，可以发现其二阶导数就是best step的分母。 得出结论：the best step $\\propto$ |First dertivative| / Second derivative。 因此，再来看两个参数的情况，比较a点和c点，a点的一阶导数更小，二阶导数也更小；c点点一阶导数更大，二阶导数也更大。 所以如果要比较a、c两点，谁离minima更远，应该比较其一阶导数的绝对值除以其二阶导数的大小。 回到 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 上一部分得出的结论是：the best step $\\propto$ |First dertivative| / Second derivative。 所以我们的learning rate 也应该和 |First dertivative| / Second derivative相关。 $g^t$也就是一阶导数，但为什么 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 能代表二阶导数呢？ 上图中，蓝色的函数图有更小的二阶导数，绿色的函数图有更大的二阶导数。 在复杂函数中，求二阶导数是一个很复杂的计算。 所以我们想用一阶导数来反映二阶导数的大小。 在一阶导数的函数图中，认为一阶导数值更小的，二阶导数也更小，但是取一个点显然是片面的，所以考虑取多个点。 也就是用 $ \\sqrt{\\text{(first derivative)}^2}$ 来代表best step中的二阶导数。 总结一下Adagrad的为了找寻最好的learning rate，从找寻best step下手，用简单的二次函数为例，得出 best step $\\propto$ |First dertivative| / Second derivative。 但是复杂函数的二阶导数是难计算的，因此考虑用多个点的一阶导数来反映其二阶导数。 得出 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 。 直观来解释公式中的一阶导数的root mean square，即来为该次迭代的一阶导数造成反差效果。 其他文献中的Adaptive Gradient理应都是为了调节learning rate使之有best step。(待补充的其他Gradient)[1] Tip 2:Stochastic Gradient DescentStochastic Gradient Descent在linear model中，我们这样计算Loss function： $L=\\sum_{n}\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ 每求一次Loss function，L都对所有training examples的 $\\text{error}^2$求和，因此每一次的loss function的计算，都是一重循环。 在Stochastic Gradient Descent中，每一次求loss function，只取一个example $x^n$，减少一重循环，无疑更快。 Stochastic Gradient Descent Pick an example $x^n$ $L=\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ 上图中，传统的Gradient Descent看完一次所有的examples，离minima还很远；而Stochastic Gradient Descent ，看完一次，已经离minima较近了。 Tip 3:Feature ScalingWhat is Feature Scaling 如上图所示，希望能让不同的feature能有相同的scale（定义域/规模） Why Feature Scaling假设model都是 $y = b+ w_1 x_1 +w_2 x_2$。 上图中，左边 $x_2$的规模更大，可以认为 $x_1$ 对loss 的影响更小， $ x_2$对loss的影响更大。 即当 $w_1,w_2$轻微扰动时，同时加上相同的 $\\Delta w$时，$x_2$ 使 $y$的取值更大，那么对loss 的影响也更大。 如图中下方的函数图 $w_1$方向的L更平滑， $w_2$ 方向更陡峭些，Gradient descent的步骤如图所示。 但当对 $x_2$进行feature scaling后，图像会更像正圆，Gradient descent使，参数更新向着圆心走，更新会更有效率。 How Feature Scaling概率论知识：标准化。 概率论： 随机变量 $X$ 的期望和方差均存在，且 $ D(X)>0$,令 $X^*=\\frac{X-E(X)}{\\sqrt{D(X)}}$ 那么 $E(X^*)=0,D(X)=1 $ , $ X^* $ 称为X的标准化随机变量。 对所有向量的每一维度，进行标准化处理： $x_{i}^{r} \\leftarrow \\frac{x_{i}^{r}-m_{i}}{\\sigma_{i}} $ （ $m_i$是该维度变量的均值， $\\sigma_i$ 是该维度变量的方差） 标准化后，每一个feature的期望都是0，方差都是1。 Gradient Descent Theory(公式推导)当用Gradient Descent解决 $\\theta^*=\\arg \\min_\\theta L(\\theta)$时，我们希望每次更新 $\\theta $ 都能得到 $L(\\theta^0)&gt;L(\\theta^1)&gt;L(\\theta^2)&gt;…$ 这样的理论结果，但是不总能得到这样的结果。 上图中，我们虽然不能一下知道minima的方向，但是我们希望：当给一个点 $\\theta^0$ 时，我们能很容易的知道他附近（极小的附近）的最小的loss 是哪个方向。 所以怎么做呢？ Tylor Series微积分知识：Taylor Series（泰勒公式）。 Tylor Series:函数 $h(x)$ 在 $x_0$ 无限可导，那么 $\\begin{aligned} \\mathrm{h}(\\mathrm{x}) &=\\sum_{k=0}^{\\infty} \\frac{\\mathrm{h}^{(k)}\\left(x_{0}\\right)}{k !}\\left(x-x_{0}\\right)^{k} \\\\ &=h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right)+\\frac{h^{\\prime \\prime}\\left(x_{0}\\right)}{2 !}\\left(x-x_{0}\\right)^{2}+\\ldots \\end{aligned}$ 当 x 无限接近 $x_0$ 时，忽略后面无穷小的高次项， $h(x) \\approx h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right) $ 上图中，用 $\\pi/4$ 处的一阶泰勒展示来表达 $\\sin(x)$ ,图像是直线，和 $\\sin(x)$ 图像相差很大，但当 x无限接近 $\\pi/4$ 是，函数值估算很好。 Multivariable Taylor Series $h(x, y)=h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right) +\\text{something raleted to} (x-x_x^0)^2 \\text{and} (y-y_0)^2+…$ 当 $(x,y)$ 接近 $(x_0,y_0)$ 时， $h(x,y)$ 用 $(x_0,y_0)$ 处的一阶泰勒展开式估计。 $ h(x, y) \\approx h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right)$ Back to Formal Derivation 当图中的红色圆圈足够小时，红色圆圈中的loss 值就可以用 $(a,b)$ 处的一阶泰勒展开式来表示。 $ \\mathrm{L}(\\theta) \\approx \\mathrm{L}(a, b)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}\\left(\\theta_{1}-a\\right)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}}\\left(\\theta_{2}-b\\right) $ $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$ ,d 足够小。 用 $s=L(a,b)$ , $ u=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}, v=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} $ 表示。 最后问题变成： $L(\\theta)\\approx s+u(\\theta_1-a)+v(\\theta_2-b)$ 找 $(\\theta_1,\\theta_2)$，且满足 $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$，使 $L(\\theta)$ 最小。 变成了一个简单的最优化问题。 令 $\\Delta \\theta_1=\\theta_1-a$ , $\\Delta\\theta_2=\\theta_2-b$ 问题简化为： $\\text{min}:u \\Delta \\theta_1+v\\Delta\\theta_2$ $\\text{subject to}:{\\Delta\\theta_1}^2+{\\Delta\\theta_2}^2\\leq d^2$ 画出图，就是初中数学了。更新的方向应该是 $(u,v)$ 向量反向的方向。 所以： $\\left[\\begin{array}{l} \\Delta \\theta_{1} \\\\ \\Delta \\theta_{2} \\end{array}\\right]=-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $\\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $ \\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}} \\\\ \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} \\end{array}\\right] $ Limitation of Gradient Descent Gradient Descent 可能会卡在local minima或者saddle point（鞍点：一个方向是极大值，一个方向是极小值，导数为0） 实践中，我们往往会在导数无穷接近0的时候停下来（&lt; 1e-7)，Gradient Descent 可能会停在plateau(高原；增长后的稳定) Reference[1] 待补充的其他Gradient","link":"/2020/03/01/Gradient/"},{"title":"「LeetCode」：Math","text":"LeetCode Math 专题记录。 10月初。 Albert Einstein: “I believe that not everything that can be counted counts, and not everything that counts can be counted” 「并非所有重要的东西都是可以被计算的，也并不是所有能被计算的东西都那么重要。」 7. Reverse Integer[E]7. Reverse Integer[E] Problem: 反转32bits的有符号数字，如果反转后会溢出则返回0. Solution： 直观的解决它，先算出反转后的数字，用比较大小来看是否溢出。（最开始还想着转换为bit串来看，就复杂了） 123456789101112131415161718class Solution: def reverse(self, x: int) -&gt; int: n_min = -(2 ** 31) n_max = 2 ** 31 - 1 s = 0 flag = True if x &lt; 0: flag = False x = -x while x != 0: r = x % 10 s = s * 10 + r x //=10 if flag is False: s = -s if s &lt; n_min or s &gt; n_max: return 0 return s 十进制转换为二进制、八进制、十六进制： 二进制：bin(a) ,也可以直接赋二进制的值0b10101 八进制：oct(a) ，赋值八进制的值0o263361 十六进制：hex(a) ,赋值十六进制0x1839ac29 165. Compare Version Numbers[M]165. Compare Version Numbers[M] Problem： 比较版本号。 Solution： 直观～Easy～ 123456789101112131415class Solution: def compareVersion(self, version1: str, version2: str) -&gt; int: li1 = version1.split('.') li2 = version2.split('.') n_1 = len(li1) n_2 = len(li2) n = max(n_1, n_2) for i in range(n): a = int(li1[i]) if i &lt; n_1 else 0 b = int(li2[i]) if i &lt; n_2 else 0 if a &gt; b: return 1 elif a &lt; b: return -1 return 0 Python中的强制转换： 字符串转换为int : int_value = int(str_value) int转换为字符串：str.value = str(int_value) int转换为unicode： unicode(int_value) unicode转换为int：int(unicode_value) 字符串转换为unicode：unicode(str_value) unicode转换为字符串：str(unicode_value) Java中的强制转换： 字符串String转化为int：int_value = String.parseInt(string_value) 或者 (int)string_value) int转化为字符串String：string_value = (String)int_value 66. Plus One[E]66. Plus One[E] Problem: 用列表表示一个正数，返回正数+1的列表结果。 Solution： 记录一个最高位的进位情况。 123456789101112131415161718192021from typing import Listclass Solution: def plusOne(self, digits: List[int]) -&gt; List[int]: c_bit = 0 n = len(digits) i = n - 1 digits[i] += 1 while i &gt;= 0: if digits[i] &lt; 10: break digits[i] %= 10 if i == 0: c_bit = 1 else: digits[i-1] += 1 i -= 1 if c_bit == 1: digits.insert(0, c_bit) return digits Python中list添加元素的集中方法：（append(); extend(); insert(); +加号） append() ：在List尾部追加单个元素，只接受一个参数，参数可以是任意数据类型。 extend() ：在list尾部追加一个列表，将该参数列表中的每个元素连接到原列表。 12345&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; b = [3,4,5]&gt;&gt;&gt; a.extend(b)&gt;&gt;&gt; a[1, 2, 3, 3, 4, 5] insert(index, object)：将一个元素插入到列表中，第一个参数是插入的索引点，第二个是插入的元素。 +加号：将两个list相加，返回一个新的list对象。 区别：前三种方法(append, extend, insert)可以对列表增加元素，没有返回值，是直接修改原数据对象，而+加号是需要创建新的list对象，需要消耗额外的内存。","link":"/2020/10/10/Leetcode-math/"},{"title":"「机器学习-李宏毅」：Regression","text":"在YouTube上看台大李宏毅老师的课，看完Regression讲座的感受就是： 好想去抓Pokemon！！！ 这篇文章将总结李宏毅老师Regression的讲座，并尝试实现其demo。 Regression（回归）DefineRegression：是找到一个$function$，进行预测。对输入的feature，输出一个$Scalar$(数值，标量)。 Example Application Look for a $function$ Stock Market Forecast（股票预测） $input$：过去的股价信息 $output$：明天的股价平均值（$Scalar$) Self-Driving Car(自动驾驶) $input$：路况信息 $output$：方向盘角度（$Scalar$) Recommendation（推荐系统） $input$：使用者A、商品B $output$：使用者A购买商品B的可能性 可见，$input$都是一些特征信息，$output$都是一个标量数值，这就是Regression。 Regression Case: Pokenmon 看完这节课，感想：好想去抓宝可梦QAQ 预测一个pokemon进化后的CP（Combat Power，战斗力）值。 为什么要预测呐？ 如果进化后的CP值高，就进化他，不然就把他当糖果，因为宝可梦很难抓的。（？没玩过，我也不懂o r z） 上图妙蛙种子的信息(可能的$input$)： $x_{cp}$：CP值 $x_s$:物种 $x_{hp}$:生命值 $x_w$:重量 $x_h$:高度 output：进化后的CP值。 $x_{cp}$：用下标表示一个object的component。 $x^1$：用上标表示一个完整的object。 Step 1: 找一个Model（function set）Model ：$y = b + w \\cdot x_{cp}$ 假设用上式作为我们的Model，那么这些函数： $ \\begin{aligned} &\\mathrm{f}_{1}: \\mathrm{y}=10.0+9.0 \\cdot \\mathrm{x}_{\\mathrm{cp}}\\\\ &f_{2}: y=9.8+9.2 \\cdot x_{c p}\\\\ &f_{3}: y=-0.8-1.2 \\cdot x_{c p} \\end{aligned} $ 等都属于这个集合，但是显然像$f_3$这种函数是bad，CP值不可能是负数。bad functions 很多，所以在下面的步骤，会说明如何判别一个函数的好坏，自动的选出最好的那个 $function$。 把Model 1一般化，得到线代中的 Linear Model：$y = b+\\sum w_ix_i$ $x_i$：x的feature $b$：bias,偏置值 $w_i$：weight，权重 Step 2: 判别Goodness of Function(Training Data)Training Data假定使用Model ：$y = b + w \\cdot x_{cp}$ Training Data：十只宝可梦，用向量的形式表示。 使用Training data来judge the goodness of function.。 Loss Function(损失函数)概率论：做线性回归，一般使用最小二乘法。一般回归，大多使用极大似然估计。 Loss function $L$ ：$L(f)=L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ 其中的 $\\hat{y}^n-(b+w\\cdot x_{cp}^n)$是Estimation error(估测误差) Loss Function的意义：它的 $input$是一个 $function$，它的 $output$体现了how bad it is,这个函数有多糟/好。 Figure the Result 上图横纵坐标是函数 $L$的参数 $w 、b$，图中的每一个point都是一个 $function $。 color：体现函数的输出，越红越大，说明选择的函数越bad。 所以我们要选择紫色区域结果最小的函数。 而这个得到best function的过程是可以通过无数次迭代实现的。（重复的迭代当时是交给计算机做了） Step 3:迭代找出Best Function$L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ 找到Best Function: $f^{*}=\\arg \\min _{f} L(f)$ 也就是找到参数 $w^{*},b^{*}=\\arg \\min_{w,b} L(w,b)=\\arg \\min_{w,b}\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ arg ：argument,变元 arg min：使之最小的变元 arg max：使之最大的变元 据悉，线性回归的参数可以用线性代数的知识，解出closed-form solution（解析解），我先挖个坑QAQ，以后来填这块知识。[1] 在机器学习中，只要$L$函数可微分， 即可用Gradient Descent（梯度下降）的方法来求解。 Gradient Decent（梯度下降）和概率论中的梯度下降估计参数的原理相同，只是计算机不能直接解出方程的解，所以计算机的方法是迭代。 考虑一个参数w*$w^*=\\arg \\min_w L(w)$ 步骤： 随机选取一个初始值 $w^0$ 计算 $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp; &nbsp; $\\begin{equation} w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{0}} \\end{equation}$ 计算 $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp;&nbsp; $\\begin{equation} w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{1}} \\end{equation}$ …until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$ 上图迭代过程的几点说明 $\\begin{equation}\\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}}\\end{equation}$的正负 如果是negative，也就是该点切线斜率是负的，那应该Increse w，以找到最低点。 Negative $\\rightarrow$ Increase w Positive $\\rightarrow$ Decrease w $-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{i}}$：步长 $\\eta$：learning rate（学习速度），事先设好的值。 $-$(负号)：如果 $\\begin{equation}\\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}}\\end{equation}$是负的，应该增加w。 Local optimal：局部最优和全局最优 如果是以上图像，则得到的w不是全局最优。 但线性回归的损失函数是凸函数，存在一个全局最优，没有局部最优。 考虑多个参数 $w^{*},b^{*}$ 微积分知识：gradient（梯度，向量)： $\\nabla L=\\left[\\begin{array}{l}\\frac{\\partial L}{\\partial w} \\\\frac{\\partial L}{\\partial b}\\end{array}\\right]$ 考虑多个参数和考虑一个参数思路相同，每次迭代，迭代两个参数。 $w^{*}, b^{*}=\\arg \\min _{w, b} L(w, b)$ 步骤： 随机选取初值 $w^0,b^0$ 计算 $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ &nbsp; &nbsp; $w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0}} \\quad b^{1} \\leftarrow b^{0}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ 计算 $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ &nbsp; &nbsp; &nbsp; $w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1}} \\quad b^{2} \\leftarrow b^{1}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ …until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$, $ \\frac{{\\rm d}L}{{\\rm d}b}|_{b=b^n}=0$ 上图，坐标为 $L(w,b)$函数的参数，Color代表 $L$的大小，越紫值越小。 每一个点都是一个 $function$，沿着梯度方向（图中法线方向）迭代，找到全局最优点。 再次说明：线性回归中，损失函数是convex（凸函数），没有局部最优解。 $\\frac{\\partial L}{\\partial w}$和 $\\frac{\\partial L}{\\partial b}$的公式推导$L(w, b)=\\sum_{n=1}^{10}\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)^{2}$ 微积分的知识，显然。 数学真香。———我自己 $\\frac{\\partial L}{\\partial w}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)（-x_{cp}^n)$ $\\frac{\\partial L}{\\partial b}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)(-1)$ 实际结果分析Training Data Training Data的Error=31.9，但我们真正关心的是Testing Data的error。 Testing Data 是new Data：另外的Pokemon！。 Testing DataModel 1： $y = b+w\\cdot x_{cp}$ error = 35,比Training Data error更大。 Model 2：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2$ Testing error=18.4，比Model 1 好。 Model 3：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3$ Testing error=18.1，比Model 2好。 Model 4:$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4$ Testing error =28.8,比Model3更差。 Model 5：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4+w_5\\cdot (x_{cp})^5$ Testing error = 232.1,爆炸了一样的差。 Overfiting（过拟合了）从上面5个Model中可以得出，越复杂的函数模型，在Testing data上不一定能得到更好的结果。（过拟合使Training data 的误差越来越小） 所以在选择Model时，需要选择合适的Model。 对模型进行改进如果收集更多的Training Data，可以发现他好像不是一个Linear Model。 Back to step 1:Redesigh the Model从上面那张图，感觉他不是一个Linear Model,而是需要if 是伊布，模型是…，if 是…,可见是和物种有关系。 （很抱歉，我只认识右上角时伊布，QAQ，我也说不出名字） 但用 $\\delta$(微积分学的狄拉克函数)表示条件语句，可以发现，他仍然是一个线性模型。 $\\delta(x_s= \\text{Pidgey)}\\left\\{\\begin{array}{ll}=1 & \\text { If } x_{s}=\\text { Pidgey } \\\\ =0 & \\text { otherwise }\\end{array}\\right.$ $y = b_1\\cdot \\delta_1+w_1\\cdot \\delta_1+b2\\cdot \\delta_2+w_2\\cdot \\delta_2+…$是一个linear model。 拟合出来，Training Data 和Testing Data的error都蛮小的。 如果想让拟合误差更小，还可以考虑其他的feature，重量、高度、HP等。 但同样的，如果函数过于复杂，也会出现Overfitting的情况。 Back to Step 2:Regularization（正则化）对于Linear Model :$y = b+\\sum w_i x_i$ 为什么要正则化？我们希望得到的函数是较平滑的，这样测试时，函数的输出对输入的noise不sensitive，即输入x的细微扰动，并不太会影响输出的结果。 所以当参数越接近0，函数越平滑。因此在原本的loss function后加入 $\\lambda \\sum(w_i)^2$项（ $\\lambda$需手调），可以保证函数较平滑。 正则化： $L = \\sum_n(\\hat{y}^n-(b+\\sum w_i x_i))^2 + \\lambda\\sum(w_i)^2$ $\\lambda $大小的选择 可以得出结论： $\\lambda $越大，Training Error变大了。 当 $\\lambda$更大，损失函数更考虑w参数的取值，更关心函数的平滑程度，而更少的关心拟合的error。 $\\lambda $越大，Testing Error变小了，当 $\\lambda$过大时，又变大。 $\\lambda $较小时，$\\lambda $增大，函数更平滑，能良好适应数据的扰动。 $\\lambda $较大时，函数过于平滑，宛如直线，这显然不能准确预测。 因此，在调节$\\lambda $大小时，也要适当选择。 正则化的一个注意点在regularization中，我们只考虑了w参数，没有考虑bias偏置值参数。 因为正则化是寻找较平滑拟合，而偏置参数只是让函数平移，与平滑无关。 Again：Regularization不考虑bias Fllowing Gradient descent[2] Overfitting and regularization[3] Validation[4] 由于博主也是在学习阶段，学习后，会po上下面内容的链接。 希望能在学习、写博客的过程中，锻炼自己的表达能力，尽量让文风言简意赅又科学严谨。 写博客是为了记录与分享，感谢指正。 Reference[1] “周志华西瓜书p55,待补充” [2] [3] [4]","link":"/2020/02/29/Regression/"},{"title":"「Cryptography-Dan」:Stream Cipher 1","text":"Stream Cipher的第一部分：介绍了One Time Pad和Stream Cipher中的PRG。其中OTP部分叙述了什么是Perfect Secrecy？为什么OTP很难在实践中应用？Stream Cipher部分中，本文主要阐述了什么是PRG？Stream Cipher的另一种安全的定义（依靠PRG的unpredictable)。本文后半部分，详细阐述了一种weak PRG——线性同余生成器，它是如何工作的？它为什么不安全？如何attack it elegantly? The One Time PadSymmetric Ciphers: difinitionDef :a cipher difined over $\\mathcal{(K,M,C)}$ is a paire of “efiicient “ algorithms $(E,D)$ where $$ E :\\mathcal{K \\times M \\longrightarrow \\mathcal{C}} \\quad ,\\quad D:\\mathcal{K\\times\\mathcal{C}\\longrightarrow\\mathcal{M}} \\\\ s.t. \\quad \\forall m\\in \\mathcal{M},k\\in \\mathcal{K}:D(k,E(k,m))=m $$ $\\mathcal{(K,M,C)}$ 分别是密钥空间、明文空间、密文空间。 对称加密其实是定义在$\\mathcal{(K,M,C)}$ 的两个有效算法 $(E,D)$ ，这两个算法满足consistence equation(一致性方程)：$D(k,E(k,m))=m$ 。 一些说明： $E$ is ofen randomized. 即加密算法E总是随机生成一些bits，用来加密明文。 $D$ is always deterministic. 即当确定密钥和明文时，解密算法的输出总是唯一的。 “efficient” 的含义 对于理论派：efficient表示 in polynomial time（多项式时间） 对于实践派：efficient表示 in a certain time One Time Pad(OTP)Definition of OTPThe one time pad(OTP) 又叫一次一密。 用对称加密的定义来表示OTP： $\\mathcal{M=C=}{0,1}^n\\quad \\mathcal{K}={0,1}^n$ $E：\\quad c = E(k,m)=k\\oplus m \\quad$ $D:\\quad m = D(k,c)=k\\oplus c$ 明文空间和密文空间相同，密钥空间也是n位01串集合。 而且，在OTP中，密钥key的长度和明文message长度一样长。 加密过程如上图所示。 证明其一致性方程 Indeed ： $D(k,E(k,m))=D(k,k\\oplus m)=k\\oplus (k\\oplus m)=0\\oplus m=m$ 但是OTP加密安全吗？ 如果已知明文(m)和他的OTP密文(c)，可以算出用来加密m的OTP key吗？ ：当然，根据异或的性质，key $k=m\\oplus c$ 所以什么是安全呢？ Perfect Security Definition根据Shannon 1949发表的论文，Shannon’s basic idea: CT(Ciphertext) should reveal no “info” about PT(Plaintext)，即密文不应该泄露明文的任何信息。 Perfect Security Def:A cipher $(E,D)$ over $\\mathcal{(K,M,C)}$ has perfect security if $\\forall m_0,m_1 \\in \\mathcal{M}\\ (|m_0|=|m_1|) \\quad \\text{and} \\quad \\forall c\\in \\mathcal{C} $$$Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$$ $k \\overset{R}\\longleftarrow \\mathcal{K}$ 的意思是 $k$ 是 从$\\mathcal{K}$ 中随机取的，即随机变量 $k$ 的取值是均匀分布。 对任意 $m_0,m_1$ （并且message长度相同），那么在密钥空间任意取 $k$ , $k$ 将 $m_0,m_1$ 加密为相同密文的概率相同。 对attacker来说 ：攻击者截取一段密文c，那么c是由 $m_0,m_1$ 加密而来的概率是相同的，即攻击者也不知道明文到底是 $m_0$ 还是 $m_1$ （因为概率相同）。 $\\Rightarrow$ Given CT can’t tell if msg is $m_0 \\ \\text{or}\\ m_1 $ (for all $m_i$ ) . 【攻击者不能区分明文到底是 $m_?$ 】 $\\Rightarrow$ most powerful adv.(adversary) learns nothing about PT from CT. 【不管攻击者多聪明，都不能从密文中得到密文的信息】 $\\Rightarrow$ no CT only attack!! (but other attackers possible). 【惟密文攻击对OTP无效】 OTP has perfect secrecyLemma : OTP has perfect secrecy. 用上一小节的perfect securecy的定义来证明这个引理。 Proof： 要证明： $Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$ 表达式： $\\forall m, c: \\quad \\operatorname{Pr}_{k}[E(k,m)=c]=\\frac{\\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c}{|\\mathcal{K}|}$ 对于任意m,c, $\\operatorname{Pr}_{k}[E(k,m)=c]$ 等于能将m加密为c的密钥个数除以密钥空间的大小。 $\\because |\\mathcal{K}|$ 是相同的，所以即证 ： $\\{ \\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c \\}=\\text{const}$ 对于任意 m,c，能将m加密为c的OTP key只有一个： $k=m\\oplus c$ $\\therefore$ OTP has perfect secrecy. key-len $\\geq$ msg-len Perfect Secrecy的性质带来了一个bad news。 Thm: perfect secrecy $\\Rightarrow$ $|\\mathcal{K}|\\geq|\\mathcal{M}|$ 如果一个cipher满足perfect secrecy,那么其密钥的长度必须大于等于明文长度。这也是perfect secrecy的必要条件。 所以OTP是perfect secrecy的最优情况，$|\\mathcal{K}|=|\\mathcal{M}|$ ，密钥长度等于明文长度。 为什么说是一个bad news呢？ 如果Alice用OTP给Bob发一段msg，在她发之前，她需要先发一个和msg等长的key，这个key只有Alice和Bob知道。 所以如果Alice有能保密传输key的方法，那她何不直接用这个方法传输msg呢？ 所以OTP : hard to use in practice! (long key-len) 因此，我们需要key-len短的cipher。 Pseudorandom Generators（伪随机数生成器）Stream Ciphers: making OTP practicalStream Ciphers（流密码）的思想就是：用PRG（pseudorandom Generators） key 代替 “random” key。 PRG其实就是一个function G：${ 0,1 }^s\\longrightarrow { 0,1 }^n \\quad, n&gt;&gt;s$ 。 通过函数将较小的seed space映射到大得多的output space。 注意： function G is eff. computable by a deterministic algorithm. 函数G是确定的，随机的只有s，s也是G的输入。 PRG的输出应该是 “look random”（下文会提到的PRG必须是unpredictable） Stream Ciphers的过程如上图所示：通过PRG，将长度较短的k映射为足够长的G(k)，G(k)异或m得到密文。 有两个问题？ 第一，Stream Cipher安全吗？为什么安全？ 第二，Stream Cipher have perfect secrecy? 现在，只能回答第二个问题。 ：流密码没有perfect secrecy。因为它不满足key-len $\\geq$ msg-len，流密码的密钥长度远小于明文长度。 流密码没有perfect secrecy，所以我们还需要引入另一种安全，这种安全和PRG有关。 PRG must be unpredictablePRG如果predictable，流密码安全吗？ Suppose predictable假设PRG是可预测的，即： $ \\exists:\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1,...,n} $ 已知G(k)输出的前i bis，存在一种算法，能计算G(k)的后面剩余的bits。 攻击如上图所示： 如果attacker has prior knowledge：已知一段密文前缀的对应明文（m斜线字段）（比如在SMTP协议中，报文的开头总是”from”） attacker将该密文字段与已知明文字段异或，得到G(k)的前缀。 因为PRG是可预测的，所以可以通过G(k)的前缀计算出G(k)的剩下部分。 得到的G(K)就可以recover m。 即使，G(k)只能预测后一位，即 $\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1}$ ，也不安全，当预测出下一位时，又得到了新的前缀，最终得到完整的G(k)。 所以当PRG可预测时，流密码就不安全了。 所以用Stream Cipher时，PRG必须unpredictable! Predictable: difinitionPredictable Def : $ \\exists $ \"eff\" alg. A and $\\exists$ $0\\leq i\\leq n-1$ ， s.t. $Pr_{k \\overset{R}\\leftarrow \\mathcal{K} } {[A(G(k)|_{1,2,...,i})=G(k)|_{i+1}]}>1/2 +\\epsilon$ for non-negligible $\\epsilon$ (e.g. $\\epsilon=1/2^{30}$) 可预测：即存在算法，通过G(k)的前i位可以计算出第i+1位的概率大于1/2 + $\\epsilon$ (不可忽略的值) Unpredictable Def : 即predictable的反面， $\\forall i$ : no “eff.” adv. can predict bit(i+1) for “non-neg” $\\epsilon$ . Q：假设 $\\mathrm{G}: \\mathrm{K} \\rightarrow{0,1}^{\\mathrm{n}} $ ，满足XOR(G(k))=1，G可预测吗？ W：G可预测，存在i = n-1,因为当已知前n-1位,可以预测第n位。 Weak PRGsLinear Congruential Generators一种应该永远不在安全应用中使用PRG——LCG（linear congruential generators）(线性同余随机生成器)。 虽然他们在应用中使用很快，而且其输出还有良好的统计性质（比如0的个数和1的个数基本相等等），但他们应该never be used for cryptographic。 因为在实践中，给出LCG输出的一些连续序列，很容易计算出输出的剩余序列。 Basic LCGDefinitionBasic LCG has four public system parameters: an integer q, two constants a,b $\\in { 0,…,q-1}$ , and a positive integer $w\\leq q$ . The constant a is taken to be relatively prime to q. 【有四个公开参数：整数q，两个q剩余系下的常数a,b，（a与q互素）一个小于等于q的正整数w。】 We use $\\mathcal{S}_q$ and $\\mathcal{R}$ to denote the sets: $\\mathcal{S}_{q}:=\\{0, \\ldots, q-1\\} ; \\quad \\mathcal{R}:=\\{0, \\ldots,\\lfloor(q-1) / w\\rfloor\\}$ Now, the generators $G_{\\mathrm{lcg}}: \\mathcal{S}_{q} \\rightarrow \\mathcal{R} \\times \\mathcal{S}_{q}$ with seed $s\\in\\mathcal{S}_{q}$ defined as follows: $G_{\\operatorname{lcg}}(s):=(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ 【LCG的输出是一对数，$(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ 】 当 $w=2^t$ 时，$\\lfloor s / w\\rfloor$ simpky erases the t lease significant bits of s【向右平移t位】。 Insecure当已知 $s^{\\prime}:=a s+b \\bmod q$ ，即可直接求出s，也就求出了所谓的随机数 $\\lfloor s/w\\rfloor$ . Variant: Blum-Micali constructionDefinition 如上图所示，变体的LCG是一个迭代，输出不包括 $s_i$ ，把 $r_1,…,r_n$ 作为一次迭代的输出。 不同的应用系统使用不同的 $q,a,b,w$ 参数，在Java 8 Development Kit（JDKv8）中，$q=2^{48}$ , $w=2^{22}$ ,constant $a=\\text{0x5DEECE66D}$ , $b=\\text{0x0B}$ 。 所以在JDKv8中, LCG的输出其实是 $s_i$（48bits） 的前48-22=26 bits 。 显然JDKv8中的参数大小应用在安全系统中，还是太不安全了。 how to attack in JDKv8 在迭代的第一次输出中，LCG就 reveal 26bits of the seed s。 对于s剩下的后22个bits，attacker can easily recover them by exhausitive search(穷举)： 对于每个可能的取值，attacker都能得到一个候选seed $\\hat{s}$ 用 $\\hat{s}$ 来验证我们所直接得到的LCG的输出。 如果 $\\hat{s}$ 验证失败，则到第三步继续穷举。直至验证成功。 当穷举至正确的s时，就可以直接预测LCG的剩余输出。 在现代处理器中，穷举 $2^{22}$ (4 million) 只需要1秒。所以LCG的参数较小时，是很容易attack。 当 $q=2^{512}$ 时，这种穷举的攻击方法就失效了。但是有一种对于LCG的著名攻击方法[1]，即使每次迭代，LCG只输出较少的bits，也能从这些较少的但连续的输出序列中预测出整个LCG输出序列。 Cryptanalysis ：elegant attackWarning of MathSupposeSuppose : q is large (e.g. $q=2^{512}$ ), and $G_{lcg}^{(n)}$ outputs about half the bits of the state s per iteration. 【q很大， $G_{lcg}^{(n)}$ 每次输出s的一半左右的bits】 More precisely, suppose: $w&lt;\\sqrt{q}/c$ for fixed c（e.g. $c=32$ ） 【保证输出s前一半左右bits的这个条件】 Suppose the attacker is given two consecutive outputs of the gnerator $r_i,r_{i+1}\\in \\mathcal{R}$ . 【已知两个连续输出 $r_i,r_{i+1}\\in \\mathcal{R}$ 】 Attacker Knows $r_{i}=\\left\\lfloor s_{i} / w\\right\\rfloor \\quad \\text { and } \\quad r_{i+1}=\\left\\lfloor s_{i+1} / w\\right\\rfloor=\\left\\lfloor\\left(a s_{i}+b \\bmod q\\right) / w\\right\\rfloor$ 【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i$ 】 $r_{i} \\cdot w+e_{0}=s \\quad \\text { and } \\quad r_{i+1} \\cdot w+e_{1}=a s+b+q x \\qquad (0\\leq e_0,e_1&lt;w&lt;\\sqrt{q}/c)$ 【 去掉floor符号和mod：$e_0,e_1$ 是 $s_i,s_{i+1}$ 除 $w$ 的余数】 【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i,e_0,e_1,x$ 】 re-arranging: put $x$ and $s$ on the left $s=r_{i} \\cdot w+e_{0} \\quad \\text { and } \\quad a s+q x=r_{i+1} w-b+e_{1}$ 【把未知参数s，x放在等式左边，方便写成矩阵形式】 $s \\cdot\\left(\\begin{array}{l}1 \\ a\\end{array}\\right)+x \\cdot\\left(\\begin{array}{l}0 \\ q\\end{array}\\right)=\\boldsymbol{g}+\\boldsymbol{e} \\quad \\text { where } \\quad \\boldsymbol{g}:=\\left(\\begin{array}{c}r_{i} w \\ r_{i+1} w-b\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{e}:=\\left(\\begin{array}{c}e_{0} \\ e_{1}\\end{array}\\right)$ 【已知： $\\boldsymbol{g},a,q$ ，未知：$\\boldsymbol{e},s,x$ 】 to break the generator it suffices to find the vector $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}$ . 【令 $u\\in {Z}^2$ , $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}=s \\cdot(1, a)^{\\mathrm{T}}+x \\cdot(0, q)^{\\mathrm{T}}$ 】 【如果我们求出了 $\\boldsymbol{u}$ ，那可以用线性代数的知识解出 $s$ 和 $x$ ,再用 $s$ 来预测PRG的剩下输出】 konws $\\boldsymbol{g}$ , knows $\\boldsymbol{e}$ is shorter, and $|\\boldsymbol{e} |_{\\infty}$ is at most $\\sqrt{q}/c$ , knows that $\\boldsymbol{u}$ is “close” to $\\boldsymbol{g}$ . 【e向量很小，$|\\boldsymbol{e} |_{\\infty}$ 上界是$\\sqrt{q}/c$ ，u离g很近】 Taxicab norm or Manhattan(1-norm) ${\\|}A{\\|}_1=\\max \\{ \\sum|a_{i1}|,\\sum|a_{i2}|,...,\\sum|a_{in}| \\}$ （列和范数，A矩阵每一列元素绝对值之和的最大值） Euclidean norm(2-norm) $\\|\\mathbf{x}\\|=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}\\right)^{1 / 2}$ $\\infty$-范数 $\\|A\\|_{\\infty}=\\max \\{ \\sum|a_{1j}|,\\sum|a_{2j}|,...,\\sum|a_{mj}| \\}$ （行和范数，A矩阵每一行元素绝对值之和的最大值） attack can figure the lattice with attacking LCG. the lattice is generated by the vectors $(1,5)^T$ and $(0,29)^T$ , the attacker has a vector $\\boldsymbol{g}=(9,7)^T$ and wishes to find the closest lattice vector $\\boldsymbol{u}$ . 【上图是 $(1,5)^T$ 和 $(0,29)^T$ 两个向量生成的的格点，希望能从以上格点找到离已知 $\\boldsymbol{g}$ 向量最近的格点】 $\\mathcal{L}_a$ :由 $(1, a)^{\\mathrm{T}},(0, q)^{\\mathrm{T}}$ 作为基向量生成的点集合。 The problem is a special case of a general problem call the closest vector problem: given a lattice $\\mathcal{L}$ and a vector $\\boldsymbol{g}$ ,find a vector in $\\mathcal{L}$ that is closest to $\\mathcal{g}$ . There is an efficient polynomial time algorithm for this problem.[2] 【问题归结于 closest vector problem问题，在已知栅格点集合中找离某一向量最近的点，此问题已有多项式时间算法】 step 8 aboveLemma : * For at least $(1-16/c^2)\\cdot q $ of the a in $\\mathcal{S}_q$ , the lattice $\\mathcal{L}_a\\sub Z_2$ has the following property: for every $\\boldsymbol{g} \\in Z^2$ there is at most one vector $\\boldsymbol{u}\\in \\mathcal{L}_a$ such that $\\|\\boldsymbol{g}-\\boldsymbol{u}\\|_{\\infty}","link":"/2020/03/15/StreamCipher1/"},{"title":"「Cryptography-Dan」:Stream Cipher 2","text":"作为Stream Cipher的第二篇文章。第一部分分析了基于Stream Cipher的两种攻击：第一种是Two time pad,第二种是对与其完整性的攻击，即流密码是可被篡改的。第二部分具体说明了一些使用流密码加密的例子。包括分析基于软件的RC4流密码、基于硬件的CSS流密码和现代的安全流密码:eStream中的Salsa20。 Attack on OTP and stream ciphersAttack1: two time pad is insecureNever use strame cipher key more than once!! why insecure使用相同的PRG(k)加密不同明文时：$$C_1 \\leftarrow m_1 \\oplus \\text{PRG(k)}\\C_2 \\leftarrow m_2 \\oplus \\text{PRG(k)}$$Eavesdropper（窃听者）截获这两段密文 $C_1\\ C_2$ ，对密文进行疑惑操作，可得： $C_1 \\oplus C_2\\rightarrow m_1\\oplus m_2$ 。 在传输中，英语字母是用ASCII编码后再传输，所以这样的编码会带来很多redundancy（冗余），即根据 $m_1\\oplus m_2$ ，可以得到 $m_1\\ m_2$ 。 因此，当一个密钥会被使用多次时，就不应该直接用stream cipher，后面的章节会介绍multi-use ciphers。 Examples: Project Venona(1941-1946)我们已经知道：加密应该用OTP，即一次性密钥。 但是，当时是通过人工掷骰子并记录得到密钥，工作费时费力。因此不得不用生成的密钥加密多条消息。 最后仅凭截获密文，就破译了3000多条消息。 Examples: MS-PPTP(Windows NT)微软在Windows NT的PPTP协议（point to point transfer protocol）中使用的流密码是：RC4。 在这个协议中允许一个端系统向另一个端系统发送加密后的信息。 过程如下： 在一次对话连接中：主机想发送$m_1\\ m_1\\ m_3$ 三条消息进行查询，服务器想发送 $s_1\\ s_1\\ s_3$ 三条消息进行响应。 主机和服务器hava a shared key:k。 知道密钥不能加密多次，于是主机将三条消息进行concatenation（联结）： $m_1||m_2||m_3$ 。 主机用k作为密钥，得到G(k)，进行加密 $[m_1||m_2||m_3]\\oplus\\text{G(k)}$ 。 同样，服务器也将响应消息进行联结： $s_1||s_2||s_3$ 。 服务器也用k作为密钥，得到相同的G(k)，对响应消息进行加密 $[s_1||s_2||s_3]\\oplus\\text{G(k)}$ 。 因此，在一次对话中，主机和服务器都使用了相同的 G(k)进行加密，也就是 two time pad。 如何改进主机和服务器have a shared pair of key, 即主机和服务器都使用不同的key进行加密。 Examples: 802.11b WEPHow it worksWEP(Wired Equivalent Privacy)，有效等效加密，是一种用于IEEE 802.11b的一种安全算法。这个算法设计的很糟糕，现已被WPA所淘汰。 WEP用于Wi-Fi通信，是他的的加密层。 WEP的算法过程如下： 主机和路由器进行通信： 主机和路由 have a shared key。 主机想要发送一段消息，包括明文m和其校验码CRC(m)。 PRG’s seed： IV||k, k is a long term key，IV is a counter. Length of IV: 24 bits. IV的作用：每一次传送数据包时，用IV来改变每次的密钥。 用(IV||k作为密钥，得到PRG(IV||k),使用流密码进行加密传输。 主机直接发送IV和密文。 路由器用收到的IV和k连接，用PRG(IV||k)，对密文解密。 Problems of IV IV 导致的问题1: two time pad Length of IV: 24 bits Related IV after $2^{24}$ (16M frames) 【当发送16百万的帧后，PRG又会重复】 On some 802.11 cards: IV rests to 0 after power cycle. 【在有些单片机上，重启后IV会变成0：每次重启都会使用PRG(0||k)加密】 IV 导致的问题2: related keys 在PRG中，key for frame is related。(IV||k)，k是104 bits, IV 是24 bits，所以key的后104位总是相同的，不同密钥之间的差异也很小。 对RC4 PRG的攻击： Fluhrer, Mantin, and Shamir在2001年:只需要监听 $10^6$ 帧即可破译密钥[1]。 Recent attacks：只需要监听4000帧，即可破译WEP网络中的密钥。 所以，密钥关联是灾难性的。 Avoid related keys！ A better construction对于WEP，一种更好的做法是：将多个要发送的帧联结起来，得到 $m_1||m_2||…||m_n$ 长流，再用PRG对这个长流加密。 如上图所示，k扩展后，被分成很多段，用第一段加密第一帧，第二段加密第二帧……。 这样，加密每一帧的密钥都是一个伪随机密钥。(no relationship, and looks like random)。 当然，更好的解决方法是使用更强的加密算法（WPA2）。 Examples: disk encryption另一个例子是硬盘加密，在这个例子中，你会发现：使用流密码对硬盘加密不是一个好的举措。 如果使用流密码： Alice 想要给Bob写一封邮件，如上图所示。 邮件经过硬盘加密（流密码）后，存入内存块。 Alice 想要对存在这个硬盘中的邮件进行修改。 Alice只把Bob改成了Eve，其他部分都没有变，如上图所示。 保存后，邮件再次经过硬盘加密（流密码）后，存入内存块。 Attacker：他得到了硬盘上最初的密文和修改后的密文。 通过分析，他发现两段密文只有前小部分不同。（用相同的流密码密钥加密，修改后，密文很容易看出变化） 虽然Attacker不知道Alice是怎么修改的，但是他知道了Alice修改的具体位置。 $\\Rightarrow$ attacker得到了他不应该知道的信息，即修改的具体位置。 在硬盘加密中，对于文本内容的修改前后，也使用了相同的密钥段加密不同的消息，即two time pad。 因此在硬盘加密中，不推荐使用流密码。 Two time pad: SummaryNever use stream cipher key more than once!! Network traffic: negotiate new key for every session. Disk encryption: typically do not use a stream cipher. Attack2: no integrity(OTP is malleable)OPTP和Stream Cipher一样，不提供完整性的保证，只提供机密性的保证。 如上图所示： 如果attacke截获：密文 $m\\oplus k$ 。 并用sub-permutation key（子置换密钥）来对密文进行修改，得到新的密文：$(m\\oplus k)\\oplus p$ 新的密文最后解密得到的明文是 $m\\oplus p$ 。 所以对于有修改密文能力的攻击者来说，攻击者很容易修改密文，并且修改后的密文，对原本解密后的明文也有很大的影响。 具体攻击如下： Bob想要发送一封邮件：消息开头是From: Bob，使用OTP加密后，发送密文。 Attacker：截获了这段密文 假设：attacker知道这封邮件是来自Bob。 attacker想修改这封密文邮件，使得它来自others。 于是它用一个子置换密钥对原密文的特定位置（即Bob密文位置）进行操作，得到新的密文：From： Eve。 这个子置换密钥是什么呢？ 如上图所示，Bob的ASCII码是 42 6F 62，Eve的ASCII码是 45 76 65。 那么Bob $\\oplus$ Eve的ASCII码是 07 19 07。 因此这个子置换密钥是07 19 07。 最后收件人进行解密，得到的是明文：From：Eve。 对attacker来说，虽然他不能创建来自Eve的密文，但是他可以通过修改原本的密文，达到相同的目的。 Conclusion: Modifications to ciphertext are undertected and have predictable impact on plaintext. Real-world Stream CiphersOld example(SW): RC4RC4流密码，是Ron RivestRC4在1987年设计的。曾用于secure Web traffic(in the SSL/TLS protocol) 和wireless traffic (in the 802.11b WEP protocol). It is designed to operate on 8-bit processors with little internal memory. At the heart of the RC4 cipher is a PRG, called the RC4 PRG. The PRG maintains an internal state consisting of an array S of 256 bytes plus two additional bytes i,j used as pointers into S. 【RC4 cipher的核心是一个PRG，called the RC4 PRG。这个PRG的内部状态是一个256字节的数组S和两个指向S数组的指针】 RC4 stream cipher generator setup algorithms: 对S数组进行初始化，0-255都只出现一次入下图所示： 伪代码 stream generator: Once tha array S is initialized, the PRG generates pseudo-random output one byte at atime, using the following stream generator: The procedure runs for as long as necessary. Again, the index i runs linearly through the array while the index j jumps around. Security of RC4具体参见「A Graduate Course in Applied Cryptography」的p76-78 挖坑，有空填坑 cryptanalysis of RC4[2] Weakness of RC4 Bias in initial output: Pr[2^nd^ byte=0]=2/256. 如果PRG是随机的，其概率应该是1/256。 而Pr[2^nd^ byte=0]=2/256的结果是：消息的第二个字节不被加密的概率比正常情况多一倍。（0异或不变） 统计的真实情况是，不止第二个字节，前面很多字节的概率很不都均匀。 因此，如果要使用RC4 PRG，从其输出的257个字节开始使用。 Prob. of (0,0) is 1/256^2^ +1/256^3^ . 如果PRG是随机的，00串出现的概率应该是(1/256)^2^ . Related key attackes. 在上小节中「Examples: 802.11b WEP」，WEP使用的RC4流密码，related key对安全通信也是灾难性的。 Old example(HW): CSS (badly broken)The Content Scrambling System (CSS) is a system used for protecting movies on DVD disks. It uses a stream cipher, called the CSS stream cipher, to encrypt movie contents. CSS was designed in the 1980’s when exportable encryption was restricted to 40-bits keys. As a result, CSS encrypts movies using a 40-bits key. 【1980的美国出口法，限制出口的加密算法只能是40位，于是CSS的密钥是40位】[amazing.jpg] While ciphers using 40-bit keys are woefully insecure, we show that the CSS stream cipher is particularly weak and can be broken in far less time than an exhaustive search over all 2^40^ keys. 【虽然40位的密钥本来就不够安全，但是我们能用远小于穷举时间的方法破解CSS】 因为博主也是第一次学，很多东西也不了解。 所以概述性语言，我用教科书的原文记录，附注一些中文。望海涵～ Linear feedback shift register(LFSR)CSS 流密码是由两个LFSR（线性反馈移位寄存器）组成的，除了CSS，还有很多硬件系统是基于LFSR进行加密操作，但无一例外，都被破解了。 DVD encryption (CSS)：2 LFSRs GSM encryption (A5/,2): 3 LFSRs 【全球通信系统】 Bluetooth(E0): 4LFSRs LFSR can be implemented in hardware with few transistors. And a result, stream ciphers built from LFSR are attractive for low-cost consumer electronics such as DVD players, cell phones, and Bluetooth devices. 【LFSR在硬件上运行很快，也很省电，所以虽然基于LFSR的算法都被破解了，但是改硬件有点困难，所以还是有很多系统在使用】 上图是一个8位LFSR{4,3,2,0}。 LFSR是由一组寄存器组成，LFSR每个周期输出一位（即0位）。 有一些位（如上图的4，3，2，0）称为tap positions(出头)，通过这些位计算出feedback bit(反馈位)。 反馈位和寄存器组的未输出位组成新的寄存器组。 伪代码如下： 所以基于LFSR的PGR的seed是寄存器组的初始状态。 how CSS worksCSS的seed=5 bytes=40 bits。 CSS由两个LFSR组成，如下图所示，一个17-bit LFSR，一个25-bit LFSR。 seed of LFSR: 17-bit LFSR: 1||first 2 bytes ，即17位。 25-bit LFSR: 1||last 3 bytes，即25位。 CSS过程： 两个LFSR分别运行8轮，输出8 bits。 两个8bits 相加mod 256（还有加上前一块的进位）即是CSS一轮的输出：one byte at a time. Cryptanalysis of CSS (2^16 time attack)当已知CSS PRG的输出时，我们通过穷举（2^40^ time）破解得到CSS的seed。 但还有一种更快的破解算法，只需要最多2^16^ 的尝试。 破解过程如下： 影片文件一般是MPEG文件，假设我们已知明文MPEG文件的前20个字节。（已知明文的prefix） CSS是流密码，可以通过已知prefix还原出CSS的prefix，即CSS PRG的前20个字节的输出。 For all possible initial settings of 17-bit LFSR do: run 17-it LFSR to get 20 bytes of output. 【先让17-bit输出20个字节】 subtract from CSS prefix $\\Rightarrow$ candidate 20 bytes output of 25-bit LFSR. 【通过还原的CSS PRG的输出，得到25-bit输出的前20个字节】 If consistent with 25-bit LFSR, found correct initial settings of both!! 【具体是如何判别这20个字节是否是一个25-bit LFSR的输出呢？】 假设前三个字节是y1, y2, y3. 那么25-bit LFSR的initial :{1, y1 , y2, y3},其中y都是8 bits. 用这个初始状态生成20个字节，如果和得到的20个字节相同，则正确，否则再次枚举。 当得到了两个LFSR的seed, 就可以得到CSS PRG的全部输出，即可破解。 Modern stream ciphers: eStreammain idea eStream PRG ： $\\{0,1\\}^s\\times R \\rightarrow \\{0,1\\}^n$ (n&gt;&gt;s) seed: ${0,1}^s$ nonce: a non-repeating value for a given key.【就对于确定的seed,nonce绝不重复】 Encryption: $\\text{E(k, m; r)}=\\text{m}\\oplus \\text{PRG(k; r)} $ The pair (k,r) is never used more than once. 【PRG的输入是(k,r), 确保OTP】 eStram: Salsa 20(SW+HW)Salsa20/12 and Salsa20/20 are fast stream ciphers designed by Dan Bernstein in 2005. Salsa 20/12 is one of four Profile 1 stream cipher selected for the eStream portfolio of stream ciphers. eStream is a project that identifies fast and secure stream ciphers that are appropriate for practicle use. Variants of Salsa20/12 and Salsa20/20, called ChaCha12 and ChaCha20 respectively, were proposed by Bernstein in 2008. These stream ciphers have been incorporated into several widely deployed protocols such as TLS and SSH. Salsa20 PRG: $\\{0,1\\}^{128 \\text { or } 256} \\times\\{0,1\\}^{64} \\longrightarrow\\{0,1\\}^{n}$ (max n = 2^73^ bits) Salsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ 通过计数器，使得输出联结，可以输出as long as you want pseudorandom segment. 算法过程如上图所示：Salsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ 32 bytes的{k,r,i}通过扩展得到64 bytes的{ $\\tau_0,k,\\tau_1,r,i,\\tau_2,k,\\tau_3$ }. k :16 bytes的seed. i: 8 bytes的index，计数器。 r: 8 bytes的nonce. $\\tau_{0,1,2,3}$ 都是4 bytes的常数，Salsa20算法说明书上公开确定的值。 64 bytes 通过h函数映射，十轮。 h : invertible function designed to be fast on x86(SEE2). 在x86上有SEE2指令可以直接运行h函数，所以很快。 h是逆函数（也是公开的函数），输出可以通过逆运算得到其输入。 h是一个一一映射的map，每一个64bytes的输入都有唯一对应的64 bytes的输出。 将第十轮H函数的输出和最开始的输入做加法运算，word by word(32位)，即每4 bytes相加。 为什么要有这一步？ h是可逆运算，如果直接将函数的输出作为PRG的输出，那可以通过h的逆运算得到原64 bytes，也就得到了(k; r). Is Salsa20 secure(unpredictable)前文我们通过unpredictable来定义PRG的安全（下一篇文章会给出安全PRG更好的定义），所以Salsa20 安全吗？是否是不可预测的？ Unknown：no known provably secure PRGs. 【不能严格证明是一个安全PRG（后文会继续讲解什么是安全的PRG），如果严格证明了，也就证明了P=NP】 In reality： no known attacks bertter than exhaustive search. 【现实中还没有比穷举更快的算法】 Performance通过下图的比较，如果在系统中需要使用流密码，建议使用eStream。 speed ：该算法每秒加密多少MB的数据。 Reference S. Fluhrer, I. Mantin, and A. Shamir. Weaknesses in the key scheduling algorithm of RC4. In proceedings of selected areas of cryptography (SAC), pages 1-24, 2001. 「A Graduate Course in Applied Cryptography」p76-78:挖坑 待补充","link":"/2020/03/19/StreamCipher2/"},{"title":"「Cryptography-Dan」:Stream Cipher 3","text":"Stream Cipher的第三篇文章。 文章主要分为两部分，前部分逐步定义Secure PRG的定义，通过引入statistical test（统计测试）和Advantage（优势）得出当且仅当PRG is unpredictable,PRG is secure的结论。 后部分介绍了密码学中的一个重要概念Semantic Security的定义，通过引入 computationally indistinguishable(计算上不可区分)的概念给出定义，并证明了OTP的语意安全和在安全PRG条件下的流密码的语意安全，得出如果流密码中使用的PRG is secure,那么流密码就具备semantic security。 文章开头，也简单介绍了密码学中negligible和non-negligible的含义。 Negligible and non-negligibleIn practice: $\\epsilon$ is a scalar and $\\epsilon$ non-neg: $\\epsilon \\geq 1/2^{30}$ (likely to happen over 1GB of data) $\\epsilon$ negligible: $\\epsilon \\leq 1/2^{80}$ (won’t happen over life of key) 在实践中，$\\epsilon$ 是一个数值，如果是non-neg不可忽略的话，大约在1GB的数据下就会发生，如果是可忽略的值，在密钥的生存周期内基本不会发生。 In theory: $\\epsilon$ is a function $\\varepsilon: Z^{\\geq 0} \\rightarrow R^{\\geq 0}$ and $\\epsilon$ non-neg: $\\exists d: \\epsilon(\\lambda)\\geq 1/\\lambda^d$ ($\\epsilon \\geq 1/\\text{poly} $, for many $\\lambda$ ) $\\epsilon$ negligible: $\\forall d, \\lambda \\geq \\lambda_{d}: \\varepsilon(\\lambda) \\leq 1 / \\lambda^{d}$ ( $\\epsilon \\leq 1/\\text{poly}$, for large $\\lambda$ ) [0]（理论中，这个还不太理解，待补充。） PRG Security DefsLet $G:K\\longrightarrow \\{0,1\\}^n$ be a PRG. Goal: define what it means that [ $k\\stackrel{R}{\\longleftarrow} \\mathcal{K}$ , output G(k) ] is “indistinguishable” from [ $r\\stackrel{R}{\\longleftarrow} \\{0,1\\}^n$ , output r] . 【使得PGR的输出和真随机是不可区分的】（ $\\stackrel{R}{\\longleftarrow}$ 的意思是在均匀分布中随机取） 下图中，红色的圈是全部的 ${0,1}^n$ 串，按照定义是均匀分布。而粉色G()是PRG的输出，由于seed很小，相对于全部的 ${0,1}^n$ ，所以G()的输出范围也很小。 因此，attacker观测G(k)的输出，是不能和uniform distribution（均匀分布）的输出区分开。 这也就是我们所想构造的安全PGR的目标。 Statistical TestsStatistical test on ${0,1}^n$ ：有一个算法A，$x\\in{0,1}^n$ ,A(x) 根据算法定义输出”0”或”1”. 统计测试是自定义的。 Example： A(x)=1 if $| \\#0(x)-\\#1(x)|\\leq 10\\cdot\\sqrt{n}$ 【期望串中0、1的数目差不多，这样look random】 A(x)=1 if $|\\#00(x)-\\frac{n}{4}\\leq10\\cdot\\sqrt{n}$ 【期望Pr(00)=1/4 ,串中00出现的概率为1/4，认为是look random】 A(x)=1if max-run-of-0(x) &lt; 10·log(n) 【期望串中0的最大游程不要超过规定值】 上面的第三个例子，如果输出为全1，满足上述的统计条件输出1，但全1串看起来并不random。 统计测试也由于是自定义的，所以通过统计测试的也不一定是random，其PRG也不一定是安全的。 所以，如何评估一个统计测试的好坏？ 下面引入一个重要的定义advantage，优势。 Advantage引入Advantage（优势）来评估一个统计测试的好坏。 Let G: $k \\rightarrow \\{0,1\\}^n$ be a PRG and A a stat. test on ${0,1}^n$ 【G是一个PRG，A是一个对01串的统计测试】 Define: the advantage of statisticaltest A relative to PRG G Adv$_\\text{PRG}[A,G]$ $\\text{Adv}_\\text{PRG}[A,G]=|Pr[A(G(k))=1]-Pr[A(r)=1]|\\in[0,1]$ , $k\\stackrel{R}{\\longleftarrow} \\mathcal{K}, r\\stackrel{R}{\\longleftarrow}\\{0,1\\}^n$ 【定义：Adv$_\\text{PRG}[A,G]$ 为统计测试A对于PRG G的优势为统计测试以PRG作为输入输出为1的概率 减去 统计测试以truly random string 作为输入输出为1的概率】 Adv close to 1 : 统计测试能区分PRG的输出和truly random string，即adversary可以利用区分PRG的输出和random的这一点从而破解系统。 Adv close to 0 : 统计测试不能区分PRG的输出和truly random string, 即adversary认为PRG的输出和random别无二致。 Advantage 优势[1] In cryptography, an adversary’s advantage is a measure of how successfully it can attack a cryptographic algorithm, by distinguishing it from an idealized version of that type of algorithm.Note that in this context, the “adversary” is itself an algorithm and not a person. A cryptographic algorithm is considered secure if no adversary has a non-negligible advantage, subject to specified bounds on the adversary’s computational resources (see concrete security). 在密码学中，adversary的优势是评估它通过某种理想算法破解一个加密算法的成功尺度。 这里的adversary是一种破解算法而不是指攻击者这个人。 当没有 adversary对该加密算法有不可忽略的优势时，该加密算法被认为是安全的，因为adversary只有有限的计算资源。 e.g.1 : A(x) = 0，统计测试A对PRG的任何输出都输出0，则Adv[A,G] = 0. e.g.2 : G: $k \\rightarrow {0,1}^n$ satisfies msb(G(k))=1 for 2/3 of keys in K. Define statistical test A(x) as : if[ msb(x)=1 ] output “1” else output “0” 【PRG G(k)的2/3的输出的最高有效位是1，定义统计测试A(x),输入的最高有效位为1输出1，否则输出0】 msb: most significant bit,最高有效位。 则 Adv[A,G] = | Pr[A(G(k))] - Pr[A(r)] | = 2/3 - 1/2 = 1/6 即 A can break the generator G with advantage 1/6, PRG G is not good. Secure PRGs: crypto definitionDef: We say that G: $k \\rightarrow {0,1}^n$ is secure PRG if $\\forall$ “eff” stat. tests A : Adv$_\\text{PRG}$ [A,G] is “negligible”. 这里的”eff”,指多项式时间内。 这个定义，“所有的统计测试”，这一点难以达到，因此没有provably secure PRGs。 但我们有heuristic candidates.（有限的stat. test 能满足） Easy fact: a secure PRG is unpredictable证明命题： a secure PRG is unpredictable. 即证明其逆否命题： PRG is predictable is insecure。 suppose A is an eddicient algorithm s.t. $\\text{Pr}_{k\\stackrel{R}{\\longleftarrow}\\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\\frac{1}{2} + \\epsilon$ for non-negligible $\\epsilon$ . 【 算法A是一个有效的预测算法， $\\text{Pr}_{k\\stackrel{R}{\\longleftarrow}\\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\\frac{1}{2} + \\epsilon$ , $\\epsilon$ 是不可忽略的值，即A能以大于1/2的概率推测下一位。】 Adversary能利用算法A来区分这个PRG和random依次来破解系统。 Define statistical test B as: B(x)=1 if $A(x|{1,…,i})=x{i+1}$ , else B(x)=0. 【定义一个统计测试B，如果预测算法A预测下一位正确输出1，否则输出0】 计算Adv$_\\text{PRG}$ : $r\\stackrel{R}{\\longleftarrow}\\{0,1\\}^n$ : Pr[B(r) = 1] = 1/2 $r\\stackrel{R}{\\longleftarrow}\\{0,1\\}^n$ : Pr[B(G(k)) = 1] = 1/2+ $\\epsilon$ Adv$_\\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | &gt; $\\epsilon$ Adversary 能以 $\\epsilon$ 的优势区分PRG和random，因此PRG 不安全。 所以，如果A是一个好的预测算法，那么B就是一个好的统计算法，Adversary就能通过B以 $\\epsilon$ 的优势破解系统。 在此，证明了 if A can predict PRG, PRG is insecure $\\Rightarrow$ A secure PRG is unpredictable. Thm(Yao’82): an unpredictable PRG is secure上节证明了A secure PRG is unpredictable. 在1982 Yao 的论文[2]中证明了其逆命题： an unpredictable PRG is secure. G: $k \\rightarrow {0,1}^n$ be PRG “Thm“ : if $\\forall i \\in$ {0,…, n-1} PRG G is unpredictable at pos. i then G is a secure PRG. 【定理：如果在任何位置PRG都是不可预测的，那么PRG就是安全的】 Proof： A: 预测算法： $\\forall i \\quad\\text{Pr}_{k\\stackrel{R}{\\longleftarrow}\\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]=\\frac{1}{2} $ B:统计测试： B(x)=1 if $A(x|{1,…,i})=x{i+1}$ , else B(x)=0. Adv$_\\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | = 0 If next-bit predictors cannot distinguish G from random then no statistial test can! 【next-bit predictor指用预测算法的统计测试】 e.g. Let G: $k \\rightarrow {0,1}^n$ be PRG such that from the last n/2 bits of G(k) it is easy to compute the first n/2 bits. Is G predictable for som i $\\in$ {0, …, n-1} ? : Yes.当n=2时，可以预测出第一位。 在此，可以得出结论： Adversary不可区分PRG的输出和truly random string时被认为是安全的。 当且仅当PRG在任意位置不可预测时，PRG是安全的。 More generally: computationally indistinguishableLet P1 and P2 be two distributions over ${0,1}^n$ Def : We say that P1 and P2 are computationally indistinguishable (denoted $P_1\\approx_p P_2$) If $\\forall$ “eff” stat. tests A $\\left|\\text{Pr}_{x\\leftarrow_{P_1}}[A(x)=1]-\\text{Pr}_{x\\leftarrow_{P_2}}[A(x)=1]\\right|","link":"/2020/06/26/StreamCipher3/"},{"title":"「PyTorch」：Tensors Explained And Operations","text":"PyTorch框架学习。 本篇文章主要介绍PyTorch中的Tensor及其基本操作，主要分为四个方面：Reshape, Element-wise, Reduction和Access。 Tensor的具体操作介绍，建议配合Colab笔记使用： PyTorch Tensors Explained Tensor Operations: Reshape Tensor Operations: Element-wise Tensor Operation: Reduction and Access 英文的表达解释都是比较清晰且精确的，所以以英语的形式作为主要记录，文中会夹带一些中文总结语句，方便阅读。 Introducing TensorsTensor Explained - Data Structures of Deep LearningWhat Is A Tensor?A tensor is the primary data structure used by neural networks. 【Tensor是NN中最主要的数据结构】 Indexes Required To Access An ElementThe relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure. 【以下pairs都是需要同等数量的indexes才能确定特定的元素。】 【而tensor是generalizations，是一种统一而普遍的定义。】 Indexes required Computer science Mathematics 0 number scalar 1 array vector 2 2d-array matrix Tensors Are GeneralizationsWhen more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language. MathematicsIn mathematics, we stop using words like scalar, vector, and matrix, and we start using the word tensor or nd-tensor. The n tells us the number of indexes required to access a specific element within the structure. 【数学中，当我们需要用大于两个的indexes才能确定特点元素时，我们使用tensor或者nd-tensor来表示该数据结构，说明需要n个index才能确定该数据结构中的特定元素。】 Computer ScienceIn computer science, we stop using words like, number, array, 2d-array, and start using the word multidimensional array or nd-array. The n tells us the number of indexes required to access a specific element within the structure. 【计算机科学中，我们使用nd-array来表示，因此，nd-array和tensor实则是一个东西。】 Indexes required Computer science Mathematics n nd-array nd-tensor Tensors and nd-arrays are the same thing! One thing to note about the dimension of a tensor is that it differs from what we mean when we refer to the dimension of a vector in a vector space. The dimension of a tensor does not tell us how many components exist within the tensor. 【需要注意的地方是，tensor中的维度和vector向量空间中的维度不是同一个东西，vector向量空间中的维度表示该vector有多少个元素组成的，而tensor中的维度是下文中rank的含义。】 Rank, Axes, And Shape Explained【下文会详细解释深度学习tensor的几个重要性质：Rank, Axes, Shape.】 The concepts of rank, axes, and shape are the tensor attributes that will concern us most in deep learning. Rank Axes Shape Rank And IndexesWe are introducing the word rank here because it is commonly used in deep learning when referring to the number of dimensions present within a given tensor. The rank of a tensor tells us how many indexes are required to access (refer to) a specific data element contained within the tensor data structure. A tensor’s rank tells us how many indexes are needed to refer to a specific element within the tensor. 【这里的rank实则就是tensor的维度。】 【tensor的rank值告诉我们需要多少个indexes才能确定该tensor中的特定元素。】 Axes Of A TensorIf we have a tensor, and we want to refer to a specific dimension, we use the word axis in deep learning. An axis of a tensor is a specific dimension of a tensor. Elements are said to exist or run along an axis. This running is constrained by the length of each axis. Let’s look at the length of an axis now. Length Of An AxisThe length of each axis tells us how many indexes are available along each axis. 【当我们关注tensor的某一具体维度时，在深度学习中我们使用axis来表达。】 【元素被认为是在某一axie上存在或延伸的，元素延伸的长度取决于axis的长度。】 【Axis的长度表示在每一维度（axis）上有多少个索引】 Shape Of A TensorThe shape of a tensor is determined by the length of each axis, so if we know the shape of a given tensor, then we know the length of each axis, and this tells us how many indexes are available along each axis. The shape of a tensor gives us the length of each axis of the tensor. 【tensor的shape由每一axis的长度决定，即每一axis的索引数目】 Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called reshaping. Reshaping changes the shape but not the underlying data elements. 【tensor的常见操作reshape只改变tensor的shape，而不改变底层的数据。】 CNN Tensors Shape ExplainedCNN的相关介绍，可见 这篇文章 What I want to do now is put the concepts of rank, axes, and shape to use with a practical example. To do this, we’ll consider an image input as a tensor to a CNN. Remember that the shape of a tensor encodes all the relevant information about a tensor’s axes, rank, and indexes, so we’ll consider the shape in our example, and this will enable us to work out the other values. 【tensor的shape能体现tensor的axes、rank、index所有信息】 【以CNN为例来说明rank, axes, shape.】 Shape Of A CNN InputThe shape of a CNN input typically has a length of four. This means that we have a rank-4 tensor with four axes. Each index in the tensor’s shape represents a specific axis, and the value at each index gives us the length of the corresponding axis. 【CNN的input 是一个rank4-tensor.】 Each axis of a tensor usually represents some type of real world or logical feature of the input data. If we understand each of these features and their axis location within the tensor, then we can have a pretty good understanding of the tensor data structure overall. 【tensor的每个axis往往代表着某一个逻辑feature，所以理解features和tensor中axis的位置的关系能帮助我们更好的理解tensor。】 Image Height And WidthTo represent two dimensions, we need two axes. The image height and width are represented on the last two axes. 【表示图像的height和width，需要2个axes，使用最后两个axes表示。】 Image Color ChannelsThe next axis represents the color channels. Typical values here are 3 for RGB images or 1 if we are working with grayscale images. This color channel interpretation only applies to the input tensor. 【下一个axis(从右至左)表示图像的color channels（颜色通道，如灰度图像就有1个颜色通道，RGB图像有三个）。】 【注意：color channel的说法只适用于input tensor。】 Image BatchesThis brings us to the first axis of the four which represents the batch size. In neural networks, we usually work with batches of samples opposed to single samples, so the length of this axis tells us how many samples are in our batch. Suppose we have the following shape [3, 1, 28, 28] for a given tensor. Using the shape, we can determine that we have a batch of three images. 【第一个axis表示batch属性，表明该batch的size。在深度学习中，我们通常使用一批样本，而不是一个单独的样本，所以这一维度表明了我们的batch中有多少样本。】 tensor：[Batch, Channels, Height, Width] Each image has a single color channel, and the image height and width are 28 x 28 respectively. Batch size Color channels Height Width NCHW vs NHWC vs CHWNIt’s common when reading API documentation and academic papers to see the B replaced by an N. The N standing for number of samples in a batch. 【在API文档或学术论文中，N经常会代替代替B，表示the number of samples in a batch。】 Furthermore, another difference we often encounter in the wild is a reordering of the dimensions. Common orderings are as follows: NCHW NHWC CHWN 【除此之外，也会经常遇到这些axes的其他顺序。】 As we have seen, PyTorch uses NCHW, and it is the case that TensorFlow and Keras use NHWC by default (it can be configured). Ultimately, the choice of which one to use depends mainly on performance. Some libraries and algorithms are more suited to one or the other of these orderings. 【PyTorch 默认使用NCHW，而TensorFlow和Keras使用NHWC】 Output Channels And Feature MapsLet’s look at how the interpretation of the color channel axis changes after the tensor is transformed by a convolutional layer. Suppose we have three convolutional filters, and lets just see what happens to the channel axis. Since we have three convolutional filters, we will have three channel outputs from the convolutional layer. These channels are outputs from the convolutional layer, hence the name output channels opposed to color channels. 【tensor送入convolutional layer（卷积层）后，color channel 这一axis的长度发生变化。 【在中解释到，有几个convolutional filters，卷积层输出的tensor就有几个channel（channel代替color channel的表达）。】 Feature MapsWith the output channels, we no longer have color channels, but modified channels that we call feature maps. These so-called feature maps are the outputs of the convolutions that take place using the input color channels and the convolutional filters. Feature maps are the output channels created from the convolutions. 【卷积层输出tensor的channel维度代替color channels的叫法。】 【卷积层的输出也叫叫feature maps】 PyTorch TensorsWhen programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form. 【数据预处理往往是编写NN的第一步，将原始数据转换为tensor form。】 Tensor的基本操作见Colab运行笔记链接：PyTorch Tensors Explained (不会用的也可以直接看github 上的) PyTorch Tensors Attributes torch.dtype：tensor包含数据类型。 常见数据类型： Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point torch.float16 torch.HalfTensor torch.cuda.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 torch.LongTensor torch.cuda.LongTensor torch.device: tensor数据所分配的设备，如CPU，cuda:0 torch.layout: tensor在内存中的存储方式。 As neural network programmers, we need to be aware of the following: Tensors contain data of a uniform type (dtype). Tensor computations between tensors depend on the dtype and the device. 【Tensors包含相同类型的数据】 【Tensors之间的计算取决于他的类型和他所分配的设备】 Creating TensorsThese are the primary ways of creating tensor objects (instances of the torch.Tensor class), with data (array-like) in PyTorch: Creating Tensors with data. 【四种用数据创建tensor的方式】 torch.Tensor(data) torch.tensor(data) torch.as_tensor(data) torch.from_numpy(data) torch.Tensor() Vs torch.tensor()The first option with the uppercase T is the constructor of the torch.Tensor class, and the second option is what we call a factory function that constructs torch.Tensor objects and returns them to the caller. However, the factory function torch.tensor() has better documentation and more configuration options, so it gets the winning spot at the moment. 【torch.Tensor(data) 是 torch.Tensor class的Constructor，而torch.tensor(data) 是生成/返回 torch.Tensor class的函数（factory functions)】 【因为torch.tensor() 有更多的选项设置，比如可以设置数据类型，所以一般用torch.tensor() 来生成。】 Default dtype Vs Inferred dtypeThe difference here arises in the fact that the torch.Tensor() constructor uses the default dtype when building the tensor. The other calls choose a dtype based on the incoming data. This is called type inference. The dtype is inferred based on the incoming data. 【torch.Tensor() 在生成tensor时，使用的是默认dtype=torch.float32 ，而其他三种是使用的引用dtype ，即生成tensor的数据类型和输入的数据类型一致。】 Sharing Memory For Performance: Copy Vs Sharetorch.Tensor() and torch.tensor() copy their input data while torch.as_tensor() and torch.from_numpy() share their input data in memory with the original input object. This sharing just means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the torch.Tensor and the numpy.ndarray. Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory. 【torch.Tensor() 和 torch.tensor() 在根据data创建tensor时，在内存中额外复制数据】 【torch.as_tensor() 和 torch.from_numpy() 在根据data创建tensor时，是和原输入数据共享的内存，即原numpy.ndarry的数据改变，相应的tensor也会改变。】 Share Data Copy Data torch.as_tensor() torch.tensor() torch.from_numpy() torch.Tensor() Some things to keep in mind about memory sharing (it works where it can): Since numpy.ndarray objects are allocated on the CPU, the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used. 【在使用GPU时， as_tensor() 也会将ndarray数据从CPU复制到GPU上。】 The memory sharing of as_tensor() doesn’t work with built-in Python data structures like lists. 【as_tensor() 在Python内置数据结构时不会共享内存】 The as_tensor() performance improvement will be greater if there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. 【as_tensor() 在ndarry和tensor之间大量连续操作时能有效提高性能】 torch.as_tensor() Vs torch.from_numpy()This establishes that torch.as_tensor() and torch.from_numpy() both share memory with their input data. However, which one should we use, and how are they different? The torch.from_numpy() function only accepts numpy.ndarrays, while the torch.as_tensor() function accepts a wide variety of array-like objects, including other PyTorch tensors. 【这两个都是和输入数据共享内存，但 torch.from_numpy() 只能接受numpy.ndarrays 类型的数据，而torch.as_tensor() 能接受array-like(像list, tuple)等类型，所以一般torch.as_tensor() 更常用。】 If we have a torch.Tensor and we want to convert it to a numpy.ndarray 【用torch.numpy() 把tensor转换为ndarray】 Creating Tensors without data. 【还有几种创建常见tensor的方式】 torch.eyes(n) : 创建2-D tensor，即n*n的单位向量。 torch.zeros(shape) : 创建shape=shape的全0tensor。 torch.ones(shape) : 创建全1tensor。 torch.rand(shape) : 创建随机值tensor。 Tensor Operation关于Tensor 操作的Colab运行笔记。对照使用最佳。如果打不开也可以看github Tensor Operations: Reshape Tensor Operations: Element-wise Tensor Operation: Reduction and Access We have the following high-level categories of operations: Reshaping operations Element-wise operations Reduction operations Access operations 【对tensor的操作主要分为4种：reshape, element-wise, reduction, access】 ReshapeAs neural network programmers, we have to do the same with our tensors, and usually shaping and reshaping our tensors is a frequent task. 【reshape在NN编程中是很常见的操作】 （具体操作见colab运行笔记本:Tensor Operations: Reshape ） 12345678import torcht = torch.tensor([ [1,1,1,1], [2,2,2,2], [3,3,3,3]], dtype=torch.float32)t.reshape([2,6])t.reshape(2,2,3) Reshaping changes the tensor’s shape but not the underlying data. Our tensor has 12 elements, so any reshaping must account for exactly 12 elements. 【reshape操作不改变底层的数据，只是改变tensor的shape】 In PyTorch, the -1 tells the reshape() function to figure out what the value should be based on the number of elements contained within the tensor. 【reshape中传入的-1参数，PyTorch可以自动计算该值，因为PyTorch要保证tensor的元素个数不变】 Squeezing And Unsqueezing Squeezing a tensor removes the dimensions or axes that have a length of one. 【Squeezing操作：移除tensor中axis长度为1的维度】 Unsqueezing a tensor adds a dimension with a length of one. 【Unsqueezing操作：增加一个axis长度为1的维度】 （具体操作见colab运行笔记本:Tensor Operations: Reshape ） 12t.squeeze()t.squeeze().unsqueeze(dim=0) Concatenation TensorsWe combine tensors using the cat() function, and the resulting tensor will have a shape that depends on the shape of the two input tensors. （具体操作见colab运行笔记本:Tensor Operations: Reshape ） 12torch.cat((t1,t2,t3), dim=0)torch.cat((t1,t2,t3), dim=1) Flatten这里从CNN的例子看Flatten，CNN的相关细节见：这篇文章 A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input. 【flatten在卷积层网络很常见，因为输入必须flatten后才能连接到一个全连接网络层】 对于MNIST数据集中18*18的手写数字，在前文说到CNN的输入是[Batch Size, Channels, Height, Width] ，怎么才能flatten tensor的部分axis，而不是全部维度。 CNN的输入，需要flatten的axes：(C,H,W) 从dim1维度开始flatten（具体操作见colab运行笔记本:Tensor Operations: Reshape ） 1t.flatten(start_dim=1, end_dim=-1) Broadcasting and Element-WiseAn element-wise operation operates on corresponding elements between tensors. 【element-wise操作两个tensor之间对应的元素。】 BroadcastingBroadcasting describes how tensors with different shapes are treated during element-wise operations. Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors. 【broadcast描述了不同shape之间的tensor如何进行element-wise操作】 【broadcast运行我们增加scalars到高维度】 Let’s think about the t1 + 2 operation. Here, the scaler valued tensor is being broadcasted to the shape of t1, and then, the element-wise operation is carried out. 【在t1+2时，scalar 2实际是先被broadcast到和t1相同的shape, 再执行element-wise操作】 We have two tensors with different shapes. The goal of broadcasting is to make the tensors have the same shape so we can perform element-wise operations on them. （具体操作见colab运行笔记本:Tensor Operations: Element-wise ） Broadcasting Details（具体操作见colab运行笔记本:Tensor Operations: Element-wise ） Same Shapes: 直接操作 Same Rank, Different Shape: Determine if tensors are compatible（兼容）. 【两个tensor兼容，才可以对tensor broadcast，再执行element-wise操作】 We compare the shapes of the two tensors, starting at their last dimensions and working backwards. Our goal is to determine whether each dimension between the two tensors’ shapes is compatible. 【从最后一个维度向前判断，每个维度是否兼容】 【判断该维度兼容的条件是满足下面两个条件其一：维度长度相同；或者其中一个为1】 The dimensions are compatible when either: They’re equal to each other. One of them is 1. Determine the shape of the resulting tensor. 【操作的结果是一个新的tensor，结果tensor的每个维度长度是原tensors在该维度的最大值】 Different Ranks: Determine if tensors are compatible.(同上) When we’re in a situation where the ranks of the two tensors aren’t the same, like what we have here, then we simply substitute a one in for the missing dimensions of the lower-ranked tensor. 【对低维度的tensor的缺失维度，用1来代替，比如shape为(1,3) 和 ()，低维度的shape变为(1,1)】 Determine the shape of the resulting tensor. ArgMax and ReductionA reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. 【reduction 操作是能减少tensor元素数量的操作。】 Reshaping operations gave us the ability to position our elements along particular axes. Element-wise operations allow us to perform operations on elements between two tensors, and reduction operations allow us to perform operations on elements within a single tensor. 【Reshape操作让我们能沿着某一axis操纵tensor 中的元素位置；Element-wise操作让我们能对tensors之间对应元素进行操作；Reduction操作能让我们对单个tensor间的元素操作。】 (具体操作见colab笔记本：Tensor Operation: Reduction and Access ) 1234t.sum()t.prod()t.mean()t.std() Reducing Tensors By Axes只需要对这些方法传一个维度对参数。 (具体操作见colab笔记本：Tensor Operation: Reduction and Access ) 12t.sum(dim=0)t.sum(dim=1) ArgmaxArgmax returns the index location of the maximum value inside a tensor. 【Argmax返回最大value的index】 (具体操作见colab笔记本：Tensor Operation: Reduction and Access ) 1t.argmax(dim=0) Aceessing Elements Inside TensorsThe last type of common operation that we need for tensors is the ability to access data from within the tensor. 【Access操作能获得tensor中的数据，即将tensor中的数据拿出来放在Python内置的数据结构中】 (具体操作见colab笔记本：Tensor Operation: Reduction and Access ) 123t.mean().item()t.mean(dim=0).tolist()t.mean(dim=0).numpy() Advanced Indexing And SlicingPyTorch Tensor支持大多数NumPy的index和slicing操作。 坑：https://numpy.org/doc/stable/reference/arrays.indexing.html Reference 挖坑：advanced indexing and slicing: https://numpy.org/doc/stable/reference/arrays.indexing.html","link":"/2020/10/21/Tensors/"},{"title":"「Math」:Entropy, Cross-Entropy and DL-Divergence","text":"在机器学习中，常用cross-entropy来作为模型的损失函数，这篇文章将阐述信息学中的entropy（熵）是什么，cross-entropy（交叉熵）又是什么，KL-Divergence和entropy、cross-entropy的关系是什么？ 如何具象的理解这些概念？ 在开始阅读这篇文章之前，先提及一下香农对bit的定义，香农认为bit是用来消除信息的不确定性的。 bit：uncertainty divided by 2. 原视频 讲的很好，本文只是在此基础上对一些总结，方便理解物质化（马原.jpg）。 公式总概bit：用来消除信息的不确定性 Entropy（熵）： $H(p)=-\\sum_i p_i\\log(p_i)$ 度量概率分布的平均信息量（即不确定性）。值越大，不确定性越大。 Cross-Entropy（交叉熵）： $H(p,q)=-\\sum_i p_i\\log(q_i)$ 度量两个分布的相似程度（一般 $p$ 为真实分布，$q$为预测分布），值越大，两个分布越不相似。 KL-Divergence（KL散度，也叫相对熵） ：$D_{KL}(p|q)=H(p,q)-H(q)$ 度量交叉熵超过熵的那一部分。 Entropy-熵实例1： sunny和rainy的发生的概率都是0.5，天气预报预测明天的天气为sunny，将sunny消息发给用户。 该条消息不管多长，有用的信息其实只有1个比特，即uncertainty divided by 2. 实例2： 有八种不同的天气，发生的概率相同，当天气预报将预测消息发送给用户时。 该条消息能使得uncertainty divided By 8.即有用信息为3个比特。 实例3: sunny发生的概率为0.75，rainy的概率为0.25，如果天气预测明天的天气： 将这个例子理解为抽球游戏，盒子里有3个红球（表示sunny天气），1个白球（rainy天气）。 事件 $X$ 表示为在盒子里抽中球的颜色，可得知抽中红球的概率为0.75，抽中白球的概率是0.25。 抽中哪个球是不确定的，即uncertainty 如果原来是4，即不知道将抽中这四个球中的哪一个。 如果抽中白球，那该信息表示：就是那4个球中的唯一一个白球，uncertainty 从原来的4变为1，即 uncertainty divided by 4.表示该信息，需要有用比特， $\\log_2(4)=\\log_2(1/0.25)=2$ 个比特来表示。即抽打白球的情况的不确定性更大，需要更多的比特来消除不确定性，来表示白球的发生。所以该条信息中只有2个比特是useful information. 如果抽中红球，该信息表示为：是那3个红球中的一个，uncertainty 从原来的4变为3 （如果和抽中白球的情况统一，最后的确定发生的uncertainty都表示为1，即在没有抽之前，抽到红球的uncertainty为 $1/0.75=4/3$ ） 即uncertainty divided by 4/3.表示该信息需要有用 $\\log_2(4/3)=\\log_2(1/0.75)=-\\log_2(0.75)=0.41$ 比特来表示。即抽到红球的情况不确定性没有那么大，只需要较少比特即可消除不确定性，来表示红球的发生。所以该条信息中只有0.41个比特是useful information. 这里也可以看出，如果一个事件的发生的概率越小（越不可能发生），即对该事件发生的不确定性越大，但一旦发生了，所携带的信息量就会很大，因为需要用更多的比特来消除不确定性。 回到本例子： 如果预测天气为rainy，将预测消息发给用户，则该条消息包含2比特（$\\log_2(1/0.25)=-\\log_2(0.25)=2$）的有用信息，即对rainy天气发送的不确定性更大，需要更多的比特来消除不确定性。 如果预测天气为sunny，因为在预测之前，用户对sunny发生的可能性就没有那么大，因此只需要0.41比特（$\\log_2(1/0.75)=-\\log_2(0.75)$)来消除不确定性。 那平均下来，气象局发送的平均信息量为 $0.75\\times 0.41+0.25\\times2=0.81$ bits. 因此我们用 $\\log_2(1/p)=-\\log_2p$ 来表示事件发生时所携带的信息量。（或者说需要这么多信息量来消除事件发生的不确定性） 用 $-\\sum_i{p_i}\\log_2{p_i}$ 来表示该事件的平均信息量（概率分布的不确定性），这就是信息熵（Entropy）。 Entropy：$$H(p)=-\\sum_i pi\\log_2(p_i)$$熵越大，说明携带的平均信息量越多，即不确定性越强，需要越多的比特来消除不确定性。所以熵是用来衡量不确定性的量。 和化学中衡量混乱程度的熵，是类似的。 Cross-Entropy-交叉熵例1： 从上面的实例2来看，即8中天气发生概率相同，对天气表示进行信息编码，为下图： entropy为3bits，而cross-entropy（交叉熵），也就是消息（比特流）的平均长度，为3bits. 例2： 但如果8种天气发生的可能性为下图： 算出来的entropy为2.23bits，即平均信息量为2.23bits。 如果仍是用这样的编码，cross-entropy为3bits，就多出一些冗余信息量。 例3: 如果换一种编码方式： 算出来的cross-entropy为 $0.35 \\times2+0.35\\times2+0.1\\times3+…+0.01\\times5=2.42$ bits，就非常接近entropy=2.23bits。 说明这种编码方式冗余量很小，非常接近真实的概率分布所包含的平均信息。 例4: 如果天气的概率分布变为下图，entropy不变仍然为2.23bits： 那么算出来的cross-entropy为 $0.01\\times2+0.01\\times 2+0.04\\times3+…+0.35\\times5=4.58$ bits，远大于entropy的值。说明这种编码方式冗余量很大。 换一种角度看例4，把信息编码认为是预测的概率分布，例4点编码表示的分布如下： Cross-Entropy：$$H(p,q)=-\\sum_i p_i\\log(q_i)$$所以cross-entropy可以理解为信息/比特流的平均长度。 如果预测的概率分布非常接近真实的概率分布，那比特流的平均长度也会非常接近原分布的平均信息量。 如果预测的概率分布 $q$ 和真实分布 $p$ 完全一样，那么cross-entropy等于entropy。 所以cross-entropy可以用来衡量两个概率分布的相似程度。 用在机器学习中用作评判模型好坏的损失函数，度量模型预测分布和真实分布的相似程度。 KL-Divergence-KL散度而如果预测的概率分布和真实分布不同，那么cross-entropy的值就会大于entropy的值，超过的部分就叫做relative entropy（相对熵），也就是KL-Divergence（Kullback-Leibler Divergence，KL散度） 即可以得到等式：$\\text{Cross-Entropy = Entropy+KL-Divergence}$ 则KL-Divergence：$$\\begin{align}D_{KL}(p|q)=H(p,q)-H(p) &amp;= -\\sum_i p_i\\ln q_i - \\sum_i p_i\\ln p_i \\&amp;= -\\sum_i p_i \\ln \\frac{p_i}{q_i} \\&amp;= \\sum_i p_i \\ln \\frac{q_i}{p_i}\\end{align}$$ Reference 视频链接：https://www.youtube.com/watch?v=ErfnhcEV1O8","link":"/2021/01/16/entropy-and-more/"},{"title":"「机器学习-李宏毅」：Error","text":"这篇文章叙述了进行regression时，where dose the error come from?这篇文章除了解释了error为什么来自bias和variance，还给出了当error产生时应该怎么办？如何让模型在实践应用中也能表现地和测试时几乎一样的好？ Error在中的2.4节，我们比较了不同的Model。下图为不同Model下，testing data error的变化。 可以发现，随着模型越来越复杂，testing data的error变小一些后，爆炸增大。 越复杂的模型在testing data上不一定能得到好的performance。 所以，where dose the error come from? ：bias and variance Bias and Variance of Estimator用打靶作比，如果你的准心，没有对准靶心，那打出的很多发子弹的中心应该离靶心有一段距离，这就是bias。 但把准心对准靶心，你也不一定能打中靶心，可能会有风速等一系列原因，让子弹落在靶心周围，这就是variance。 上图中，可以直观体现出bias 和 variance的影响。 概率论中 ： 一个通过样本值得到了估计量，有三个评判准则：无偏性、有效性和相和性。 这里的无偏性的偏也就是bias。 概率论中定义：设 $\\hat{\\theta}(X_1,X_2,…,X_n)$ 是未知参数 $\\theta$ 的估计量，若 $E(\\hat{\\theta})=\\theta$ ，则称 $\\hat{\\theta}$ 是 $\\theta$ 的无偏估计。 变量 $x$ ，假设他的期望是 $\\mu$ ，他的方差是 $\\sigma^2$. 对于样本： $x^1,x^2,…,x^N$ ，估计他的期望和方差。 概率论的知识： $m=\\frac{1}{N} \\sum_{n} x^{n} \\quad s^{2}=\\frac{1}{N} \\sum_{n}\\left(x^{n}-m\\right)^{2}$ $E(m)=\\mu$ ，所以用 $m$ 是 $\\mu$ 的无偏估计。(unbiased) 但是 $E\\left[s^{2}\\right]=\\frac{N-1}{N} \\sigma^{2} \\quad \\neq \\sigma^{2}$ ，所以这样的估计是有偏差的。(biased) 因此统计学中用样本估算总体方差都进行了修正。 而在机器学习中，Bias和Variance通常与模型相关。 上图中，假设黑色的线是 true function，红色的线是训练得到的函数，蓝色的线是，训练函数的平均函数。 可见，随着函数模型越来越复杂，bias在变小，但variance也在增大。 右下角图中，红色的线接近铺满了，variance已经很大了，模型过拟合了。 对机器学习中模型对bias影响的直观解释 左图的model简单，右图的model复杂。 简单的model，包含的函数集较小，可能集合圈根本没有包括target（true function），因此在这个model下，无论怎么训练，得到的函数都有 large bias。 而右图中，因为函数非常复杂，所以大概率包含了target，因此训练出的函数可能variacne很大，但有 small bias。 what to do with large bias/variance 上图中，红色的线表示bias的误差，绿色的线表示variance的误差，蓝色的线表示观测的误差。 当模型过于简单时：来自bias的误差会较大，来自vaiance的误差较小，也就是 Large Bias Small Variance 当模型过雨复杂时：来自bias的误差会较小，来自variance的误差会很大，也就是 Small Bias Large Variance 2 case : Underfitting ：If your model cannot even fit the training examples, then you have large bias. Overfitting : If you can fit the traning data, but large error on testing data , then you probably have large variance. With Large BiasFor bias, redesign your model. Add more features as input. A more complex model. 考虑更多的feature；使用稍微复杂些的模型。 With Large Variance More data Regularization (在这篇2.5.2文章中有叙述什么是regularization) Model Selection There is usually a trade-off beween bias and variance. Select a model that balances two kinds of error to minimize total error. 选择模型需要在bias和variance中平衡，尽量使得总error最小。 What you should NOT do: 以上，描述的是这样的一个情形：在traning data中，得到了三个自认不错的模型，kaggle的公开的testing data测试，分别得到三个模型的error，认为第三个模型最好！ 但是，当把kaggle用private的testing data 进行测试时，error肯定是大于0.5的，最好的model也不一定是第三个。 同理，当把我们训练出的model拿来实际应用时，可能会发现情况很糟，并且，这个model可能选的是测试中最好的，但在应用中并不是最好的。 Cross Validation什么是Cross Validation(交叉验证)？ 在机器学习中，就是下图过程： 把Traning Set 分成两个部分：Training Set和Validation Set。 在Training Set部分选出模型。 用Validation Set来判断哪个模型好：计算模型在Validate Set的error。 再用模型预测Testing Set(public)，得到的error一定是比Validation Set中大的。 Not recommend : Not用public testing data的误差结果去调整你的模型。 这样会让模型在public的performance比private的好。 但模型在private testing data的performance才是我们真正关注的。 那么当模型预测private testing set时（投入应用时），能尽最大可能的保证模型和在预测public testing data相近。 N-fold Cross ValidationN-fold Cross Validation（N-折交叉验证）的过程如下： 把Training Set 分为3（3-fold）份，每一次拿其中一份当Validation Set，另外两份当作Training Set。 每一次用Train Set来训练。得到了三个Model。 要判断哪一个Model好？ 每一个Model都计算出不同Validation Set的error。 得到一个Average Error。 最后选这个average error最小的model。 最后应用在public traning set，来评估模型应用在private training set的performance。","link":"/2020/03/15/error/"},{"title":"「LeetCode」：Array","text":"8月某司实训+准备开学期末考，我可太咕了q w q…dbq，（希望）高产博主我.我..又回来了。 LeetCode Array专题，持久更新。（GitHub) Array27-Remove Elements27-Remove Elements Solution 123456789101112131415from typing import Listclass Solution: def removeElement(self, nums: List[int], val: int) -&gt; int: tot = 0 for element in nums: if element == val: continue else: nums[tot] = element tot = tot + 1 return tot Python的参数传递和函数返回值： 1def removeElement(self, nums:List[int], val:int) -&gt; int: 题目要求： “remove all instances of that value in-place and return the new length.” “Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.” “Confused why the returned value is an integer but your answer is an array? Note that the input array is passed in by reference, which means modification to the input array will be known to the caller as well.” 题目要求是在原数组上删除数值，不能额外开新的空间存储数组。 意思就是说，虽然函数返回的是一个数值，但实际返回答案是一个数组。 因为数组的传递是指针传递，返回的是数组长度，则相当于返回了这个in-place的新数组。 26-Remove Duplicates from Sorted Array26-Remove Duplicates from Sorted Array Solution： 1234567891011121314151617from typing import Listclass Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: tot = 0 before = None for index in range(len(nums)): if nums[index] != before: nums[tot] = nums[index] before = nums[index] tot = tot + 1 else: continue return tot 第一次提交的时候before = nums[0] - 1 报错了，原因是传入数组长度为0，下标越界。 注意空数组的下标越界问题。 80-Remove Duplicates from Sorted Array II80-Remove Duplicates from Sorted Array II Solution: 1234567891011121314151617181920from typing import Listclass Solution: def removeDuplicates(self, nums: List[int]) -&gt; int: before = None before_cnt = 0 length = 0 for index in range(len(nums)): if nums[index] != before: nums[length] = nums[index] before = nums[index] length += 1 before_cnt = 1 else: before_cnt += 1 if before_cnt &lt;= 2: nums[length] = nums[index] length += 1 return length 189-Rotate Array（S123）189-Rotate Array Problem: 简述题目大意，给一个列表nums，一个 $k$ 值，要求原址让列表循环右移 $k$ 位。 Solution: 其实以下三种做法时间空间复杂度差别不大，主要看个思路吧。 S Runtime Memory Language S1 64ms 15.2MB pyhon3 S2 64ms 15.1MB python3 S3 116ms 15.1MB python3 S1-简单做法：空间换时间时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ ，循环右移时多开了一个数组。 12345678910111213from typing import Listimport queueclass Solution: def rotate(self, nums: List[int], k: int) -&gt; None: \"\"\" Do not return anything, modify nums in-place instead. \"\"\" length = len(nums) a = [0] * length for index in range(length): a[(index + k)%length] = nums[index] nums[:] = a S2-利用数学同余关系时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ ，循环右移时只多开了一个变量。 原理： 定理[1]： 设 $m$ 是正整数，整数 $a$ 满足 $(a,m)=1$ ，$b$ 是任意整数。若 $x$ 遍历模 $m$ 的一个完全剩余系，则 $ax+b$ 也遍历模 $m$ 的一个完全剩余系。 由以上定理可以得知，设 $n$ 为列表长度， $x$ 是列表的下标，遍历 $n$ 的一个完全剩余系。 如果 $(k,n)=1$ ， $kx$ 也遍历 $n$ 的一个完全剩余系。这种情况，列表下标通过 $k$ 的倍数的顺序连成一个环。 ：只需要额外一个变量 $temp$ 存储移动占用的值。 如果 $(k,n)\\neq 1$ ，那么 $kx$ 不会遍历一个 $n$ 的完全剩余系，会出现下图的情况（如绿色的线的元素的 $idx = kx+0$ ，红色线的元素都是 $idx=kx+1$ ），会在 $k$ 的某个剩余类一直循环。 ：遍历每个 $k$ 的剩余类。 在每次循环移位时，需要记录该次循环的起始位，防止重复。 123456789101112131415161718class Solution: def rotate(self, nums: List[int], k: int) -&gt; None: \"\"\" Do not return anything, modify nums in-place instead. \"\"\" length = len(nums) k %= length start = 0 cnt = 0 while cnt &lt; length: current, prev = start, nums[start] while True: current = (current + k) % length prev, nums[current] = nums[current], prev cnt += 1 if current == start: break start += 1 S3-利用反转列表的思路时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ 原理： 1234Original List : 1 2 3 4 5 6 7After reversing all numbers : 7 6 5 4 3 2 1After reversing first k numbers : 5 6 7 4 3 2 1After revering last n-k numbers : 5 6 7 1 2 3 4 --&gt; Result 123456789101112131415161718from typing import List# Solution 3: Reverseclass Solution: def reverse(self, nums: list, begin: int, end: int) -&gt; None: while begin &lt; end: nums[begin], nums[end] = nums[end], nums[begin] begin += 1 end -= 1 def rotate(self, nums: List[int], k: int) -&gt; None: n = len(nums) k %= n self.reverse(nums, 0, n-1) self.reverse(nums, 0, k-1) self.reverse(nums, k, n-1) Python用引用管理对象。 12int a1 = 1, *p = &amp;a1;int a2 = 2, &amp;b = a2; 指针：指针变量是一个新变量，这个变量存储的是（变量a1的）地址，该地址指向一个存储单元。（该存储单元存放的是a1的值）。 引用：引用的实质是变量的别名，所以a2和b实际是一个东西，在内存中占有同一个存储单元。 所以python中交换对象可以直接a,b = b,a Python 列表的操作：切片。 41-First Missing Positive41-First Missing Positive Solution: 排一下序，维护一个expect变量就行了。 时间复杂度：$\\mathcal{O}(n\\log{n})$ ，题目没有卡常。 Runtime: 36 ms, faster than 70.96% of Python3 online submissions for First Missing Positive. 空间复杂度：$\\mathcal{O}(n)$ Memory Usage: 13.8 MB, less than 69.19% of Python3 online submissions for First Missing Positive. 12345678910111213from typing import Listclass Solution: def firstMissingPositive(self, nums: List[int]) -&gt; int: nums.sort() expect = 1 for element in nums: if element == expect: expect += 1 elif element &gt; expect: return expect return expect 299-Bulls and Cows299-Bulls and Cows Problem: 题目大意是：给定两个相同长度的字符串，计算这两个字符串有多少个对应位数字相同，和多少个位置不对应但数字相同的个数。 Solution: 应用字符0-9本身数字的性质。 时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ 123456789101112131415161718192021class Solution: def getHint(self, secret: str, guess: str) -&gt; str: # 0-9 cnt for guess (expect the same digit) cnt = [0] * 10 bulls = cows = 0 g = lambda a: ord(a) - ord('0') for si, gi in zip(secret, guess): if si == gi: bulls += 1 else: cnt[g(gi)] += 1 for si, gi in zip(secret, guess): if si == gi: continue elif cnt[g(si)] &gt; 0: cows += 1 cnt[g(si)] -= 1 output = \"{}A{}B\".format(bulls, cows) return output ord()函数和chr()函数 ord()返回字符的ASCII码，chr函数返回ASCII码对应的字符。 浅析lambda表达式，匿名函数，类似于C语言的宏。 格式：lambda [arg1[, arg2,...]] : expression 双变量同时遍历使用zip()函数 134-Gas Station（S12）134-Gas Station Problem： 题目大意是：有N个环形加油站，每个加油站能加油gas[i]，一个汽车起始油量为0，且从i个站开到第i+1个站需要花费cost[i]的油量。找出这个车能顺时针跑完一圈的起始点（如果有，则唯一），如果不能返回-1。 Solution： Solution Runtime Memory Language S1-简单做法 3244ms 14.9MB python3 S2-原理优化 104ms 14.8MB python3 S1-简单解法时间复杂度：$\\mathcal{O}(n^2)$ ，实际远达不到 $n^2$ ，算有一点贪心叭。 空间复杂度：$\\mathcal{O}(n)$ 该汽车从起点i能跑完的必要条件： 起始点 gas[i] - cost[i] &gt;= 0 。并且维护一个数组存放 gas[i] - cost[i] ，即这个站自给自足的油量余量。 如果从满足条件1的起点开始跑一圈， 要求路程中的油量必须大于等于0。维护一个汽车当前总油量 $S$ （用前缀和维护），每跑过一段路程，都要求 $S&gt;=0$ 。 123456789101112131415161718192021from typing import Listclass Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: n = len(gas) remain = [] for g, c in zip(gas, cost): remain.append(g - c) for i in range(n): if remain[i] &lt; 0: continue else: S = 0 for j in range(n): S += remain[(i + j) % n] if S &lt; 0: break if S &gt;= 0: return i return -1 S2-对问题分析进行再简化时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ 12345 i : 0 1 2 3 4g[i]: 1 2 3 4 5c[i]: 3 4 5 1 2g-c :-2-2-2 3 3sum :-2-4-6-3 0 如果 $S=\\sum_{i=0}^{n-1} g[i]-c[i],S&lt;0$ 那么一定无解， $S$ 称为总积累油量。 如果 $S&gt;=0$，如果有找出最优解的方法，则一定有解。 起点满足 $g[i]-c[i]&gt;=0$ ，把这些点称为正余量点。 用 $\\mathcal{O}(1)$ 算出从第 $i$ 个点出发到第 $n$ （n就是第0个点） 个点所积累的油量： $res[i] =S-sum[i-1]$ .即用总积累油量减去前 $i-1$ 段路程能积累的油量（一般积累为负）。(sum数组就是 g-c的前缀和) 对于满足起点要求 $g[i]-c[i]&gt;=0$ 的所有点，计算从第 $i$ 个点出发到第 $n$ 个点到油量积累。那么有最大油量积累的点即为最优起始点。（题目规定如果存在，则点唯一） 因此，因为 $S$ 固定，只需要找到 $sum$ 数组中的最小值的下标，下标+1即是结果。 证明其正确性： 如果满足 $g[i]-c[i]&gt;=0$ 的上述点 $i,j（i&lt;j)$ ，如果 $res[i]&gt;=res[j]$ ，说明从 $i$ 到 $j$ 是正油量积累，贪心的思想，那肯定积累的油量越多越好， $i$ 比 $j$ 优。 如果 $res[i]&lt;=res[j]$ ，说明从 $i$ 到 $j$ 是负油量积累，如果从 $i$ 点出发，到 $j$ 点就负油量了；如果从 $j$ 点出发，该车最后再跑 $i$ 到$j$ 段，因为保证了总积累油量是正，所以最后一定有足够的油量能跑完 $i$ 到 $j$ 段。 再证只要 $S&gt;=0$ 则一定有解。是动态尝试起始点，( $i,j$ 都满足 $g[i]-c[i]&gt;=0$ ）从点 $i$ 开始，跑到点 $j$ 时，如果该途中途有出现油量不够，那就把 $i$ 到 $j$ 的这段路程放到路途的后面来跑，等油量积累够了再跑这段。 Code: 1234567891011121314151617from typing import Listclass Solution: def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int: n = len(gas) sum = 0 remain = [] for g, c in zip(gas, cost): remain.append(g - c) sum += g - c if sum &lt; 0: return -1 for i in range(1, n): remain[i] += remain[i - 1] min_idx = remain.index(min(remain)) return (min_idx + 1) % n 前缀和 list.index(value) 找出list中值为value 的第一个下标。 min(list) 返回list中的最小值。 118-Pascal’s Triangle118-Pascal’s Triangle Problem: 给定一个数字，输出如下规则的值。 Solution： 注意边界吧。（不太喜欢这种题qwq 123456789101112from typing import Listclass Solution: def generate(self, numRows: int) -&gt; List[List[int]]: ans = [] for i in range(0, numRows): temp = [1] * (i+1) for j in range(1, i): temp[j] = ans[i-1][j-1] + ans[i-1][j] ans.append(temp) return ans Python 中的append会出现值被覆盖的情况：变量在循环外定义，但在循环中对该变量做出一定改变，然后append到列表，最后发现列表中的值都是一样的。 因为Python中很多时候都是以对象的形式管理对象，因此append给列表的是一个地址。 119-Pascal’s Triangle II119-Pascal’s Triangle II Problem： 给定一个数字，输出某一行。 Solution： 1234567891011from typing import Listclass Solution: def getRow(self, rowIndex: int) -&gt; List[int]: temp = [1] for i in range(0, rowIndex): for j in range(i, 0, -1): temp[j] += temp[j-1] temp.append(1) return temp 169-Majority Element169-Majority Element Problem: 给一串数字，找到出现次数大于 n/2 的数字。 Solution： 用字典计数。 12345678910111213from typing import Listclass Solution: def majorityElement(self, nums: List[int]) -&gt; int: n = len(nums) cnt = {} for ele in nums: if ele in cnt: cnt[ele] += 1 else: cnt[ele] = 1 return max(cnt, key=cnt.get) 返回值最大/最小的键/索引。 列表： 最大值的索引：list.index(max(list)) 最小值的索引：list.index(min(list)) 字典： 最大值的键：max(dict, key=dict.get) 最小值的键：min(dict, key=dict.get) 229-Majority Element II229-Majority Element II Problem: 给一串数字，返回出现次数大于 n/3 的数字。 Solution： 1234567891011121314151617from typing import Listclass Solution: def majorityElement(self, nums: List[int]) -&gt; int: n = len(nums) cnt = {} ans = [] for ele in nums: if ele in cnt: cnt[ele] += 1 else: cnt[ele] = 1 for (k, v) in cnt.items(): if v &gt; n/3: ans.append(k) return ans 字典的实用方法： 操作 实现方法 删除字典元素 del dict['Name'] 清空字典所有条目 dict.clear() 删除字典 del dict 返回指定键的值，如果值不存在返回default的值 dict.get(key, default) 如果键不存在字典中，添加键并将值设为default,于get类似 dict.setdefault(key, default=None) 判读键是否存在 1. if k in dict 2. dict.has_key(key) 存在返回true 以列表返回可遍历的（键，值）元祖数组 dict.items() 以列表返回一个字典的所有键 dict.keys() 以列表返回字典中的所有值 dict.values() 返回最大值的键值 max(dict, key=dict.get) 返回最小值的键值 min(dict, key=dict.get) 遍历字典的方法： 123456789101112131415161718192021222324252627# -*- coding:utf-8 -*-dict={\"a\":\"Alice\",\"b\":\"Bruce\",\"J\":\"Jack\"}# 实例一：键循环for i in dict: print \"dict[%s]=\" % i,dict[i]# 结果:# dict[a]= Alice# dict[J]= Jack# dict[b]= Bruce# 实例二：键值元组循环for i in dict.items(): print i# 结果:# ('a', 'Alice')# ('J', 'Jack')# ('b', 'Bruce')# 实例三：键值元组循环for (k,v) in dict.items(): print \"dict[%s]=\" % k,v# 结果:# dict[a]= Alice# dict[J]= Jack# dict[b]= Bruce 274-H-Index274-H-Index Problem: 给出研究人员论文的论文引用次数，计算它的H指数（有h篇论文的引用次数至少为h，剩下N-h篇论文的引用次数不超过h）。 Solution： 时间复杂度：$\\mathcal{O}(n\\log{n})$ 空间复杂度：$\\mathcal{O}(n)$ 排序后，再二分。（感觉自己的二分写的有点丑qwq 还有一种思路是，排序完，从最大的h开始递减遍历，满足条件就返回。反正排序也要$\\mathcal{O}(n\\log{n})$ 的复杂度… 12345678910111213141516171819from typing import Listclass Solution: def hIndex(self, citations: List[int]) -&gt; int: n = len(citations) citations.sort() begin = 0 end = n - 1 while begin &lt;= end: mid = (begin + end) &gt;&gt; 1 h = n - mid if citations[mid] &gt;= h: end = mid if begin == end: return h else: begin += 1 return n-begin 非递归写二分：while begin &lt;= end 275-H-Index II275-H-Index II Problem: 和274一样，给了递增的论文引用数，希望能用指数时间返回H指数。 Solution： 啊，就二分鸭。 123456789101112131415161718from typing import Listclass Solution: def hIndex(self, citations: List[int]) -&gt; int: n = len(citations) begin = 0 end = n - 1 while begin &lt;= end: mid = (begin + end) &gt;&gt; 1 h = n - mid if citations[mid] &gt;= h: end = mid if begin == end: return h else: begin += 1 return n - begin 243-Shortest Word Distanceqwq 这道题还收费来着，于是于是就开了个中国区的会员（中国区的会员便宜好多啊！！） 243-Shortest Word Distance Problem： 给定一串单词，单词1和单词2，计算单词1单词2在单词列表中的距离。 Solution： Solution Runtime Memory Language S1-二分查找 44ms 15.7MB python3 S2-线性维护 40ms 15.6MB python3 S1-二分查找时间复杂度：$\\mathcal{O}(n\\log{n})$ 空间复杂度：$\\mathcal{O}(n)$ （最开始还很疑惑啥是单词距离…单词1和单词2可能在单词列表中重复出现） 计算出单词1和单词2在单词列表中出现的索引值列表，是递增有序的。 对于单词1索引列表中的每个值，在单词2索引列表中查找该值的lower_bound，计算距离。 同理，对于单词2索引列表中的每个值，也同样计算距离。 找出最小距离。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from typing import Listdef lower_bound(a: list, x: int) -&gt; int: n = len(a) begin = 0 end = n - 1 while begin &lt;= end: mid = (begin + end) &gt;&gt; 1 if a[mid] &gt;= x: end = mid if begin == end: return begin else: begin += 1 return -1class Solution: ans = None def findShortest(self, li1: list, li2: list) -&gt; None: for idx in li1: min_dis_idx = lower_bound(li2, idx) if min_dis_idx == -1: continue else: self.ans = min(self.ans, li2[min_dis_idx] - idx) if self.ans == 1: return def shortestDistance(self, words: List[str], word1: str, word2: str) -&gt; int: n = len(words) li1 = [] li2 = [] self.ans = n for idx in range(n): # li1 and li2 are ordered if words[idx] == word1: li1.append(idx) if words[idx] == word2: li2.append(idx) self.findShortest(li1, li2) if self.ans &gt; 1: self.findShortest(li2, li1) return self.ans S2-线性维护：时间复杂度：$\\mathcal{O}(n)$ 空间复杂度：$\\mathcal{O}(n)$ 问题还能再简化，线性扫描单词列表，维护两个变量，单词1出现的最近索引，单词2出现的最近索引。扫描时计算距离，每当单词1或单词2出现时，就用另一个单词的最近索引计算。 12345678910111213141516171819from typing import Listclass Solution: def shortestDistance(self, words: List[str], word1: str, word2: str) -&gt; int: n = len(words) lst1 = None lst2 = None ans = n for idx in range(n): if words[idx] == word1: lst1 = idx if lst2 is not None: ans = min(ans, lst1-lst2) if words[idx] == word2: lst2 = idx if lst1 is not None: ans = min(ans, lst2-lst1) return ans C++中： lower_bound(begin, end, num)：返回num的下界，即大于等于num的第一个索引位置。 upper_bound(begin, end, num)：返回num的上界，即大于num的第一个索引位置。 Python中用二分实现这两个函数。 244-Shortest Word Distance II244-Shortest Word Distance II Problem: 和上题题干类似，计算单词距离，但是此问是每一个单词列表，可能有多个询问。 Solution： 对每一个单词列表，都可能有多个询问。 因此，之前243的解法每次询问都会遍历一遍单词列表。如果对每个单词列表询问数为 $M$ ，那么时间复杂度为 $\\mathcal{O}(NM)$ ，会超时，所以希望能将单词列表的有关信息存下来，再用常数时间处理每一个询问。 这里的解法是用一个字典把每个单词出现的index列表存下来，键是单词，值是index列表。这个列表相对于单词列表的数目应该是远远小于的，因此用二重循环应该也能过吧（没有尝试二重循环解法） 这里有两种思路，一种是自己想的归并思路，还有一种是官方解答的思路，官方思路比归并的思路更优雅一些，问题抽象的更好。（代码差距不大，时间差距也不太大） Solution Runtime Memory Language S1-归并思路查询 96ms 20.8MB python3 S2-交叉跳跃查询 80ms 20.4MB python3 双指针：$i$ 指向列表1，$j$指向列表2. S1-归并思路归并思路：列表的值都是有序的，再把两个列表的值按归并的思想再排序，可以想成把点一个一个有序放在数轴上。 $i$指针前进的情况：（排序时，取列表1的下一个数字） $i+1&lt;len1$ and $li1[i+1] &lt; li[j]$ $i+1&lt;len1$ and $li1[i+1] &lt; li[j+1]$ $i+1 &lt; len1$ and $j+1 == len2$ ($j$ 已无法移动) 其余情况：$j$ 移动。 1234567891011121314151617181920212223242526272829303132333435from typing import Listclass WordDistance: def __init__(self, words: List[str]): self.words = words self.len = len(words) self.dict = {} for index in range(self.len): word = words[index] if word in self.dict: temp = self.dict[word] temp.append(index) else: temp = [index] self.dict[word] = temp def shortest(self, word1: str, word2: str) -&gt; int: ans = self.len li1 = self.dict[word1] li2 = self.dict[word2] len1 = len(li1) len2 = len(li2) i = j = 0 while i &lt; len1 and j &lt; len2: ans = min(ans, abs(li1[i] - li2[j])) if ans == 1: return ans # i goes ahead if i + 1 &lt; len1 and ((li1[i + 1] &lt; li2[j]) or (j+1 == len2) or (j + 1 &lt; len2 and li1[i + 1] &lt; li2[j + 1])): i += 1 else: j += 1 return ans S2-交叉比较对于当前指向列表1和列表2的两个元素 $li1[i]$ 和 $li2[j]$ ，对 $li1[i]$来说，只需要和旁边的属于列表2的元素比较，对 $li2[j]$ 同理。 因此，当 $li1[i]&gt;li2[j]$ 时，下一次的比较应该让 $j$ 指针前移一位，继续计算指针 $i$ 所指元素和其旁边的列表2的元素。同理，当 $li1[i]&lt;li2[j]$ 时，下一次的比较应该让 $i$ 指针前移一位，继续计算指针 $j$ 和其旁边的列表1的元素。 具体移动如下图。 1234567891011121314151617181920212223242526272829303132333435from typing import Listclass WordDistance: def __init__(self, words: List[str]): self.words = words self.len = len(words) self.dict = {} for index in range(self.len): word = words[index] if word in self.dict: temp = self.dict[word] temp.append(index) else: temp = [index] self.dict[word] = temp def shortest(self, word1: str, word2: str) -&gt; int: ans = self.len li1 = self.dict[word1] li2 = self.dict[word2] len1 = len(li1) len2 = len(li2) i = j = 0 while i &lt; len1 and j &lt; len2: ans = min(ans, abs(li1[i] - li2[j])) if ans == 1: return ans # i goes ahead if li1[i] &lt; li2[j]: i += 1 else: j += 1 return ans 277-Find the Celebrity277-Find the Celebrity Problem: 已有know(i, j) API，判断i是否知道j，i是名人的充要条件是其他所有人知道i，而i不知道其他所有人。 Solution： 两种思路，第一种较为直观，使用二重循环，但剪枝多，远达不到 $\\mathcal{O}(n^2)$ ，第二种稍做优化。因此两种解法差距不太大。 提交时间 运行时间 内存消耗 语言 S几秒前 1896ms 13.6MB python3 5 分钟前 1772ms 13.6MB python3 S1-直观思路时间复杂度：远不到 $\\mathcal{O}(n^2)$ 用了二重循环，但剪枝很多，所以远达不到 $\\mathcal{O}(n^2)$ 12345678910111213141516171819202122232425class Solution: def findCelebrity(self, n: int) -&gt; int: for i in range(n): fg = True # i knows j ? for j in range(n): if i == j: continue if knows(i, j): fg = False break if not fg: continue # j knows i ? for j in range(n): if i == j: continue if not knows(j, i): fg = False break if not fg: continue else: return i return -1 S2-排除法时间复杂度：$\\mathcal{O}(n)$ 排除i：根据know(i, j)=True 可以认为i不是名人，j可能是名人。 对于n-1个关系，最后从n个人中选出一个可能的人，再根据名人的充要条件去判断他是否是名人。 123456789101112131415class Solution: def findCelebrity(self, n: int) -&gt; int: celebrity = 0 for i in range(1, n): if knows(celebrity, i): celebrity = i continue for i in range(n): if celebrity == i: continue if (not knows(celebrity, i)) and knows(i, celebrity): continue else: return -1 return celebrity 245-Shortest Word Distance III245-Shortest Word Distance III Problem: 题意增加了两个单词可能相同，分两种情况就好了。 Solution： 123456789101112131415161718192021222324from typing import Listclass Solution: def shortestWordDistance(self, words: List[str], word1: str, word2: str) -&gt; int: n = len(words) ptr1 = None ptr2 = None ans = n if word1 == word2: for idx in range(n): if words[idx] == word1: ptr1, ptr2 = idx, ptr1 if (ptr1 is not None) and (ptr2 is not None): ans = min(ans, ptr1-ptr2) else: for idx in range(n): if words[idx] == word1: ptr1 = idx if words[idx] == word2: ptr2 = idx if (ptr1 is not None) and (ptr2 is not None): ans = min(ans, abs(ptr1-ptr2)) return ans 217-Contains Duplicate[E]217-Contains Duplicate Problem: 判断数组中有无重复元素出现。 Solution： 12345678class Solution: def containsDuplicate(self, nums: List[int]) -&gt; bool: S = set() for i in nums: if i in S: return True S.add(i) return False 219-Contains Duplicate II[E]219-Contains Duplicate II Problem: 判断数组中是否有两个相同的值，他们的索引之差小于等于k。 Solution： 存放出现该值的最近的索引，扫一遍。 123456789101112class Solution: def containsNearbyDuplicate(self, nums: List[int], k: int) -&gt; bool: dict = {} n = len(nums) for i in range(n): if nums[i] not in dict: dict.setdefault(nums[i], i) else: if i - dict[nums[i]] &lt;= k: return True dict[nums[i]] = i return False 4-Median of Two Sorted Arrays[H] （2S）4-Median of Two Sorted Arrays[H] Problem: 给两个排好序的数组，返回一个中位数 Solution： S 运行时间 内存消耗 S1 $\\mathcal{O}(m+n)$ ：92ms 14.3MB S2 $\\mathcal{O}(\\log(m+n))$ ：52ms 13.3MB S1:归并排序的做法。 时间复杂度：$\\mathcal{O}(m+n)$ 12345678910111213141516171819202122232425262728293031from typing import Listclass Solution: def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: n1 = len(nums1) n2 = len(nums2) median1 = median2 = None i = j = 0 tot = 0 while i &lt; n1 or j &lt; n2: if (i &lt; n1 and j &lt; n2 and nums1[i] &lt;= nums2[j]) or (i &lt; n1 and j &gt;= n2): tot += 1 if tot == (n1 + n2)//2: median1 = nums1[i] if tot == (n1 + n2)//2 + 1: median2 = nums1[i] break i += 1 else: tot += 1 if tot == (n1 + n2) // 2: median1 = nums2[j] if tot == (n1 + n2) // 2 + 1: median2 = nums2[j] break j += 1 if (n1 + n2) % 2 == 1: return median2 else: return (median1 + median2)/2 S2:二分的思路 时间复杂度：$\\mathcal{O}(\\log(m+n))$ 一、首先讨论一个数组的中位数，数组有n个元素，如果n为奇数，则第(n+1)/2个是中位数；如果n为偶数，则第(n+1)/2和第(n+2)/2的平均值为中位数。 回到本题，因为是两个数组，如果根据奇偶性分类讨论就过于麻烦了，所以将两种情况统一以简化解题思路。 即无论n是奇数还是偶数，数组的中位数都是第(n+1)/2和第(n+2)/2的平均数。 回到本题，设数组1有n个元素，数组2有m个元素，则中位数为两个数组的有序序列的第(n+m+1)/2个和第(n+m+2)/2个的平均数。 二、因此，题目需要求解的问题改为求这两个有序数组的有序序列的第k个数。 二分思想：两个数组分别找第k/2个数，（假设都存在），比较，如果第一个数组的这个数小于第二个数组，说明第k个数肯定不在第一个数组的前k/2个数中，因此就可以直接去掉数组1的前k/2个元素，查找有序序列的第k-k/2个数；同理，如果大于，则说明第k个数肯定不在第二个数组的前k/2个数中，去掉数组2的前k/2个元素。 使用一个数组起始指针l1和l2来实现数组的“去掉”前k/2个元素。 设数组1的元素个数为n，数组2的元素个数为m。 递归函数Find(l1, l2, k)：查找起始指针为l1, l2的两个有序数组的第k个数。 讨论边界情况，有数组为空的情况。即 l1 == n 或者 l2 == m . 如果第一个数组已为空，则直接返回第二个数组的第k个数； 同理，如果第二个数组为空，则直接返回第一个数组的第k个数。 两个数组都不为空的情况。即 l1 &lt; n 或者 l2 &lt; m . 递归边界： k == 1 ,即返回 nums1[l1] 和 nums2[l2] 中较小的那一个。 数组长度边界：即有数组的剩余元素个数小于 k/2 ，那么拿出来比较的就应该是数组的最后一个元素。 维护两个值 k1 和 k2 来分别表示用两个数组的第 k1和k2个来比较。 (k1 k2都小于等于k/2) 123# to avoid the rest length of nums1/nums2 is shorter than k//2k1 = k//2 if l1+k//2 &lt;= n else n-l1k2 = k//2 if l2+k//2 &lt;= m else m-l2 比较nums1[l1+k1-1] 和 nums2[l2+k2-1] 的大小，递归： 相等： 如果 k-k1-k2 == 0 说明nums1的前k1个和nums2的前k2个就是有序序列的前k个，返回 nums1[l1+k1-1] 。 否则，（即某一个数组的剩余长度小于k/2），分别去掉两个数组的前k1和k2个数，递归调用 Find(l1+k1, l2+k2, k-k1-k2) 。 nums1[l1+k1-1] &gt; nums2[l2+k2-1] 说明可以去掉数组2的前k2个数，递归调用 Find(l1, l2+k2, k-k2) nums1[l1+k1-1] &gt; nums2[l2+k2-1] 说明可以去掉数组1的前k1个数，递归调用 Find(l1+k1, l2, k-k1) Code： 123456789101112131415161718192021222324252627282930313233343536class Solution: def __init__(self): self.nums1 = None self.nums2 = None def findKthOfTwo(self, l1: int, l2: int, k: int) -&gt; int: nums1 = self.nums1 nums2 = self.nums2 n = len(nums1) m = len(nums2) # nums1 is empty if l1 == n: return nums2[l2+k-1] # nums2 is empty if l2 == m: return nums1[l1+k-1] # both not empty if k == 1: return nums1[l1] if nums1[l1] &lt;= nums2[l2] else nums2[l2] # to avoid the rest length of nums1/nums2 is shorter than k//2 k1 = k//2 if l1+k//2 &lt;= n else n-l1 k2 = k//2 if l2+k//2 &lt;= m else m-l2 if nums1[l1+k1-1] == nums2[l2+k2-1]: return nums1[l1+k1-1] if k-k1-k2 == 0 else self.findKthOfTwo(l1+k1, l2+k2, k-k1-k2) elif nums1[l1+k1-1] &gt; nums2[l2+k2-1]: return self.findKthOfTwo(l1, l2+k2, k-k2) else: return self.findKthOfTwo(l1+k1, l2, k-k1) def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float: n = len(nums1) m = len(nums2) self.nums1 = nums1 self.nums2 = nums2 # median: the average of (n+m+1)//2 th and (n+m+2)//2 th return (self.findKthOfTwo(0, 0, (n+m+1)//2) + self.findKthOfTwo(0, 0, (n+m+2)//2)) / 2 reference 《信息安全数学基础》 2.2同余类和剩余系。","link":"/2020/09/15/LeetCode_array/"},{"title":"「机器学习-李宏毅」:HW1-Predict PM2.5","text":"在本篇文章中，用手刻Adagrad完成了「机器学习-李宏毅」的HW1-预测PM2.5的作业。其中包括对数据的处理，训练模型，预测，并使用sklearn toolkit的结果进行比较。有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub Task Descriptionkaggle link 从中央气象局网站下载的真实观测资料，必须利用linear regression或其他方法预测PM2.5的值。 观测记录被分为train set 和 test set, 前者是每个月前20天所有资料；后者是从剩下的资料中随机取样出来的。 train.csv: 每个月前20天的完整资料。 test.csv: 从剩下的10天资料中取出240笔资料，每一笔资料都有连续9小时的观测数据，必须以此观测出第十小时的PM2.5. Process Datatrain data如下图，每18行是一天24小时的数据，每个月取了前20天（时间上是连续的小时）。 test data 如下图，每18行是一笔连续9小时的数据，共240笔数据。 最大化training data size 每连续10小时的数据都是train set的data。为了得到更多的data，应该把每一天连起来。即下图这种效果： 每个月就有： $20*24-9=471$ 笔data 123456789# Dictionary: key:month value:month datamonth_data = {}# make data timeline continuousfor month in range(12): temp = np.empty(shape=(18, 20*24)) for day in range(20): temp[:, day*24: (day+1)*24] = data[(month*20+day)*18: (month*20+day+1)*18, :] month_data[month] = temp 筛选需要的Features : 这里，我就只考虑前9小时的PM2.5，当然还可以考虑和PM2.5等相关的氮氧化物等feature。 training data 1234567# x_data v1: only consider PM2.5x_data = np.empty(shape=(12*471, 9))y_data = np.empty(shape=(12*471, 1))for month in range(12): for i in range(471): x_data[month*471+i][:] = month_data[month][9][i: i+9] y_data[month*471+i] = month_data[month][9][i+9] testing data 12345# Testing data featurestest_x = np.empty(shape=(240, 9))for day in range(240): test_x[day, :] = test_data[18*day+9, :]test_x = np.concatenate((np.ones(shape=(240, 1)), test_x), axis=1) Normalization 123456789101112# feature scale: normalizationmean = np.mean(x_data, axis=0)std = np.std(x_data, axis=0)for i in range(x_data.shape[0]): for j in range(x_data.shape[1]): if std[j] != 0: x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]for i in range(test_data.shape[0]): for j in range(test_data.shape[1]): if std[j] != 0: test_data[i][j] = (test_data[i][j] - mean[j])/std[j] Training手刻Adagrad 进行training。（挖坑：RMSprop、Adam[1] Linear Pseudo code 123456Declare weight vector, initial lr ,and # of iterationfor i_th iteration : y’ = the product of train_x and weight vector Loss = y’ - train_y gradient = 2*np.dot((train_x)’, Loss ) weight vector -= learning rate * gradient 其中的矩阵操作时，注意求gradient时矩阵的维度。可参考下图。 Adagrad Pseudo code 123456789Declare weight vector, initial lr ,and # of iterationDeclare prev_gra storing gradients in every previous iterations for i_th iteration : y’ = the inner product of train_x and weight vector Loss = y’ - train_y gradient = 2*np.dot((train_x)’, Loss ) prev_gra += gra**2 ada = np.sqrt(prev_gra) weight vector -= learning rate * gradient / ada 注：代码实现时，将bias存在w[0]处，x_data的第0列全1。因为w和b可以一同更新。（当然，也可以分开更新） Adagrad training 123456789101112131415161718192021# train-adagradbatch = x_data.shape[0] # full batchepoch = 400# some parameters for trainingdim = x_data.shape[1]+1w = np.zeros(shape=(dim, 1)) # concatenate bias = w[0]lr = np.full((dim, 1), 0.8) # learning rategrad = np.empty(shape=(dim, 1)) # gradient of loss to every paragradsum = np.zeros(shape=(dim, 1)) # sum of gradient**2x_data = np.concatenate((np.ones(shape=(x_data.shape[0], 1)), x_data), axis=1)loss_his = np.empty(shape=(epoch, 1))for T in range(epoch): L = y_data - np.dot(x_data, w) loss_his[T] = np.sum(L**2) / x_data.shape[0] grad = (-2)*np.dot(np.transpose(x_data), L) gradsum = gradsum + grad**2 w = w - lr*grad/(gradsum**0.5) Testing1answer = np.dot(test_x, w) Draw and Analysis在每次迭代更新时，我将Loss的值存了下来，以便可视化Loss的变化和更新速度。 Loss的变化如下图：(红色的是sklearn toolkit的loss结果) 此外，在源代码中，使用sklearn toolkit来比较结果。 结果如下： 123456789101112131415161718192021222324252627v1: only consider PM2.5Using sklearnLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)bias= [21.37402689]w= [[ 0.00000000e+00] [-5.54801503e-01] [-4.32873874e-01] [ 3.63669814e+00] [-3.99037687e+00] [-9.07364636e-01] [ 8.83495803e+00] [-9.51785135e+00] [ 1.32734655e-02] [ 1.81886444e+01]]In our modelbias= [19.59387132]w= [[-0.14448468] [ 0.39205748] [ 0.26897134] [-1.02415371] [ 1.21151411] [ 2.21925424] [-5.48242478] [ 4.01080346] [13.56369122]] 发现参数有一定差异，于是我在testing时，也把sklearn的结果进行预测比较。 一部分结果如下： 1234567891011121314151617['id', 'value', 'sk_value']['id_0', 3.551092352912313, 5.37766865368331]['id_1', 13.916795471648756, 16.559245678900034]['id_2', 24.811333478647043, 23.5085950470451]['id_3', 5.101440436158914, 6.478306159981166]['id_4', 26.7374726797937, 27.207516152986663]['id_5', 19.43735346531517, 21.916809502961648]['id_6', 22.20460696285646, 24.751295357256392]['id_7', 29.660872382552682, 30.24344042612033]['id_8', 17.5964527734513, 16.64242443764712]['id_9', 56.58017426943178, 59.760988216575115]['id_10', 13.767504260132299, 10.808372404511037]['id_11', 11.743000466164233, 11.526958393801682]['id_12', 59.509878887026105, 64.201008247897]['id_13', 53.19824337746267, 54.3856368053018]['id_14', 21.97191108867921, 24.530720709840974]['id_15', 10.833283625735444, 14.350345549104446] Code有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137########################## Date: 2020-4-4# Author: FredLau# HW1: predict the PM2.5##########################import sysimport numpy as npimport pandas as pdimport csvfrom sklearn import linear_modelimport matplotlib.pyplot as plt###################### process data# process train dataraw_data = np.genfromtxt('data/train.csv', delimiter=',')data = raw_data[1:, 3:]data[np.isnan(data)] = 0 # process nan# Dictionary: key:month value:month datamonth_data = {}# make data timeline continuousfor month in range(12): temp = np.empty(shape=(18, 20*24)) for day in range(20): temp[:, day*24: (day+1)*24] = data[(month*20+day)*18: (month*20+day+1)*18, :] month_data[month] = temp# x_data v1: only consider PM2.5x_data = np.empty(shape=(12*471, 9))y_data = np.empty(shape=(12*471, 1))for month in range(12): for i in range(471): x_data[month*471+i][:] = month_data[month][9][i: i+9] y_data[month*471+i] = month_data[month][9][i+9]# process test datatest_raw_data = np.genfromtxt('data/test.csv', delimiter=',')test_data = test_raw_data[:, 2:]test_data[np.isnan(test_data)] = 0# feature scale: normalizationmean = np.mean(x_data, axis=0)std = np.std(x_data, axis=0)for i in range(x_data.shape[0]): for j in range(x_data.shape[1]): if std[j] != 0: x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]for i in range(test_data.shape[0]): for j in range(test_data.shape[1]): if std[j] != 0: test_data[i][j] = (test_data[i][j] - mean[j])/std[j]# Testing data featurestest_x = np.empty(shape=(240, 9))for day in range(240): test_x[day, :] = test_data[18*day+9, :]test_x = np.concatenate((np.ones(shape=(240, 1)), test_x), axis=1)################################# train-adagradbatch = x_data.shape[0] # full batchepoch = 400# some parameters for trainingdim = x_data.shape[1]+1w = np.zeros(shape=(dim, 1)) # concatenate bias = w[0]lr = np.full((dim, 1), 0.8) # learning rategrad = np.empty(shape=(dim, 1)) # gradient of loss to every paragradsum = np.zeros(shape=(dim, 1)) # sum of gradient**2x_data = np.concatenate((np.ones(shape=(x_data.shape[0], 1)), x_data), axis=1)loss_his = np.empty(shape=(epoch, 1))for T in range(epoch): L = y_data - np.dot(x_data, w) loss_his[T] = np.sum(L**2) / x_data.shape[0] grad = (-2)*np.dot(np.transpose(x_data), L) gradsum = gradsum + grad**2 w = w - lr*grad/(gradsum**0.5)f = open('output/v1.csv', 'w')sys.stdout = fprint('v1: only consider PM2.5\\n')################################ train by sklearn linear modelprint('Using sklearn')reg = linear_model.LinearRegression()print(reg.fit(x_data, y_data))print('bias=', reg.intercept_)print('w=', reg.coef_.transpose())print('\\n')# In our modelprint('In our model')print('bias=', w[0])print('w=', w[1:])############################ draw change of lossplt.xlim(0, epoch)plt.ylim(0, 10)plt.xlabel('$iteration$', fontsize=16)plt.ylabel('$Loss$', fontsize=16)iteration = np.arange(0, epoch)plt.plot(iteration, loss_his/100, '-', ms=3, lw=2, color='black')sk_w = reg.coef_.transpose()sk_w[0] = reg.intercept_sk_loss = np.sum((y_data - np.dot(x_data, sk_w))**2) / x_data.shape[0]plt.hlines(sk_loss/100, 0, epoch, colors='red', linestyles='solid')plt.legend(['adagrad', 'sklearn'])plt.show()# plt.savefig('output/v1.png')f.close()############### test (sklearn vs our adagradf = open('output/v1test.csv', 'w')sys.stdout = ftitle = ['id', 'value', 'sk_value']answer = np.dot(test_x, w)sk_answer = np.dot(test_x, sk_w)print(title)for i in range(test_x.shape[0]): content = ['id_'+str(i), answer[i][0], sk_answer[i][0]] print(content)f.close() Reference 待完成","link":"/2020/04/06/ml-lee-hw1/"},{"title":"「Web」:HTML and CSS","text":"温故知新：对Web基础知识——HTML和CSS的持续更新。 说在前面B/S 软件结构C/S： Client Server（JavaSE） B/S：Browser Server（JavaEE） 前端开发流程 美术实现：网页设计 前端工程师：设计为静态网页 Java程序员：后端工程师修改为动态页面 网页端组成部分内容：页面中可以看到的数据。一般使用html技术。 表现：内容在页面上的展示形式。一般使用CSS。 行为：页面中的元素与输入设备交互。一般使用javascript技术。 HTML创建HTML文件 创建一个Web静态工程 在工程下创建html页面 12345678910&lt;!DOCTYPE html&gt;&lt;!--声明--&gt;&lt;html lang=\"zh_CN\"&gt;&lt;!--html中包含两部分：head和body--&gt;&lt;head&gt;&lt;!--head中包含：title标签、CSS样式、js代码--&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;Hello World!&lt;/body&gt;&lt;/html&gt; HTML标签 标签名大小写不敏感 标签有自己的属性 基本属性：修改简单样式 事件属性：设置事件响应后的代码 标签分为单标签&lt;标签/&gt;和双标签&lt;标签&gt;&lt;/标签&gt; 标签的属性必须要有值，属性值加双引号。 显示特殊标签：&lt; &gt; 空格等等，建议查阅文档。 字体标签12345&lt;body&gt; &lt;font color=\"red\" size=\"7\"&gt; 哒哒哒。 &lt;/font&gt;&lt;/body&gt; 标题标签：h1 到 h612&lt;h1 align=\"center\"&gt;标题1&lt;/h1&gt;&lt;h2 align=\"left\"&gt;标题2&lt;/h2&gt;&lt;!--align：显示位置,默认左--&gt; 超链接123&lt;a href=\"https://baidu.com\" target=\"_self\"&gt;百度&lt;/a&gt;&lt;!--_self属性：当前窗口跳转--&gt;&lt;br/&gt;&lt;a href=\"https://baidu.com\" target=\"_blank\"&gt;百度&lt;/a&gt;&lt;!--_blank属性：打开新窗口跳转--&gt; 列表标签12345678910111213&lt;ul type=\"none\"&gt;&lt;!--无序列表--&gt;&lt;!--type属性可以更改列表前的符号--&gt; &lt;li&gt;百度&lt;/li&gt; &lt;li&gt;百度&lt;/li&gt; &lt;li&gt;百度&lt;/li&gt; &lt;li&gt;百度&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;!--有序表格--&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt;&lt;/ol&gt; img标签 属性src:图片等路径位置 JavaSE中路径 相对路径：从工程名字开始算 绝对路径：硬盘中的路径 Web中的路径 相对路径： . ：表示当前文件所在的目录 .. ：表示当前文件所在的上级目录 文件名：表示当前文件所在目录的文件，相当于./文件名 绝对路径：http://ip:port/工程名/资源路径 属性：weight; height; border：设置图片边框大小。 alt：当指定路径找不到图片时，用来代替显示的文本内容。 表格标签：实现跨行跨列1234567891011121314151617181920212223&lt;table border=\"1\" width=\"300\"&gt;&lt;!--表格标签--&gt; &lt;!--border：设置边框、width：设置宽度、height：设置高度--&gt; &lt;!--align：设置表格对齐方式--&gt; &lt;!--cellspacing:单元格间距--&gt; &lt;tr&gt;&lt;!--行标签--&gt; &lt;th&gt;h1&lt;/th&gt;&lt;!--表头标签--&gt; &lt;th&gt;h2&lt;/th&gt; &lt;th&gt;h3&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.1&lt;/td&gt;&lt;!--单元格标签--&gt; &lt;td align=\"center\"&gt;1.2&lt;/td&gt; &lt;!--align：设置单元格文本对齐方式--&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=\"2\"&gt;2.1&lt;/td&gt;&lt;!--colspan:列的宽度,实现单元格跨列--&gt; &lt;td rowspan=\"2\"&gt;2.2&lt;/td&gt;&lt;!--rowspan:行的宽度，实现单元格跨行--&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=\"2\"&gt;3.1&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; iframe框架标签可以在html页面上开辟一个小区域加载单独的页面，实现内嵌窗口。 12345&lt;iframe src=\"hello.html\" width=\"400\" height=\"600\" name=\"abc\"&gt;&lt;/iframe&gt;&lt;!--name：表示该区域的名字--&gt;&lt;a href=\"welcome.html\" target=\"abc\"&gt;欢迎&lt;/a&gt;&lt;!--target：打开窗口显示的位置--&gt;&lt;!--a标签的target属性设置为iframe的name属性，就在开辟的区域打开链接窗口--&gt; 表单标签表单：html中用来收集用户信息的元素集合，将这些信息发送给服务器处理。 1234567891011121314151617181920212223242526272829303132333435&lt;form&gt;&lt;!--表单标签--&gt; 用户名称：&lt;input type=\"text\" value=\"User\"/&gt;&lt;br/&gt;&lt;!--input输入框标签--&gt; &lt;!--type：输入类型 value：默认值--&gt; &lt;!--text：文本类型--&gt; 用户密码：&lt;input type=\"password\" /&gt;&lt;br/&gt; &lt;!--password：密码类型--&gt; 确认密码：&lt;input type=\"password\"/&gt;&lt;br/&gt; 性别：&lt;input type=\"radio\" name=\"sex\" checked=\"checked\"/&gt;男 &lt;input type=\"radio\" name=\"sex\"/&gt;女&lt;br/&gt; &lt;!--radio：单选框; name属性：可对其分组; checked：默认选项--&gt; 兴趣爱好：&lt;input type=\"checkbox\" checked=\"checked\"/&gt;Java &lt;input type=\"checkbox\"/&gt;JavaScript&lt;br/&gt; &lt;!--checkbox：复选框; checked:默认选项--&gt; 国籍： &lt;select&gt;&lt;!--下拉列表框标签--&gt; &lt;option&gt;--请选择国籍--&lt;/option&gt;&lt;!--选项标签--&gt; &lt;option selected=\"selected\"&gt;中国&lt;/option&gt; &lt;!--selected：默认选择--&gt; &lt;option&gt;美国&lt;/option&gt; &lt;option&gt;日本&lt;/option&gt; &lt;/select&gt;&lt;br/&gt; 自我评价：&lt;textarea rows=\"10\" cols=\"30\"&gt;默认值&lt;/textarea&gt;&lt;br/&gt; &lt;!--textarea标签：多行文本输入框；属性 rows:行数; 属性 cols：列数--&gt; &lt;!--textarea起始标签和结束标签中的内容是默认值--&gt; &lt;input type=\"reset\" value=\"重新输入\"/&gt;&lt;br/&gt; &lt;!--reset：重置按钮; value属性：更改按钮文本--&gt; &lt;input type=\"submit\" value=\"submit\"&gt;&lt;br/&gt; &lt;!--submit：提交按钮--&gt; &lt;input type=\"button\" value=\"按钮\"/&gt;&lt;br/&gt; &lt;!--button:按钮--&gt; &lt;input type=\"file\"/&gt;&lt;br/&gt; &lt;!--file:文件上传--&gt; &lt;input type=\"hidden\"/&gt;&lt;br/&gt; &lt;!--hidden:隐藏域，需要发送一些不需要用户参与的信息至服务器，可使用隐藏域--&gt;&lt;/form&gt; 表单格式化把表单放入表格，使表单排列整齐。 12345678910111213141516171819202122&lt;form&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户名称：&lt;/td&gt; &lt;td&gt;&lt;input type=\"text\" value=\"User\"/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;用户密码：&lt;/td&gt; &lt;td&gt;&lt;input type=\"password\" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;确认密码：&lt;/td&gt; &lt;td&gt;&lt;input type=\"password\"/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;性别：&lt;/td&gt; &lt;td&gt; &lt;input type=\"radio\" name=\"sex\" checked=\"checked\"/&gt;男 &lt;input type=\"radio\" name=\"sex\"/&gt;女&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt; 表单提交的细节以下格式化的表单： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;form action=\"https://localhost:8080\" method=\"get\"&gt; &lt;!--form标签属性--&gt; &lt;!--action：设置提交的服务器地址--&gt; &lt;!--method：设置提交的方式，默认GET（也可以是POST）--&gt; &lt;input type=\"hidden\" name=\"action\" value=\"login\"&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户名称：&lt;/td&gt; &lt;td&gt;&lt;input type=\"text\" value=\"User\"/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;用户密码：&lt;/td&gt; &lt;td&gt;&lt;input type=\"password\" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;确认密码：&lt;/td&gt; &lt;td&gt;&lt;input type=\"password\"/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;性别：&lt;/td&gt; &lt;td&gt; &lt;input type=\"radio\" name=\"sex\" checked=\"checked\"/&gt;男 &lt;input type=\"radio\" name=\"sex\"/&gt;女&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;兴趣爱好：&lt;/td&gt; &lt;td&gt; &lt;input type=\"checkbox\" checked=\"checked\"/&gt;Java &lt;input type=\"checkbox\"/&gt;JavaScript &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;国籍：&lt;/td&gt; &lt;td&gt; &lt;select&gt; &lt;option&gt;--请选择国籍--&lt;/option&gt; &lt;option selected=\"selected\"&gt;中国&lt;/option&gt; &lt;option&gt;美国&lt;/option&gt; &lt;option&gt;日本&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;自我评价:&lt;/td&gt; &lt;td&gt;&lt;textarea rows=\"10\" cols=\"30\"&gt;默认值&lt;/textarea&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=\"reset\" value=\"重新输入\"/&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=\"submit\" value=\"submit\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt; 格式化后的表单显示为： 表单提交后，url显示为：https://localhost:8080/?action=login&amp;sex=on 该url体现了三部分 提交表单的服务器地址/action属性的值：localhost:8080/ 分隔符：? 请求参数/表单信息：action=login; sex=on 表单提交的时候，数据没有发送给服务器的三种情况： 表单项input标签没有name属性值。 单选、复选输入标签以及下拉列表的option标签，还需要加value属性值，以便发送给服务器具体值，而不是on。 表单项不在提交的form标签中。 修改后的表单代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;form action=\"https://localhost:8080\" method=\"get\"&gt; &lt;!--action：设置提交的服务器地址--&gt; &lt;!--method：设置提交的方式，默认GET--&gt; &lt;input type=\"hidden\" name=\"action\" value=\"login\"&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户名称：&lt;/td&gt; &lt;td&gt;&lt;input type=\"text\" name=\"user\" value=\"User\"/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;用户密码：&lt;/td&gt; &lt;td&gt;&lt;input type=\"password\" name=\"password\"/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;确认密码：&lt;/td&gt; &lt;td&gt;&lt;input type=\"password\" name=\"password\"/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;性别：&lt;/td&gt; &lt;td&gt; &lt;input type=\"radio\" name=\"sex\" checked=\"checked\" value=\"boy\"/&gt;男 &lt;input type=\"radio\" name=\"sex\" value=\"girl\"/&gt;女&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;兴趣爱好：&lt;/td&gt; &lt;td&gt; &lt;input type=\"checkbox\" checked=\"checked\" name=\"hobby\" value=\"Java\"/&gt;Java &lt;input type=\"checkbox\" name=\"hobby\" value=\"js\"/&gt;JavaScript &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;国籍：&lt;/td&gt; &lt;td&gt; &lt;select name=\"country\"&gt; &lt;option value=\"none\"&gt;--请选择国籍--&lt;/option&gt; &lt;option selected=\"selected\" value=\"中国\"&gt;中国&lt;/option&gt; &lt;option value=\"美国\"&gt;美国&lt;/option&gt; &lt;option value=\"日本\"&gt;日本&lt;/option&gt; &lt;/select&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;自我评价:&lt;/td&gt; &lt;td&gt;&lt;textarea rows=\"10\" cols=\"30\"&gt;默认值&lt;/textarea&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=\"reset\" value=\"重新输入\"/&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=\"submit\" value=\"submit\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt; 表单提交后的url: https://localhost:8080/?action=login&amp;user=fred&amp;password=123&amp;password=123&amp;sex=girl&amp;hobby=Java&amp;hobby=js&amp;country=中国 表单标签method属性参数的区别 GET： 浏览器的地址栏为：action属性值 + ? + 请求参数 请求参数格式为：name=value&amp;name=value 不安全 有数据长度限制 POST请求的特点： 浏览器上的地址栏为：action属性值（没有请求参数） 相当于GET请求更安全 理论上没有数据长度限制 div和span div 标签：默认独占一行 span 标签：长度是封装数据长度 p 标签：默认在段落的上方或下方各空出一行（如果已有空行则不空） label标签label标签为input元素定义标注。 该标签不会为用户呈现特殊的效果，但为鼠标用户改进了可用性，即在label元素内点击文本，就会触发该控件。即当用户选择该标签时，浏览器会自动将焦点转到和label标签绑定的表单项上。 常见的应用情况是：单选框/复选框，点击文本即可勾选，而不需要去点那个框。 for : 表示该label是为表单中哪个控件服务，for属性点值设置为该元素的id属性值 CSSCSS简介CSS：层叠样式表单，用于增强/控制网页样式，且允许将样式信息和网页内容分离的一种标记性语言。 语法规则： 选择器：浏览器根据选择器决定受CSS样式影响到HTML元素/标签。 属性：属性:值; 形成一个完成的declaration。 CSS中的注释：/**/ CSS与HTML结合方式标签中的style在标签的style属性设置style=&quot;key: value1 value2;&quot; 这种方式可读性差，且没有复用性。 head标签中使用style标签在head标签中，用style标签定义需要的css样式。 style标签中的语句是CSS语法。 123456789&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style&gt; div{ border: 1px solid red; } &lt;/style&gt;&lt;/head&gt; 可以在同一页面复用代码，不能在多个页面复用CSS代码，且维护不方便，需要修改每个页面。 CSS文件把CSS样式写成CSS文件，在html文件的head标签中通过link标签引用。 style.css 123456div{ border: 1px red solid;}span{ border: 1px red solid;} div.html 123456789&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--link标签专门在head中用来引入CSS样式代码--&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\"/&gt; &lt;!--rel:文档间的关系--&gt; &lt;!--type:目标URL的类型--&gt; &lt;!--href:URL--&gt;&lt;/head&gt; 可以在多个页面中复用CSS样式，且维护方便。 CSS选择器标签名选择器1234标签名{ 属性:值; 属性:值;} 标签名选择器决定哪些标签被动的使用这个样式。 id选择器1234#id选择器{ 属性:值; 属性:值;} id选择器通过id属性选择性的使用这个样式。 html文件 12&lt;div id=\"id001\"&gt;div1&lt;/div&gt;&lt;!--标签的id属性--&gt; &lt;div id=\"id002\"&gt;div2&lt;/div&gt; CSS文件： 123456789101112&lt;style&gt; #id001{ border: yellow 1px solid; font-size: 30px; color: blue; } #id001{ border: 5px blue dotted; font-size: 20px; color: red; }&lt;/style&gt; class 选择器1234.class属性值{ 属性:值; 属性:值;} class属性多用来分组定义CSS样式。 class选择器通过class属性值选择性使用这个样式。 html文件 12&lt;div class=\"class0\"&gt;div1&lt;/div&gt;&lt;!--标签的class属性--&gt; &lt;div class=\"class0\"&gt;div2&lt;/div&gt; CSS文件： 1234567&lt;style&gt; .div{ color: blue; font-size: 30px; border: 1px yellow solid; }&lt;/style&gt; 组合选择器12345.class0, #id001{ color: blue; font-size: 30px; border: 1px yellow solid;} 组合选择器可以让多个选择器共用同样的CSS样式。 常用样式具体可查阅 字体颜色 color : red; color : rgb(33,33,13); color : #00F666; 宽度 width : 19px; width : 20%; 高度 height : 19px; height : 20%; 背景颜色 background-color : #0F2222; 字体大小 font-size : 20px; 边框 border : 1px solid red; DIV居中（相当于页面的居中） 12margin-left : auto;margin -right : auto; 文本居中 text-align : center; 超链接去下划线 text-decoration : none; 表格细线 1234567table{ border : 1px solid black; border-collapse : collapse;/*合并表格边框*/}td,th{ border : 1px, solid black;} 列表去修饰符 list-style : none","link":"/2020/07/22/html-css/"},{"title":"「Python」：Module & Method","text":"长期记录帖：关于遇到过的那些Python 的Packets &amp; Module &amp; Method &amp; Attribute。中英记录。 Trickylist comprehension List comprehension provides a concise way to create lists. e.g. : squares = [x**2 for x in range(10)] A list comprehension consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result will be a new list resulting from evaluating the expression in the context of the for and if clauses which follow it. e.g. : Double loop: [(x,y) for x in [1,2,3] for y in [3,1,4] if x != y] 输出7个 e.g. : (Using zip() to loop together): [(x, y) for x,y in zip([1,2,3], [3,1,4]) if x!=y] 输出2个 Python-Build functionprint print(*objects, sep=’ ‘, end=’\\n’, file=sys.stdout) len Return the length(the number of items) of an object. str.format() 字符串格式化 eg: “{} {}”.format(“Hello”,”World) ‘Hello World’ zip(*iterables) Make an iterator that aggregates【聚集】 elements from each of the iterales. Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. 【返回一个元组的迭代器】 使用zip可以同时对多个迭代器进行迭代 enumerate Enumerate is a built-in funciton of Python. It allows us to loop over something and have an automatic counter. e.g. for counter, value in enumerate(some_list): print(counter, value) e.g. : an optional argument: tell enumerate from where to start the index. for c, value in enumerate(my_list, 1): print(c, value) with open(path) as f: 由于读写文件都可能产生IOError，一旦出错，后面的f.close()就不会调用。 用try……finally来实现，比较麻烦。 try: ​ f = open(path, ‘r’) ​ print(f.read()) finally: ​ if f: ​ f.close() 用with as 简化 with open(path, ‘r’) as f: ​ print(f.read()) numpynumpy.argsort numpy.argsort(a, axis=-1, kind=None, order=None) Returns the indices that would sort an array. Perform an indirect sort along the given axis using the algorithm specified by the kind keyword. It returns an array of indices of the same shape as a that index data along the given axis in sorted order. Parameters: a :array_like. axis : int or None, optional Axis along which to sort. The default is -1(the last axis). 【默认按照最后一个维度】 2-D: axis = 0按列排序 2-D: axis = 1 按行排序 kind :{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, optional The default is ‘quicksort’ Return: index_array: ndarray, int.【返回的是降序排列的索引数组】 e.g.: x = np.array([5, 1, 2]) np.argsort(x) # 降序 array([1,2,0]) np.argsort(-x) # 升序 array([0,2,1]) Linear algebra(numpy.linalg)numpy.dot numpy.dot(a,b) Dot product of two arrays. If both a and b are 1-D arrays, it is inner porduct of vectors. If both a and b are 2-D arrays, it is matrix multiplication, but using matmul is preferred. Id either a or b is 0-D(scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a*b is preferred. …… numpy.matmul Matrix product of two arrays. numpy.matmul(x1, x2) numpy.linalg.inv(a) Compute the inverse of a matrix. Given a square matrix a, return the matrix ainv satisfying dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])/ numpy.linalg.inv(a) Parameters: a :(…, M, M) array_like. Matrix to e inverted. Return: ainv. numpy.linalg.svd numpy.linalg.svd(a, full_matrices=True, compute_uv=True, hermitian=False) Singular Value Decomposition 矩阵的奇异值分解 A = u @ s @ vh u, vh是标准正交矩阵, inv(u) = uh s是对角矩阵 Parameters: a :array_like, a real or complex array with a.ndim &gt;=2 full_matrices :bool, optional. True(default) If True, u and vh have the shapes(…, M, M) and (…, N, N), respectively. Otherwise, the shapes are(…, M, K) and (…, K, N), respectively, where K = min(M,N) compute_uv : bool, optional.True(default) Whether or not to compute u and vh in addition to s.【注，vh就是v的转置】 Return： u: array s: array vh:array numpy.zeros numpy.zeros(shape, dtype=float, order=’C’) Return a new array of given shape and type, filled with zeros. parameters: shape: int or truple of ints. e.g.,(2,3) or 2 dtype: data-type, optional.(Defaults is numpy.float64) oder: optional Returns: out ndarray numpy.full numpy.full(shape, fill_value, dtype=None) Return a new array of given shape and type, filled with fill_value. parameters: shape :int or sequence of ints (2,3) or 2 fill_value :scalar dtype :data-type, optional numpy.arange numpy.arange([start, ]stop, [step, ]dtype=None) Return evenly spaced values within a given interval. parameters: start: number, optional. (Defaults is 0) stop :number. [start,stop) step :number, optional(Defaults is 1) dtype : Returns :ndarray differ with built-in range function numpy.arange returnan ndarray rathan than a list. numpy.arange’s step can be float. numpy.meshgrid numpy.meshgrid(x, y) 生成用x向量为行，y向量为列的矩阵（坐标系） 返回 X矩阵和Y矩阵 X矩阵：网格上所有点的x值 Y矩阵：网格上所有点的y值 e.g., X, Y = np.meshgrid(x, y) 【X,Y 都是网格点坐标矩阵】 numpy.genfromtext numpy.genfromtxt (fname, delimiter=None) Load data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments characters are discarded. Parameters: fname :file, str, list of str, generator. dtype :dtype, optional. delimiter :str, int, or sequence, optional. (default = whitespace) The strin used to separate values. Python的列表读取处理数据很慢，numpy.genfromtext就很棒。 numpy.isnan numpy.isnan(x) Test element-wise for NaN(Not a number) and return result as a boolean array. Parameters: x :array_like Returns: y:ndarray or bool. True where x is NaN, false otherwise. numpy.empty nmpy.empty(shape, dtype=float, order=’C’) Return a new arry of given shape and type, without initializing entries. Parameters: shape :int or tuple of int dtype :data-type,optional Default is numpy.float64. Returns: out: ndarray numpy.reshape numpy.reshape(a, newshape, order=’C’) Gives a new shape to an array without changing its data.【改变张量的shape，不改变张量的数据】 Parameters: a : array-like newshape : int or tuple of ints One shape dimension can be -1. The value is inferred from the length of the array and remaning dimensions. Returns: reshaped_array:ndarray numpy.mean numpy.mean(a, axis=None) Compute the arithmetic mean along the specifiied axis.(the average of the array elements) Parameters: a : array_like axis ：None or int or tuple of ints, optional Axis or axes along which the means are computed. axis=0 ：沿行的垂直往下（列） axis=1 ：沿列的方向水平向右（行） numpy.std numpy.std(a, axis=None,) Compute the standard deviation along the specified axis.【标准差】 Parameters: a :array_like axis :Axis or axes along which the means are computed. numpy.shape attribute Tuple of array dimensions. numpy.concatenate numpy.concatenate((a1, a2, …), axis=0) Join a sequence of arrays along an existing axis. Parameters: a1, a2, … :sequence of array_like The arrays must have the same shape, excepting in the dimension corresponding to axis.【除了axia方向，其他维度的shape要相同】 If axis is None, arrays are flattened before use.【值为None，就先将向量变成一维的】 Default=0 numpy.ndarray.astype method numpy.ndarray.astype(dtype) Copy of the array cast to a specified type.【强制转换数据类型】 Parameters: dtype : str or dtype numpy.ones numpy.ones(shape, dtype=None) Return a new array of given shape and type, filled with ones. Parameters: shape : int or sequence of ints. dtype : data-type, optional numpy.array numpy.array(object, dtype = none) Create an array Parameters: object :array_like An array, any object exposing the array interface, an object whose array method returns an array, or any(nested) sequence. numpy ndarray 运算 [[1]]*3 = [[1],[1],[1]] A * B 元素相乘 numpy.dot(A, B) 矩阵相乘 numpy.power numpy.power(x1, x2) First array elements raised to powers from second array. Parameters: x1 :array_like . The bases. x2 :array_like The exponents. numpy.sum numpy.sum(a, axis=None, dtype=None) Sum of arrays elements over a given axis. Parameters: a :array_like Elements to sum. axis :None or int or tuple of ints, optional Axis or axes along which a sum is perfomed. The default, None, will sum all of the elementsof the input array. numpy.transpose numpy.transpose(a, axes=None) Permute the dimensions of the array.【tensor的维度换位】 Parameters: a : array_like axes : list of ints, optinal Default, reverse the dimensions. Otherwise permute the axes according to the values given. Returns : ndarray 张量a的shape是(10,2,15), numpy.transport(2,0,1)的shape就是(15,10,2) 对于一维：行向量变成列向量 对于二维：矩阵的转置 numpy.save numpy.save(file, arr) Save an array to a binary file in Numpy .npy format. Parameters: file :file, str, or pathlib arr :array_like Array data to be saved. numpy.clip Clip(limit) the values in an array numpy.clip(a, a_min, a_max) Parameters: a :array_like a_min : scalar or array_like a_max :scalar or array_like numpy.around Evenly round to the given number of decimals(十进制) numpy.around(a) Parameters： a :array_like Notes: For values exactly halfway between rounded decimal values, Numpy rounds to the nearest even values. 【这什么意思呢？ 就是说对于0.5的这种，为了统计上平衡，不会全部向上取整或者向下取整，会向最近的偶数取整，around（2.5）=2】 numpy.log The natural logarithm log is the inverse of exponential functions, so that log(exp(x))=x. numpy.log(x) Parameters: x : array_like numpy.ndarray.T attribute, the transpose array. ndarray.T numpy.random.shuffle Modify a sequence in-space by shufflng its contents. This function only shuffles the array along the first axis of a multi-diensional array. The order of sub-arrays is changed but their contents remains the same. numpy.random.shuffle(x) Parameters: x : array_like e.g. shuffle two list, X and Y, together. 【以相同的顺序打乱两个array】 np.random.seed(0) randomize = np.arrange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize] sklearnskelearn.linear_model.LinearRegression class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None) Ordinary least squares Linear Regress(最小二乘法回归运算) Parameters: fit_intercept :bool, optional, defalut True【True：需要bias的截距项】 normalize :bool, optional, default False 【True：对样本做feature scaling】 Attributes： coef_ :array of shape(n_features) 【权重】 intercept_ :bias Methods： fit(self,X,y[,sample_weight]) :Fit linear model e.g. , LinearRegression().fit(x_data, y_data) matplotlib.pyplotmatplotlib.pyplot.contourf contour and contourf draw contour lines and filled contours, respectively.【一个画等高线，一个填充等高线/轮廓】 contour([X, Y, ] Z, [levels], **kwargs) Parameters X, Y: The coordinates of the values in Z. A and Y must both be 2-D with the sanme shape as Z(e.g. created via numpy.meshgrid), or they must both be 1-D such that len(X) == M is the number of columns in Z and len(Y) = N is the number of rows in Z. 【X，Y要么是由像numpy.mershgrid(x, y) 生成的网格点坐标矩阵，要么X，Y是（基）向量，X向量是x轴的，对应到Z矩阵，是Z矩阵的列数，Y向量同理】 Z ：array-like(N,M) levels : int or array-like, optional. Determines the number and positions of contour lines / religions.【划分多少块等高区域】 alpha :float, optional. Between 0(transparent) and 1(opaque).【透明度】 cmap :str or Colormap, optional. e.g., pyplot.contourf(x, y, Z, 50, alpha=0.5, cmap=pyplot.get_cmap(‘jet’))【‘jet’是常用的那种红橙黄绿青蓝紫】 matplotlib.pyplot.plot plot([x], y, [fmt], , data=None, *kwargs) The coordinates of the points or line nodes are given by x, y. Parameters: x, y :array-like or scalar. fmt :str, optional. A format string. e.g., ‘.’, point marker. ‘-‘, solid line style. ‘–’,dashed line style. ‘b’, blue. ms/markersize : float lw/linewidth :float color : matplotlib.pyplot.xlim xlim(args, *kwargs) Get or set the x limits of the current axes. e.g. left, right = xlim() :get xlim(left, right) :set matplotlib.pyplot.show show(args, *kwargs) display a figure. matplotlib.pyplot.vlines Plot vertical lines. vlines(x, ymin, ymax, color=’k’, linestyles=’solid’) Parameters: x :scalar or 1D array_like ymin, ymax :scalar or 1D array_like matplotlib.pyplot.hlines Plot horizontal lines. vlines(y, xmin, xmax, color=’k’, linestyles=’solid’) Parameters: y :scalar or 1D array_like xmin, xmax :scalar or 1D array_like matplotlib.pyplot.savefig Save the current figure. savefig(fname) Parameters: fname :str ot Pathlike matplotlib.pyplot.legend Place a lengend on the axes. e.g. : legend() Labeling exisiting plot elements plt.plot(train_loss) plt.plot(dev_loss) plt.legend([‘train’, ‘dev’]) e.g. : le syssys.argv[] python a.py data.csv sys.argv = [‘a.py’, ‘data.csv’] 重定向到文件 f = open(‘out.csv’, ‘w’) sys.stdout = f print(‘此时print掉用的就是文件对象的write方法’)","link":"/2020/03/07/python/"},{"title":"「机器学习-李宏毅」：Recurrent Neural Network（RNN）","text":"这篇文章中首先从RNN能解决什么问题入手，分析了RNN与一般NN的区别。然后着重讲解了基于RNN的LSTM模型，包括LSTM的细节、和一般NN的区别，以及如何训练LSTM模型。具体阐述了在模型（RNN类模型）训练过程中为什么会遇到剧烈抖动问题和如何解决抖动的工程解决方法。 Example applicationSolt filling先从RNN的应用说起，RNN能做什么？ RNN可以做智慧系统： 如下图中，用户告诉订票系统：”I would like to arrive Taipei on November 2nd”. 订票系统能从这句话中得到Destination: Taipei，time of arrival: November 2nd. 这个过程也就是Solt Filling （槽位填充）。 如果用Feedforward network来解决solt filling问题，输入就是单词，输出是每个槽位（slot）的单词，如下图。 上图中，如何将word表示为一个vector？ EncodingHow to represent each word as a vector? 1-of-N encoding最简单的方式是1-of-N encoding方式（独热方式）。 向量维度大小是整个词汇表的大小，每一个维度代表词汇表中的一个单词，如果该维度置1，表示这个维度代表的单词。 Beyond 1-of-N encoding对1-of-N encoding方式改进。 第一种：Dimension for “Other” 在1-of-N的基础上增加一维度——‘other’维度，即当单词不在系统词汇表中，将other维度置1代表该单词。 第二种：Word hashing 即便是增加了”other”维度，编码vector的维度也很大，用word hashing的方式将大幅减少维度。 以apple为例，拆成app, ppl, ple三个部分，如上图所示，vector中表示这三个部分的维度置1。 用这样的word hashing方式，vector的维度只有 $26\\times 26\\times26$ ，大幅减少词向量的维度。 Example通过encoding的方式，单词用vector来表示，用前馈神经网络来解决solt filling问题。 如下图. input:一个单词（encoding为vector） output: input单词中属于该槽位(solts)的概率分布(vector)。 但用普通的前馈神经网络处理solt filling问题会出现下图问题： 上图中，arrive Taipei on November 2nd 和 leave Taipei on November 2nd，将这两句话的每个单词（vector）放入前馈神经网络，得出的dest槽位都应该是Taipei。 但，通过之前的语意，arrive Taipei的Taipei应该是终点，而leave Taipei的Taipei是起点。 因此，在处理这种问题时，我们的神经网络应该需要memory，对该输入的上下文有一定的记忆存储。 Recurrent Neural Network(RNN)Basic structure因此，我们对一般的前馈神经网络加入记忆元件a, a 存储hidden layer的输出，同时a也作为下一次计算的输入部分,下图就是最基础的RNN模型。 举一个例子来说明该过程： Input sequence: $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$ $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$ $\\begin{bmatrix}2 \\ 2 \\end{bmatrix}$ … RNN模型如下图所示：所有的weight都是1，没有bias; 所有的神经元的activation function 都是线性的。 input : $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$, 记忆元件初值 a1=0 a2=0. 记忆元件也作为输入的一部分，hidden layer的输出为 2 2, 更新记忆元件的值. output: $\\begin{bmatrix}4 \\ 4 \\end{bmatrix}$ , 记忆元件存储值 a1=2 a2=2. input : $\\begin{bmatrix}1 \\ 1 \\end{bmatrix}$ , 记忆元件存储值 a1=2 a2=2. 记忆元件也作为输入的一部分，hidden layer 的输出为6 6,更新记忆元件的值。 output: $\\begin{bmatrix}12 \\ 12 \\end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6. 这里可以发现，第一次和第二次的输入相同，但是由于有记忆元件的缘故，两次输出不同。 input : $\\begin{bmatrix}2 \\ 2 \\end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6. 记忆元件也作为输入的一部分，hidden layer 的输出为16 16,更新记忆元件的值。 output: $\\begin{bmatrix}32 \\ 32 \\end{bmatrix}$ , 记忆元件存储值 a1=16 a2=16. RNN中，由于有memory，会和一般前馈模型有两个不同的地方：一是输入相同的vector，输出可能是不同的；二是将一个sequence连续放进RNN模型中，如果sequence中改变顺序，输出也大多不同。 用这个RNN模型来解决之前的solt filling问题，就可以解决上下文语意不同影响solt的问题。 将arrive Taipei on November 2nd的每个单词都放入同样的模型中。 因此将RNN展开，如上图，像不同时间点的模型，但其实是不同时间点循环使用同一个模型。 由于左边的前文是arrive，右边的前文是leave，所以存储在memory中的值不同，Taipei作为input的输出（槽位的概率分布）也不同。 Elman Network &amp; Jordan Network上文中只是RNN模型中的一种，即Elman Network，记忆元件存储的是上一个时间点hidden layer的输出。 而Jordan Network模型中,他的记忆元件存储的是上一时间点的output。 （据说，记忆元件中存储output的值会有较好的performance，因为output是有target vector的，因此能具象的体现放进memory的是什么） Bidirectional RNN上文中的RNN模型，记忆元件中存储的都是上文的信息，如果要同时考虑上下文信息，即是bidirectional RNN(双向RNN)。 模型如下图。 双向RNN的好处是看的范围比较广，当计算输出 $y^t$ 时，上下文的内容都有考虑到。 Long Short-term Memory(LSTM)现在最常用的RNN模型是LSTM，Long Short-term Memory，这里的long是相当于上文中的RNN模型，因为上文提到的RNN模型都是short-term,即每一个时间点，都会把memory中的值洗掉，LSTM的long，就是会把memory的值保留的相对于久一些。 LSTM如下图，与一般NN不同的地方是，他有4个inputs,一个outputs。 LSTM主要有四部分组成： Input Gate：输入门，下方箭头是输入，左方箭头是输入信号控制输入门的打开程度，完全打开LSTM才能将输入值完全读入，打开的程度也是NN自己学。 Output Gate：输出门，上方箭头是输出，左方箭头是输入信号控制输出门的打开程度，同理，打开程度也是NN自己学习。 Memory Cell：记忆元件。 Forget Gate：遗忘门，右边的箭头是输入信号控制遗忘门的打开程度，控制将memory cell洗掉的程度。 LSTM更详细的阐述LSTM的内部机制： 注意： $z_o,z_i,z_f$ 是门的signal control,其实就等同于一般NN中neuron的输入z，是scalar。 gate其实就是一个neuron，通常gate neuron 的activation function f取 sigmod,因为值域在0到1之间，即对应门的打开程度。 input/forget/output gate的neuron的activation function是f(sigmod function), input neuron的activation function是g。 input gate控制输入:$g(z)f(z_i)$ input: z $\\rightarrow$ $g(z)$ input gate signal control: $z_i \\rightarrow f(z_i)$ multiply：$g(z)f(z_i)$ forget gate 控制memory：$cf(z_f)$ forget gate signal control: $z_f\\rightarrow f(z_f)$ 如果 $f(z_f)=1$ ,说明memory里的值保留；如果 $f(z_f)=0$ ,说明memory里的值洗掉。 更新当前时间点的memory(输入+旧的memory值) ：$c’=g(z)f(z_i)+cf(z_f)$ output gate 控制输出：$h(c’)f(z_o)$ output: $c’ \\rightarrow h(c’)$ output gare signal control: $z_o \\rightarrow f(z_o)$ multiply: $h(c’)f(z_o)$ LSTM模型（trained）如下图： 输入序列为 $\\begin{bmatrix}3 \\ 1 \\ 0 \\end{bmatrix}$$\\begin{bmatrix}4 \\ 1 \\ 0 \\end{bmatrix}$ $\\begin{bmatrix}2 \\ 0 \\ 0 \\end{bmatrix}$ $\\begin{bmatrix}1 \\ 0 \\ 1 \\end{bmatrix}$ $\\begin{bmatrix}3 \\ -1 \\ 0 \\end{bmatrix}$ 该LSTM activation function: g、h都为linear function（即输出等于输入），f为sigmod. 通过该LSTM的输出序列为： 0 0 0 7 0 0 （建议手算一遍） Compared with Original Networkoriginal network如下图： LSTM 的NN即用LSTM替换原来的neuron，这个neuron有四个inputs，相对于original network也有4倍的参数，如下图： 所以原来RNN的neuron换为LSTM，就是下图： 上图中： 这里的 $z^f,z^u,z,z^o$ 都是 $x^t \\begin{bmatrix} \\quad\\end{bmatrix}$ 矩阵运算得到的vector, 因为上图中有多个LSTM，因此 $z^i$ 的第k个元素，就是控制第k个LSTM的input signal control scalar。所以，$z^f,z^u,z,z^o$ 的维度等于下一层neuron/LSTM的个数。 所以这里memory（cell）$c^t$ 也是一个vector，第k个元素是第k个LSTM中cell存储的值。 向量运算和scalar一样，LSTM细节如下图： Extension：“peephole”上小节的LSTM是simplified，将LSTM hidden layer的输出 $h^t$ 和cell中存储的值 $c^t$ 和下一时间点的输入 $x^{t+1}$ 一同作为下一时间点的输入，就是LSTM的扩展版”peephole”。 如下图： Multi-layer LSTM多层的peephole LSTM如下图： （：wtf 我到底看到了什么 不要怕：Keras PyTorch等套件都有 “LSTM”，“GUR，”SimpleRNN“ 已实现好的layers. Learning训练RNN时，输入与target如下所示： 估测模型的好坏，计算RNN的Loss时，需要看作一个整体，计算每个时间点RNN输出与target的crossentropy的和。 训练也可同样用Backpropagation，但考虑到时间点，有一个进阶版的”Backpropogation through time(BPTT)”[1]。 RNN一般就用BPTT训练。 How to train wellnot easy to trainRNN-based network is not always easy to learn. 但基于RNN的模型往往不太好训练，总是会出现下图中的绿色线情况（即抖动）。 error surface is rougherror surface，即total loss在参数变化时的函数图。 会发现基于RNN的模型的error surface会长下图这个样子：有时很平坦(flat)有时很陡峭(steep) 橙色点出发： 起初处在flat的位置。 随着一次次更新，gradient在变小，learning rate即会变大。 可能稍微不幸，就会出现跨过悬崖，即出现了剧烈震荡的问题。 如果刚好当前处在悬崖低，这时的gradient很大，learning rate也很大，step就会很大，飞出去，极可能出现segment fault(NaN). Thomas Mikolv 用工程师的角度来解决这个问题，即当此时的gradient大于某个阈值(threshold)时，就不要让当前的gradient超过这个阈值（通常取15）。 这样处在悬崖低的橙色点，（Clipping路线），更新就会到绿色的，继续更新。 Why为什么RNN模型会出现抖动的情况呢？ 用下图这个简单例子说明（一般activation function用sigmod,而ReLu的performance一般较差）： 上图中，输入序列是1 0 0 0 …，memory连接下一个时间点的权重是w，可以轻易得到最后一个时间点的输出 $y^{1000}=w^{999}$ 。 上图中，循环输出1000次，如果w变化 $\\Delta w$ ，看输出 $y^{1000}$ 的变化，来直观体现gradient 的变化： 上图中，可以看出： 绿色部分：当w从1变化为1.01时， $y^{1000}$ 的输出变化即大，既有较大的gradient，理应有小的learning rate。 黄色部分：当w从0.99变化为0.01时， $y^{1000}$ 的输出几乎不变化，即有较小的gradient，理应有大大learning rate. 在很小的地方（0.01 到 1.01），他的gradient就变化即大，即抖动的出现。 Reason：RNN，虽然可以看作不同时间点的展开计算，但始终是同一个NN的权重计算（cell连接到下一个时间点的权重），在不同时间中，反复叠乘，因此会出现这种情况。 Helpful Techniques LSTM几乎已经算RNN的一个标准了，为什么LSTM的performance比较好呢。 为什么用LSTM替换为RNN？ :Can deal with gradient vanishing(not gradient explode). 可以解决gradient vanish的问题（gradient vanish problem 具体见 这篇文章2.1.1） 为什么LSTM可以解决gradient vanish问题 ：memory and input are added.（LSTM的的输出与输入和memory有关） : The influence never disappears unless forget gate is closed.（memory的影响可以很持久） GRU[2]（Gated Recurrent Unit）：是只有两个Gate，比LSTM简单，参数更少，不容易overfitting 玄学了叭 More Applications【待更新】 Many to OneMany to ManyBeyond SequenceSeq2SeqAuto-encoder-TextAuto-encoder-SpeechChat-botReference BPTT GRU","link":"/2020/06/11/rnn/"},{"title":"「机器学习-李宏毅」:Semi-supervised Learning","text":"这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？ 再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。 对于Generative Model，文章重点讲述了如何用EM算法来训练模型。 对于Low-density，文章重点讲述了如何让模型进行Self-training，并且在训练中引入Entropy-based Regularization term来尽可能low-density的假设。 对于Smoothness Assumption，文章重点讲述了Graph-based Approach（基于图的方法），并且在训练中引入Smoothness Regularization term来尽可能满足Smoothness Assumption的假设。 对于Better Representation，本篇文章只是简单阐述了其思想，具体介绍见这篇博客。 Introduction什么是Semi-supervised learning(半监督学习)？和Supervised learning（监督式学习）的区别在哪？ Supervised learning（监督式学习）： 用来训练的数据集 $R$ 中的数据labeled data，即 ${(x^r,\\hat{y}^r)}_{r=1}^R$ . 比如在图像分类数据集中： $x^r$ 是image，对应的target output $y^r$ 是分类的label。 而Semi-supervised learning（半监督式学习）： 用来的训练的数据集由两部分组成 $\\{(x^r,\\hat{y}^r)\\}_{r=1}^R$ , $\\{x^u\\}_{u=R}^{R+U}$ ，即labeled data和unlabeled data，而且通常情况下，unlabeled data的数量远远高于labeled data是数量，即 $U&gt;&gt;R$ . Semi-supervised learning 又分为两种，Transductive learning （转导/推论推导）和 Inductive learning（归纳推理） Transductive learing: unlabeled data is the testing data. 即测试数据在训练中用过。 Inductive learning: unlabeled data is not the testing data.测试数据是训练中没有用过的数据。 这里的使用testing data是指用testing data的feature，而不是使用testing data的label。 为什么会有semi-supervised learning？ Collecting data is easy, but collecting “labelled” data is expensive. 【收集数据很简单，但收集有label的数据很难】 We do semi-supervised learning in our lives 【在生活中，更多的也是半监督式学习，我们能明白少量看到的事物，但看到了更多我们不懂的，即unlabeled data】 Why Semi-supervised learning helps为什么半监督学习能帮助解决一些问题？ 如上图所示，如果只有labeled data，分类所画的boundary可能是一条竖线。 但如果有一些unlabeled data（如灰色的点），分类所画的boundary可能是一条斜线。 The distribution of the unlabeled data tell us something. 半监督式学习之所以有用，是因为这些unlabeled data的分布能告诉我们一些东西。 通常这也伴随着一些假设，所以半监督式学习是否有用往往取决于这些假设是否合理。 Semi-supervised Learning for Generative ModelSupervised Generative Model在这篇文章中，有详细讲述分类问题中的generative model。 给定一个labelled training data $x^r\\in C_1,C_2$ 训练集。 prior probability（先验概率）有 $P(C_i)$ 和 $P(x|C_i)$ ，假设是Gaussian模型，则 $P(x|C_i)$ 由Gaussian模型中的 $\\mu^i,\\Sigma$ 参数决定。 根据已有的labeled data，计算出假设的Gaussian模型的参数（如下图），从而得出prior probability。 即可算出posterior probability $P\\left(C_{1} \\mid x\\right)=\\frac{P\\left(x \\mid C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x \\mid C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x \\mid C_{2}\\right) P\\left(C_{2}\\right)}$ Semi-supervised Generative Model在只有labeled data的图中，算出来的 $\\mu,\\Sigma$ 参数如下图所示： 但如果有unlabeled data（绿色点），会发现分布的模型参数更可能是是下图： The unlabeled data $x^u$ help re-estimate $P(C_1),P(C_2),\\mu^1,\\mu^2,\\Sigma$ . 因此，unlabeled data会影响分布，从而影响prior probability，posterior probability，最终影响 boundary。 EM所以有unlabeled data, 这个Semi-supervised 的算法怎么做呢？ 其实就是EM（Expected-maximization algorithm，期望最大化算法。） Initialization : $\\theta={P(C_1),P(C_2),\\mu^1,\\mu^2,\\Sigma}$ . 初始化Gaussian模型参数，可以随机初始，也可以通过labeled data得出。 虽然这个算法最终会收敛，但是初始化的参数影响收敛结果，就像gradient descent一样。 E：Step 1: compute the posterior probability of unlabeled data $P_\\theta(C_1|x^u)$ (depending on model $\\theta$ ) 根据当前model的参数，计算出unlabeled data的posterior probability $P(C_1|x^u)$ .(以$P(C_1|x^u)$ 为例) M：Step 2: update model. Back to step1 until the algorithm converges enventually. 用E步得到unlabeled data的posterior probability来最大化极大似然函数，更新得到新的模型参数，公式很直觉。(以 $C_1$ 为例) （$N$ ：data 的总数，包括unlabeled data; $N_1$ :label= $C_1$ 的data数） $P(C_1)=\\frac{N_1+\\Sigma_{x^u}P(C_1|x^u)}{N}$ 对比没有unlabeled data之前的式子， $P(C_1)=\\frac{N_1}{N}$ ，除了已有label= $C_1$ ，还多了一部分，即unlabeled data中属于 $C_1$ 的概率和。 $\\mu^{1}=\\frac{1}{N_{1}} \\sum_{x^{r} \\in C_{1}} x^{r}+\\frac{1}{\\sum_{x^{u}} P\\left(C_{1} \\mid x^{u}\\right)} \\sum_{x^{u}} P\\left(C_{1} \\mid x^{u}\\right) x^{u}$ 对比没有unlabeled data的式子 ，$\\mu^{1}=\\frac{1}{N_{1}} \\sum_{x^{r} \\in C_{1}} x^{r}$ ，除了已有的label= $C_1$ ，还多了一部分，即unlabeled data的 $x^u$ 的加权平均（权重为 $P(C_1\\mid x^u)$ ，即属于 $C_1$ 的概率）。 $\\Sigma$ 公式也包括了unlabeled data. 所以这个算法的Step 1就是EM算法的Expected期望部分，根据已有的labeled data得出极大似然函数的估计值； Step 2就是EM算法的Maximum部分，利用unlabeled data（通过已有模型的参数）最大化E步的极大似然函数，更新模型参数。 最后反复迭代Step 1和Step 2，直至收敛。 Why EM[1]挖坑EM详解。 为什么可以用EM算法来解决Semi-supervised? 只有labeled data 极大似然函数 $\\log{L(\\theta)}=\\sum_{x^r}\\log{P_\\theta(x^r,\\hat{y}^r)}$ , 其中 $P_\\theta(x^r,\\hat{y}^r)=P_\\theta(x^r\\mid \\hat{y}^r)P(\\hat{y}^r)$ . 对上式子求导是有closed-form solution的。 有labeled data和unlabeled data 极大似然函数增加了一部分 $\\log L(\\theta)=\\sum_{x^{r}} \\log P_{\\theta}\\left(x^{r}, \\hat{y}^{r}\\right)+\\sum_{x^{u}} \\log P_{\\theta}\\left(x^{u}\\right)$ . 将后部分用全概率展开， $P_{\\theta}\\left(x^{u}\\right)=P_{\\theta}\\left(x^{u} \\mid C_{1}\\right) P\\left(C_{1}\\right)+P_{\\theta}\\left(x^{u} \\mid C_{2}\\right) P\\left(C_{2}\\right)$ . 如果要求后部分，因为是unlabeled data, 所以模型 $\\theta$ 需要得知unlabeled data的label，即 $P(C_1\\mid x^u)$ ,而求这个式子，也需要得到 prior probability $P(x^u\\mid C_1)$ ,但这个式子需要事先得知模型 $\\theta$ ，因此陷入了死循环。 因此这个极大似然函数不是convex（凸），不能直接求解，因此用迭代的EM算法逐步maximum极大似然函数。 Low-density Separation Assumption另一种假设是Low-density Separation的假设，即这个世界是非黑即白的”Black-or-white”。 两种类别之间是low-density，交界处有明显的鸿沟，因此要么是类别1，要么是类别2，没有第三种情况。 Self-training对于Low-density Separation Assumption的假设，使用Self-training的方法。 Given：labeled data set $={(x^r,\\hat{y}^r}{r=1}^R$ ,unlabeled data set $={x^u}{u=l}^R+U$ . Repeat： Train model $f^$ from labeled data set. ($f^$ is independent to the model) 从labeled data set中训练出一个模型 Apply $f^*$ to the unlabeled data set. Obtain pseudo-label ${(x^u,y^u}_{u=l}^{R+U}$ . 用这个模型 $f^*$ 来预测unlabeled data set， 获得伪label Remove a set of data from unlabeled data set, and add them into the labeled data set. 拿出一些unlabeled data(pseudo-label)，放到labeled data set中，回到步骤1，再训练。 how to choose the data set remains open 如何选择unlabeled data 是自设计的 you can also provide a weight to each data. 训练中可以对unlabeled data(pseudo-label)和labeled data 赋予不同的权重. 注意： Regression模型是不能self-training的，因为unlabeled data和其pseudo-label放在模型中的loss为0，无法再minimize。 Hard LabelV.S. semi-supervised learning for generative model Semi-supervised learning for generative model和Low-density Separation的区别其实是soft label 和soft label的区别。 generative model是利用来unlabeled data的 $P(C_1|x^u)$ posterior probability来计算新的prior probability，迭代更新模型。 而low-density是计算出unlabeled data的pseudo-label，选择性扩大labeled data set(即加入部分由pseudo-label的unlabeled data)来迭代训练模型。 因此，如果考虑Neural Network： ($\\theta^*$ 是labeled data计算所得的network parameters) 如下图，unlabeled data $x^u$ 放入模型中预测，得到 $\\begin{bmatrix} 0.7 \\ 0.3\\end{bmatrix}$ . 如果是使用hard label，则 $x^u$ 的target是 $\\begin{bmatrix} 1 \\ 0\\end{bmatrix}$ . 如果是使用soft label，则 $x^u$ 的target是 $\\begin{bmatrix} 0.7 \\ 0.3\\end{bmatrix}$ . 如果是使用soft label，则self-training不会有效，因为新的data加进去，不会增大模型的loss，也就无法再minimize. 所以基于Low-density Separation的假设，是非黑即白的，需要使用hard label来self-training。 Entropy-based Regularization在训练模型中，我们需要尽量保证unlabeled data在模型中的分布是low-density separation。 即下图中，unlabeled data得到的pseudo-label的分布应该尽量集中，而不应该太分散。 所以，在训练中，如何评估 $y^u$ 的分布的集中度？ 根据信息学，使用 $y^u$ 的entropy，即 $E\\left(y^{u}\\right)=-\\sum_{m=1}^{5} y_{m}^{u} \\ln \\left(y_{m}^{u}\\right)$ (注：这里的 $y^u_m$ 应该是 $y^u=m$ 的概率) 当 $E(y^u)$ 越小，说明 $y^u$ 分布越集中，如下图。 因此，在self-training中： $L=\\sum_{y^r} C(x^r,\\hat{y}^r)+\\lambda\\sum_{x^u}E(y^u)$ Loss function的前一项（cross entropy）minimize保证分类的正确性，后一项（entropy of $y^u$ ) minimize保证 unlabeled data分布尽量集中，最大可能满足low-density separation的假设。 training：gradient decent. 因为这样的形式很像之前提到过的regularization(具体见这篇文章的3.2)，所以又叫entropy-based regularization. Outlook: Semi-supervised SVMSVM也是解决semi-supervised learning的方法. 上图中，在有unlabeled data的情况下，希望boundary 分的越开越好（largest margin）和有更小的error. 因此枚举unlabeled data所有可能的情况，但枚举在计算量上是巨大的，因此SVM（Support Vector Machines）可以实现枚举的目标，但不需要这么大的枚举量。 Smoothness AssumptionSmoothness Assumption的思想可以用以下话归纳： “You are known by the company you keep” 近朱者赤，近墨者黑。 蓬生麻中，不扶而直。白沙在涅，与之俱黑。 Assumption：“similar” $x$ has the same $\\hat{y}$ . 【意思就是说：相近的 $x$ 有相同的label $\\hat{y}$ .】 More precise assumption： x is not uniform if $x^1$ and $x^2$ are close in a hign density region, $\\hat{y}^1$ and $\\hat{y}^2$ are the same. Smoothness Assumption假设更准确的表述是： x不是均匀分布，如果 $x^1$ 和 $x^2$ 通过一个high density region的区域连在一起，且离得很近，则 $\\hat{y}^1$ 和 $\\hat{y}^2$ 相同。 如下图， $x^1$ 和 $x^2$ 通过high density region连接在一起，有相同的label，而 $x^2$ 和 $x^3$ 有不同的label. Smoothness Assumption通过观察大量unlabeled data，可以得到一些信息。 比如下图中的两张人的左脸和右脸图片，都是unlabeled，但如果给大量的过渡形态（左脸转向右脸）unlabeled data，可以得出这两张图片是相似的结论. Smoothness Assumption还可以用在文章分类中，比如分类天文学和旅游学的文章。 如下图， 文章 d1和d3有overlap word（重叠单词），所以d1和d3是同一类，同理 d4和d2是一类。 如果，下图中，d1和d3没有overlap word，就无法说明d1和d3是同一类。 但是，如果我们收集到足够多但unlabeled data，如下图，通过high density region的连接和传递，也可以得出d1和d3一类，d2和d4一类。 Cluster and then Label在Smoothness Assumption假设下，直观的可以用cluster and then label，先用所有的data训练一个classifier。 直接聚类标记(比较难训练）。 Graph-based Approach另一种方法是利用图的结构（Graph structure）来得知 $x^1$ and $x^2$ are close in a high density region (connected by a high density path). Represent the data points as a graph. 【把这些数据点看作一个图】 建图有些时候是很直观的，比如网页中的超链接，论文中的引用。 但有的时候也需要自己建图。 注意： 如果是影像类，base on pixel，performance就不太好，一般会base on autoencoder，将feature抽象出来，效果更好。 Graph Construction建图过程如下： Define the similarity $s(x^i, x^j)$ between $x^i$ and $x^j$ . 【定义data $x^i$ 和 $x^j$ 的相似度】 Add edge【定义数据点中加边（连通）的条件】 K Nearest Neighbor【和该点最近的k个点相连接】 e-Neighborhood【与离该点距离小于等于e的点相连接】 Edge weight is proportional to $s(x^i, x^j)$ 【边点权重就是步骤1定义的连接两点的相似度】 Gaussian Radial Basis Function： $s\\left(x^{i}, x^{j}\\right)=\\exp \\left(-\\gamma\\left\\|x^{i}-x^{j}\\right\\|^{2}\\right)$ 一般采用如上公式（经验上取得较好的performance）。 因为利用指数化后（指数内是两点的Euclidean distance），函数下降的很快，只有当两点离的很近时，该相似度 $s(x^i,x^j)$ 才大，其他时候都趋近于0. Graph-based Approach图建好后： The labeled data influence their neighbors. Propagate through the graph. 【label data 不仅会影响他们的邻居，还会一直传播下去】 如果data points够多，图建的好，就会像下图这样： 但是，如果data较少，就可能出现下图这种label传不到unlabeled data的情况： Smoothness Definition因为是基于Smoothness Assumption，所以最后训练出的模型应让得到的图尽可能满足smoothness的假设。 注意： 这里的因果关系是，unlabeled data作为NN的输入，得到label $y$ ，该label $y$ 和labeled data的 label $\\hat{y}$ 一起得到的图是尽最大可能满足Smoothness Assumption的。 （而不是建好图，然后unlabeled data的label $y$ 是labeled data原有的 $\\hat{y}$ 直接传播过来的，不然训练NN干嘛） 把unlabeled data作为NN的输入，得到label ，对labeled data和”unlabeled data” 建图。 为了在训练中使得最后的图尽可能满足假设，定义smoothness of the labels on the graph. $S=\\frac{1}{2} \\sum_{i,j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}$ （对于所有的labeled data 和 “unlabeled data”（作为NN输入后，有label）） 按照上式计算，得到的Smoothness如下图所示： Smaller means smoother. 【Smoothness $S$ 越小，表示图越满足这个假设】 计算smoothness $S$ 有一种简便的方法： $S=\\frac{1}{2} \\sum_{i, j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}=y^{T} L y$ (这里的1/2只是为了计算方便) $y$ : (R+U)-dim vector，是所有label data和”unlabeled data” 的label，所以是R+U维。 $y=\\begin{bmatrix}…y^i…y^j…\\end{bmatrix}^T$ $L$ :(R+U) $\\times$ (R+U) matrix，也叫Graph Laplacian（调和矩阵，拉普拉斯矩阵） $L$ 的计算方法：$L=D-W$ 其中 $W$ 矩阵算是图的邻接矩阵（区别是无直接可达边的值是0） $D$ 矩阵是一个对角矩阵，对角元素的值等于 $W$ 矩阵对应行的元素和 矩阵表示如下图所示： （证明据说很枯燥，暂时略[2]) Smoothness Regularization$S=\\frac{1}{2} \\sum_{i, j} w_{i, j}\\left(y^{i}-y^{j}\\right)^{2}=y^{T} L y$ $S$ 中的 $y$ 其实是和network parameters有关的（unlabeled data的label），所以把 $S$ 也放进损失函数中minimize，以求尽可能满足smoothness assumption. 以满足smoothness assumption的损失函数： $L=\\sum_{x^r} C\\left(y^{r}, \\hat{y}^{r}\\right)+\\lambda S$ 损失函数的前部分使labeled data的输出更贴近其label，后部分 $\\lambda S$ 作为regularization term，使得labeled data和unlabeled data尽可能满足smoothness assumption. 除了让NN的output满足smoothness的假设，还可以让NN的任何一层的输出满足smoothness assumption，或者让某层外接一层embedding layer，使其满足smoothness assumption，如下图： Better RepresentationBetter Presentation的思想就是：去芜存菁，化繁为简。 Find the latent(潜在的) factors behind the observation. The latent factors (usually simpler) are better representation. 【找到所观察事物的潜在特征，即该事物的better representation】 该部分后续见这篇博客。 Reference 挖坑：EM算法详解 挖坑：Graph Laplacian in smoothness. Olivier Chapelle：Semi-Supervised Learning","link":"/2020/07/03/semi-supervised/"},{"title":"「算法导论」:排序-总结","text":"本篇文章review了算法中的排序算法，包括冒泡排序、插入排序、归并排序、堆排序（以及用堆实现优先队列）、快速排序和计数排序。 分别从算法思路、算法伪代码实现、算法流程、算法时间复杂度四个方面阐述每个算法。 排序排序问题： 输入：一个 $n$个数的序列 $&lt;a_1,a_1,…,a_n&gt;$ 输出：输入序列的一个重拍 $&lt;a_1’,a_2’,…,a_n’&gt;$ ，使得 $a_1’\\leq a_2’ \\leq…\\leq a_n’$ . 在实际中，待排序的数很少是单独的数值，它们通常是一组数据，称为记录(record)。每个记录中包含一个关键字(key)，这就是需要排序的值。记录剩下的数据部分称为卫星数据(satellite data)，通常和关键字一同存取。 原址排序：输入数组中仅有常数个元素需要在排序过程中存储在数组之外。 典型的原址排序有：插入排序、堆排序、快速排序。 符号说明： $\\Theta$ 记号： $\\Theta$ 记号渐进给出一个函数的上界和下界。 $\\Theta(g(n))=\\left\\{f(n): \\text { there exist positive constants } c_{1}, c_{2}, \\text { and } n_{0}\\text { such that } \\right. \\left.0 \\leq c_{1} g(n) \\leq f(n) \\leq c_{2} g(n) \\text { for all } n \\geq n_{0}\\right\\}$ $g(n)$ 称为 $f(n)$ 的一个渐进紧确界(asymptotically tight bound) $O$ 记号 $O$ 记号只给出了函数的渐进上界。 $O(g(n))=\\left\\{f(n): \\text { there exist positive constants } c \\text { and } n_{0}\\text { such that } \\right. \\left.0 \\leq f(n) \\leq c g(n) \\text { for all } n \\geq n_{0}\\right\\}$ $\\Omega$ 记号 $\\Omega$ 记号给出了函数的渐进下界。 $\\Omega(g(n))=\\left\\{f(n): \\text { there exist positive constants } c \\text { and } n_{0}\\text { such that } \\right. \\left.0 \\leq c g(n) \\leq f(n) \\text { for all } n \\geq n_{0}\\right\\}$ 符号比较如下图： 排序算法运行时间一览 ： 算法 最坏情况运行时间 平均情况/期望运行时间 插入排序 $\\Theta (n^2)$ $\\Theta(n^2)$ 归并排序 $\\Theta(n\\lg{n})$ $\\Theta(n\\lg{n})$ 堆排序 $O(n\\lg{n})$ - 快速排序 $\\Theta(n^2)$ $\\Theta(n\\lg{n})$ (expected) 计数排序 $\\Theta(k+n)$ $\\Theta(k+n)$ 基数排序 $\\Theta(d(n+k))$ $\\Theta(d(n+k))$ 桶排序 $\\Theta(n^2)$ $\\Theta(n)$ (average-case) 冒泡排序反复交互相邻未按次序排列的元素。 BUBBLESORT(A) 参数：A待排序数组 1234for i = 1 to A.lengh-1 for j = A.length downto i+1 //每次迭代找出A[i..j]中最小的元素放在A[i]位置 if A[j] &lt; A[j-1] exchange A[j] with A[j-1] 冒泡排序是原址排序，流行但低效。 插入排序如下图所示，插入排序就像打牌时排序一手扑克牌。 开始时，我们的左手为空，桌子上的牌面向下。 然后，我们每次从桌子上拿走一张牌，想把它放在左手中的正确位置。 为了找到这张牌的正确位置，我们从右到左将这张牌和左手里的牌依次比较，放入正确的位置。 左手都是已排序好的牌。 INSERTION-SORT(A) A：待排序数组 12345678for j = 2 to A.length key = A[j] //将key插入到已排序好的A[1..j-1] i = j - 1 //pointer for sorted sequence A[1..j-1] while i &gt; 0 and A[i] &gt; key A[i+1] = A[i] i-- A[i+1] = key 插入排序是原址排序，对于少量元素是一个有效的算法。 最坏情况的运行时间： $\\Theta(n^2)$ . 归并排序分治分治： 将原问题分解为几个规模较小但类似于原问题的子问题，递归地求解这些子问题，然后再合并这些子问题的解来建立原问题的解。 分治的步骤： 分解：分解原问题为若干规模较小的子问题。 解决：递归地求解各子问题，规模较小，可直接求解。 合并：合并这些子问题的解成原问题的解。 算法核心归并排序中的分治 ： 分解：分解待排序的n个元素序列成各n/2个元素序列的两个子序列。 解决：使用归并排序递归地排序两个子序列，当序列长度为1时，递归到达尽头。 合并：合并两个已经排序好的子序列以产生排序好的原序列。 核心 ：合并两个已经排序好的子序列——MERGE(A, p, q, r) A: 待排序原数组。 p, q, r: 数组下标，满足 $p\\leq q&lt;r$ 。 假设子数组 A[p..q] 和A[q+1..r]都已经排好序，合并这两个数组代替原来的A[p..r]子数组。 MERGE算法理解： 牌桌上有两堆牌面朝上，每堆都已排好序，最小的牌在顶上。希望将两堆牌合并成排序好的输出牌堆，且牌面朝下。 比较两堆牌顶顶牌，选取较小的那张，牌面朝下的放在输出牌堆。 重复步骤2直至某一牌堆为空。 将剩下的另一堆牌面朝下放在输出堆。 MERGE合并的过程如下图所示： MERGE算法分析： 在上述过程中，n个元素，我们最多执行n个步骤，所以MERGE合并需要 $\\Theta(n)$ 的时间。 伪代码MERGE(A, p, q, r) 功能：合并已排序好的子数组A[p..q]和A[q+1..r] 参数：A为待排序数组，p, q, r为数组下标，且满足 $p\\leq q&lt;r$ 12345678910Let S[p..r] be new arraysk = p //pointer for S[]i = p, j = q+1 //pointer for subarraywhile k &lt;= r while ( i &lt;= q and j &lt;= r and A[i] &lt;= A[j] ) or j &gt; r // 取A[p..q]牌堆 S[k++] = A[i++] while ( i &lt;= q and j &lt;= r and A[i] &gt;= A[j] ) or i &gt; q //取A[q+1..r]牌堆 A[p..r] = S[p..r] MERGE-SORT(A, p, r) 功能：排序子数组A[p..r] 12345if p &lt; r q = (p+r)/2 MERGE-SORT(A, p, q) MERGE-SORT(A, q+1, r) MERGE(A, p, q, r) 算法分析分治算法运行时间分析分治算法运行时间递归式来自三个部分。 假设 $T(n)$ 是规模为 $n$ 的一个问题的运行时间。若规模问题足够小，则直接求解需要常量时间，将其写作 $\\Theta(1)$ 。 假设把原问题分解成 $a$ 个子问题，每个子问题的规模是原问题的 $1/b$ (在归并排序中， $a$ 和 $b$ 都为2，但很多分治算法中 $a\\neq b$ )。为了求解一个规模为 $n/b$ 规模的子问题，需要 $T(n/b)$ 的时间，所以需要 $aT(n/b)$ 的时间求解 $a$ 个子问题。 如果分解子问题需要 $D(n)$ 时间，合并子问题需要 $C(n)$ 时间。 递归式： $$ T(n)=\\left\\{\\begin{array}{ll}\\Theta(1) & \\text { if } n \\leq c \\\\ a T(n / b)+D(n)+C(n) & \\text { otherwise }\\end{array}\\right. $$ 归并排序分析前文分析了MERGE(A, p, q, r) 合并两个子数组的时间复杂度是 $\\Theta(n)$ ，即 $C(n)=\\Theta(n)$ ，且 $D(n)=\\Theta(n)$ . 归并排序的最坏情况运行时间 $T(n)$ : $$ T(n)=\\left\\{\\begin{array}{ll}\\Theta(1) & \\text { if } n=1 \\\\ 2 T(n / 2)+\\Theta(n) & \\text { if } n>1\\end{array}\\right. $$ 用递归树的思想求解递归式： 即递归树每层的代价为 $\\Theta(n)=cn$ ，共有 $\\lg{n}+1$ 层，所以归并排序的运行时间结果是 $\\Theta(n\\lg{n})$ . 堆排序堆自由树 ：连通的、无环的无向图。 有根数 ：是一棵自由树，其顶点存在一个与其他顶点不同的顶点，称为树的根。 度 ：有根树中一个结点 $x$ 孩子的数目称为 $x$ 的度。 深度 :从根 $r$ 到结点 $x$ 的简单路径。 二叉树 ：不包括任何结点，或者包括三个不相交的结点集合：一个根结点，一棵称为左子树的二叉树和一棵称为右子树的二叉树。 完全k叉树 ：所有叶结点深度相同，且所有内部结点度为k的k叉树。 （二叉）堆 ：是一个数组，它可以被看成一个近似的完全二叉树，树上的每个结点对应数组中的一个元素。除了最底层外，该树被完全填满，并且是从左到右填充。如下图所示。 堆的数组$A$ 有两个属性： $A.length$ ：数组元素的个数，A[1..A.length]中都存有值。 $A.heap-size$ ：有多少个堆元素在数组，A[1..heap-size]中存放的是堆的有效元素。 （$0\\leq A.heap-size\\leq A.lengh$ ) 堆的性质： $A[1]$ :存放的是树的根结点。 对于给定的一个结点 $i$ ，很容易计算他的父结点、左孩子和右孩子的下标。 PARENT(i) 1return i/2 //i&gt;&gt;&gt;1 LEFT(i) 1return 2*i //i&lt;&lt;&lt;1 RIGHT(i) 1return 2*i+1 //i&lt;&lt;&lt;1 | 1 包含$n$ 个元素的堆的高度为 $\\Theta(\\lg{n})$ 堆结构上的基本操作的运行时间至多和堆的高度成正比，即时间复杂度为 $O(\\lg{n})$ . 叶子结点：n/2+1 , n/2+2 , … , n 堆的分类： 最大堆： 除了根以外的结点 $i$ 都满足 $A[\\text{PARENT}(i)]\\geq A[i]$ . 某个结点最多和其父结点一样大。 堆的最大元素存放在根结点中。 最小堆： 除了根以外的结点 $i$ 都满足 $A[\\text{PARENT}(i)]\\leq A[i]$ . 堆的最小元素存放在根结点中。 堆的基本过程 : MAX-HEAPIFY：维护最大堆的过程，时间复杂度为 $O(\\lg{n})$ BUILD-MAX-HEAP：将无序的输入数据数组构造一个最大堆，具有线性时间复杂度 $O(n\\lg{n})$ 。 HEAPSORT：对一个数组进行原址排序，时间复杂度为 $O(n\\lg{n})$ MAX-HEAP-INSERT、HEAP-EXTRACT-MAX、HEAP-INCREASE-KEY和HEAP-MAXIMUM：利用堆实现一个优先队列，时间复杂度为 $O(\\lg{n})$ . 维护：MAX-HEAPIFY调用MAX-HEAPIFY的时候，假定根结点LEFT(i)和RIGHT(i)的二叉树都是最大堆，但A[i]可能小于其左右孩子，因此违背了堆的性质。 MAX-HEAPIFY通过让 A[i]“逐级下降”，从而使下标为i的根结点的子树满足最大堆的性质。 MAX-HEAPIFY(A, i) 功能：维护下标为i的根结点的子树，使其满足最大堆的性质。 参数：i 是该子树的根结点，其左子树右子树均满足最大堆的性质。 12345678910l = LEFT(i)r = RIGHT(i)if l &lt;= A.heap-size and A[l] &gt; A[i] largest = lelse largest = iif r &lt;= A.heap-size and A[r] &gt; A[i] largest = rif largest != i exchange A[i] with A[largest] MAX-HEAPIFY(A, largest) 下图是执行 MAX-HEAPIFY(A, 2)的执行过程。A.heap-size=10, 图(a)(b)(c)依次体现了值为4的结点依次下降的过程。 时间复杂度分析 ： MAX-HEAPIFY的时间复杂度为 $O(lg{n})$. 建堆：BUILD-MAX-HEAP堆的性质： 子数组A[n/2+1..n]中的元素都是树的叶子结点。因为下标最大的父结点是n/2，所以n/2以后的结点都没有孩子。 建堆 ：每个叶结点都可以看成只包含一个元素的堆，利用自底向上的方法，对树中其他结点都调用一次MAX-HEAPIFY，把一个大小为n = A.length的数组A[1..n]转换为最大堆。 BUILD-MAX-HEAP(A) 功能：把A[1..n]数组转换为最大堆 123A.heap-size = A.lengthfor i = A.length/2 downto 1 MAX-HEAPIFY(A, i) 下图是把A数组构造成最大堆的过程： 时间复杂度分析 ： BUILD-MAX-HEAP需要 $O(n)$ 次调用MAX-HEAPIFY，因此构造最大堆的时间复杂度是 $O(n\\lg{n})$ . 排序：HEAPSORT算法思路： 初始化时，调用BUILD-MAX-HEAP将输入数组A[1..n]建成最大堆，其中 n = A.length。 调用后，最大的元素在A[1]，将A[1]和A[n]互换，可以把元素放在正确的位置。 将n结点从堆中去掉(通过减少A.heap-size实现)，剩余结点中，原来根的孩子仍是最大堆，但根结点可能会违背堆的性质，调用MAX-HEAPIFY(A, 1)，从而构造一个新的最大堆。 重复步骤3，直到堆的大小从n-1降为2. HEAPSORT(A) 功能：利用堆对数组排序 12345BUILD-MAX-HEAP(A)for i = A.length downto 2 exchange A[1] with A[i] A.heap-size = A.heap-size - 1 MAX-HEAPIFY(A, 1) 下图为调用HEAPSORT的过程图： 时间复杂度分析 ： 建堆BUILD-MAX-HEAP的时间复杂度为 $O(n\\lg{n})$ ，n-1次调用MAX-HEAPIFY的时间复杂度为 $O(n\\lg{n})$ ，所以堆排序的时间复杂度为 $O(n\\lg{n})$ . 堆的应用：优先队列这里关注如何用最大堆实现最大优先队列。 优先队列(priority queue)： 一种用来维护由一组元素构成的集合S的数据结构，其中每一个元素都有一个相关的值，称为关键字(key)。 （最大）优先队列支持的操作 ： INSERT(S, x)：把元素 $x$ 插入集合S中，时间复杂度为 $O(\\lg{n})$ 。 MAXIMUM(S)：返回S中具有最大关键字的元素，时间复杂度为 $O(1)$ 。 EXTRACT-MAX(S)：去掉并返回S中的具有最大关键字的元素，时间复杂度为 $O(\\lg{n})$ 。 INCREASE-KEY(S, x, k)：将元素 $x$ 的关键字值增加到k，这里假设k的大小不小于元素 $x$ 的原关键字值，时间复杂度为 $O(\\lg{n})$ 。 MAXIMUM将集合S已建立最大堆的前提下，调用HEAP-MAXIMUM在 $\\Theta(1)$ 实现MAXIMUM的操作。 HEAP-MAXIMUM(A) 功能：实现最大优先队列MAXIMUM的操作，即返回集合中最大关键字的元素。 1return A[1] 时间复杂度分析 ：$\\Theta(1)$ EXTRACT-MAX类似于HEAPSORT的过程。 A[1]为最大的元素，A[1]的孩子都是最大堆。 将A[1]和A[heap-size]交换，减少堆的大小(heap-size)。 此时根结点的孩子满足最大堆，而根不一定满足最大堆性质，维护一下当前堆。 HEAP-EXTRACT-MAX(A) 功能：实现最大优先队列EXTRACT-MAX的操作，即去掉并返回集合中最大关键字的元素。 1234567if A.heap-size &lt; 1 error \"heap underflow\"max = A[1]A[1] = A[A.heap-size]A.heap-size = A.heap-size - 1MAX-HEAPIFY(A, 1)return max 时间复杂度分析 ：$O(\\lg{n})$ . INCREASE-KEY如果增加A[i]的关键词，可能会违反最大堆的性质，所以实现HEAP-INCREASE-KEY的过程类似插入排序：从当前i结点到根结点的路径上为新增的关键词寻找恰当的插入位置。 当前元素不断和其父结点比较，如果当前元素的关键字更大，则和父结点进行交换。 步骤1不断重复，直至当前元素的关键字比父结点小。 HEAP-INCREASE-KEY(A, i, key) 功能：实现最大优先队列INCREASE-KEY的功能，即将A[i]的关键字值增加为key. 参数：i为待增加元素的下标，key为新关键字值。 123456if key &lt; A[i] error \"new key is smaller than current key\"A[i] = keywhile i &gt; 1 and A[PARENT(i)] &lt; A[i] exchange A[i] with A[PARENT(i)] i = PARENT(i) 下图展示了HEAP-INCREASE-KEY的过程： 时间复杂度分析 ：$O(\\lg{n})$ INSERT如何插入一个元素扩展最大堆？ 先通过增加一个关键字值为 $-\\infin$ 的叶子结点扩展最大堆。 再调用HEAP-INCREASE-KEY过程为新的结点设置对应的关键字值。 MAX-HEAP-INSERT(A, key) 功能：实现最大优先队列的INSERT功能，即将关键字值为key的新元素插入到最大堆中。 参数：key是待插入元素的关键字值。 123A.heap-size = A.heap-size + 1A[A.heap-size] = -∞HEAP-INCREASE-KEY(A, A.heap-size, key) 时间复杂度分析 ：$O(\\lg{n})$ . 快速排序对于包含 $n$个数的输入数组来说，快速排序是一个最坏情况时间复杂度为 $\\Theta(n^2)$ 的排序算法。 虽然最坏情况时间复杂度很差，但是快速排序通常是实际排序应用中最好的选择，因为他的平均性能非常好：他的期望时间复杂度为 $\\Theta(n\\lg{n})$ ，而且 $\\Theta(n\\lg{n})$ 中隐含的常数因子非常小。 另外，它还能进行原址排序。 分治对A[p..r]子数组进行快速排序的分治过程： 分解： 数组A[p..r]被划分为两个（可能为空）的子数组A[p..q-1]和A[q+1..r]。 使得A[p..q-1]中的每个元素都小于等于A[q]，A[q+1..r]中的每个元素都大于等于A[q]。 其中计算下标q也是分解过程的一部分。 解决：通过递归调用快速排序，对子数组A[p..q-1]和A[q+1..r]进行排序。 合并：因为子数组都是原址排序的，所以不需要合并操作，A[p..r]已经排好序。 快速排序：QUICKSORT按照分治的过程。 QUICKSORT(A, p, r) 功能：快速排序子数组A[p..r] 1234if p &lt; r q = PARTITION(A, p, r) QUICKSORT(A, p, q-1) QUICKSORT(A, q+1, r) 数组的划分：PARTITION快速排序的关键部分就在于如何对数组A[p..r]进行划分，即找到位置q。 PARTITION(A, p, r) 功能：对子数组A[p..r] 划分为两个子数组A[p..q-1]和子数组A[q+1..r]，其中A[p..q-1] 小于等于A[q]小于等于A[q+1..r] 返回：数组的划分下标q 12345678x = A[r]i = p - 1for j = p to r - 1 // j is pointer for comparation if A[j] &lt;= x i = i+1 exchange A[i] with A[j]exchange A[i+1] with A[r]return i+1 PARTITION总是选择一个 $x=A[r]$ 作为主元(pivot element)，并围绕它来划分子数组A[p..r]。 在循环中，数组被划分为下图四个（可能为空的）区域： $p\\leq k\\leq i$ ，则 $A[k]\\leq x$ . $i+1\\leq k \\leq j-1$ ，则 $A[k]&gt;x$. $k = r$ ，则 $A[k]=x$ . $j\\leq k\\leq r-1$ ，则 $A[k]$ 与 $x$ 无关。 下图是将样例数组PARTITION的过程： 快速排序的性能[*]待补充 快速排序的随机化版本与始终采用 $A[r]$ 作为主元的方法不同，随机抽样是从子数组A[p..r]随机选择一个元素作为主元。 加入随机抽样，在平均情况下，对子数组A[p..r]的划分是比较均匀的。 RANDOMIZED-PEARTITION(A, p, r) 功能：数组划分PARTITION的随机化主元版本 123i = RANDOM(p, r)exchange A[r] with A[i]return PARTITION(A, p, r) RANDOMIZED-QUICKSORT(A, p, r) 功能：使用随机化主元的快速排序 1234if p &lt; r q = RANDOMIZED-PARTITION(A, p, r) RANDOMIZED-QUICKSORT(A, p, q-1) RANDOMIZED-QUICKSORT(A, q+1, r) 计数排序计数排序 ： 假设 $n$ 个输入元素中的每一个都是在 0到 $k$ 区间内到一个整数，其中 $k$ 为某个整数。当 $k = O(n)$ 时，排序的运行时间为 $\\Theta(n)$ . 计数排序的思想 ： 对每一个输入元素 $x$，确定小于 $x$ 的元素个数。利用这个信息，就可以直接把 $x$ 放在输出数组正确的位置了。 COUNTING-SORT(A, B, k) 功能：计数排序 参数： A[1..n]输入的待排序数组，A.length = n B[1..n] 存放排序后的输出数组； 临时存储空间 C[0..k] ，A[1..n]中的元素大小不大于k. 123456789101112let C[0..k] be a new arrayfor i = 0 to k C[i] = 0for j = 1 to A.length C[A[j]] = C[A[j]] + 1//C[i] now contains the number of elements equal to i.for i = 1 to k C[i] = C[i] + C[i-1]//C[i] now contains the number of elements less than or equal to i.for j = A.length downto 1 B[C[A[j]]] = A[j] C[A[j]] = C[A[j]] - 1 下图是计数排序的过程： Reference Introduction to Algorithms. 算法导论","link":"/2020/06/29/sort-preview/"},{"title":"「机器学习-李宏毅」：Tips for Deep Learning","text":"这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。tips从Training和Testing两个方面展开。在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有RMSprop、Momentum、Adam等）。当在Training中得到好的performance，但在testing中perform bad时，即遇到了overfitting，又该怎么处理呢？文章后半部分详尽介绍了EarlyStopping、Regularization和Dropout三个solution。 Recipe of Deep LearningDeep Learning 的三个步骤： 如果在Training Data中没有得到好的结果，需要重新训练Neural Network。 如果在Training Data中得到好的结果，在Testing Data（这里的Testing Data是指有Label的Data，比如Kaggle的Public Data或者是从Training Data中划分出的Development Data）没有得到的好的结果，说明Overfitting了，需要重新设计Neural Network的结构。 Do not always blame Overfitting 如果在Testing Data中，看到上图，20-layer的error小，56-layer的error大，56-layer一定overfitting了。 No!!!不要总把原因归咎于Overfitting。 再看Testing Data error之前，先看看Training Data的error。上图中，56-layer的DNN在Training Data的error本来就比20-layer的大，说明56-layer的DNN根本没有train好。 所以56-layer的DNN在Testing Data上的error大，原因不是overfitting，而是模型根本没有train好。 注： Overfitting是在Training Data上error小，但在Testing Data上的error大。 因此，对于在Training Data上得到不好的结果和在Training Data上得到好的结果但在Testing Data上得到不好的结果这两种情况，需要不同的解决方法。 Bad Results on Training Data在不重新设计DNN结构时，如果在Training Data中得到Bad Results，一般有两种方法来改进结果： New activation function【neuron换新的激活函数】 Adaptive Learning Rate New activation functionVanishing Gradient Problem 上图表示，在手写数字辨识中，Deeper layers并不能有好的performance。 为什么会这样呢？ 因为出现了Vanishing Gradient Problem，即gradient随着deeper layer逐渐消失的问题。 上图中，假设neuron的activation function是sigmod函数。 靠近Input layer层的参数的变化对Loss的影响很小，所以对Loss function做微分，gradient很小，参数更新慢。 而靠近Output layer层的参数的编号对Loss的影响更大，所以对Loss function做微分，gradient很大，参数更新快。 因为靠近Output Layer层的参数更新快，所以很快converge（收敛、趋于稳定）；但靠近Input Layer层的参数更新慢，几乎还处在random（随机）的状态。 当靠近Output Layer层的参数趋于稳定时，由于靠近Output Layer层的参数对Loss影响大，所以观察到的Loss的值也趋于稳定，于是，你就把training停掉了。 但是，靠近Input层的参数几乎处在random状态，所以拿模型用在Testing Data上，发现结果几乎是随机的。 怎么直观理解靠近Input Layer的参数的gradient小呢？ 用微分的直观含义来表示gradient $\\partial{l}/\\partial{w}$ : 当 $w$ 增加 $\\Delta{w}$ 时，如果 $l$ 的变化 $\\Delta{l}$ 变化大，说明 $\\partial{l}/\\partial{w}$ 大，否则 $\\partial{l}/\\partial{w}$ 小。 我们在DNN中使用的activation function是sigmod函数，sigmod函数会把值压到0和1之间。 因此，上图中，其他值不变，只有连接 $x_N$ 的参数 $w$ 增加 $\\Delta w$ 时，输入通过neuron的sigmod函数，函数的输出增加的 $\\Delta$ 会变小，随着Deeper Layer，neuron的输出的 $\\Delta$ 会越变越小，趋至0。 最后DNN输出的变化对 loss的影响小，即 $\\Delta{l}$ 趋至0，即参数的gradient $\\partial{l}/\\partial{w}$ 趋至0。（即 Vanishing Gradient） ReLu ：Rectified Linear Unit为了防止发生Vanishing Gradient Problem，在DNN中选择使用新的activation function。 ReLu长下面这个样子： z: input a: output 当 $z\\leq0$ 时， $a=0$ ；当 $z &gt;0$ 时， $a=z$ 。 Reason : Fast to compute Biological reason【有生物上的原因】 Infinite sigmod with different biases. 【是无穷个 有不同bias的sigmod函数 的叠加】 Vanishing gradient problem 【最重要的是没有vanishing gradient problem】 为什么ReLu没有vanishing gradient problem 上图DNN中，ReLu在输入是负数时，输出是0。因此这些输出是0的neuron可以去掉。 就变成了下图这个A Thinner linear network。由于ReLu函数的性质，靠近Input Layer的参数不会有smaller gradient。 这里有一个Q&amp;A: Q1: function变成linear的，会不会DNN就变弱了？ ： 当neuron的operation region不变的话，DNN的确是linear的，但是当neuron的operation region改变后，就是unlinear的。 ：即，当input的变化小，operation region不变（即输入不会从大于0变成小于0，小于0变成大于0这种），model还是linear的；但当input的变化大时，很多neuron的operation region都变化了，model其实就是unlinear的。 Q2: ReLu 怎么微分？ ：ReLu在0点不可微，那就随便指定为0这样（台湾腔QAQ）。 ReLu - variant当 $z\\leq 0$ 时，输出为0，就不能更新参数了。于是就有下图变体： 当 $z\\leq0$ 时，gradient都为0.01，为什么不能是其他值。于是就有下图变体：其中 $\\alpha$ 也是一个需要学习的参数 MaxoutMaxout，如下图，在设计neural network时，会给每一层的neuron分组，成为一个新的neuron。 Maxout也是一个Learnable activation function。 ReLu是Maxout学出来的一个特例。 上图中，左图是ReLu。 ReLu的输入 $z = wx+b$ ，输出 $a$ 如上图的绿色的线。 右图是Maxout。Maxout的输入 $z_1 =wx+b,z_2=0$ ，那么输出取max，输出 $a$ 如上图中绿色的线，和左图的ReLu相同。 Maxout is more than ReLu。 当参数更新时，Maxout的函数图像如下图： DNN中的参数是learnable的，所以Maxout也是一个learnable的activation function。 Reason ： Learnable activation function [Ian J. Goodfellow, ICML’13] Activation function in maxout network can be any piecewise linear convex function. 在maxout神经网络中的激活函数可以是任意的分段凸函数。 How many pieces depending on how many elements in a group. 分段函数分几段取决于一组中有多少个元素。 Maxout : how to trainGiven a training data x, we know which z would be the max. 【当给出每笔training data时，我们能知道Maxout neuron中哪一个最大】 如上图，在这笔training data x中，我们只train this thin and linear network 的参数，即max z相连的参数。 每笔不同的training data x，会得到不同的thin and linear network，最后，会train到每一个参数。 Adaptive Learning RateReview Adagrad在这篇文章： Gradient 第一小节讲到一种adaptive learning rate的gradient 算法：Adagrad 算法。在那篇文章中，我们得出的结论是 the best step $\\propto$ |First dertivative| / Second derivative. 在上图中，两个方向，因为蓝色方向的二阶微分更小，所以蓝色方向应该有更大的learning rate。 因此，在Adagrad中，我们用一阶微分来估量二阶微分的大小： $$ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t} $$ RMSProp但是，在训练NN时，Error Surface（Total Loss对参数的变化）的图像可能会更复杂，如下图： 因为函数图像过于复杂，可能在同一方向的不同位置，也需要有不同的learning rate。 RMSProp是Adagrad的进阶版。 RMSProp过程： $w^{1} \\leftarrow w^{0}-\\frac{\\eta}{\\sigma^{0}} g^{0} \\quad \\sigma^{0}=g^{0}$ $w^{2} \\leftarrow w^{1}-\\frac{\\eta}{\\sigma^{1}} g^{1} \\quad \\sigma^{1}=\\sqrt{\\alpha (\\sigma^{0})^2+(1-\\alpha)(g^1)^2}$ $w^{3} \\leftarrow w^{2}-\\frac{\\eta}{\\sigma^{2}} g^{2} \\quad \\sigma^{2}=\\sqrt{\\alpha (\\sigma^{1})^2+(1-\\alpha)(g^2)^2}$ … $w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sigma^{t}} g^{t} \\quad \\sigma^{t}=\\sqrt{\\alpha (\\sigma^{t-1})^2+(1-\\alpha)(g^t)^2}$ $\\sigma^t$ 也是在算gradients的 root mean squar。 但是在RMSProp中，加入了参数 $\\alpha$ (需要手动调节大小的参数)，可以给当前算出来的gradient $g^t$ 更大的权重，即更相信现在gradient的方向，不那么相信以前gradient的方向。 MomentumMomentum，则是引用物理中的惯性。 上图中，当小球到达local minima时，会因为惯性继续往前更新，则有可能到达minima的位置。 这里的Momentum，就代指上一次前进（参数更新）的方向。 Vanilla Gradient Descent 如果将Gradient的步骤画出图来，就是下图这样： 过程： Start at position $\\theta^0$ Compute gradietn at $\\theta^0$ Move to $\\theta^1=\\theta^0-\\eta\\nabla{L(\\theta^0)}$ Compute gradietn at $\\theta^1$ Move to $\\theta^2=\\theta^1-\\eta\\nabla{L(\\theta^1)}$ … …Stop until $\\nabla{L(\\theta^t)}\\approx0$ Momentum 在Momentum中，参数更新方向是当前Gradient方向和Momentum方向（上一次更新方向）的叠加。 Movement方向：上一次更新方向 - 当前gradient方向。 过程： Start at position $\\theta^0$ Movement: $v^0=0$ Compute gradient at $\\theta^0$ Movement $v^1=\\lambda v^0-\\eta\\nabla{L(\\theta^0)}$ Move to $\\theta^1=\\theta^0+v^1$ Compute gradient at $\\theta^1$ Movement $v^2=\\lambda v^1-\\eta\\nabla{L(\\theta^1)}$ Move to $\\theta^2=\\theta^1+v^2$ … …Stop until $\\nabla{L(\\theta^t)}\\approx0$ 和Vanilla Gradient Descent比较，$v^i$ 其实是过去gradient( $\\nabla{L(\\theta^0)}$ 、$\\nabla{L(\\theta^1)}$ 、… 、 $\\nabla{L(\\theta^{i-1})}$ )的加权和。 迭代过程： $v^0=0$ $v^1=-\\eta\\nabla{L(\\theta^0)}$ $v^2=-\\lambda\\eta\\nabla{L(\\theta^0)}-\\eta\\nabla{L(\\theta^1)}$ … 再用那个小球的例子来直觉的解释Momentum： 当小球在local minima时，gradient为0，但是Momentum（即上次移动方向）是继续往前，于是小球可以继续向前更新。 Adam = RMSProp + Momentum Algorithm：Adam, our proposed algorithm for stochastic optimization. 【Adam，是为了优化stochastic gradient】（至于什么是stochastic gradient，建议戳) $g_t^2$ indicates the elementwise square $g_t\\odot g_t$ . 【$g_t^2$ 是gradient $g_t$ 向量和 $g_t$ 的元素乘】 Good default settings for the tested machine learning problems are $\\alpha=0.001$ , $\\beta_1=0.9$ , $\\beta_2=0.999$ and $\\epsilon=10^{-8}$ . All operations on vectors are element-wise. With $\\beta_1^t$ and $\\beta_2^t$ we denote $\\beta_1$ and $\\beta_2$ to the power t. 【参数说明：算法默认的参数设置是 $\\alpha=0.001$ , $\\beta_1=0.9$ , $\\beta_2=0.999$ ， $\\epsilon=10^{-8}$ 。算法中所有vector之间的操作都是对元素操作。 $\\beta_1^t$ 和 $\\beta_2^t$ 是 $\\beta_1$ 和 $\\beta_2$ 的 $t$ 次幂】 Adam Pseudo Code： Require：$\\alpha$ : Stepsize 【步长/learning rate $\\eta$ 】 Require：$\\beta_1,\\beta_2\\in\\left[0,1\\right)$ : Exponential decay rates for the moment estimates. Require：$f(\\theta)$ : Stochastic objective function with parameters $\\theta$ .【参数 $\\theta$ 的损失函数】 Require: $\\theta_0$ ：Initial parameter vector 【初值】 $m_0\\longleftarrow 0$ (Initial 1st moment vector) 【 $m$ 是Momentum算法中的更新参数后的方向 $v$ 】 $v_0\\longleftarrow 0$ (Initial 2nd moment vector) 【 $v$ 是RMSprop算法中gradient的root mean square $\\sigma$ 】 $t\\longleftarrow 0$ (Initial timestep) 【更新次数】 while $\\theta_t$ not concerged do 【当 $\\theta$ 趋于稳定，即 $\\nabla{f(\\theta)}\\approx0$ 时】 $t\\longleftarrow t+1$ $g_t\\longleftarrow \\nabla{f_t(\\theta_{t-1})}$ (Get gradients w.r.t. stochastic objective at timestep t) 【算第t次时 $\\theta$ 的gradient】 $m_{t} \\leftarrow \\beta_{1} \\cdot m_{t-1}+\\left(1-\\beta_{1}\\right) \\cdot g_{t}$ (Update biased first momen t estimate) 【用Momentum算更新方向】 $v_{t} \\leftarrow \\beta_{2} \\cdot v_{t-1}+\\left(1-\\beta_{2}\\right) \\cdot g_{t}^{2}$ (Update biased second raw moment estimate) 【RMSprop估测最佳步长（ 和$v$ 负相关） 】 $\\widehat{m}_{t} \\leftarrow m_{t} /\\left(1-\\beta_{1}^{t}\\right)$ （Comppute bbi. as-corrected first momen t estima te) 【算出来的值有bias，论文中有具体解释为什么有。当更新次数增加时， $1-\\beta_1^t$ 也趋近于1】 $\\widehat{v}_{t} \\leftarrow v_{t} /\\left(1-\\beta_{2}^{t}\\right)$ (Compute bias-corrected second raw momen t estimate) 【和上同理】 $\\theta_{t} \\leftarrow \\theta_{t-1}-\\alpha \\cdot \\widehat{m}_{t} /(\\sqrt{\\widehat{v}_{t}}+\\epsilon)$ （Update parameters） 【 $\\widehat{m}t$ 相当于是更准确的gradient的方向，$\\sqrt{\\widehat{v}{t}}+\\epsilon$ 是为了估测最好的步长，调节learning rate】 Gradient Descent Limitation？在Gradient这篇文章中，讲到过Gradient有一些问题不能处理： Stuck at local minima Stuck at saddle point Very slow at the plateau （李老师说的，不是我说的QAQ）：但是Andrew（吴恩达）在2017年说过，不用太担心这个问题。为什么呢？ 如果要stuck at local minima，前提是每一维度都是local minima。 如果在一个维度遇到local minima的概率是p，当NN很复杂时，有很多参数时，比如1000，那么遇到local minima的概率是 $p^{1000}$ ，趋近于0了，几乎不会发生。 ：所以不用太担心Gradient Descent的局限性。 Bad Results on Testing DataEarly Stopping在更新参数时，可能会出现这样曲线图： 图中，Total Loss在training set中逐渐减小，但在validation set中逐渐增大。 而我们真正关心的其实是validation set的Loss。 所以想让参数停在validation set中loss最低时。 Keras能够实现EarlyStopping功能[1]：click here 123from keras.callbacks import EarlyStoppingearly_stopping = EarlyStopping(monitor='val_loss', patience=2)model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) RegularizationRegularization：Find a set of weight not only minimizing original cost but also close to zero. 构造一个新的loss function，除了最小化原来的loss function，还能使得参数趋紧0，使得function更平滑。 function的曲线更平滑，当输入有轻微扰动，不会太影响输出的结果。 L2 norm regularizationNew loss function: $$ \\begin{equation} \\begin{aligned} \\mathrm{L}^{\\prime}(\\theta)&=L(\\theta)+\\lambda \\frac{1}{2}\\|\\theta\\|_{2} \\\\ \\theta &={w_1,w_2,...} \\\\ \\|\\theta\\|_2&=(w1)^2+(w_2)^2+... \\end{aligned} \\end{equation} $$ 其中用第二范式 $\\lambda\\frac{1}{2}|\\theta|_2$ 作为regularization term。做regularization是为了使函数更平滑，所以一般不考虑bias) New gradient: $$ \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w}=\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda w $$ New update: $$ \\begin{equation} \\begin{aligned} w^{t+1} &\\longrightarrow w^{t}-\\eta \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w} \\\\ &=w^{t}-\\eta\\left(\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda w^{t}\\right) \\\\ &=(1-\\eta \\lambda) w^{t}-\\eta \\frac{\\partial \\mathrm{L}}{\\partial w} \\end{aligned} \\end{equation} $$ 在更新参数时，先乘一个 $(1-\\eta\\lambda)$ ，再更新。 weight decay（权值衰减）：由于 $\\eta,\\lambda$ 都是很小的值，所以 $w^t$ 每次都会先乘一个小于1的数，即逐渐趋于0，实现regularization。但是，因为更新中还有gradient部分，所以不会等于0。 L1 norm regularizationRegularization除了用第二范式，还可以用其他的，比如第一范式 $|\\theta|_1=|w_1|+|w_2|+…$ New loss function: $$ \\begin{equation}\\begin{aligned}\\mathrm{L}^{\\prime}(\\theta)&=L(\\theta)+\\lambda \\frac{1}{2}\\|\\theta\\|_1\\\\ \\theta &={w_1,w_2,...} \\\\ \\|\\theta\\|_1&=|w_1|+|w_2|+...\\end{aligned}\\end{equation} $$ 用sgn()符号函数来表示绝对值的求导。 符号函数：Sgn(number) 如果number 大于0，返回1；等于0，返回0；小于0，返回-1。 New gradient: $$ \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w}=\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda \\text{sgn}(w) $$ New update: $$ \\begin{equation} \\begin{aligned} w^{t+1} &\\longrightarrow w^{t}-\\eta \\frac{\\partial \\mathrm{L}^{\\prime}}{\\partial w} \\\\ &=w^{t}-\\eta\\left(\\frac{\\partial \\mathrm{L}}{\\partial w}+\\lambda \\text{sgn}(w^t)\\right) \\\\ &=w^{t}-\\eta \\frac{\\partial \\mathrm{L}}{\\partial w}-\\eta \\lambda \\operatorname{sgn}\\left(w^{t}\\right) \\end{aligned} \\end{equation} $$ 在用第一范式做regularization时，每次 $w^t$ 都要减一个值 $\\eta\\lambda\\text{sgn}(w^t)$ ，和用第二范式做regularization比较，后者每次都要乘一个小于1的值，即使是乘0.99，w下降也很快。 Weight decay（权值衰减）的生物意义： Our brain prunes（修剪） out the useless link between neurons. DropoutWiki: Dropout是Google提出的一种正则化技术，用以在人工神经网络中对抗过拟合。Dropout有效的原因，是它能够避免在训练数据上产生复杂的相互适应。Dropout这个术语代指在神经网络中丢弃部分神经元（包括隐藏神经元和可见神经元）。在训练阶段，dropout使得每次只有部分网络结构得到更新，因而是一种高效的神经网络模型平均化的方法。[2] 这里讲Dropout怎么做。 Training Each time before updating the parameters: Each neuron has p% to dropout. Using the new thin network for training. 【如上图，每个neuron有p的概率被dropout。于是NN就变成了下图thinner的NN】 For each mini-batch, we resample the dropout neurons. 【每次mini-batch，都要重新dropout，更新NN的结构】 TestingTesting中不做dropout If the dropout rate at training is p%, all the weights times 1-p%. 【如果在training中 dropout rate是 p%，在testing是，每个参数都乘 （1-p%)】 【比如dropout rate 是0.5。如果train出来的w是 1，那么testing中 w=0.5】 Why dropout in training：Intuitive Reason 这是一个比较有趣的比喻： 这也是一个有趣的比喻hhh: 即，团队合作的时候，如果每个人都认为队友在带我，那每个人都可能划水。 但是，（training中）如果你知道你的队友在划水，那你可能会做的更好。 但是，（testing中）发现每个人都有更好地做，都没有划水，那么结果就会很好。 （hhhh，李老师每次讲Intuitive Reason的时候，都觉得好有道理hhh，科学的直觉orz给我也整一个） Why multiply (1-p%) in testing: Intuitive reason为什么在testing中 weights要乘（1-p%)? 用一个具体的例子来直观说明： 上图中，如果dropout rate=0.5，假设只训练一次， $w_2,w_4$ 相连的neuron都被dropout。 在testing中，因为不对neurondropout，所以如果不改变weight，计算出的结果 $z’\\approx 2z$ 。 因此将所有weight简单地和(1-p%) 相乘，能尽量保证计算出的结果 $z’\\approx z$ 。 Dropout is a kind of ensembleEnsemble(合奏)，如下图，将testing data丢给train好的NN来估计，最后的估计值取所有NN输出的平均，如下图： 为什么说dropout is a kind of ensemble? Using one mini-batch to train one network 【dropout相当于每次用一个mini-batch来训练一个network】 Some parameters in the network are shared 【有些参数可能会在很多个mini-batch都被train到】 由于每个神经元有 p%的概率被dropout，因此理论上，如果有M个neuron，可能会训练 $2^M$ 个network。 但是在Ensemble中，将每个network存下来，testing的时候输出取平均，这样的过程太复杂了，结果也不一定会很好。 所以在testing中，no dropout，对原始network中的每个参数乘 (1-p%)，用这样简单的操作来达到ensemble的目的。 Reference Keras: how can i interrupt training when the validation loss isn’t decresing anymore. Dropout-wiki：https://zh.wikipedia.org/wiki/Dropout","link":"/2020/04/21/tips-for-DL/"},{"title":"「机器学习-李宏毅」:Unsupervised-PCA","text":"这篇文章详细讲解了无监督学习（Unsupervised learning）的PCA（主成分分析法）。 文章开篇从聚类（Clustering）引出Distributed Represention，其中粗略阐述了聚类中K-means和HAC（层次聚类）的思想。 文章的后半部分具体阐述了PCA的数学细节，PCA的去相关性性质，PCA的另一种解释角度（component的角度），PCA的不足等。 Unsupervised Learning无监督学习分为两种： Dimension Reduction：化繁为简。 function 只有input，能将高维、复杂的输入，抽象为低维的输出。 如下图，能将3D的折叠图像，抽象为一个2D的表示（把他摊开）。 Generation：无中生有。 function 只有output。 （后面的博客会提及） Dimension Reduction此前，在semi-supervised learning的最后，提及过better presentation的思想，Dimension Reduction 其实就是这样的思想：去芜存菁，化繁为简。 比如，在MNIST中，一个数字的表示是28*28维度的向量（图如左），但大多28 *28维度的向量（图为右）都不是数字。 因此，在表达下图一众“3”的图像中，根本不需要28*28维的向量表示，1-D即可表示一张图（图片的旋转角度）。28 * 28的图像表示就像左边中老者的头发，1-D的表示就像老者的头，是对头发运动轨迹一种更简单的表达。 Clustering在将Dimension Reduction之前，先将一种经典的无监督学习——clustering. clustering也是一种降维的表达，将复杂的向量空间抽象为简单的类别，用某一个类别来表示该数据点。 这里主要讲述cluster的主要思想，算法细节可参考其他资料 。（待补充） K-meansK-means的做法是： Clustering $X=\\left\\{x^{1}, \\cdots, x^{n}, \\cdots, x^{N}\\right\\}$ into K clusters. 把所有data分为K个类，K的确定是empirical的，需要自己确定 Initialize cluster center $c^i, i=1,2,…,K$ .(K random $x^n$ from $X$) 初始化K个类的中心数据点，建议从training set $X$ 中随机选K 个点作为初始点。 不建议直接在向量空间中随机初始化K个中心点，因为很可能随机的中心点不属于任何一个cluster。 Repeat：根据中心点标记所属类，再更新新的中心点，重复直收敛。 For all $x^n$ in $X$ : 标记所属类。 $$ b_{i}^{n}\\left\\{\\begin{array}{ll}1 & x^{n} \\text { is most \"close\" to } c^{i} \\\\ 0 & \\text { Otherwise }\\end{array}\\right. $$ Updating all $c^i$ : $c^{i}=\\sum_{x^{n}} b_{i}^{n} x^{n} / \\sum_{x^{n}} b_{i}^{n}$ (计算该类中心点) HAC：Hierarchical Agglomerative Clustering(HAC)另一种clustering的方法是层次聚类（Hierarchical Clustering），这里介绍Agglomerative（自下而上）的策略。 Build a tree. 如上图中，计算当前两两数据点（点或组合）的相似度（欧几里得距离或其他）。 选出最相近的两个合为一组（即连接在同一父子结点上，如最左边的两个） 重复1-2直至最后合为root。 该树中，越早分支的点集合，说明越不像。 Pick a threshold. 选一个阈值，即从哪个地方开始划开，比如选上图中红色的线作为阈值，那么点集分为两个cluseter，蓝色、绿色同理。 HAC和K-means相比，HAC不直接决定cluster的数目，而是通过决定threshold的值间接决定cluster的数目。 Distributed RepresentationCluster：an object must belong to one cluster. 在做聚类时，一个数据点必须标注为某一具体类别。这往往会丢失很多信息，比如一个人可能是70%的外向，30%的内敛，如果做clustering，就将这个人直接归为外向，这样的表示过于粗糙。 因此仍用vector来表示这个人，如下图。 Distributed Representation（也叫Dimension Reduction）就是：一个高维的vector通过function，得到一个低维的vector。 Distributed的方法有常见的两种： Feature selection： 如下图数据点的分布，可以直接选择feature $x_2$ . 但这种方法往往只能处理2-D的情况，对于下图这种3-D情况往往不好做特征选择。 Principle component analysis（PCA） 另一种方法就是著名的PCA，主成分分析法。 PCA中，这个function就是一个简单的linear function（$W$），通过 $z=Wx$ ，将高维的 $x$ 转化为低维的 $z$ . PCA：Principle Component AnalysisPCA的参考资料见Bishop, Chapter12. PCA就是要找 $z=Wx$ 中的 $W$ . Main IdeaReduce 1-D如果将dimension reduce to 1-D，那么可以得出 $z_1 = w^1\\cdot x$ . $w^1$ 是vector，$x$ 是vector，做内积。 如下图，内积即投影，将所有的点 $x$ 投影到 $w^1$ 方向上，然后得到对应的 $z_1$ 值。 而对于得到的一系列 $z_1$ 值，我们希望 $z_1$ 的variance越大越好。 因为 $z_1$ 的分布越大，用 $z_1$ 来刻画数据，才能更好的区分数据点。 如下图，如果 $w^1$ 的方向是small variance的方向，那么这些点会集中在一起，而large variance方向，$z_1$ 能更好的刻画数据。 $z_1$ 的数学表达是： $ \\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2} \\quad \\left\\|w^{1}\\right\\|_{2}=1$ (后文解释为什么要 $w^1$ 的长度为1) Reduce 2-D同理，如果将dimension reduce to 2-D . $z=Wx$ 即 $$ \\left\\{ \\begin{array}{11}z_1=w^1\\cdot x \\\\ z_2=w^2 \\cdot x \\end{array} \\right. ,\\quad W=\\begin{bmatrix}(w^1)^T \\\\ (w^2)^T \\end{bmatrix} $$ 将所有点 $x$ 投影到 $w^1$ 方向，得到对应的 $z_1$ ，且让 $z_1$ 的分布尽可能的大： $$ \\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2} ,\\quad \\left\\|w^{1}\\right\\|_{2}=1 $$ 将所有点投影到 $w^2$ 方向，得到对应的 $z_2$ ，同样让 $z_2$ 的分布也尽可能大，再加一个约束条件，让 $w^2$ 和 $w^1$ 正交（后文会具体解释为什么） $$ \\operatorname{Var}\\left(z_{2}\\right)=\\frac{1}{N} \\sum_{z_{2}}\\left(z_{2}-\\overline{z_{2}}\\right)^{2} ,\\quad \\left\\|w^{2}\\right\\|_{2}=1 ,\\quad w^1\\cdot w^2=0 $$ 因此矩阵 $W$ 是Orthogonal matrix (正交矩阵)。 Detail[Warning of Math想跳过math部分的，可以直接看Conclusion。 1-D中： Goal：find $w^1$ to maximum $(w^1)^T S w^1$ s.t.$(w^1)^Tw^1=1$ 结论：$w^1$ 就是协方差矩阵 $S$ 最大特征值 $\\lambda_1 $ 对应的特征向量。 s.t.$(w^1)^Tw^1=1$ 2-D中： Goal：find $w^2$ to maximum $(w^2)^TSw^2 $ s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ 结论：$w^2$ 就是协方差矩阵$S$ 第二大特征值 $\\lambda_2 $ 对应的特征向量。 s.t.$(w^2)^Tw^2=1$ k-D中： 结论：$w$ 就是协方差矩阵 $S$ 前 $k$ 大的特征值对应的特征向量。s.t. $W$ 是正交矩阵。 1-DGoal：Find $w^1$ to maximum the variance of $z_1$ . $\\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2}$ $z_1=w^1\\cdot x ,\\quad \\overline{z_{1}}=\\frac{1}{N} \\sum_{z_{1}}=\\frac{1}{N} \\sum w^{1} \\cdot x=w^{1} \\cdot \\frac{1}{N} \\sum x=w^{1} \\cdot \\bar{x}$ $\\operatorname{Var}\\left(z_{1}\\right)=\\frac{1}{N} \\sum_{z_{1}}\\left(z_{1}-\\overline{z_{1}}\\right)^{2}=(w^1)^T\\operatorname{Cov}(x)w^1$ $=\\frac{1}{N} \\sum_{x}\\left(w^{1} \\cdot x-w^{1} \\cdot \\bar{x}\\right)^{2} $ $=\\frac{1}{N} \\sum\\left(w^{1} \\cdot(x-\\bar{x})\\right)^{2}$ $a,b$ 是vector： $(a\\cdot b)^2=(a^Tb)^2=a^Tba^Tb$ $a^Tb$ 是scalar: $(a\\cdot b)^2 = (a^Tb)^2=a^Tba^Tb =a^Tb(a^Tb)^T=a^Tbb^Ta$ $=\\frac{1}{N} \\sum\\left(w^{1}\\right)^{T}(x-\\bar{x})(x-\\bar{x})^{T} w^{1}$ $ = \\left(w^{1}\\right)^{T}\\sum\\frac{1}{N}(x-\\bar{x})(x-\\bar{x})^{T} \\ w^{1}$ $=(w^1)^T\\operatorname{Cov}(x)w^1$ 令 $S=\\operatorname{Cov}(x)$ 之前遗留的两个问题： $\\left|w^1\\right|_2=1$ ? $w^1\\cdot w^2=1$ ? 现在来看第一个问题，为什么要 $\\left|w^1\\right|_2=1$ ？ 现在的目标，变成了 maximum $(w^1)^T S w^1$ ，如果不限制 $\\left|w^1\\right|_2$ ，让 $\\left|w^1\\right|_2$ 无穷大，那么 $(w^1)^T S w^1$ 的值也会无穷大，问题无解了。 Goal：maximum $(w^1)^T S w^1$ s.t. $(w^1)^Tw^1=1$ Lagrange multiplier[挖坑] 求解多元变量在有限制条件下的驻点。 构造拉格朗日函数： $g\\left(w^{1}\\right)=\\left(w^{1}\\right)^{T} S w^{1}-\\alpha\\left(\\left(w^{1}\\right)^{T} w^{1}-1\\right)$ ，$\\alpha\\neq 0$ 为拉格朗日乘数 $\\nabla_{w^1}g=0$ 的值为驻点（会单独写一篇博客来讲拉格朗日乘数） $\\frac{\\partial g}{\\partial \\alpha}=0$ 为限制函数 对矩阵微分：详情见wiki scalar-by-vector(scalar对vector微分) $S$ 是对称矩阵，不是 $w^1$ 的函数，结果用 $w^1$ 表达：$2Sw^1-2\\alpha w^1=0$ maximum: $(w^1)^T S w^1=\\alpha (w^1)^Tw^1=\\alpha$ *Goal：find $w^1$to maximum $\\alpha$ * $\\alpha$ 满足等式：$Sw^1=\\alpha w^1$ $\\alpha$ 是 $S$ 的特征向量，$w^1$ 是 $S$ 对应于特征值 $\\alpha$ 的特征向量。 关于特征值和特征向量的知识参考：参考下面线代知识 $w^1$ is the eigenvector(特征向量) of the covarivance matrix S corresponding to the largest eigenvalue $\\lambda_1$ . 结论：$w^1$ 就是协方差矩阵最大特征值对应的特征向量。 梦回线代QWQ（自己线代学的太差啦 啊这！ 特征向量，特征值定义： $A$ 是n阶方阵，如果存在数 $\\lambda$ 和n维非零向量 $\\alpha$ ，满足 $A\\alpha=\\lambda \\alpha$ , 则称 $\\lambda$ 为方阵 $A$ 的一个特征值，$\\alpha$ 为方阵 $A$ 对应于特征值 $\\lambda$ 的一个特征向量。 求解特征向量和特征值： $A\\alpha -\\lambda \\alpha=(A-\\lambda I)\\alpha=0$ 齐次方程有非零解的充要条件是特征方程 $det(A-\\lambda I)=0$ （行列式为0） 根据特征方程先求解出 $\\lambda$ 的所有值。 再根将 $\\lambda$ 代入齐次方程，求解齐次方程的解 $\\alpha$ ，即为对应 $\\lambda$ 的特征向量。 2-DGoal：find $w^2$ to maximum $(w^2)^TSw^2 $ s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ 构造拉格朗日函数： $g\\left(w^{2}\\right)=\\left(w^{2}\\right)^{T} S w^{2}-\\alpha\\left(\\left(w^{2}\\right)^{T} w^{2}-1\\right)-\\beta\\left(\\left(w^{2}\\right)^{T} w^{1}-0\\right)$ 对 $w^2$ 求微分，所求点满足等式： $S w^{2}-\\alpha w^{2}-\\beta w^{1}=0$ 左乘 $(w^1) ^T$： $(w^1)^TSw^2-\\alpha (w^1)^Tw^2-\\beta(w^1)^Tw^1=0$ 已有： $(w^1)^Tw^2=0, (w^1)^Tw^1=1$ 证明：$ (w^1)^TSw^2=0$ $\\because (w^1)^TSw^2$ 是scalar $\\therefore (w^1)^TSw^2=((w^1)^TSw^2)^T=(w^2)^TS^Tw^1$ $\\because S^T=S$ (协方差矩阵是对称矩阵) $\\because Sw^1=\\lambda_1 w^1$ $\\therefore (w^1)^TSw^2=(w^2)^TSw^1=\\lambda_1(w^2)^Tw^1=0$ $(w^1)^TSw^2-\\alpha (w^1)^Tw^2-\\beta(w^1)^Tw^1=0-\\alpha\\cdot 0-\\beta \\cdot 1=0$ $\\therefore \\beta=0$ $w^2$ 满足等式：$S w^{2}-\\alpha w^{2}=0$ 和1-D的情况相同：find $w^2$ maximum $(w^2)^TSw^2$ $(w^2)^TSw^2=\\alpha$ $w^2$ is the eigenvector(特征向量) of the covarivance matrix S corresponding to the largest eigenvalue $\\lambda_2$ . OVER! Conclusion最后解决之前的Q2：$(w^1)^Tw^2=0$ ? 先说明一下$S$ 的性质： 是对称矩阵，对应不同特征值对应的特征向量都是正交的。 （参考1，2） 也是半正定矩阵，其特征值都是非负的。 （参考4，5，6） 其次关于 $W$ 的性质 $ W=\\begin{bmatrix}(w^1)^T \\\\ (w^2)^T \\\\ ...\\end{bmatrix}$ ,易得 $W$ 是orthogonal matrix(正交矩阵)。 所以这是一个约束条件，能让PCA的最优化问题转化为求其特征值的问题。 （具体见下一小节：PCA-decorrelation） 其次 $z=Wx$ ，也因为 $W$ 的正交性质，让 $z$ 的各维度（特征）decorrelation，去掉相关性，降维后的特征相互独立，方便后面generative model的假设。 $S=Cov(x)$ 为实对称矩阵。 实对称矩阵的性质：$A$ 是一个实对称矩阵，对于于 $A$ 的不同特征值的特征向量彼此正交。 正交矩阵的性质：$W^TW=WW^T=I$ $Var(z)=(w^1)^T S w^1\\geq 0$ ，方差一定大于等于0 。 半正定矩阵的定义： 实对称矩阵 $A$ ，对任意非零实向量 $X$ ，如果二次型 $f(X)=X^TAX\\geq0$ ， 则有实对称矩阵 $A$ 是半正定矩阵。 半正定矩阵的性质：半正定矩阵的特征值都是非负的。 1-D中： Goal：find $w^1$ to maximum $(w^1)^T S w^1$ s.t.$(w^1)^Tw^1=1$ 结论：$w^1$ 就是协方差矩阵 $S$ 最大特征值 $\\lambda_1 $ 对应的特征向量。 s.t.$(w^1)^Tw^1=1$ 2-D中： Goal：find $w^2$ to maximum $(w^2)^TSw^2 $ s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ 结论：$w^2$ 就是协方差矩阵$S$ 第二大特征值 $\\lambda_2 $ 对应的特征向量。 s.t.$(w^2)^Tw^2=1$ k-D中： 结论：$w$ 就是协方差矩阵 $S$ 前 $k$ 大的特征值对应的特征向量。s.t. $W$ 是正交矩阵。 PCA-decorrelation$z=Wx$ 通过PCA找到的 $W$ ，$x$ 得到新的presentation $z$ ，如下图。 可见，经过PCA后，original data变为decorrelated data，各维度（feature）是去相关性的，即各维度是独立的，方便generative model的假设（比如Gaussian distribution). $z$ 是docorrelated，即 $Cov(z)=D$ 是diagonal matrix(对角矩阵) 证明：$Cov(z)=D$ is diagonal matrix $W=\\begin{bmatrix}(w^1)^T \\\\ (w^2)^T \\\\ ...\\end{bmatrix}$ ，$S=\\operatorname{Cov}(x)$ $\\operatorname{Cov}(z)=\\frac{1}{N} \\sum(z-\\bar{z})(z-\\bar{z})^{T}=W S W^{T}$ $=W S\\left[\\begin{array}{lll}w^{1} & \\cdots & w^{K}\\end{array}\\right]=W\\left[\\begin{array}{lll}S{w}^{1} & \\cdots & S w^{K}\\end{array}\\right]$ $=W\\left[\\lambda_{1} w^{1} \\quad \\cdots \\quad \\lambda_{K} w^{K}\\right]=\\left[\\lambda_{1} W w^{1} \\quad \\cdots \\quad \\lambda_{K} W w^{K}\\right]$ ($\\lambda$ is scalar) $=\\left[\\begin{array}{lll}\\lambda_{1} e_{1} & \\cdots & \\lambda_{K} e_{K}\\end{array}\\right]=D$ ($W$ is orthogonal matrix) PCA-Another Point of ViewMain Idea: ComponentPCA看作是一些basic component的组成，如下图，手写数字都是一些基本笔画组成的，记做 $\\{u^1,u^2,u^3,...\\}$ 因此，下图的”7”的组成为 $\\{u^1,u^3,u^5\\}$ 所以原28*28 vector $x$ 表示的图像能近似表示为： $$ x \\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}+\\bar{x} $$ 其中 $\\{u^1,u^2,u^3,...\\}$ 是compoment的vector表示， $\\{c^1,c^2,c^3,...\\}$ 是component的系数，$\\bar{x}$ 是所有images的平均值。 因此 $\\begin{bmatrix}c_1 \\\\c_2 \\\\... \\\\ c_k \\end{bmatrix}$ 也能表示一个数字图像。 现在问题是找到这些component $\\{u^1,u^2,u^3,...\\}$ , 再得到 他的线形表出 $\\begin{bmatrix}c_1 \\\\c_2 \\\\... \\\\ c_k \\end{bmatrix}$ 就是我们想得到的better presentation. Detail要满足：$x \\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}+\\bar{x}$ 即，$x -\\bar{x}\\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}$ ，等式两边的误差要尽量小。 问题变成：找 $\\{u^1,u^2,u^3,...\\}$ minimize the reconstruction error = $\\|(x-\\bar{x})-\\hat{x}\\|_2$ . 损失函数： $L=\\min _{\\left\\{u^{1}, \\ldots, u^{K}\\right\\}} \\sum\\left\\|(x-\\bar{x})-\\left(\\sum_{k=1}^{K} c_{k} u^{k}\\right)\\right\\|_{2}$ 而求解PCA的过程就是在minimize损失函数 $L$ ，PCA中求解出的 $\\{w^1,w^2,...,w^K\\}$ 就是这里的component $\\{u^1,u^2,...,u^K\\}$ .(Proof 见Bisho, Chapter 12.1.2) *Goal: minimize the reconstruction error = $\\|(x-\\bar{x})-\\hat{x}\\|_2$ * $x -\\bar{x}\\approx c_{1} u^{1}+c_{2} u^{2}+\\cdots+c_{K} u^{K}$ 每个sample: $\\left\\{ \\begin{matrix} x^{1}-\\bar{x} \\approx c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\\cdots \\\\ x^{2}-\\bar{x} \\approx c_{1}^{2} u^{1}+c_{2}^{2} u^{2}+\\cdots \\\\x^{3}-\\bar{x} \\approx c_{1}^{3} u^{1}+c_{2}^{3} u^{2}+\\cdots \\\\ ...\\end{matrix} \\right.$ 下图中 $X=x-\\bar{x}$ 矩阵的第一列都和上面的 $x^1-\\bar{x}$ 对应： 而上面的 $c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\\cdots$ 和下图的component矩阵乘系数矩阵的第一列对应： 因此，是要让下图矩阵的结果 minimize error： 如何求解: SVD矩阵分解-其实就是最大近似分解（挖坑） SVD能将一个任意的矩阵，分解为下面三个矩阵的乘积。 $X = U\\Sigma V$ $U,V$ 都是orthogonal matrix，$\\Sigma$ 是diagonal matrix。 组成$U$ (M*K) 的K个列向量是 $XX^T$ 矩阵的前K大特征值对应的特征向量。 组成 $V$ (K*N)的K个行向量是 $X^TX$ 矩阵的前K大特征值对应的特征向量。 $XX^T$ 和 $X^TX$ 的特征值相同 $\\Sigma$ 的对角值 $\\sigma_i=\\sqrt{\\lambda_i}$ 解：$U$ 矩阵作为 component矩阵， $\\Sigma V$ 乘在一起作为系数矩阵。 $U=\\{u^1,u^2,u^3,...\\}$ 矩阵是$XX^T$ 的特征向量组成正交矩阵。 而PCA的解 $W^T=\\{w^1,w^2,...,w^K\\}$ 也是特征向量组成的正交矩阵。 所以和PCA的关系：$U$ 矩阵是 $XX^T=Cov(x)$ 的特征向量，所以$U$ 矩阵就是PCA的解。 PCA-NN：Autoencoder上文说到求解PCA的解 $\\{w^1,w^2,...,w^K\\}$ 就是在最小化restruction error $x -\\bar{x}\\approx \\sum_{k=1}^K c_kw^k$ . 两者的联系就是PCA的解 $\\{w^1,w^2,...,w^K\\}$ 就是component $\\{u^1,u^2,u^3,...\\}$ ,且PCA的表示是 $z$ 对应这里的 $c_k$ (第k个image的表示）. PCA视角： $z=c_k=(x-\\bar{x})\\cdot w^k$ PCA looks like a neural network with one hidden layer(linear activation function)。 把PCA视角看作一个NN，如下图，其hidden layer的激活函数是一个简单的线性激活函数。 再看component视角： $\\hat{x}=\\sum_{k=1}^K c_kw^k\\approx x-\\bar{x}$ PCA就构成了下面的NN，hidden layer可以是deep，这就是autoencoder(后面的博客会再详细讲)。 用Gradient Descent对输入输出做minimize error，hidden layer的输出 $c$ 就是我们想要的编码（降维后的编码）。 Q：用PCA求出的结果和用Gradient Descent训练NN的结果一样吗？ A：当然不一样，PCA的 $w$ 都是正交的，而NN的结果是gradient descent迭代出来的，并且该结果还会于初值有关。 Q：有了PCA，为什么还要用NN呢？ A：因为PCA只能处理linear的情况，对前文那种高维的非线形的无法处理，而NN可以是deep的，能较好处理非线形的情况。 tips: how many components?比如在对Pokemon进行PCA时，有六个features，如何确定principle component的数目？ 往往在实际操作中，会对每个component计算一个ratio，如图中的公式： 因为每一个component对应一个eigenvector，每个eigenvector对应一个eigenvalue，而这个eigenvalue的值代表了在这个component的维度的variance有多大，越大当然能更好的表示。 因此计算eigenvalue的ratio，来找出分布较大的component作为主成分。 More About PCA如果对MNIST做PCA分析，结果如下图，会发现下面eigen-digits这些并不像数字的某个组成部分： 同样，对face做PCA分析，结果下图： 为什么呢？ 在MNIST中，一张image的表示如下图： 其中，$\\alpha$ 可以是任意实数，那么就有正有负，所以PCA的解包含了一些真正component的adding up and subtracting，所以MNIST的解不像这些数字的一部分。 如果想得到的解看起来像真正的component，可以规定图像只能是加，即 $\\alpha$ 都是非负的。 Non-negative matrix factorization(NMF) Forcing $\\alpha$ be non-negative: additive combination Forcing $w$ be non-negative: components more like “parts of digits” Weakness of PCA PCA是unsupervised，因此可能不能区分本来是两个类别的东西。 如图，PCA的结果可能是上图的维度方向，但如果引入labeled data，更好的表达应该按照下图LDA的维度方向。 LDA (Linear Discriminant Analysis) 是一种supervised的分析方法。 PCA是Linear的，前文已经提及过，除了可以用NN的方式也有很多其他的non-linear的解法。 Reference HAC的算法细节待补充完善：https://zhuanlan.zhihu.com/p/34168766 PCA: Bishop, Chapter12. 线代知识：特征值、特征向量、实对称矩阵等： 拉格朗日乘数：Bishop, Appendix E 矩阵微分：https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors Proof-PCA的过程就是在minimize损失函数 $L$ :Bisho, Chapter 12.1.2 SVD： https://www.cnblogs.com/pinard/p/6251584.html https://www.youtube.com/watch?v=rYz83XPxiZo NMF：Non-negative matrix factorization LDA：Linear Discriminant Analysis","link":"/2020/10/31/unsupervised-learning-pca/"},{"title":"「机器学习-李宏毅」:Unsupervised Learning：Word Embedding","text":"这篇文章主要是介绍一种无监督学习——Word Embedding（词嵌入）。 文章开篇介绍了word编码的1-of-N encoding方式和word class方式，但这两种方式得到的单词向量表示都不能很好表达单词的语义和单词之间的语义联系。 Word Embedding可以很好的解决这个问题。 Word Embedding有count based和prediction based两种方法。文章主要介绍了prediction based的方法，包括如何predict the word vector? 为什么这样的模型works？介绍了prediction based的变体；详细阐述了该模型中sharing parameters的做法和其必要性。 文章最后简单列举了word embedding的相关应用，包括multi-lingual embedding, multi-domain embedding, document embedding 等。 Word to Vector如何把word转换为vector? 1-of-N Encoding第一种方法是1-of-N Encoding： Vector的维度是单词总数，每一维度都代表一个单词。 1-of-N Encoding的方法简单，但这种向量的表示方式not imformative，即向量表示不能体现单词之间的语义关系。 Word Class对1-of-N Encoding方式改进，Word Class采用聚类cluster的方式，根据类别训练一个分类器。 但这种人为分类的方式，信息是会部分丢失的，即光做clustering是不够的，会丢失单词的部分信息。 Word Embedding第三种方式是Word Embedding。（词嵌入） Word Embedding: Machine learns the meaning of words from reading a lot of documents without supervision. Word Embedding，机器通过阅读大量文章学习单词的含义，用vector的形式表示单词的语义。训练时只需要给机器大量文章，不需要label，因此是无监督学习。 Word Embedding如何做Word Embedding呢？ auto-encoder？能否用auto-encoder的方式来做词嵌入呢？ 即用1-of-N encoding的方式对单词编码，作为训练的输入和输出。 word2vec时，把model中的某一hidden layer的输出作为该单词的向量表示。 这种方式是不可以的，不可以用auto-encoder。因为auto-encoder不能学到informative的信息，即用auto-encoder表示的向量不能表达word的语义。 Exploit the ContextA word can be understood by its context. 所以Word Embedding可以利用上下文来学习word的语义。 如何利用单词的上下文来学习呢？ Count based 如果两个单词 $w_i$ 和 $w_j$ 在文章中经常同时出现，那么 $V(w_i)$ ( $w_i$ 的向量表示)和 $V(w_j)$ 的向量表示会很close. E.g. Glove Vector: https://nlp.stanford.edu/projects/glove/ GloVe的表示法有两个亮点： Nearest neighbors：vectors之间的欧几里得距离（或者余弦相似度）能较好表示words之间的语义相似度。 Linear substructures：用GloVe方法表示的vectors有有趣的线性子结构。 Prediction based 使用预测的方式来表示。 Prediction basedHow to predict？prediction based的方法是用前一个单词来预测当前单词。 训练时： $w_{i-1}$ 的1-of-N encoding编码作为输入，$w_i$ 的1-of-N encoding的编码作为输出。 NN如上图，$w_{i-1}$ 的1-of-N encoding编码作为输入，输出的vector表示下一个单词是 $w_i$ 的概率。 word2vec : $w_{i-1}$ 的1-of-N encoding编码作为NN的输入，$w_i$ 的向量表示为第一个hidden layer的neurons的输入 $z$ 。 Why it works?直觉的解释他为什么能work。 如上图，蔡英文 宣誓就职 和 马英九 宣誓就职，虽然 $w_{i-1}$ 不同，但NN的输出中，“宣誓就职”的概率应该最大。 即hidden layers必须把不同的 $w_{i-1}$ project到相同的space，要求hidden layer的input是相近的，NN的输出才是相近的。 Prediction-based ：Various Architecture因为一个单词的下一个单词范围非常大，所以使用前一个单词预测当前单词的方法，performance是较差的。 因此常常会使用多个单词来预测下一个单词，NN的输入是多个单词连接在一起组成的向量，一般NN的输入至少为10个单词，word embedding的performance较好。 除了使用多个单词的方法，prediction-based的方法还用两种变体结构。 Continuous bag of word (CBOW) model: predicting the word given its context. 使用单词的前后文（前一个单词和后一个单词）来预测当前单词。 Skip-gram: predicting the context given a word. 使用中间单词来预测单词的前一个单词和后一个单词。 Sharing Parameters使用多个单词作为NN的输入，提高了word embedding的performance，但也大幅增加了模型训练的参数数量。 使用sharing parameters（共享参数）能大量减少模型的参数数量。 如上图，输入单词连接到neurons的权重应该是相同的。 除了能减少参数，sharing parameters也是必要的。否则，如果NN的输入的单词顺序交换，那么得到的单词向量是不同的。 How to train sharing parameters? 假设两个单词相同维度连接到neuron的weight是 $w_i,w_j$ ，在训练中，如何让 $w_i=w_j$ ? Given the same initialization.(相同的初始化) 原来的参数更新：$$w_i \\longleftarrow w_i - \\frac{\\partial C}{\\partial w_i} \\w_j \\longleftarrow w_j - \\frac{\\partial C}{\\partial w_j}$$虽然有相同的初始化，但在Backpropagation求偏微分时，$\\frac{\\partial C}{\\partial w_i}$ 和 $\\frac{\\partial C}{\\partial w_j}$ 不一样，那么参数 $w_i$ 和 $w_j$ 更新一次后就不同了。 在训练sharing parameters的参数更新：$$w_i \\longleftarrow w_i - \\frac{\\partial C}{\\partial w_i} -\\frac{\\partial C}{\\partial w_j}\\w_j \\longleftarrow w_j - \\frac{\\partial C}{\\partial w_j}-\\frac{\\partial C}{\\partial w_i}$$这样更新后，$w_i$ 和 $w_j$ 仍保持一致。如果有多个单词，亦然。 Word2Vec 在word2vec时，根据sharing parameters的性质，计算单词的向量表示时，可以简化运算。 如上图，用前文单词 $x_{i-1},x_{i-2}$ 表示单词 $x_i$ 的向量表示 $z=W_1x_{i-2}+W_2x_{i-1}=W(x_{i-2}+x_{i-1})$ . 其中 $x_{i-1},x_{i-2}$ 的维度是|V|，$x_i$ 的向量表示 $z$ 的维度是 |Z|，$W_1=W_2=W$ 的维度为|Z|*|V|。 Advantages of Word EmbeddingWord Embedding能得到一些有趣的特性。 向量之间有趣的线性子结构 相近的向量有相近的语义 向量之间表示的语义特性 其他应用Multi-lingual Embedding：实现翻译 不同语言之间分开训练，训练出的不同语言所对应词汇的向量表示肯定不同，再将对应词汇的向量project到同一点，即实现了翻译。 Multi-domain Embedding还可以做影像嵌入。 Document Embedding：将文件表示为一个向量 Bag of Word: 用Bag-of-word的方式编码文件，再实现semantic embedding。得到的文件表示向量可以表示文件的语义主题。 Beyond Bag of Word: 句子中单词的顺序也很大程度影响句子的语义。 因此，下图的两句话有相同的bag-of-word，但表达的含义完全相反。 关于beyond bag of word的相关工作参考reference 2. Reference GloVe: Global Vectors for Word Representation beyond bag of word:","link":"/2020/10/11/unsupervised-learning-word-embedding/"},{"title":"「机器学习-李宏毅」：HW2-Binary Income Predicting","text":"这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。包括对数据集的处理，训练模型，可视化，预测等。有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub Task introduction and Dataset Kaggle competition: link Task: Binary Classification Predict whether the income of an individual exceeds $50000 or not ? *Dataset: * Census-Income (KDD) Dataset (Remove unnecessary attributes and balance the ratio between positively and negatively labeled data) Feature Format train.csv, test_no_label.csv【都是没有处理过的数据，可作为数据参考和优化参考】 text-based raw data unnecessary attributes removed, positive/negative ratio balanced. X_train, Y_train, X_test【已经处理过的数据，可以直接使用】 discrete features in train.csv =&gt; one-hot encoding in X_train (education, martial state…) continuous features in train.csv =&gt; remain the same in X_train (age, capital losses…). X_train, X_test : each row contains one 510-dim feature represents a sample. Y_train: label = 0 means “&lt;= 50K” 、 label = 1 means “ &gt;50K ” 注：数据集超大，用notepad查看比较舒服；调试时，也可以先调试小一点的数据集。 Logistic RegressionLogistic Regression 原理部分见这篇博客。 Prepare data本文直接使用X_train Y_train X_test 已经处理好的数据集。 1234567891011121314# prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_Train'X_test_fpath = './data/X_test'output_fpath = './logistic_output/output_logistic.csv'fpath = './logistic_output/logistic'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:] 统计一下数据集： 1234567891011train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In logistic model:\\n') f.write('Size of Training set: {}\\n'.format(train_size)) f.write('Size of development set: {}\\n'.format(dev_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n'.format(data_dim)) 结果如下： 12345In logistic model:Size of Training set: 48830Size of development set: 5426Size of test set: 27622Dimension of data: 510 normalizenormalize data. 对于train data，计算出每个feature的mean和std，保存下来用来normalize test data。 代码如下： 123456789101112131415def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X. # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std # Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std) Development set split在logistic regression中使用的gradient，没有closed-form解，所以在train set中划出一部分作为development set 优化参数。 12345678def _train_dev_split(X, Y, dev_ratio=0.25): # This function splits data into training set and development set. train_size = int(X.shape[0] * (1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]# Split data into train data and development datadev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio) Useful function_shuffle(X, Y)本文使用mini-batch gradient。 所以在每次epoch时，以相同顺序同时打乱X_train,Y_train数组，再mini-batch。 1234567np.random.seed(0)def _shuffle(X, Y): # This function shuffles two two list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize] _sigmod(z)计算 $\\frac{1}{1+e^{-z}}$ ，注意：防止溢出，给函数返回值规定上界和下界。 1234def _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8)) _f(X, w, b)是sigmod函数的输入，linear part。 输入： X：shape = [size, data_dimension] w：weight vector, shape = [data_dimension, 1] b: bias, scalar 输出： 属于Class 1的概率（Label=0，即收入小于$50k的概率） 123456789def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b) _predict(X, w, b)预测Label=0？（0或者1，不是概率） 123def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int) _accuracy(Y_pred, Y_label)计算预测出的结果（0或者1）和真实结果的正确率。 这里使用 $1-\\overline{error}$ 来表示正确率。 12345def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc _cross_entropy_loss(y_pred, Y_label)计算预测出的概率（是sigmod的函数输出）和真实结果的交叉熵。 计算公式为： $\\sum_n {C(y_{pred},Y_{label})}=-\\sum[Y_{label}\\ln{y_{pred}}+(1-Y_{label})\\ln(1-{y_{pred}})]$ 12345678def _cross_entropy_loss(y_pred, Y_label): # This function calculates the cross entropy of Y_pred and Y_label # # Argument: # y_pred: predictions, float vector # Y_label: truth labels, bool vector cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((1 - Y_label).T, np.log(1 - y_pred)) return cross_entropy[0][0] _gradient(X, Y_label, w, b)和Regression的最小二乘一样。（严谨的说，最多一个系数不同） 12345678def _gradient(X, Y_label, w, b): # This function calculates the gradient of cross entropy # X, Y_label, shape = [batch_size, ] y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = - np.dot(X.T, pred_error) b_grad = - np.sum(pred_error) return w_grad, float(b_grad) Training (Adagrad)初始化一些参数。 这里特别注意 : 由于adagrad的参数更新是 $w \\longleftarrow w-\\eta \\frac{gradient}{ \\sqrt{gradsum}}$ . 防止除0，初始化gradsum的值为一个较小值。 123456789101112# training by logistic model# Initial weights and biasw = np.zeros((data_dim, 1))b = np.float(0.)w_grad_sum = np.full((data_dim, 1), 1e-8) # avoid divided by zerosb_grad_sum = np.float(1e-8)# Some parameters for trainingepoch = 20batch_size = 2**3learning_rate = 0.2 AdagradAagrad具体原理见这篇博客的1.2节。 迭代更新时，每次epoch计算一次loss和accuracy，以便可视化更新过程，调整参数。 12345678910111213141516171819202122232425262728293031323334353637# Keep the loss and accuracy history at every epoch for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Iterative trainingfor it in range(epoch): # Random shuffle at every epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for id in range(int(np.floor(train_size / batch_size))): X = X_train[id*batch_size: (id+1)*batch_size] Y = Y_train[id*batch_size: (id+1)*batch_size] # calculate gradient w_grad, b_grad = _gradient(X, Y, w, b) # adagrad gradient update w_grad_sum = w_grad_sum + w_grad**2 b_grad_sum = b_grad_sum + b_grad**2 w_ada = np.sqrt(w_grad_sum) b_ada = np.sqrt(b_grad_sum) w = w - learning_rate * w_grad / np.sqrt(w_grad_sum) b = b - learning_rate * b_grad / np.sqrt(b_grad_sum) # compute loss and accuracy of training set and development set at every epoch y_train_pred = _f(X_train, w, b) Y_train_pred = np.around(y_train_pred) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size) train_acc.append(_accuracy(Y_train_pred, Y_train)) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.around(y_dev_pred) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size) dev_acc.append(_accuracy(y_dev_pred, Y_dev)) Loss &amp; accuracy输出最后一次迭代的loss和accuracy。 结果如下： 1234Training loss: 0.2933570286596322Training accuracy: 0.8839238173254147Development loss: 0.31029505347634456Development accuracy: 0.8336166253549906 画出loss 和 accuracy的更新过程： loss： accuracy： 由于Feature数量较大，将权重影响最大的feature输出看看： 12345678910Other Rel &lt;18 spouse of subfamily RP: [7.11323764] Grandchild &lt;18 ever marr not in subfamily: [6.8321061] Child &lt;18 ever marr RP of subfamily: [6.77322397] Other Rel &lt;18 ever marr RP of subfamily: [6.76688406] Other Rel &lt;18 never married RP of subfamily: [6.37488958] Child &lt;18 spouse of subfamily RP: [5.97717831] United-States: [5.53932651] Grandchild 18+ spouse of subfamily RP: [5.42948497] United-States: [5.41543809] Mexico: [4.79920763] Code完整数据集、代码等，欢迎光临小透明GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222################## Data:2020-04-05# Author: Fred Lau# ML-Lee: HW2 : Binary Classification###########################################################import numpy as npimport csvimport sysimport matplotlib.pyplot as plt########################################################### prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_Train'X_test_fpath = './data/X_test'output_fpath = './logistic_output/output_logistic.csv'fpath = './logistic_output/logistic'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X. # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_stddef _train_dev_split(X, Y, dev_ratio=0.25): # This function splits data into training set and development set. train_size = int(X.shape[0] * (1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)# Split data into train data and development datadev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In logistic model:\\n') f.write('Size of Training set: {}\\n'.format(train_size)) f.write('Size of development set: {}\\n'.format(dev_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n'.format(data_dim))np.random.seed(0)################################################################ useful functiondef _shuffle(X, Y): # This function shuffles two two list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize]def _sigmod(z): # Sigmod function can be used to calculate probability # To avoid overflow return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))def _f(X, w, b): # This is the logistic function, parameterized by w and b # # Arguments: # X: input data, shape = [batch_size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probability of each row of X being positively labeled, shape = [batch_size, 1] return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This fucntion returns a truth value prediction for each row of X by logistic regression return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return accdef _cross_entropy_loss(y_pred, Y_label): # This function calculates the cross entropy of Y_pred and Y_label # # Argument: # y_pred: predictions, float vector # Y_label: truth labels, bool vector cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((1 - Y_label).T, np.log(1 - y_pred)) return cross_entropy[0][0]def _gradient(X, Y_label, w, b): # This function calculates the gradient of cross entropy # X, Y_label, shape = [batch_size, ] y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = - np.dot(X.T, pred_error) b_grad = - np.sum(pred_error) return w_grad, float(b_grad)######################################## training by logistic model# Initial weights and biasw = np.zeros((data_dim, 1))b = np.float(0.)w_grad_sum = np.full((data_dim, 1), 1e-8) # avoid divided by zerosb_grad_sum = np.float(1e-8)# Some parameters for trainingepoch = 20batch_size = 2**3learning_rate = 0.2# Keep the loss and accuracy history at every epoch for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Iterative trainingfor it in range(epoch): # Random shuffle at every epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for id in range(int(np.floor(train_size / batch_size))): X = X_train[id*batch_size: (id+1)*batch_size] Y = Y_train[id*batch_size: (id+1)*batch_size] # calculate gradient w_grad, b_grad = _gradient(X, Y, w, b) # adagrad gradient update w_grad_sum = w_grad_sum + w_grad**2 b_grad_sum = b_grad_sum + b_grad**2 w_ada = np.sqrt(w_grad_sum) b_ada = np.sqrt(b_grad_sum) w = w - learning_rate * w_grad / np.sqrt(w_grad_sum) b = b - learning_rate * b_grad / np.sqrt(b_grad_sum) # compute loss and accuracy of training set and development set at every epoch y_train_pred = _f(X_train, w, b) Y_train_pred = np.around(y_train_pred) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size) train_acc.append(_accuracy(Y_train_pred, Y_train)) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.around(y_dev_pred) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size) dev_acc.append(_accuracy(y_dev_pred, Y_dev))with open(fpath, 'a') as f: f.write('Training loss: {}\\n'.format(train_loss[-1])) f.write('Training accuracy: {}\\n'.format(train_acc[-1])) f.write('Development loss: {}\\n'.format(dev_loss[-1])) f.write('Development accuracy: {}\\n'.format(dev_acc[-1]))#################### Plotting Loss and accuracy curve# Loss curveplt.plot(train_loss, label='train')plt.plot(dev_loss, label='dev')plt.title('Loss')plt.legend()plt.savefig('./logistic_output/loss.png')plt.show()plt.plot(train_acc, label='train')plt.plot(dev_acc, label='dev')plt.title('Accuracy')plt.legend()plt.savefig('./logistic_output/acc.png')plt.show()################################## Predictpredictions = _predict(X_test, w, b)with open(output_fpath, 'w') as f: f.write('id, label\\n') for id, label in enumerate(predictions): f.write('{}, {}\\n'.format(id, label[0]))################################ Output the weights and biasind = (np.argsort(np.abs(w), axis=0)[::-1]).reshape(1, -1)with open(X_test_fpath) as f: content = f.readline().strip('\\n').split(',')content = content[1:]with open(fpath, 'a') as f: for i in ind[0, 0: 10]: f.write('{}: {}\\n'.format(content[i], w[i])) Generative ModelGenerative Model 原理部分见 这篇博客 Prepare data这部分和Logistic regression一样。 只是，因为generative model有closed-form solution，不需要划分development set。 12345678910111213141516171819202122232425262728293031323334353637383940# Prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './generative_output/output_{}.csv'fpath = './generative_output/generative'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)train_size = X_train.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In generative model:\\n') f.write('Size of training data: {}\\n'.format(train_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n\\n'.format(data_dim)) Useful functions12345678910111213141516171819202122232425# Useful functionsdef _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8))def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function computes prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc Training公式再推导计算公式： $$ \\begin{equation}\\begin{aligned}P\\left(C_{1} | x\\right)&=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\\\&=\\frac{1}{1+\\frac{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}}\\\\&=\\frac{1}{1+\\exp (-z)} =\\sigma(z)\\qquad(z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\end{aligned}\\end{equation} $$ 计算z的过程： 首先计算Prior Probability。 假设模型是Gaussian的，算出 $\\mu_1,\\mu_2 ,\\Sigma$ 的closed-form solution 。 根据 $\\mu_1,\\mu_2,\\Sigma$ 计算出 $w,b$ 。 计算Prior Probability。 程序中用list comprehension处理较简单。 123# compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1]) 计算 $\\mu_1,\\mu_2 ,\\Sigma$ （Gaussian） $\\mu_0=\\frac{1}{C0} \\sum_{n=1}^{C0} x^{n} $ (Label=0) $\\mu_1=\\frac{1}{C1} \\sum_{n=1}^{C1} x^{n} $ (Label=0) $\\Sigma_0=\\frac{1}{C0} \\sum_{n=1}^{C0}\\left(x^{n}-\\mu^{}\\right)^{T}\\left(x^{n}-\\mu^{}\\right)$ (注意 ：这里的 $x^n,\\mu$ 都是行向量，注意转置的位置） $\\Sigma_1=\\frac{1}{C1} \\sum_{n=1}^{C1}\\left(x^{n}-\\mu^{}\\right)^{T}\\left(x^{n}-\\mu^{}\\right)$ $\\Sigma=(C0 \\times\\Sigma_0+C1\\times\\Sigma_1)/(C0+C1)$ (shared covariance) 123456789101112131415mean_0 = np.mean(X_train_0, axis=0)mean_1 = np.mean(X_train_1, axis=0)# compute in-class covariancecov_0 = np.zeros(shape=(data_dim, data_dim))cov_1 = np.zeros(shape=(data_dim, data_dim))for x in X_train_0: # (D,1)@(1,D) np.matmul(np.transpose([x]), x) cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# shared covariancecov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train.shape[0]) 计算 $w,b$ 在 这篇博客中的第2小节中的公式推导中， $x^n,\\mu$ 都是列向量，公式如下： $$ z=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} $$ $w^T=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} \\qquad b=-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}}$ 但是 ，一般我们在处理的数据集，$x^n,\\mu$ 都是行向量。推导过程相同，公式如下： （主要注意转置和矩阵乘积顺序） $$ z=x\\cdot \\Sigma^{-1}\\left(\\mu^{1}-\\mu^{2}\\right)^{T} -\\frac{1}{2} \\mu^{1}\\Sigma^{-1}\\left(\\mu^{1}\\right)^{T}+\\frac{1}{2}\\mu^{2}\\Sigma^{-1} \\left(\\mu^{2}\\right)^{T} +\\ln \\frac{N_{1}}{N_{2}} $$ $w=\\Sigma^{-1}\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\qquad b=-\\frac{1}{2} \\mu^{1}\\Sigma^{-1}\\left(\\mu^{1}\\right)^{T}+\\frac{1}{2}\\mu^{2}\\Sigma^{-1} \\left(\\mu^{2}\\right)^{T} +\\ln \\frac{N_{1}}{N_{2}}$ 但是，协方差矩阵的逆怎么求呢？ numpy中有直接求逆矩阵的方法(np.linalg.inv)，但当该矩阵是nearly singular，是奇异矩阵时，就会报错。 而我们的协方差矩阵（510*510）很大，很难保证他不是奇异矩阵。 于是，有一个 牛逼 强大的数学方法，叫SVD(singular value decomposition, 奇异值分解) 。 原理步骤我……还没有完全搞清楚QAQ（先挖个坑）[1] 利用SVD，可以将任何一个矩阵（即使是奇异矩阵），分界成 $A=u s v^T$ 的形式：其中u,v都是标准正交矩阵，s是对角矩阵。（numpy.linalg.svd方法实现了SVD） 可以利用SVD求矩阵的伪逆 $A=u s v^T$ u,v是标准正交矩阵，其逆矩阵等于其转置矩阵 s是对角矩阵，其”逆矩阵“（注意s矩阵的对角也可能有0元素） 将非0元素取倒数即可。 $A^{-1}=v s^{-1} u$ 计算 $w,b$ 的代码如下： 123456789101112131415# compute weights and bias# Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.# cov = u@s@vh# cov_inv = dot(vh.T * 1 / s, u.T)u, s, vh = np.linalg.svd(cov, full_matrices=False)s_inv = s # s_inv avoid &lt;1e-8for i in range(s.shape[0]): if s[i] &lt; (1e-8): break s_inv[i] = 1./s[i]cov_inv = np.matmul(vh.T * s_inv, u.T)w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))b = (-0.5) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (0.5) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) Accuracy accuracy结果： 1Training accuracy: 0.8756450899439694 也将权重较大的feature输出看看： 12345678910age: [-0.51867291] Masters degree(MA MS MEng MEd MSW MBA): [-0.49912643] Spouse of householder: [0.49786805]weeks worked in year: [-0.44710924] Spouse of householder: [-0.43305697]capital gains: [-0.42608727]dividends from stocks: [-0.41994666] Doctorate degree(PhD EdD): [-0.39310961]num persons worked for employer: [-0.37345994] Prof school degree (MD DDS DVM LLB JD): [-0.35594107] Code具体数据集和代码，欢迎光临小透明GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133import numpy as npnp.random.seed(0)############################################### Prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './generative_output/output_{}.csv'fpath = './generative_output/generative'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)train_size = X_train.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In generative model:\\n') f.write('Size of training data: {}\\n'.format(train_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n\\n'.format(data_dim))######################### Useful functionsdef _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8))def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function computes prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc######################## Generative Model: closed-form solution, can be computed directly# compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])mean_0 = np.mean(X_train_0, axis=0)mean_1 = np.mean(X_train_1, axis=0)# compute in-class covariancecov_0 = np.zeros(shape=(data_dim, data_dim))cov_1 = np.zeros(shape=(data_dim, data_dim))for x in X_train_0: # (D,1)@(1,D) np.matmul(np.transpose([x]), x) cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# shared covariancecov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train.shape[0])# compute weights and bias# Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.# cov = u@s@vh# cov_inv = dot(vh.T * 1 / s, u.T)u, s, vh = np.linalg.svd(cov, full_matrices=False)s_inv = s # s_inv avoid &lt;1e-8for i in range(s.shape[0]): if s[i] &lt; (1e-8): break s_inv[i] = 1./s[i]cov_inv = np.matmul(vh.T * s_inv, u.T)w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))b = (-0.5) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (0.5) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0])# compute accuracy on training setY_train_pred = 1 - _predict(X_train, w, b)with open(fpath, 'a') as f: f.write('\\nTraining accuracy: {}\\n'.format(_accuracy(Y_train_pred, Y_train)))# Predictpredictions = 1 - _predict(X_test, w, b)with open(output_fpath.format('generative'), 'w') as f: f.write('id, label\\n') for i, label in enumerate(predictions): f.write('{}, {}\\n'.format(i, label))# Output the most significant weightwith open(X_test_fpath) as f: content = f.readline().strip('\\n').split(',')content = content[1:]ind = np.argsort(np.abs(np.concatenate(w)))[::-1]with open(fpath, 'a')as f: for i in ind[0:10]: f.write('{}: {}\\n'.format(content[i], w[i])) Reference SVD原理，待补充","link":"/2020/04/15/ml-lee-hw2/"},{"title":"「政治」:马克思主义","text":"这学期马原毛概并举，政治知识储备达到巅峰qwq 看着徐涛老师的政治课复习，做了些许笔记，做个总结分享出来。 「哲学们只是用不同的方式解释世界，而问题在于改变世界。——卡尔·马克思」 马克思主义真是大智慧！！！ 马克思主义哲学马克思主义导论1.马克思主义的内涵 马克思主义是什么 （创造者）马克思主义是马克思和恩格斯创立并为后继者所不断发展的科学理论体系， （内容）是关于自然、社会、人类思维发展的一般规律的学说。， （目的）是关于社会主义必然代替资本主义，最终实现共产主义的学说， （立场）是关于无产阶级解放、全人类解放，和每个人自由而全面发展的学说， （作用）是指引人民创造美好生活的行动指南。 马克思主义的构成 （方法）马克思主义哲学 （主体）马克思主义政治经济学 （归属）科学社会主义 用了马克思主义哲学的方法，创立了马克思主义政治经济学，得出了科学社会主义的结论。 2.马克思主义基本原理马克思主义基本原理是对马克思主义立场、观点、方法的集中概括 基本立场 （人的解放）马克思主义以无产阶级的解放和全人类的解放为己任， （人的发展）以人的全面发展为目标， （人为中心）以人民为中心，一切为了人民，一切依靠人民。 基本观点 （科学认识）是对自然、社会、人类思维发展一般规律的科学认识 （科学总结）是对人类思想成果和社会实践经验的科学总结。 基本方法 辩证唯物主义和历史唯物主义的世界观和方法论。 实事求是的方法、辩证分析的方法、社会基本矛盾和主要矛盾分析的方法、阶级分析的方法、群众路线的方法 3.马克思主义的来源 第一个无产阶级同盟：正义者同盟 第一个无产阶级政党：共产主义者同盟 1844年马恩《德法年鉴》：完成了唯心主义向唯物主义的、从革命民主主义向共产主义的转变，为创立马克思主义奠定思想基础。 1844年马恩《德意志形态》：首次系统阐述了历史唯物主义的基本观点，历史观的伟大变革。 1848年马恩《共产党宣言的发表》标志马克思主义的公开问世。 马克思《资本论》：系统阐述剩余价值学说，揭示资本主义生产关系的密码 马克思一生的伟大发现：唯物史观和剩余价值学说 马克思《法兰西内战》：高度赞扬巴黎工人的伟大创举，科学总结了巴黎公社的历史经验 马克思《哥达纲领批判》：丰富科学社会主义学说 恩格斯《反杜林论》：全面阐述马克思主义理论体系 4.马克思主义的产生条件马克思主义的产生具有深刻的社会根源、阶级基础和思想渊源 社会渊源：马克思、恩格斯生活的时代，资本主义生产方式在西欧已经有相当的发展。 资本主义一方面带来社会化大生产的迅猛发展，一方面又造成严重的社会灾难： 社会两极化，工人极端困苦 周期性经济危机频繁爆发 阶级基础/实践基础：无产阶级在反抗资产阶级剥削和压迫的斗争中，逐渐走向自觉，并迫切渴望科学的理论指导。 工人运动：法、英、德 1831年法国里昂工人 1836年英国工人运动——宪章运动：集中反映工人的政治需求 1844年德国西里西亚纺织工人举行起义：标志现代无产阶级作为独立的政治力量登上历史舞台 思想渊源：德国古典哲学、英国古典政治经济学、英法两国空想社会主义 德国古典哲学：辩证法 英国古典政治经济学：资本主义生产关系的分析和关于劳动创造价值的思想 空想社会主义：对资本主义社会的批判和对未来新社会的发展 19世纪三大科学发现：细胞学说、能量守恒和能量转化定律、生物进化论 5.马克思主义的特征 考：特征和解释对应，一段材料反映什么样的特征。 科学性：马克思主义是对自然、社会和人类思维发展本质和规律的正确反映 。 革命性：马克思主义的革命性集中表现为彻底的批判精神和鲜明的无产阶级立场。 实践性：马克思主义是从实践中来，到实践中去，在实践中接受检验，并随实践不断发展的学说。 实践性是马克思主义独有的特征，可以区分其他派别。 人民性：人民至上是马克思主义的政治立场。 马克思主义的人民性是以阶级性为深刻基础的，是无产阶级先进性的的体现 以阶级性为基础，表示只要你是无产阶级，你就代表人民 发展性：马克思是不断发展的学说，具有与时俱进的品质。马克思主义理论体系是开放的，不断吸收人类最新的文明成果来充实和发展自己。 哲学基本问题6.哲学的基本问题 哲学的基本问题：思维和存在的问题。（意识/精神和物质的问题。） 存在和思维何者是第一性，何者是第二性。 该问题的回答，构成了划分唯物主义和唯心主义的标准 物质和意识是否具有同一性。 即思维能否正确认识存在的问题 6.哲学的不同流派通过对哲学基本问题的回答来划分哲学的流派 存在和思维何者是第一性，何者是第二性。 唯物主义：物质第一性 什么才是物质？划分唯物主义中的流派 古代朴素（古典）唯物主义：物质是某一种或者几种 五行；火；一生二，二生三，三生万物。 近代形而上学唯物主义：物质是原子或粒子 其中机械唯物主义是近代形而上学唯物主义 近代形而上学唯物主义&gt;机械唯物主义 现代辩证唯物主义：一切客观存在的都是物质 把历史作为物质。 唯物史观：把历史作为物质来分析，得出的观点和结论。 =马克思主义 唯心主义：意识第一性 什么才是意识 主观唯心主义：世界的本原是人的意识。 客观唯心主义：世界的本原是独立于“我”之上的。 比如上帝、道、理、缘说等 物质和意识是否具有同一性 可知论：意识可以认识物质。 唯物主义和唯心主义都属于可知论，只是唯物主义是物质决定意识，顺序不同。 不可知论：意识不可以或者不能完全认识物质。 二元论：认为物质和意识都是世界的本原。 二元论属于不完全的唯心主义。 6.哲学的重要问题 哲学的重要问题：世界是什么样的。 先回答哲学的基本问题：唯物主义或唯心主义 再回答哲学的重要问题：形而上学或辩证法。 形而上学：世界是孤立的、片面的、静止的、无矛盾的 辩证法：世界是联系的、全面的、发展的、矛盾的 辩证法是一种观点，回答世界是什么样的观点。 哲学流派的划分 先回答哲学的基本问题，再回答哲学的重要问题 形而上学的唯物主义 形而上学的唯心主义 辩证唯心主义 在马克思主义出现之前，没有辩证唯物主义的组合 辩证唯物主义 马克思在哲学的两大贡献 创立了历史唯物主义（唯物史观）：将历史作为物质来研究得出的观点和结论。 形成了辩证唯物主义：将辩证法的观点和唯物主义结合。 马克思主义的当代价值 观察当代世界变化的工具 指引当代中国发展的行动指南 引领人类社会进步的科学真理 自觉学习和运用马克思主义 唯物论-物质观框架： 物质范畴：什么是物质 物质和运动 运动和静止 物质运动和时空 7.物质范畴：什么是物质马克思批判了旧唯物主义对物质世界的直观、消极理解，强调要从能动的实践出发去把握客观世界的意义 恩格斯的物质概念：物、物质是各种物的总和，这个概念是从这个总和中抽象而来的。 考点：物质和具体物的关系：抽象和具体的关系 列宁的定义： 定义方式：通过物质和意识的关系定义 （客观实在）物质是标志客观实在的哲学范畴， （人的感觉）这种客观实在是人通过感觉感知的，不依赖于人的感觉而存在，为我们的感觉所复写、摄影、反映。 物质的唯一特性（最本质的特性）：客观实在性 马克思的物质范畴从客观存在的物质世界抽象出万物的共同特性：客观实在性 马克思主义的物质范畴理论的意义：（两个坚持两个体现） 坚持唯物主义一元论，同唯心主义一元论和二元论划清界限。 马克思主义是从物质与意识的对立统一关系中把握和规定物质的，物质最本质的特征是客观实在性，这就指明了物质对于意识的独立性、根源性，以及意识对于物质的依赖性、派生性。因为意识不过是物质的反映，不能离开物质而独立存在，所以意识不可能成为另一种本原 坚持能动反映论和可知论，批判不可知论。 世界上还有很多事物未被人类认识，但这并不意味着它们不可认识 体现了唯物论和辩证法的统一，克服了形而上学唯物主义的缺陷 体现了唯物主义自然观和唯物主义历史观的统一 8.物质和运动 物质的定义：不依赖于人的意识而存在，并能为人类的意识所反映的客观存在。 运动的定义：运动是标志一切事物和现象的变化及其过程的哲学范畴 物质和运动的关系： 物质和运动的关系：不可分割 不可分割：我是你的我，你是我的你 物质是运动着的物质，运动是物质在运动。 物质的根本属性（固有属性）是运动。 物质的唯一特性：客观实在性。 物质的根本属性：运动。 物质的存在方式是运动。 批判两种错误的观点：运动——物质 脱离物质谈运动，则是唯心主义。 脱离运动谈物质，则是形而上学。 9.运动和静止 相对静止的定义：静止是物质运动在一定条件下的稳定状态，包括两种状态：空间的相对位置暂时不变和事物的根本性质暂时不变。 运动和静止的关系：对立统一 对立统一：相互区别；相互联系。（相同词性） 不可分割：我是你的我，你是我的你。（不同词性） 相互区别：运动是绝对的、无条件性的；静止是相对的、有条件性的。 相互联系：运动和静止是相互依赖、相互渗透，“动中有静，静中有动“。 鸟鸣山更幽，风定花犹落。 批判两种错误的观点：运动——静止 夸大静止，否定运动，导致形而上学。 夸大运动，否定静止，导致诡辩论。 人不能两次踏入同一条河流。（正确） 人没有一次能踏入同一条河流。（错误） 10.物质运动和时空 时空的概念：时间是物质运动的持续性、顺序性，特点是一维性，即时间一去不复返。空间是物质运动的广延性、伸展性，特点是三维性。 时空和物质运动的关系： 不可分割：时空是物质运动的时空，物质运动是时空中的物质运动。 物质运动的存在形式是时空。 物质运动的存在形式是时空。 时间是物质运动的存在形式。正确 空间是物质运动的存在形式。正确 时空的特点：（五个特点） 考察：给一段材料，体现了时空的什么特点 客观性：不以人的意志为转移。 物质运动与时空不可分割，证明时间和空间的客观性 绝对性：时空是绝对存在的。 相对性：当速度突破极限，时空会发生变化。 有限性：具体物质形态的时空是有限的。 无限性：整个物质世界的时空是无限的。 唯物论-意识观框架： 实践：物质世界和意识世界的桥梁 意识的起源 意识到本质 意识到作用 物质和意识的辩证关系 11.实践：是自然存在和社会存在区分和统一的基础人类的产生使自然界的运动变化发生了新的飞跃，并通过人的实践形成了自然界与人类社会的区别 社会生活的本质是实践。 全部社会生活的本质是实践。 从实践出发理解社会生活的本质 实践是使物质世界分化为自然界和人类社会的历史前提，又是使自然界与人类社会统一起来的现实基础。 物质世界：一切客观存在。 在没有人之前，物质世界是自然界，有人之后，人的实践使其分为自然界和人类社会。 也通过实践，两部分相互转换，统一。 实践活动过程中，物质世界被区分为自然界和人类社会两大领域 劳动是人的存在方式，也是人类社会存在与发展的基础。通过劳动实践，人不再是单纯的自然存在物，更主要是社会存在物。 实践是人类社会的基础，是理解和解释一切社会现象的钥匙。 人类社会的本质是实践，把社会生活当作实践去理解。 社会生活的实践性体现在（为什么说实践是社会生活的本质） 社会生活是对人民各种社会活动的总称 实践是社会关系形成的基础。 包含了全部社会关系，是社会关系的发源地。 实践形成了社会生活的基本领域。 即社会的物质生活、政治生活和精神生活领域。 实践 构成了 社会发展的动力。 考：构成动力，而不是 是动力，社会发展的动力是社会基本矛盾。 实践是自然存在和社会存在区分和统一的基础。 自然存在是自然界，社会存在是人类社会。 12.意识的起源 意识的概念： （物质的产物）物质世界长期发展的产物， （本质）意识是人脑这样一种特殊物质的机能和属性， （最本质）是物质/客观世界的主观映像。 意识的起源： 由一切事物所具有的反映特性到低等生物所具有的刺激感应性， 一切事物的物理性质，化学性质，到低等生物的应激性反应。 再到高等动物的感觉和心理， 高等动物的开心、难过等情绪和疼痛等感觉 最终发展为人的意识。 意识是人所特有的 意识形成过程中的影响因素： 意识是自然界长期发展的产物，也是社会历史发展的产物 意识有很多影响因素，其中 劳动：社会实践，特别是劳动，在意识到产生和发展中起决定作用。 语言：人们在劳动和交往中形成的语言，促进了意识的发展。 语言是意识的物质外壳。 语言是物质，语言的含义是意识。 12.意识的本质 意识的本质：意识是客观世界的主观映像，是客观内容和主观形式的统一。 意识在内容上客观的，在形式上主观的，是客观内容和主观形式的统一 意识是物质的产物，又不是物质本身，因此在内容上是反应物质的，是客观存在的，在形式上是主观的映像。 12.意识的作用 意识的作用：能动作用 物质决定意识，意识对物质具有反作用，这种反作用就是意识的能动作用 意识到能动作用表现在： 意识活动具有自觉性，具有目的性和计划性。 人认识世界时，根据一定的目的和要求去确定反映什么、不反映什么、怎么反应，表现出主体的选择性。 意识活动具有创造性。 意识是对所见的东西，在头脑中进行加工想象。 意识具有指导实践改造客观世界的作用。 意识到能动性不限于从实践中形成思想，形成活动的目的、计划、方法等观念的东西，更重要的是，将这些观念作为指导，通过实践使之成为客观现实。 可以创造出世界上原本没有的东西。 意识具有指导、控制人的行为和生理活动的作用。 意识、心理对人的行为和健康有重要影响，”笑一笑十年少，愁一愁，白了头。 想做的事，能通过意识不做这件事。 13.物质和意识的辩证关系 两大关系：不可分割和对立统一，对立统一即辩证关系。 物质和意识相互区别： 物质是本原，意识是派生。 物质不是意识，意识不是物质。 大脑的分泌物是意识。错误。 物质不能代替意识，意识不能代替物质。 物质和意识相互联系： 物质可以转换为意识，意识可以转换为物质。 意识对物质既有依赖性，又有相对独立性。 把物质作为强，意识作为弱，但没有那么弱，弱的要依赖于强的，但也要有自己的独立性。 物质决定意识，意识反作用于物质。 强的决定弱的。 主观能动性和客观规律性的统一 正确认识和处理物质和意识的辩证统一，还需要处理好主观能动性和客观规律性的关系 主观能动性和客观规律性的关系（物质世界） 一方面，尊重客观规律是正确发挥主观能动性的前提。 另一方面，只有充分发挥主观能动性，才能正确认识和利用客观规律。 正确发挥人的主观能动性的前提和条件： 前提：从实际出发是发挥人的主观能动性的前提。 基本途径：实践是正确发挥人的主观能动性的基本途径。 正确发挥人的主观能动性，还需要依赖一定的物质条件和物质手段。 类比考研：从实际出发，实践，依赖一定的条件和手段 主观能动性和客观规律性的关系（人类历史的角度：社会领域） 社会历史领域，主观能动性和客观规律的辩证关系具体表现在社会历史趋向与主体选择的关系 社会历史趋向，即，客观规律。 主体选择，即，主观能动。 社会历史趋向是指社会历史规律的客观性和必然性 主体选择是指社会主体在社会发展中的能动性和选择性 社会历史规律的客观性和必然性规定了人的活动要受到规律的限制，但又不能否认人作为历史主体的能动性和选择性 14.世界的物质统一性原理 总结性，命题的方向 世界是统一的，即世界的本原只有一个。（批判二元论） 世界的统一性在于他的物质性，即世界统一的基石是物质。（批判唯心主义） 物质世界的统一性是多样性的统一，而不是单一的无差别的统一。（批判旧唯物主义，即马哲之前的唯物主义） 马克思主义哲学与唯心主义的比较 联系：都是可知论。 都是一元论，认为世界的本原只有一个。 区别： （本原）马克思主义哲学认为物质是本原，唯心主义认为意识是本原。 （认识论）马克思主义哲学在认识问题上坚持能动反映论，唯心主义坚持先验论。 （辩证法）马克思主义哲学坚持彻底的辩证法，唯心主义部分坚持辩证法。 唯心主义中也有形而上学的唯心主义和辩证唯心主义。 （历史观）马克思主义哲学在历史观上是唯物的，唯心主义在历史观上是唯心的。 马克思主义与旧唯物主义的比较。 联系：都是唯物主义，认为物质是世界的本原。 区别： （认识论）马克思主义哲学在认识论上坚持能动反映论，旧唯物主义坚持机械反映论。 （辩证论）马克思主义哲学彻底坚持辩证法，旧唯物主义完全不认同辩证法，坚持形而上学。 （历史观）马克思主义哲学在历史观上唯物，旧唯物主义在历史观上唯心。 （实践）马克思主义哲学坚持实践的观点，旧唯物主义没有实践的观点。 实践是马克思独有的观点 世界统一于物质的观点 （意识）世界的物质统一性首先体现在，意识统一于物质。 意识是物质发展的产物。 （人类社会）世界的物质统一性还体现在，人类社会也统一于物质。 人类社会（历史）是物质。 人类社会的物质性 （物质世界）人类社会是物质世界的组成部分。 物质世界包含人类社会。 （物质资料的获取）人类获取物质生活资料的实践活动，虽然是意识指导，但仍然是物质性的活动。 实践活动是物质： 社会生活的本质是实践。 社会生活/人类社会是物质的，实践是物质。 （物质资料的生产）物质资料的生产方式，是人类存在和发展的基础，集中体现人类社会的物质性。 生产方式是物质： 实践是社会关系的基础，包含所有社会关系。 实践是物质的，物质资料的生产方式也是物质的。 14.世界的物质统一性原理的意义世界的物质统一性原理的作用 （理论上）世界的物质统一性是马克思主义的基石。 （实践上：一切从实际出发）在认识和改造世界过程中，坚持实事求是，一切从实际出发，是世界的物质统一原理体现在现实生活中和实际工作中的生动体现。 考：材料关于一切从实际出发，问题问这段材料体现什么哲学原理 ：世界的物质统一性原理 辩证法-两大总特征 两大总特征 普遍联系 永恒发展 五对范畴：联系和发展环节上的逻辑 怎样的联系。 怎么发展 三大规律 事物怎么联系 对立统一 质量互变 否定之否定 15.唯物辩证法两大总特征唯物辩证法的总观点和总特征：联系和发展 两大总特征： 普遍联系 永恒发展 普遍联系 联系的概念：联系是指事物内部各要素之间 和 事物之间相互影响、相互制约、相互作用的关系。 联系是以区别为前提。 联系的特点： 考：材料体现联系什么特点 客观性：联系是事物本身固有的，不是主观臆想的。 普遍性： （事物内部）任何事物内部的不同部分和要素是相互联系，即任何事物都有内部结构性。 （事物间）任何事物都不能孤立存在，都同其他事物处于一定的相互联系中中。 （世界是一个联系网）整个世界是相互联系的统一整体，每个事物都是世界普遍联系中的一个成分或环节，并通过这个联系之网体现出联系的普遍性。事物的普遍联系是通过中介实现的，是通过中间性的联系和过渡性环节而实现的。 多样性：世界上的事物是多样的，事物之间的联系也是多样的。 条件性 条件是对事物存在和发展发生作用的诸要素的总和。 （支持或制约）条件对事物发展和人的活动具有支持或制约作用。 有利条件促进发展，不利条件抑制发展。 条件是可以改变的。 将不利条件转化为有利条件。 改变和创造条件不是任意的，必需尊重事物发展的客观规律。 永恒发展 发展的概念：概括一切形式的变化是运动，运动变化的趋势是发展。 运动=变化 &gt; 发展 运动：标志一切事物和现象变化及其过程的哲学范畴。 运动可以是前进或者后退的方向，变化也是可以往好的和不好的方向。 但发展是运动变化的趋势，是前进的，是上升的。 因此运动是绝对的无条件的，发展不是，发展是永恒的。 发展的实质：前进的上升的运动，新事物的产生和旧事物的灭亡。 新事物：合乎历史前进方向、具有远大前途的东西。 旧事物：丧失历史必然性、日趋灭亡的东西。 新旧事物的区分是：通过历史趋势来看的。而与时间的先后无关。 新事物是不可战胜的： 新事物具有新结构，适应新环境。 新事物之所以新，是因为新事物有新的要素、结构和功能，它适应已经变化了的环境和条件；旧事物之所以旧，是因为他的各种要素和功能已经不适应环境和客观条件的变化 新事物是旧事物的改良，吸收了旧的优点。 新事物否定了旧事物中消极腐朽的东西，又保留了旧事物中合理的、适应新条件的因素，并添加了旧事物所不能容纳的新内容 新事物符合群众利益，受到群众拥护。 过程的观点 恩格斯指出：“世界不是既成事物的集合体，而是过程的集合体。” 事物的发展是一个过程，只有经过一定的过程，事物才能实现自身的发展。 事物发展的过程，从形式上看，是事物在时间上持续性和空间上广延性的交替；从内容上看，是事物在运动形式、形态、结构、功能和关系上的更替。 一切在历史上产生的都要在历史上灭亡。 任何事物都有它的过去、现在和将来。 辩证法-五对范畴联系和发展是通过一系列基本环节得以实现的。 五对范畴：联系和发展环节上的逻辑问题 原因与结果 必然与偶然 可能与现实 现象与本质 内容与形式 概念、概念、方法论展开阐述 16.原因和结果 原因和结果：是揭示事物前后相继、彼此制约的关系范畴 原因和结果是揭示事物引起和被引起关系的一对范畴 前后相继，则是因果，错误。 发生在你之前的不一定是原因。比如雷电不是因果。 原因：引起某种现象的现象 结果：被某种现象引起的现象 辩证关系：对立统一 （确定又不确定）原因和结果的区分既是确定的，又是不确定。 在某一种具体因果关系中，原因和结果是相互区别的，原因就是原因，结果就是结果。 但整体来说，一种现象在一种联系中是原因，在另一种现在中可能是结果。 （互为因果）原因和结果相互作用，原因产生结果，结果反影响原因。互为因果。 （互相表达）原因和结果互相渗透，结果存在在原因中，原因出现在结果之中。 知道原因，可以预测结果，知道结果，可以反推原因。原因结果互相表达。 （关系复杂）原因和结果的关系复杂多样，有一因多果、同因异果、一果多因、异因同果、多因多果、复因多果。 有其因，必有其果。错误。原因结果的关系复杂，有一因多果的情况。 有因必有果。正确。 方法论：凡事预则立不预则废。 做什么事都要先有计划和准备。有因，结果自然就有了。 考：故事体现什么哲理。 如果故事讲的是凡事预则立不预则废，为因果关系哲理，因为该方法论是因果范畴的出的启示。 17.必然和偶然 必然和偶然：解释客观事物产生、发展和衰亡过程中不同趋势的一对范畴。 事物的发展既包含必然的方面，也包含偶然的方面。 不是说这件事是必然，这件事是偶然。每一件事都有必然和偶然的成分。 必然：事物联系与发展中一定要发生、确定不移的趋势。 偶然：事物联系和发展过程中并非确定出现，可以出现，可以不出现，可以这样出现，也可以那样出现，具有不确定的趋势。 关系：对立统一 相互区别： 产生和形成的原因不同：必然是内部原因；偶然是外部原因。 他们表现的形式不同：必然是稳定的；偶然是不稳定的。 他们在事物发展中地位和作用不同：必然起决定作用，偶然起影响作用。 相互联系： 一方面，没有脱离偶然的必然（都有偶然的一面，必然是通过多次偶然表现的）。现实事物的发展，不通过偶然而只表现为纯粹的必然的情况是不存在的。必然伴随着偶然，必然要通过偶然表现出来。 我对芒果过敏，但如果我永远不吃芒果，这件事情就不会发生，不会有我过敏的偶然事件发生。 另一方面，没有脱离必然的偶然（都有必然的一面）。似乎是偶然起支配作用的地方，实际上是必然起决定作用，制约着偶然的作用形式及其变化。 我对芒果过敏，但我不知道。我吃了一次，过敏了，我以为是偶然，我又吃了一次，又过敏了，又以为是偶然。 虽然是偶然，但其实是我芒果过敏，因此是必然的。 必然与偶然相互转化。 对于某一过程来说是必然的东西，对另一过程就可能成为偶然的东西。 在生物进化中，某个基因变异会导致新物种的产生，这是偶然转化为必然。 旧物种的基本性状在新物种中表现为返祖现象，这是必然转化为偶然。 方法论：重视事物发展的必然，把握事物发展的总趋势，又要善于从偶然中发现必然，有利于事物发展的机遇。 18.现实与可能 现实和可能：揭示事物过去、现在和将来的相互关系的一对范畴。 可能（潜在的）：事物发展过程中潜在的东西，包含在事物中，是事物发展前途的种种趋势。 现实（实际存在）：相互联系着的实际存在的事物的综合。 关系：对立统一 区别：一个潜在的，一个实际存在的。 现实是当下的客观存在，标志着事物的当前状况；可能是事物潜在的趋势，标志着事物的发展方向。 联系： （现实包含可能、现实产生新的可能）一方面，现实蕴藏着未来的发展方向，会不断产生新的可能。 （可能 包含发展为现实的依据，条件成熟可以转化为现实）另一方面，可能包含着发展为现实的因素依据，一旦主客观条件成熟，可能就会转化为现实。 区分可能性和不可能性的根据是：现实中是否有依据。 如果现实有依据，则是可能性。 如果现实没有依据，则是不可能。 区分现实的可能和抽象的可能（潜在的可能）的根据是：在现实中是否有充分的依据 如果有充分的依据，则是现实的可能。 如果没有充分的依据，则是抽象的可能。 方法论： （立足现实，分析可能）一方面，要求人民立足现实，展望未来，对可能性作全面的分析和预判。 （防止坏的可能变成现象，争取好的可能变为现实）另一方面着远长远，防止坏的可能变为现实，同时善于创造条件，促使好的可能获得实现。 19.现象和本质 考试重要 现象和本质：解释客观事物外部表现和内在联系的一对范畴。 本质：事物内在关系和根本性质，只有靠人的理性思维才能把握。 现象：事物的外部联系和表面特性，通过人的感官直接感知 关系：对立统一 区别： 现象是个别的、具体的，本质是一般的、普遍的。 现象是多变的，本质是相对稳定的。 现象是生动的、丰富的，本质是深刻单纯的。 现象有真象和假象之分。 科学的任务就在于准确辨别真象和假象 真象和假象都是现象，都是事物外部联系和表面特性，都通过人的感官直接感知。 都是客观存在的反映，没有正确和错误之分。 真象是正面直接的反映本质，假象是侧面歪曲的反映本质。 假象和错觉不是一回事。 假象也是现象，是客观事物的外在表现，而错觉是错误的感觉，是主观的。 联系： 本质决定现象，本质总是通过一定的现象表现自己的存在； 现象表现本质，现象的存在和变化归根到底依赖于本质。假象也是本质的表现。 不表现为现象的本质和不表现本质的现象是不存在的。是错觉。 例： 错觉一定是被假象迷惑。错误。（错在一定） 真象是正确的现象，错觉是错误的感觉。错误。（现象是本质的表现，是客观的，没有正确错误之分，后半句正确） 真象往往隐藏在内部，假象往往外露在外部。错误。（真象、假象都是现象，都是事物本质的外在表现，本质才是在事物内部） 方法论： （现象和本质有联系）现象和本质是统一的，所以我们能够通过现象认识事物的本质。 （现象和本质是有区别的）又因为现象和本质是对立的，所以要求我们不能停留在现象中，要透过现象，解释本质。 20.内容和形式 内容和形式：从构成要素和表现方式反映事物的一对基本范畴。 内容：构成事物的一切要素的总和，是事物存在的基础。 形式：把诸要素统一起来的结构或表现内容的方式。 关系：相互依赖、不可分割 任何事物都是内容与形式的统一。任何事物的内容都有一定的形式，任何形式也都有一定的内容，没有无内容的空洞形式，也没有无形式的纯内容。 内容决定形式，形式反作用于内容。当形式适合内容的时候，对内容的发展起积极的推动作用；当形式不适合内容的时候，对内容的发展起消极的阻碍作用。 生产力是社会生产的内容。生成关系是社会生产的形式。 当生产关系适应生产力发展时，会推动生产力。当生产关系不适应生产力时，就会阻碍生产力。 方法论： （注重内容，反对形式主义）在我们的认识和实践中，根据内容决定形式的原理，注重事物的内容，反对忽视内容、夸大形式作用的形式主义； （利用形式促进内容的发展）又要积极利用合适的形式促进内容的发展，不能忽视形式对内容的能动促进作用。 辩证法-三大规律三大规律： 对立统一规律：事物联系的内容和发展的动力 唯物辩证法的实质和核心 同一性和斗争性的辩证关系原理 同一性和斗争性在事物发展中的作用原理 普遍性和特殊性的辩证规律关系原理 矛盾的不平衡发展原理 矛盾分析法（方法论总结） 质量互变规律：事物发展过程中的状态 否定之否定规律：回答事物发展的方向和最终归属 辩证否定观 否定之否定规律 方法论 21.对立统一规律（很重要） 唯物辩证法的实质和核心：对立统一规律 为什么怎么说 对立统一规律揭示了事物普遍联系的根本内容和变化发展的内在动力，从根本上回答了事物为什么会发展的问题。 对立统一规律贯穿质变量变规律、否定之否定规律以及唯物辩证法基本范畴的中心线索。 对立统一规律提供矛盾分析法，他是对事物辩证认识的实质。 是否承认对立统一学说是唯物辩证法和形而上学对立的实质。 矛盾：反映事物内部和事物之间对立统一关系的哲学范畴。 矛盾的两种基本属性：对立和统一。 矛盾的对立属性又称斗争性 矛盾的统一属性又称同一性 （1）同一性和斗争性的辩证关系原理 同一性；斗争性；关系；方法论 矛盾的同一性：矛盾双方相互依存、相互贯通的性质和趋势。 相互依存：对立面相互依存，互为存在的前提，共处一个统一体中。 比如，没有上，就没有下。没有强就没有弱。 上下、强弱都是相互存在的前提。 相互贯通：对立面相互贯通，在一定条件下可以相互转化。 矛盾的斗争性：矛盾的对立面相互排斥、相互分离的性质和趋势。 矛盾的性质不同，矛盾的斗争形式也不同 对抗性矛盾 非对抗性矛盾 资本主义终将灭亡。 因为资本主义中间存在需要对抗性的激烈的矛盾，在对抗性矛盾运动中，资本主义走向灭亡。 而社会主义中间也存在矛盾，但是是非对抗性的人民内部的矛盾，可以通过民主集中制解决，不会走向灭亡。 同一性和斗争性的关系：对立统一 矛盾的同一性和斗争性的辩证关系不是时而同一性时而斗争性的关系，而是每时每刻既同一又斗争的关系。 矛盾的同一性和斗争性相结合，构成了事物的矛盾运动，推动着事物的变化发展 相互联系：矛盾的同一性和斗争性相互联结相辅相成。没有斗争性就没有同一性，没有同一性就没有斗争性，斗争性寓于同一性之中，同一性通过斗争性来体现。 现实中，越相似的两个事物，同一性越强，斗争性就会越强。 因此，想要减少斗争性，那就要消除同一性。 相互区别：在事物的矛盾中，矛盾的同一性是有条件的、相对的，矛盾的斗争性是无条件的、绝对的。 方法论的意义： 矛盾的同一性和斗争性是同时存在的，因此事物总是具有两面性，这要求我 们看待事物时要做到“一分为二”。例如，对待传统文化要“批判地继承”，对待外来文化应该 “批判地吸收”。 看问题要一分为二。（矛盾的同一性和斗争性是同时存在的） 求同存异（承认事物存在斗争性，追求同一性） 批判的继承（事物都有两面性） 事物之间会相互转化。（同一性和斗争性是相互转化的） 考：材料说明了同一性和斗争性的辩证关系原理。 （2）同一性和斗争性在事物发展中的作用原理 同一性的作用；斗争性的作用；方法论 矛盾的同一性在事物发展中的作用： （相互发展）由于矛盾双方相互依存、互为存在的条件，矛盾双方可以利用对方的发展使自己得到发展。 你好我也好。 （各自发展）同一性可以使矛盾双方相互吸取有利于自身的因素，在相互作用中各自得到发展。 相互吸取优点，各自发展的更好。 （转化发展）由于矛盾双方彼此互通，矛盾双方可以向彼此的对立面转化而得到发展，并规定着事物的发展方向。 你强我弱，变成我强你弱，向对立面转化了，但我的强不是原来你的强了，而是更强，因此是发展。 矛盾的斗争性在发展中的作用： （量变）矛盾双方的斗争促进矛盾双方力量的变化，竞长争高，此消彼长，造成事物的量变。 （质变）矛盾双方的斗争，促使矛盾双方的的地位和性质发生变化，实现事物的质变。 矛盾双方的斗争是一种矛盾统一体向另一种矛盾统一体过渡的决定力量 方法论： 事物的发展不仅表现为“相辅相成”，而且表现为“相反相成”。 矛盾的斗争性处于主要方面：“相反相成”是从事物的对立面也能达到统一，达到想要的效果。即逆向思维。 矛盾的同一性处于主要方面：“相辅相成”是从事物的同一性达到统一，达到想要的效果。 学会从事物的对立面把握事物的统一，逆向思考； 正确把握和谐对事物发展的作用。和谐是对立统一，而不是无差别的一致。 和谐是矛盾的一种特殊表现形式，体现着，矛盾双方的相互依存、相互促进、共同发展，和谐并不意味着矛盾的绝对同一。 （3）矛盾的普遍性和特殊性的辩证关系原理 矛盾的普遍性：（矛盾无处不在，矛盾无时不有）矛盾的普遍性是指矛盾存在与一切事物中，存在一切事物发展过程的始终，旧的矛盾解决了，新的矛盾又产生，事物始终在矛盾运动。 矛盾的特殊性：各个具体事物的矛盾、每一个矛盾的各个方面在发展的不同阶段上各有特点。 不同事物的矛盾各有特点。 同一事物的矛盾在不同发展过程和发展阶段各有不同。 构成事物的 诸多矛盾 以及 每一矛盾的不同方面 各有不同的性质、地位和作用。 关系： 相互区别： 矛盾的共性，即矛盾的普遍性，是无条件的、绝对的。 矛盾的个性，即矛盾的特殊性，是条件的、相对的。 相互联系：任何现实存在的事物的矛盾都是共性和个性的有机统一，共性寓于个性之中，没有离开个性的共性，也没有离开共性的个性。 方法论： 考：材料体现矛盾的普遍性和特殊性的辩证关系原理 “具体问题具体分析“，对症下药，量体裁衣，因材施教。 只有具体分析矛盾的特殊性，才能认清事物的本质和发展规律，并采取正确的方法和措施去解决矛盾，推动事物的发展。 （4）矛盾的不平衡发展原理事物是由多种矛盾构成的。 主要矛盾：主要矛盾是矛盾体系中处于支配地位，对事物发展、对事物发展起决定性作用的矛盾。 次要矛盾：次要矛盾是矛盾体系中处于从属地位、对事物的发展起次要作用的矛盾。 每一对矛盾： 主要方面：有一方处于支配地位，起主导地位，是矛盾的主要方面。 次要方面：被支配的一方则是矛盾的次要方面。 原理：事物的性质主要是由主要矛盾的主要方面决定的。 方法论：“两点论”与“重点论”相结合；看问题既要全面的看，又要抓关键，看主流。 “两点论”（面面俱到）：分析事物的矛盾，不仅要看的矛盾双方的对立，而且还要看到矛盾双方的统一；还要看到矛盾体系中存在主要矛盾、矛盾的主要方面，而且还有看到次要矛盾、矛盾的次要方面。 “重点论”（突出重点）：把握主要矛盾、矛盾的主要方面，并以此作为解决问题的出发点。 考：材料体现矛盾的不平衡发展原理。 矛盾分析法 与矛盾有关的所有原理的所有方法论。 考：根据题干材料判断矛盾分析法。 从事物的对立面把握事物的统一，反向思考，逆向思维。2 矛盾的同一性和斗争性在事物发展中的作用原理。 物极必反；否极泰来；福祸相依的对立面把握事物的统一。1 矛盾的同一性和斗争性的辩证关系原理。 中庸、和谐不走极端的思考方法和态度。1 矛盾的同一性和斗争性的辩证关系原理。 具体问题具体分析，对症下药，量体裁衣。3 矛盾的普遍性和特殊性的辩证关系的原理。 求同存异；差异中谋求共识。1 矛盾的同一性和斗争性的辩证关系原理。 两点论和重点论；抓关键，看主流。4 矛盾的不平衡发展原理。 22.量变质变规律 概念；关系；方法论 质：事物成为其自身并区别于其他事物的内在规定性。 考点： 认识质是认识和实践的起点和基础。 只有认识质，才能区别事物。 量：事物的规模、程度、速度 以及 它的构成成分在空间上的排列组合等可以用数量关系表示的规定性。 量：两种情况，一种是数量上的，一种是空间上的排列组合。 考点：量的意义： 认识事物的量是认识的深化和精确化。 只认识质是不够的，量才能深化认识，准确认识。 只有正确了解事物的量，才能正确估计事物在实践中的地位和作用。 度：保持事物 质的稳定性的数量界限，即事物的限度、幅度和范围，度的两端叫关节点或临界点。 度是一个区间，关节点/临界点时端点。 量变：事物 数量的增减 和 次序的变动 ，是保持事物的质的相对稳定性的不显著变化，体现了事物渐进过程的连续性。 量变有两种情况，一种是数量上的变化，一种是空间排列次序的变动。 质变：事物性质的 根本变化 ，是事物由一种质态向另一种质态的飞跃，体现了事物渐进过程和连续性的中断。 关系：对立统一 相互区别： 量变是事物质相对稳定的不显著变化，而质变是事物性质的根本变化。 量变是事物渐进过程的连续性，而质变是事物渐进过程和连续性的中断。 相互联系: （必要准备）量变是质变的必要准备。 任何事物的变化都有一个量变的积累过程，没有量变的积累，质变就不会发生。 想要质变，就必须要量变。 激变论：夸大质变，否定量变，认为可以不通过量变产生质变。 （必要结果）质变是量变的必然结果。 单纯的量变不会永远持续下去，量变达到一定的程度必然引起质变。 持续的量变一定会引起质变。 庸俗进化论：夸大量变，否定质变，认为可以一直量变下去，而没有质变。 量变和质变是相互渗透的。 事物的总量变认为是一个连续性过程，在某些点是阶段性的质变。 这些点的前后是旧质在量上的收缩和新质在量上的扩张。 一方面，在总的量变过程中有阶段性和局部性的部分质变。 另一方面，在质变过程中也有旧质在量上的收缩和新质在量上的扩张。 方法论： 理论上的方法论： 可以用来批判其他理论观点。 激变论：夸大质变，否定量变，认为可以不通过量变产生质变。 庸俗进化论：夸大量变，否定质变，认为可以一直量变下去，而没有质变。 实践中的方法论： 指导实践 适度原则。 否则会引起质变。 对社会主义初级阶段的认识。 社会主义是质，初级阶段是量。 改革、发展和稳定。 改革、发展追求的是速度，但也要把握好度，保持稳定。 23.否定之否定规律 概念 万事万物都同时存在着肯定因素和否定因素 肯定因素：维持现存事物存在的因素。 存在的原因是因为事物身上有肯定因素。 否定因素：促使现存事物灭亡的因素。 灭亡的原因是因为事物身上有否定的因素。 辩证否定观： 否定是事物的自我否定。 是事物内部矛盾运动的结果。 形而上学：认为是外在力量对事物进行消灭，错误的。 否定是事物发展的环节。 是旧事物向新事物的转变，是从旧质到新质到飞跃。只有经过否定，旧事物才能向新事物转变 否定是新旧事物联系的环节。 新事物孕育产生于旧事物，新旧事物是通过否定环节联系起来的。 辩证否定的实质是 扬弃 。 新事物对旧事物既批判又继承，既克服其消极因素又保留其积极因素 形而上学：认为要么肯定一切，要么否定一切。错误的。 对比： 辩证否定法观 形而上学否定观 “自我否定” 外在力量对事物进行否定和消灭 ”扬弃“ ”要么肯定一切，要么否定一切“ 否定之否定规律 事物的辩证发展经过肯定-否定-否定之否定三个阶段。 第一次否定：使矛盾初步解决，而处于否定阶段的事物仍然有片面性。 第二次否定：还需要经过再次否定，即否定之否定，实现对立面的统一，使矛盾得到根本解决。 每一个事物都是由许多矛盾组成的，一对矛盾，A方面和B方面组成这对矛盾。 否定是事物发展的环节，最开始事物处于A的方面，第一次否定，A - &gt; B 。 第二次发展，否定之否定，是更高形态的A，变成A‘。 事物的辩证发展就是经过两次否定、三个阶段，形成一个周期。其中，否定之否定阶段仿佛是向原来出发点的“回复”，但这是在更高阶段的“回复”，是“扬弃”的结果。 事物的发展呈现出周期性，不同周期的交替使事物的发展呈现波浪式前进或螺旋式上升的趋势。 方法论 理论上的方法论： 循环论：只看到回归，没有看到发展。 直线论：只看到发展，看不到回归。 实践中的方法论： 否定之否定规律揭示了事物发展的前进性和曲折性 事物发展的前进性：前途是光明的 每一次否定都是质变，都把事物推进到新阶段；每一个周期都是开放的，前一个周期的终点是下一个周期的起点，不存在不被否定的终点。 事物发展的曲折性：道路是曲折的 曲折性体现在回复性上，其中有暂时的停顿甚至是倒退。但是，曲折性终将为事物的发展开辟道路。表明，事物的发展是螺旋式上升的，而不是直线式前进的 24.客观辩证法与主观辩证法 客观辩证法：客观事物或客观存在的辩证法。 客观事物以相互作用、相互联系的形式呈现出的各种物质形态的辩证运动和发展规律。 大自然本身就存在的辩证法。 客观辩证法采取外部必然性形式 ，不以人的意志为转移，是物质世界本身的联系和发展。 主观辩证法：人类认识和思维运动的辩证法。 以概念作为思维细胞的辩证思维运动和发展规律 是人头脑中思维的辩证法。 主观辩证法则是采取观念的、逻辑的形式，是同人类思维的自觉活动相联系的，是以概念为基础的辩证思维规律，是辩证法的科学体系。 考： 客观辩证法是唯物的，主观辩证法是唯心的。（错误） 主观和客观辩证法是反映和被反映的关系，没有正确错误之分。 而唯物是正确的，唯心是错误的。 25.辩证思维方法 归纳与演绎 分析与综合 抽象与具体 历史与逻辑 认识论框架： 认识的来源和本质 来源：实践 本质：主体在实践的基础上对客体的能动反映 认识的过程和规律 两次飞跃：感性认识到理性认识到飞跃；理论到实践的飞跃。 规律：反复性和无限性。 认识的结果和检验标准 物质和意识：物质决定意识，意识反作用于物质，意识对物质具有依赖性，又有相对独立性。 实践和认识：实践决定认识，认识反作用于实践，认识对实践具有依赖性，又有相对独立性。 实践是物质性活动。 认识是意识活动。 26.实践的本质和特征 错误的实践观： 中国古代哲学：实践被称为“践行””实行“或”行“与”知“相对于，（知行合一的行是实践），但主要是指道德伦理行为。 错在道德伦理，太局限。不是道德伦理行为也是实践。 康德：把实践看作理性自主的道德活动。 错在道德活动。 错在理性自主，只强调主体，忽略客体。 黑格尔：把实践理解为主体改造客观对象的创造性的精神活动。 和康德相比，主体改造客观对象。 错在精神活动，实践是物质性活动，具有客观性。 费尔巴哈：把实践与物质的活动联系起来，但他所理解的实践仅限于日常生活活动，并将实践等同于生物适应环境的活动。 和黑格尔相比，认为实践是物质的活动。 错在日常活动，生理活动。实践只能是人的行为，动物没有实践。 马克思正确的实践观：实践是感性的、对象性的物质活动。 马克思科学阐明了人类实践的本质及其在认识世界和改造世界中的作用，创立了科学的实践观。 他在《关于费尔巴哈的提纲》阐明了实践是感性的、对象性的物质活动，提出全部社会生活的本质是实践，并鲜明指出哲学家们只是用不同的方式解释世界，而问题在于改变世界。 感性的：是人的意识指导的，具有目的性和计划性。 对象性：实践有客体。 物质活动：实践是物质性活动，具有客观性。 实践的本质含义：实践是人类 能动地改造世界的社会性的物质活动。 人类：实践是人独有的活动，动物没有实践。 能动的：和感性的相同，是由意识指导的，意识的反作用。 实践的基本特征：直接现实性、自觉能动性/主体能动性、社会历史性。 考：材料体现实践的特点。 直接现实性（实践最本质的特性）：实践具有将人脑中观念的存在变为现实的可能。 实践是改造世界的物质活动，不是纯粹的精神活动，是以感性事物为对象的现实的物质活动。 实践所具有的直接现实性也就是实践活动的客观实在性。（物质的唯一特性） 能将头脑中的想法变成现实存在。 自觉能动性/主体能动性：实践受意识的指导，体现主体的目的性。 与动物的本能的、被动的适应活动不同，人的实践活动是一种有意识、有目的的活动。 和动物的本能活动不同（包括人的本能活动：吃饭/睡觉），实践是有目的的。 因此人的活动中，本能活动不属于实践。 社会历史性：不同历史阶段的实践内涵不同。 实践的内容、性质、范围、水平以及方式都收到一定社会条件的制约，随着一定社会历史条件的变化而变化。 比如现在人们可以通过互联网实践，虚拟实践。 27.实践的基本结构和形式 实践的基本结构： 实践主体：指具有一定的主体能力、从事现实社会实践活动的人，是实践活动中自主性和能动性的因素，担负着设定实践目的、操作实践中介、改造实践客体的任务。 实践主体只能是人，但不是所有人都是实践主体，只有具有一定主体能力、从事现实实践活动的人才是主体。 实践主体的能力：自然能力和精神能力。 自然能力：如力量大。 精神能力：知识性因素和非知识性因素。 知识性因素：对理论知识的掌握，对经验知识的掌握。 知识性因素是首要的实践主体的能力 非知识性因素：情感和意志因素。 实践主体的基本形态：个体主体、群体主体、人类主体。 实践客体：指实践活动所指向的对象。 实践客体和客观存在的事物不完全等同，客观事物只有在被纳入主体实践活动的范围之内，为主体实践活动所指向并与主体相互作用时才成为现实的实践客体 。 实践客体是客观存在的事物，但不是所有客观存在的物都是实践客体，必需是实践活动所指向的，而像外太空、深海未被人所认知到的物都不是实践的客体。 实践中介：各种形式的工具、手段 以及 运用、操作这些工具、手段的程序和方法。 实践的中介系统： 正是依赖这些中介系统，实践的主体和客体才能够相互作用。 作为人的肢体延长、感官延伸、体能放大的物质性工具系统。 火车、电脑、雷达分别是对人的腿、脑、眼功能的延伸和放大。 语言符号工具系统。 语言符号是主体思维活动的现实形式，也是人们社会交往得以进行的中介。 主体和客体相互作用的关系：实践关系、认识关系和价值关系。 实践关系是最根本的关系。 实践的主体和客体与认识的主体和客体在本质上是一致的。 认识关系和价值关系都是基于实践关系。 实践结构的变化 主体客体化和客体主体化的双向运动是人类实践活动两个不可分割的方面。 主体客体化：人通过实践使自己的本质力量作用于客体，使其按照主体的需要发生结构和功能上的变化，形成了世界上本来不存在的对象。 实际上，人类一切实践活动的结果都是主体客体化的结果。 比如：人通过实践，使树变成纸张和筷子。 客体主体化：客体从客观对象的存在形式转化为主体生命结构的因素 或 本质力量的因素，客体失去客体性的形式，变成主体的一部分。 比如： 主体把物质工具如电脑、汽车等作为自己身体器官的延长 包括 在主体的活动 属于客体主体化。 把作为精神性客体的精神产品、先进理念和思想 转化为 主体意识的一部分，属于客体主体化。 实践的形式： 人类实践的具体形式日益多样化，从内容上看，实践可分为三种基本形式 注意：人的活动分为本能活动和实践活动，本能活动不属于实践活动，只有下列的劳动、搞关系、探索才算实践活动。 物质生产实践（最基本的实践活动） 物质生成实践：就是劳动。 社会政治实践：人们之间的社会交往和政治活动。 社会政治实践：人和人之间搞关系。 科学文化实践：创造精神文化产品的实践。 科学社会实践：探索创新，比如科学、艺术、教育等。 虚拟实践：实践活动的派生形式，具有相对独立性。 虚拟实践是基于人类实践的三种形式：物质生产实践、社会政治实践和科学文化实践，只是实践的载体发生了变化，变成了网络世界。 虚拟实践是一般实践活动的派生。 特点：交互性、开放性、间接性。 交互性：人与人之间。 开放性：互联网是共享的。 间接性：通过数字中介 28.实践决定认识辩证唯物主义认为，在实践和认识之间，实践是认识的基础，实践在认识活动中起着决定性的作用。 实践在认识活动中的决定作用（实践决定认识的原因） 实践是认识的来源（是唯一来源） 形成认识的因素中： 实践是决定因素 天赋（生理因素）和间接经验等起影响作用，是重要因素 认识的形成是多因素的，实践是决定因素。 类似意识的形成： 意识也是多因素形成的，实践特别是劳动，对意识的形成起决定作用，语言也促进意识的形成。 实践是认识的动力 恩格斯：社会一旦有技术上的需要，这种需要就会比十所大学更能把科学推向前进。 实践是认识的目的 不是为了认识而认识，其最终目的是为实践服务，指导实践 实践是检验认识真理性的唯一标准 考： 认识总是滞后于实践。错在总是。 实践是认识的先导。错在先导，实践指导认识不对，实践是一种行为，应该是认识/意识指导实践。 实践高于认识。正确，实践决定认识，高于认识。 实践和认识是合一的。知行合一。正确。 （但王阳明的知行合一，是作为的，王阳明的行局限于道德伦理行为） 29.认识的本质 各流派的认识论 唯心主义先验论：从思想和感觉到物 认识不是对事物的反映，而是先于事物的存在。 唯物主义反映论：从物到思想和感觉 先有客观事物，才有我们的认识，认识是对客观事物的反映 旧唯物主义机械反映论：直观的、消极被动的反映论 辩证唯物主义能动反映论：反映是一个能动的过程 区别在于机械反映论只有反应，即物到人的大脑中的直观反映，是什么就是什么，没有加工创造。 而辩证唯物主义能动反映是，先反映，再能动创造加工。 认识的本质：主体在实践基础上对客体的能动反映。 能动反映： 认识的反映特性：具有反映客体内容的反映性特征 认识的反映特性指人的认识必然要 以客观事物为原型和摹本 ，在思维中再现或摹写客观事物的状态、属性和本质。 （错误）虚幻的观念也是对事物本质的反映。错在本质，一切观念，即意识，都是对事物的反映，但不一定都是对事物的本质的反映，可能是对事物的现象、属性的反映。 回顾：本质和现象，现象反映本质，真象和假象都是对本质的反映。 认识的能动反映具有创造性 ：具有实践的主体能动的、创造的特征 认识是一种在思维中能动的、创造性的活动，而不是主观对客观对象简单、直接的描摹或照镜子式的原物映现。 （错误）一切观念都是现实的模仿。错在模仿，认识是具有能动的创造性。 认识的反映特性和能动的创造特性之间的关系：不可分割 人的认识是反映性或摹写性与创造性的统一 反映是能动创造的反映，创造是在基于反映创造。 旧唯物主义直观反映论：只坚持认识的反映特性，看不到认识的能动和创造性。 唯心主义和不可知论：只坚持认识能动的创造性，使创造脱离反映论的前提。 能动反映论的两个优点/特点： 把实践的观点引入认识论。 把辩证法应用于反映论考察认识的发展过程。 科学揭示认识过程中多方面的辩证关系，把认识看成一个由不知到可知、由浅入深的充满矛盾的能动的认识过程，全面揭示了认识过程的辩证特征。 30.认识过程的两次飞跃 从感性认识到理性认识（第一次飞跃） 比第二次飞跃考的多 感性认识（通过感觉感官直接感受）：人们在实践基础上，由感觉器官直接感受到的关于事物的现象、事物的外部联系、事物的各个方面的认识。 对象：事物的现象、事物的外部联系、事物的各个方面 形式：感觉、知觉和表象 感觉：对客观事物的个别属性、个别方面的直接反映。 知觉：对客观事物外部特征的整体反映。 表象：人脑对过去感觉和知觉的回忆 特点：直接性、具体性 理性认识（需要抽象、归纳、总结）：人们借助抽象思维，在概括整理大量感性材料的基础上达到关于事物的本质、全体、内部联系和事物自身的规律性的认识。 对象：事物的本质、全体、内部联系和事物自身的规律性的认识。 形式：概念、判断、推理 概念：同类事物的一般特性和本质属性的概括和反映 判断：展开了的概念，是对事物之间的联系和关系的反映，是什么或不是什么 推理：形式上表现为判断和判断之间的关系 特点：间接性和抽象性 感性认识和理性认识的辩证关系：对立统一 （感性到理性）感性认识有待发展和深化为理性认识 （理性依赖感性）理性认识依赖于感性认识 （相互渗透）感性认识和理性认识相互渗透、相互包含。 感性中有理性，理性中有感性，具有交融性。 感性认识和理性认识的辩证统一关系是在实践的基础上形成的，也需要在实践中发展。 如果割裂辩证统一关系： 教条主义-唯理论：否认感性认识而片面夸大理性认识。（否定实践） 经验主义：否认理性认识而片面夸大感性认识。 考：选择题：材料中是认为感性认识更重要还是理性认识更重要 尽信书，则不如无书。感性认识。 饱经风霜的老人与缺乏阅历的少年对同一句格言的理解不同。感性认识。 感性认识，即要实践。 感性认识上升到理性认识到的条件： （实践）投身实践，深入调查，获取十分丰富和合乎实际的感性材料。 （思考能动性）必须经过思考的作用，运用理论思维和科学抽象，将丰富的感性材料加工制作。 从认识到实践（理性认识到实践的飞跃） 从认识到实践，是认识过程的第二个阶段，是第二次能动的飞跃，也是认识过程中最重要的一次飞跃 重要性和必要性：一是认识世界的目的是为了改造世界。而是认识的真理性只有在实践中才能得到检验和发展。 31.认识过程中的理性因素和非理性因素 即形成认识，获得认识中的影响因素 理性因素：人的理性直观、理性思维的能力。 在认识活动中的作用：指导作用、解释作用和预见作用。 知识 非理性因素（感性因素）：人的情感和意志 在认识过程中的作用：激活、驱动和控制作用 联想、想象、猜测、直觉、灵感 区分感性认识和理性认识｜感性因素和理性因素： 感性认识是理性认识是认识过程已经结束，已经获得了认识。 感性因素和理性因素是在认识过程中的影响因素，还没有获得认识。 在感性认识/理性认识中都有感性因素和理性因素在起作用。 32.认识的规律 认识过程的反复性（反复循环）：人们对于一个复杂事物的认识往往要经过由感性认识到理性认识、再由理性认识到实践的多次反复才能完成。 原因： 客观：事物暴露有一个过程 主观：主体认识能力提高有个过程 认识过程的无限性（无限发展）：事物发展过程的推移来说，人类的认识永无止境，无限发展，表现为“实践、认识、再实践、再认识”，由低级阶段向高级阶段不断推移的永无止境的前进运动。 认识的无限发展过程：形式上是循环往复，实质上是前进上升。 认识是一个波浪式前进和螺旋式上升的过程。（否定之否定在认识论中） 33.认识与实践的具体的历史统一性在实践和认识的辩证运动中，主观必须统一于客观，认识必须统一于实践 方法论 实践超前于认识：冒进主义（左） 比如跑步进入中国特色社会主义道德的观点，左派。 实践落后于认识：保守主义（右） 先学西方搞几百年资本主义，再搞社会主义的观点，右派。 认识的结果34.真理及其特点 错误的真理观： 马赫主义：真理是“思想形式”，是社会性组织起来的经验，凡事大多数人承认的就是真理。 错在，大多数人。 实用主义：“有用即真理”，把真理的有用性与真理本身等同起来。 错在，真理和有用等同起来。 正确的真理观：（马克思）真理是标志主观和客观相符合的哲学范畴，是对客观事物的正确反映。 谬误：同客观事物及其发展规律相违背的认识（主客观不相符合），是对客观事物的错误/歪曲反映。 真理的特点： 客观性（真理的本质属性）：真理的内容是对客观事物及其规律的正确反映，真理中包含着不依赖人和人的意识的客观内容。 客观性体现在：真理的内容和检验标准上 真理的内容是客观的：对物质世界的正确反映 真理的检验标准是客观的：实践是物质性的活动。 真理的形式是主观的：通过感觉、知觉、表象、概念、判断、推理等主观形式表达出来 真理的内容是客观的，形式是主观的。真理是对客观事物及其规律的正确反映。 意识到内容是客观的，形式是主观的。意识是对物质世界的主观反映。 真理的客观性决定了真理的一元性：在同一条件下对特定的认识客体的真理性认识只有一个，而不可能是多个。 绝对性：真理主客观统一的确定性和发展的无限性。 绝对性体现在： 今天认为正确的东西，主客观的符合，是真理。 但明天认为昨天的不对，新的主客观符合，也是真理。 （承认了真理的客观性就是承认了真理的绝对性）任何真理都标志主观和客观的符合，包含着不依赖于人和人的意识的客观内容，都同谬误有原则的界限。这一点是绝对的、无条件的。 （承认世界的可知性，承认人能够获得关于无限发展的物质世界的正确认识，也就是承认了真理的绝对性）人类认识按其本性来说，能够正确认识无限发展着的物质世界，认识每前进一步，都是对无限发展着物质世界的接近，这一点也是绝对的、无条件的。 相对性：人们在一定条件下对客观事物及其本质和发展规律的正确认识总是有限度的、不完善的。 相对性体现在： 今天认为正确的东西，明天可能不对了。 （客观世界在发展，认识的广度有待扩展）客观世界的整体来看，任何真理都只是对客观世界的某一阶段、某一部分的正确认识，人类已经达到的认识的广度总是有限度的，认识有待扩展。 （一定程度，认识反映事物的深度有限）特定事物而言，任何真理只是对客观对象一定方面、一定层次和一定程度的正确认识，认识反映事物的深度是有限的 真理的绝对性和相对性的关系：辩证统一 辩证统一不是这个真理是绝对的，这个真理是相对的，而是每一个真理都有绝对性和相对性的一面。 相互依存 真理的绝对性是基于相对性的（一定条件下的），真理的相对性也是基于绝对性的（主观和客观的相符合）。 相互包含： 没有离开绝对真理的相对真理，也没有离开相对真理的绝对真理 真理的绝对性寓于真理的 相对性之中。 任何真理所包含的客观内容都只能是人们在特定条件下所把握到的，都是对客观世界及其事物的一定范围、一定程度的正确反映。 真理的相对性必然包含并表现着真理的绝对性。 真理都是对无限发展着的物质世界的正确认识，包含着正确的客观内容 无数相对的真理的总和，就是绝对的真理。 真理永远处在由相对向绝对的转化和发展中，是真理的相对性走向绝对性、接近绝对性的过程。 真理发展的规律就是真理的相对性无限接近绝对性。 真理的绝对性与相对性根源于人认识世界的能力的无限性（至上性）与有限性（非至上性）、绝对性与相对性的矛盾。 总体来看，人是能够完全认识世界的，是人认识世界的能力的无限性，对应真理的绝对性。 但现在的人囿于一定条件还没有完全认识世界，是人认识世界的能力的有限性，对应真理的相对性。 方法论： 教条主义：只看到真理的绝对性，忽视相对性。 诡辩论（怀疑主义）：只看到真理的相对性，忽视绝对性。 在运动和静止中，诡辩论也是夸大运动，忽视静止。认为在一直运动。 35.真理与谬误 真理与谬误的关系：对立统一 真理和谬误的对立只是在非常有限的范围内才具有绝对的意义，超出这个范围，二者的对立就是相对的。 相互区别：在确定的对象和范围内，真理与谬误的对立是绝对的，与对象相符合的认识就是真理，与对象不相符合的认识就是谬误。在确定条件下，真理和谬误存在着原则界限。 相互联系：在一定条件下，真理和谬误能相互转化。 真理和谬误在一定范围内的对立是绝对的，但超出一定范围，真理和谬误就会相互转化，真理变成谬误，谬误变成真理。 36.真理的检验标准 实践是检验真理的唯一标准这是由真理的本性和实践的特点决定的。 真理的本性：主观和客观相符合。 真理是人们对客观事物及其发展规律的正确反应，他的本性在于主观和客观相符合。 实践是连接物质和意识的桥梁，只有通过实践才能判断主客观是否相符合。 实践的本质特点：直接现实性。 实践是人们改造世界的客观的物质性活动，具有直接现实性的特点。 实践是标准，并不排斥逻辑证明的作用。 不排斥，但实践是唯一标准。 实践标准的确定性和不确定性 实践标准的确定性和不确定性是说实践标准既有确定性又有不确定性，而不是这个实践标准是确定的，另一个实践标准是不确定的。 实践标准的确定性 检验真理的唯一标准 不可推翻 即使当前不能检验，但最终能裁决 实践标准的不确定性 一定时期的实践受到主客观的制约具有局限性，不能完全证明或驳倒一切 实践检验真理不是一次完成的 已被检验的仍需接受再检验。 37.真理与价值的辩证统一 价值：价值是指在实践基础上形成的主体和客体之间的意义关系，是客体对个人、群体乃至整个社会的生活和活动所具有的积极意义。 价值：客体对…的意义 价值的特点 考：材料体现了价值的什么特点 客观性：客体对于主义的意义不依赖于主体的主观意识存在。 所有的客观性：不依赖于人的意识存在 主体性：主体不同，价值不同 主体性是指价值直接同主体相联系，始终以主体为中心 主体性不等同于主观性。 多维性：维度不同，价值不同。 社会历史性：历史时期不同，价值不同。 价值评价（价值判断）：主体对客体的价值以及价值大小所作的评判或判断。 认识分为两种，一种是知识性认识，一种是评价性认识。 知识性认识：以客体为对象，主要是是什么。 评价性认识：以主客体的关系为对象，比如主体喜不喜欢客体等。 价值评价的特点： 评价以主客体的价值关系为认识对象。 评价结果与评价主体直接相关。 评价结果的正确与否 依赖于对客体状况和主体需要的认识。 评价性认识依赖于知识性认识。 即知识越多，评价越好，评价认识越充分。 （充分了解后，再来说喜欢与否。） 价值评价的特点表明，评价并不是一种主观随意性的认识，而是具有客观性的认识活动。 评价作为一种价值评判活动，虽具有主观性，但不是一种主观随意性的认识，只有正确反映价值关系的评价才是正确认识。 对于任何价值评价的主体而言，其价值评价只有与人类整体的要求或理由相一致，才是正确的价值评价。 真理和价值在实践中的辩证统一 人们的实践活动总是受着真理尺度和价值尺度的制约。 实践的真理尺度是指在实践中人们必须遵循正确反映客观事物本质和规律的真理。 实践的价值是指在实践中人们都是按照自己的尺度和需要去认识世界和改造世界。 这一尺度体现了人的活动的目的性 任何实践活动都是在这两种尺度共同制约下进行的，任何成功的实践都是真理尺度和价值尺度的统一。 紧密联系、不可分割的辩证统一关系 价值尺度必须以真理为前提。真理又必然是具有价值的。 真理必然有价值，有价值的不一定是真理。 真理一定有用，有用的不一定是真理。 人类自身需要的内在尺度，推动着人们不断发现新的真理。 38.认识世界和改造世界必须勇于创新39.自由和必然 自由：人想怎样就怎样。 必然：客观规律，该怎样就怎样。 自由：标示人的活动状态的范畴，是指人在活动中 通过认识和利用必然 所表现出的一种自觉自主的状态。自由是对必然的认识和对客观世界的改造。 必然：必然性即规律性，指的是不依赖于人的意识而存在的自然和社会发展所固有的客观规律。 认识必然和争取自由，是人类认识世界和改造世界的根本目标，是一个历史性的过程。 由必然到自由的表现为人类不断从必然王国向自由王国发展的 历史。 以前只能顺应自然的一切规律。是必然王国。 现在通过认识和利用必然，能做一些人类想做的事，是向自由王国的发展。 自由是有条件的： 认识条件：要有对客观事物的正确认识，最主要的是对客观事物运动发展规律性和必然性的正确认识。 实践条件：能够将获得的规律性认识运用于指导实践，实现改造世界的目的，才是真正的自由。 唯物史观人类社会历史 社会发展的物质动因 人民群众创造历史 40.唯物史观和唯心史观的对立 唯心史观的缺陷： 只看到历史发展背后的精神力量，没有看到精神力量背后的 物质动因 和 经济根源 。 只看到历史发展中少数英雄人物的力量，而没有看到人们群众在社会历史发展中的决定作用。 41.社会存在和社会意识及其辩证关系 社会存在和社会意识是社会历史领域的物质和意识 社会存在：社会物质生活条件 自然地理环境：影响因素，非决定力量 人口因素：影响因素，非决定力量 考：是影响因素，而非决定力量 低级概念，夸大说是决定力量，拉低说是没有作用 物质生产方式（物质生活的生产方式，生产方式）：决定力量 是指人民为获取物质生活资料而进行的生产活动的方式，它是生产力和生产关系的统一体。 生产力 + 生产关系 = 生产方式 生产力 劳动资料：生产工具是生产力发展水平的标准 劳动对象：与劳动资料合成为生产资料 劳动者：生产力中最活跃的因素 生产关系 生产资料所有制关系：最基本内容 生产中人与人的关系 产品分配关系 社会意识：社会生活的精神方面，是社会存在的反映 根据不同的层次： 社会心理：低层次的社会意识，是自发的、不系统的、不定型的社会意识 表现为：人民的感知、情绪、情感、心态、习俗 社会意识形式：高层次的社会意识，是自觉的、系统的、定型的社会意识 社会意识形式以社会心理为基础，并对社会心理起指导和影响作用。 包括：政治法律思想、道德、艺术、宗教、哲学、科学，以理性认识为主。 社会意识形态是和阶级有关的，不同的阶级社会意识形态不同。 非社会意识形态和阶级无关，不同的阶级非社会意识形态相同。 社会意识形态：反映社会的经济关系、阶级关系的社会意识形式 包括：政治法律思想、道德、艺术、宗教、哲学 非社会意识形态：不具有社会经济形态和政治制度的性质 包括：自然科学、语言学、形式逻辑、心理学 社会存在和社会意识的辩证关系 社会存在和社会意识是辩证统一的。社会存在决定社会意识，社会意识是社会存在的反映。 社会存在决定社会意识 社会存在是社会意识内容的客观来源，社会意识是社会物质生活过程及其条件的主观反映。 物质是意识内容的客观来源，意识是物质的主观反映。 社会意识是人们进行社会物质交换的产物。 社会意识同语言一样，是在生产中由于交往活动的需要而产生的。 随着社会存在的发展，社会意识也相应地或早或迟地发生变化和发展。 考：将或早或迟改为一定立刻 社会存在发展，社会意识也在变，但不一定是一致的、同步的变化，而是不一致不同步。 社会意识反作用与社会存在 社会意识的相对独立性： 社会意识与社会存在发展的不完全同步性和不平衡性 社会意识和社会存在的发展不同步、不平衡，有些经济发展水平较高的国家社会意识水平未必很高。 社会意识内部各形式之间的相互作用 及 各自具有的历史继承性 社会意识形式中各要素相互作用社会意识。 但有些社会意识还具有历史继承性，比如一些封建残余观念。 社会意识对社会存在能动的反作用。（双向） 先进的社会意识促进社会发展，落后的社会意识阻碍社会发展。 42.生产力与生产关系矛盾运动的规律 生产力：人们解决社会同自然矛盾的实际能力，是人类在生产实践中形成的改造和影响自然以使其适合社会需要的物质力量 。 它表示人和自然的关系 生产力基本要素： 劳动资料（劳动手段）：人们在劳动过程中所运用的物质资料或物质手段 最重要的是生产工具，他是生产力发展水平的客观尺度，是区分社会经济时代的客观依据。 劳动对象 劳动资料+劳动对象 = 生产资料 劳动者 劳动者是生产力中最活跃的因素： 科学技术日益成为生产发展的决定性因素 注意： 科学技术属于生产力，但不是生产力的独立的基本要素，科学技术是与生产力的三个基本要素结合，发挥作用。 考：问生产力的因素还是生产力的独立要素/基本要素。 （正确）生产力在生产劳动中起决定作用。 （错误）生产力在社会历史中起决定作用。 科学技术是先进生产力的集中体现和主要标志，是第一生产力 生产关系：人们在物质生产过程中形成的不以人的意志为转移的经济关系。 不以人的意志为转移，生产关系是物质 生产关系是社会关系中最基本的关系，政治关系、家庭关系、宗教关系等其他社会关系，都受生产关系的支配和制约。 包括： 生产资料所有制关系 生产资料所有制是最基本、具有决定意义的方面。 它构成全部社会关系的基础，是区分不同生产方式、判定社会经济结构的客观依据。 生产工具：区分社会经济时代的客观依据。 生产资料所有制关系：判定社会经济结构的客观依据。 生产中人与人的关系 产品分配关系 生产关系是一种人和人的关系，但他在物质生产过程中结成的关系，是不以人的意志为转移。 生产力：人和自然的关系，是物质。 生产关系：人和人的关系，是物质。 生产力和生产关系的关系：不可分割 生产力和生产关系是社会生产不可分割的两个方面 在社会生产中：生产力是生产的物质内容，生产关系是生产的社会形式，二者的有机结合统一构成社会的生产方式。 在辩证法的内容和形式的范畴中： 内容决定形式，形式反作用与内容，当形式适合内容，对内容发展起积极作用，当形式不适合内容是，对内容发展起消极作用。 生产力决定生产关系 生产关系反作用与生产力（双向） 当生产关系适合生产力发展的客观要求时，对生产力的发展起推动作用；当生产关系不适合生产力发展的客观要求时，就会阻碍生产力的发展。 社会发展第一规律：生产关系一定要适应生产力发展状况的规律 形式要适应内容。 43.经济基础与上层建筑矛盾运动的规律 经济基础：是指 由社会一定发展阶段的生产力 所决定的生产关系的总和 经济基础是生产关系。 经济基础和生产关系是同级概念，生产力决定生产关系，生产力也决定 经济基础。 上层建筑：建立在一定的经济基础之上的 意识形态 以及 相应的制度、组织和设施 组成 区分： 观念上层建筑是无形的，政治上层建筑是有形的。 意识形态（观念上层建筑） 包括政治法律思想、道德、意识、宗教、哲学等思想观念。 辨析：社会意识中的意识形态和上层建筑中的意识形态 社会意识中的意识形态和上层建筑中的意识形态是同一个东西 社会意识： 社会心理 社会意识形式 社会意识形态 非社会意识形态 上层建筑： 意识形态（观念上层建筑） 政治上层建筑 属于上层建筑的意识形态是社会意识形态。 政治法律制度及设施 和 政治组织（政治上层建筑） 包括：国家政治制度、立法司法制度和行政制度，以及国家政权机构、政党、军队、警察、法庭、监狱等政治组织形态和设施 国家 国家不是从来就有的，而是社会发展到一定历史阶段的产物。 这种从社会中产生但又自居于社会之上并且日益同社会相异化的力量，就是国家。 产生： 国家是按照地域来划分国民 的，而不再以血缘关系来划分。 国家依靠强制性或暴力手段以及征收赋税来维系。 国家是阶级矛盾不可调和的产物 实质：一个阶级统治另一个阶级的工具，具有政治统治和社会管理职能的有组织的力量。 它是经济上占支配地位的阶级为维护其根本利益而建立起来的强制性暴力机关，以保证其在政治上也成为统治阶级。 国家和社会完全统一之日，也就是国家消亡之时。 国体：社会各阶级在国家的地位，国家政权掌握在哪个阶级手中。 政体：统治阶级实现其阶级统治的具体组织形式，统治阶级采取什么样的形式去组织自己的政权，实现自己的统治。 国体决定政体，政体服务于国体。 上层建筑中，政治上层建筑占主导地位，国家政权是核心。 政治上层建筑可以主导 观念上层建筑，决定法律思想等。 经济基础和上层建筑的关系中 辩证统一关系与生产力和生产关系的矛盾相同 经济基础决定上层建筑 上层建筑对经济基础具有反作用 反作用集中表现在 为自己的经济基础服务。上层建筑的反作用是巨大的，但不是无限的，它可以影响社会性质和历史进程，但不能决定历史发展的总趋势。 生产力 决定 生产关系，经济基础 决定 上层建筑。 生产关系和经济基础是同级概念，因此，生产力 可以 决定 上层建筑， 生产关系可以决定上层建筑。 上层建筑反作用的性质，取决于它所服务的经济基础的性质，归根到底取决于它是否有利于生产力发展 上层建筑这种反作用的后果可能有两种：当他 为适合生产力发展要求的经济基础服务时，就成为推动社会发展的进步力量；反之，当它为落后的经济基础服务时，就成为阻碍社会发展的消极力量。 辨析： 判断生产关系是否先进：看生产关系是否适应生产力的发展。 判断上层建筑是否先进：看上层建筑 所服务的经济基础 是否适应生产力的 发展。 如果生产关系适应生产力的发展，生产力是先进的，经济基础也适应生产力发展，是先进的，那上层建筑服务于先进的经济基础，上层建筑就是先进的，推动社会发展。 反之，上层建筑 为落后的经济基础服务，即经济基础不适应于生产力，上层建筑阻碍社会发展。 社会发展的第二规律：上层建筑一定要适应经济基础发展状况的规律。 44.社会形态更替的一般规律和特殊形式马克思、恩格斯揭示的生产力与生产关系矛盾运动的规律和经济基础与上层建筑矛盾运动的规律，是人类社会发展的一般规律。 社会形态：关于社会运动的具体形式、发展阶段和不同质态的范畴，是 同生产力发展一定阶段相适应的 经济基础与上层建筑的统一体。 社会形态 = 经济基础 + 上层建筑 总结： 生产方式 = 生产力 + 生产关系 社会形态 = 经济基础 + 上层建筑 生产资料 = 劳动资料 + 劳动对象 社会形态包括：社会的经济形态、政治形态和意识形态 社会形态 = 经济基础 + 上层建筑 经济基础 对应 社会的经济形态，上层建筑（观念上层建筑 和 政治上层建筑） 对应政治形态和意识形态。 马克思主义的社会形态范畴 深刻揭示了人类社会的本质结构及其发展的客观规律。 全面的： 社会形态的内容是全面的，既包括经济基础，又包括上层建筑，两者缺一不可，犹如“骨骼”和“血肉”。经济基础是社会的“骨骼系统”，上层建筑是社会的“血肉系统”，上层建筑不过是经济基础的政治和思想的表现形态。 骨骼 决定 血肉 具体的： 社会形态是具体的，不是抽象的。 历史的： 社会形态是历史的，有它产生、发展和灭亡的过程。 社会形态更替的统一性和多样性 社会形态更替的统一性： 社会历史可以划分为五种社会形态：原始社会、奴隶社会、封建社会、资本主义社会和共产主义社会（第一阶段是社会主义社会） 这五种社会形态的依次更替，是社会历史运动的一般过程和一般规律，表现社会形态更替的统一性。 社会形态更新的多样性： 有些国家在发展中经历了几种社会形态依次更替的典型过程，也有些国家在发展中超越了一个甚至几个社会形态而跨越式向前发展，甚至多种社会形态特征交叉渗透。 即使是同一种社会形态，在不同国家也会显现不同特点。 社会形态更替统一性与多样性辩证关系 列宁指出：世界历史发展的一般规律，不仅丝毫不排斥个别发展阶段在发展的形式或顺序上表现出的特殊性，反而是以此为前提的。 社会形态更替的必然性与人们的历史选择性 必然性，即客观必然性，是社会历史的客观规律。 人们的历史选择性，是能动的选择。 社会形态更替的必然性：社会形态依次更替的过程和规律是客观的，发展趋势是确定不移的。 社会形态更替归根结底是社会基本矛盾运动的结果。 生产力与生产关系矛盾运动的规律性，从根本上规定了社会形态更替的客观必然性。 人们的历史选择性： 社会发展的客观必然性造成了一定历史阶段社会发展的基本趋势，为人们的历史选择提供了基础、范围和可能性空间。 社会形态更替的过程也是一个主观能动性与客观规律性相统一的过程。 人们的历史选择性归根结底社会人们群众的选择性，取决于人们群众的根本利益： 取决于民族利益 取决于交往 取决于对历史必然性以及本民族特点的把握程度 社会形态更替的前进性与曲折性 否定之否定在社会形态更替上的应用。 社会形态的更替还表现为历史的前进性与曲折性、顺序性与跨越性的统一。 社会形态更替的前进性、顺序性：五种社会形态依次演进的基本趋势，其历史过程是一个“扬弃”的过程。 表明社会发展的总趋势是前进的。 社会形态的曲折性、跨越性：每一次社会制度的变革，无不经过曲折反复的斗争。 例题： （错误）社会发展过程与自然界演变过程一样都是自觉的。 社会发展过程是自觉的，能动的，有人们的历史选择性。而自然界演变过程，没有人的参与，是自发的。 辨析：自觉和自发 自觉和自发是标志人们行为活动的觉悟程度的一对范畴。 同动物相比，人类一切有意思的活动都是自觉的。自然界的规律都是自发的。 而就人的活动本身而言，自发是指人们在社会活动中盲目地为历史必然性所支配。 （正确）人类总体历史进程是不可超越的。 总体，是站在人类历史发展规律的角度，即社会更替的统一性，是不可超越的。 45.社会基本矛盾在历史发展中的作用：社会发展的根本动力 社会基本矛盾：生产力和生产关系、经济基础和上层建筑的矛盾 社会基本矛盾是社会发展的根本动力 她在历史发展中的作用主要表现在： 生产力是社会基本矛盾运动中最基本的动力因素，是人类社会发展和进步的最终决定力量。 社会基本矛盾特别是生产力和生产关系的矛盾，是“一切历史冲突的根源”，决定着社会中其他矛盾的存在和发展。 社会基本矛盾具有不同的表现形式和解决方式，并从根本上影响和促进社会形态的变化和发展。 46.阶级斗争在阶级社会发展中的作用 阶级斗争是社会基本矛盾在阶级社会中的直接表现，是阶级社会发展的直接动力。 47.社会革命在阶级社会发展中的作用 革命是实现社会形态更替的重要手段 48.改革的性质及其在社会发展中的作用 改革是推动社会发展的又一重要动力 49.科学技术在社会发展中的作用 科学技术革命是社会动力体系的一种重要动力 考：夸大科学技术的作用，说根本动力，贬低科学技术的作用，没有影响。 现代科技革命推动生产方式的变革 现代科技革命推动生活方式的变革 现代科技革命推动思维方式的变革 50.关于历史创造者的问题唯物史观和唯心史观的对立，在历史创造者问题上表现为群众史观和英雄史观的对立。 英雄史观：唯心史观从社会意识决定社会存在的基本前提，否认物质资料生产方式是社会发展的决定力量，抹杀人民群众的历史作用，宣扬少数英雄人物创作历史。 唯物史观考察历史创造者问题的原则： 唯物史观立足于 现实的人及其本质 来把握历史的创造者。 唯物史观立足 整体的社会历史过程来探究历史创造者问题。 唯物史观从社会历史发展的必然性入手来考察和说明历史创造者及其活动。 唯物史观从人与历史关系的不同层次 考察谁是历史的创造者。 人与历史的关系具有类与历史、群体与历史、个体与历史三层关系。 51.现实的人及其本质 唯物史观认为：人不是 抽象的而是现实的，现实的人，不是处在某种虚幻的离群索居和固定不变状态的人，而是处在现实的、可以通过经验观察到的、在一定条件下进行的发展过程的人。 人与动物相区别的层次上，人的本质在于劳动。 从人与人层相区别的层次上，人的本质是一切社会关系的总和。 马克思指出：人的本质不是单个人所固有的抽象物，在其现实性上，它是一切社会关系的总和。 有许多社会关系，而我是我的原因是，我是这些社会关系的这个节点。 人的本质是社会属性，而不是自然属性。 52.人民群众创造历史原理 人民群众：是一个历史范畴。 从质上看，人民群众是指一切对社会历史发展起推动作用的人，从量上看，人民群众是指社会人口中的绝大多数。 人民群众：推动顺应社会历史发展。 反动派：阻碍社会历史发展。 人民群众的最稳定的主体部分始终是从事物质资料生产的劳动群众及其知识分子。 人民群众创造历史的原因（为什么说人民群众创造历史，起决定作用） 人民群众是社会物质财富的创造者。 人类社会赖以存在和发展的基础是物质资料的生产方式。 人民群众是社会精神财富的创造者。 人民群众是社会变革的决定力量。 人民群众在创造社会财富的同时，也创造并改造着社会关系。生产关系的变革，社会制度的更替，最终取决于生产力的发展。但不会随着生产力的发展自发地实现和完成，而必须借助人民群众的力量。 人民群众既是先进生产力和先进文化的创造主体，也是实现自身利益的根本力量。 人民群众创造历史的活动收到一定社会历史条件的制约 经济条件：根本制约因素 政治条件 精神文化条件 人民群众创造历史原理的方法论： 马克思主义群众观点：坚信人民群众自己解放自己的观点，全心全意为人民服务的观点，一切向人民群众负责的观点，虚心向群众学习的观点。 群众路线：一切为了群众，一切依靠群众，从群众中来，到群众中去。 为什么要一切为了群众，一切依靠群众，从群众中来，到群众中去？ 考：方法论后面依靠的原理，即因为人民群众创造历史。 53.个人在社会历史中的作用唯物史观从人民群众创造历史这一基本前提出发，既明确了人民群众是历史的创造者，也不否认个人在历史上的作用。 历史人物对历史发展的具体过程始终起着一定的作用，有时甚至对历史事件的进程和结局发生决定性的影响，但不能决定历史发展的基本趋势。 个人是与人民群众一起创造历史。 反动派不能创造历史（历史是哲学范畴的历史） 政治经济学 简单商品经济（54-60） 价值是什么 价值如何衡量 价值如何表现 价值有何规律 发达商品经济（资本主义以后的商品经济） 自由竞争阶段 （重点）（61-70） 垄断阶段 简单商品经济马克思主义不仅揭示了人类社会发展的一般规律，而且揭示了资本主义社会发展的特殊规律。特别是马克思的劳动价值论和剩余价值论，科学揭示了资本主义生产方式的本质和资本主义剥削的秘密。 54.资本主义生产关系的产生和生产方式的形成资本主义生产关系的产生和资本主义生产方式的形成的过程，与商品经济的发展有着密不可分的关系。 商品经济的发展 简单商品经济：以生产资料私有制和个体劳动力为基础 资本主义商品经济：以生产资料私有制和雇佣劳动为基础，是商品经济的高级发达形态。 资本主义产生的途径 从小商品经济分化出来 从商人和高利贷者转化而成 资本原始积累的主要途径 用暴力手段剥夺农民土地 用暴力手段掠夺货币财富 通过资本主义革命和产业革命，最终建立起资本主义生产方式 55.价值是什么 商品经济：是以交换为目的而进行生产的经济形式 商品经济产生的历史条件： 存在社会分工 生产资料和劳动产品属于不同的所有者 商品的二因素：商品的使用价值和价值 商品：用来交换、能满足人的某种需要的劳动产品 使用价值：商品能满足人民某种需要的属性，即商品的有用性。 反映人与自然之间的物质关系。 是商品的自然属性 是一切劳动产品共有的属性 劳动产品一定有使用价值，而有些劳动产品，不是为了交换，不是商品，比如为了自己用，也具有使用价值。 任何有用的物品都具有使用价值，诸如空气，也具有使用价值。 使用价值构成社会财富的物质内容。 价值： 凝结在商品中的无差别的一般人类劳动，即人的脑力和体力的耗费。 价值的本质 是 劳动，是藏在商品里的抽象劳动。 因此，没有蕴含劳动的物品就没有价值，比如空气，没有劳动蕴含其中，本身存在的，就没有价值。 价值是商品特有的社会属性 本质上体现了生产者之间的社会关系 商品的价值是劳动创造的，实质是凝结在商品中的无差别的一般人类劳动，商品交换实际上是商品生产者之间互相交换劳动的关系，商品价值的本质体现了生产者之间一定的社会关系。 交换价值：首先表现为一种使用价值同另一种使用价值想交换的量的关系或比例，决定商品交换的比例的不是商品的使用价值，而是价值。 注意：商品的二因素只有使用价值和价值，交换价值是另一个概念。 交换价值，表面上是有用性比例的关系，但其实是商品背后蕴藏的劳动的比例关系。 价值是交换价值的基础，交换价值是价值的表现形式。 商品的价值和使用价值的关系：对立统一 对立性：商品的使用价值和价值是相互排斥的，二者不可兼得。 要获得商品的价值，就必须放弃商品的使用价值；要得到商品的使用价值，就不能得到商品的价值。 统一性：作为商品，必须同时具有使用价值和价值两个因素，二者缺一不可。 劳动的二重性 商品是劳动产品，生产商品的劳动可区分为具体劳动和抽象劳动。 具体劳动：生产一定使用价值的具体形式的劳动，即有用劳动。 形成商品的使用价值 抽象劳动：撇开一切具体形式的、无差别的一般人类劳动，即人的体力和脑力的消耗。 形成商品的价值实体 具体劳动和抽象劳动的关系：对立统一 具体劳动和抽象劳动是同一劳动的两种规定。任何一种劳动，既是特殊的具体劳动，又是一般的抽象劳动，这就是劳动的二重性。 正是劳动的二重性决定了商品的二因素。 一方面，具体劳动和抽象劳动在时间上和空间上是统一的，是商品生产者同一劳动过程的两个方面。 另一方面，具体劳动和抽象劳动又分别反映劳动的不同属性 具体劳动反映的是人与自然的关系，是劳动的自然属性 使用价值反映的是人与自然的物质关系，是商品的自然属性。 具体劳动决定商品的使用价值。 抽象劳动反映的是商品生产者的社会关系，是劳动的社会属性 价值本质体现了生产者之间的社会关系，是商品的社会属性。 抽象劳动决定商品的价值 因此，价值 就是 藏在商品里的抽象劳动。 56.价值如何衡量 决定商品价值量的不是生产商品的个别劳动时间，而只能是社会必要劳动时间。 社会必要劳动时间：在现有的社会正常的生产条件下，在社会平均的劳动熟练程度和劳动强度下制造的某种使用价值所需要的劳动时间。 是社会平均劳动时间。 商品的价值量与生产商品所耗费的劳动时间成正比，与劳动生产率成反比。 与生产商品所耗费的劳动时间成正比，指的是社会必要劳动时间。 与劳动生产率成反比，劳动生产率是劳动者生产使用价值的效率，生产效率越高，所耗费的劳动时间越少，商品价值量也就越低。 影响劳动生产率的因素包括： 劳动者的平均熟练程度、科学技术的发展程度及其在生产中的应用、生产过程的社会结合、生产过程的社会结合、生产资料的规模和效能以及自然条件等。 生产过程的社会结合，就是分工 劳动生产率的变化对商品价值总量的影响 商品价值总量 = 单位商品对价值量 * 相同时间生产商品的数量 单位商品的价值量 相同时间生产商品数量 商品价值总量 社会劳动生产率增加 减少 增多 不变 个别劳动生产率增加 不变 增多 增多 商品价值量同简单劳动与复杂劳动有密切的关系 简单劳动：不需要经过专门训练的培养的一般劳动者都能从事的劳动 复杂劳动：需要经过专门训练和培养，具有一定文化知识和技术专长的劳动者所从事的劳动 形成商品价值量的劳动是以简单劳动为尺度计量的，复杂劳动等于自乘的或多倍的简单劳动。 也就是说，少量的复杂劳动等于多量的简单劳动。在相同的劳动时间里，复杂劳动创造的价值大于简单劳动创造的价值。 在以私有制为基础的商品经济条件下，复杂劳动转化为简单劳动，不是商品生产者自觉计算出来的，而是在商品交换过程中自发实现的。 57.价值如何表现 通过交换表现价值 商品的价值形式的发展经历了四个阶段： 简单的或偶然的价值形式：1只绵羊 = 2把石斧 总和的或扩大的价值形式 一般的价值形式 货币形式 金银天然不是货币，货币天然是金银。 金银只是大自然之物，而货币天生就应该是金银来充当，因为金银具备了充当货币的优良特点。 其他东西充当一般等价物时，不能叫货币，只有金银充当一般等价物时才能叫货币。 货币：在长期交换过程中形成的固定地充当一般等价物的商品。 货币的五种基本职能： 价值尺度（最基本职能）：货币衡量和表现一切商品价值大小的作用 原因：货币也是商品，也有价值，可以衡量其他商品的价值。 特点：可以是观念上的货币。 衡量其他商品价值时，不一定有这么多货币，可以是头脑中想象的。 流通手段（最基本职能） ：货币作为商品交换的媒介 用货币可以交换其他商品。 必须是现实的货币 现实的货币，电子钱包也是现实的货币。 可以不足值 金银，可能由于磕碰，其价值可能不足它代表的值，但还是按照它代表的值流通。 经济现象：劣币驱逐良币 因为货币作为流通手段时，可以不足值，所以衍生出了纸币，纸币 = 0价值（忽略纸币的价值），纸币是一个工具，代替货币作为流通手段。 但纸币没有价值，所以纸币不能执行价值尺度的功能。 储藏手段：货币推出流通领域作为社会财富的一般代表被保存起来的职能 支付手段：货币被用来清偿债务或支付赋税、租金、工资 辨析： 支付手段 vs 流通手段 流通手段：现货交易，与商品交换。 支付手段：没有现货。 世界货币 货币的产生使整个商品世界分化为两级： 一极是各种各样的具体商品，分别代表不同的使用价值。 一极是货币，只代表商品的价值。 这样就使商品 内在的使用价值和价值的矛盾 发展成为 外在的商品和货币的矛盾。 一切商品只要转化为货币，商品使用价值和价值的矛盾就能得到解决，从而使商品的价值得到实现。 货币的出现有利于商品交换的困难性，但不能解决也不可能解决商品经济的基本矛盾。 58.价值规律及其作用 价值规律的基本内容： 商品的价值量由生产商品的社会必要劳动时间决定， 商品交换以价值量为基础，按照等价交换的原则进行。 价值规律的表现形式：是商品的价格围绕商品的价值自发波动。 决定商品价格的因素： 供给：影响因素 币值：影响因素 价值：决定因素 价值规律的作用： 积极作用： 自发地调节 生产资料和劳动力在社会各生产部门之间的分配比例。 自发地刺激社会生产力的发展 自发地调节社会收入的分配 消极后果： 导致社会资源浪费。 （价值规律自发调节社会资源在社会生产各部门的配置，可能出现比例失调的情况） 阻碍技术的进步。 （垄断的发生） 导致收入两极分化。 59.私有制基础上商品经济的基本矛盾 私人劳动和社会劳动的矛盾构成 私有制商品经济的基本矛盾（简单商品经济的基本矛盾）。 在以私有制为基础的商品经济中，商品生产者的劳动具有两重性，既是具有社会性质的社会劳动，又是具有私人性质的私人劳动。 是每一种劳动，既是私人劳动，又是社会劳动，角度不同。 从生产资料私有制来看，是私人劳动，从社会分工的角度看，又是社会劳动。 在商品经济条件下，每个生产者的劳动本身是私人劳动，而私人劳动要转化为社会劳动，就必须用自己的产品去同别人的产品交换。 交换是解决私人劳动和社会劳动之间矛盾的唯一途径。 私人劳动和社会劳动之间的矛盾 在资本主义制度下，进一步发展成资本主义的基本矛盾，即生产资料资本主义私人占有 和 生产社会化之间的矛盾，正是这一矛盾不断运动，才使资本主义制度最终被社会主义制度所代替具有了客观必然性。 60.马克思劳动价值论的意义 马克思继承了古典政治经济学劳动创造价值理论的同时，创立了劳动二重性理论。 劳动二重性理论称为理解政治经济学的枢纽 。 深化对马克思劳动价值论的认识 走进21世纪，面对新的情况，必须深化对马克思劳动价值论的认识，根据变化了的实践在继承的基础上有所创新、有所前进。 深化对创造价值的劳动的认识，对生产性劳动作出新的界定。 深化对科技人员、经营管理人员在社会生产和价值创造中所起的作用的认识 深化对科技、知识、信息等新的生产要素在财富和价值创造中的作用等认识。 深化对价值创造和价值分配关系对认识。 发达商品经济-自由竞争阶段 简单商品经济和发达商品经济的区别：劳动力成为商品 61.劳动力成为商品与货币转化为资本 劳动力：指人的劳动能力，是人的体力和脑力的总和。劳动力的使用即劳动。 劳动者：人。 劳动力：体力和脑力的总和。 劳动力成为商品的基本条件： 劳动者是自由人，能够把自己的劳动力当作自己的商品来支配。 劳动者没有别的商品可以出卖，自由得一无所有，没有任何实现自己的劳动力所必需的物质条件。 劳动力的价值：是由生产、发展、维持和延续劳动力所必需的生活必需品的价值决定的。 包括三个部分： 维持劳动者本人生存所必需的生活资料的价值 维持劳动者家属的生存所必需的生活资料的价值 劳动者接受教育和训练所支出的费用 劳动力价值的构成包含一个历史的和道德的因素。 劳动力价值和历史时期、地区有关。 劳动力商品在使用价值上有一个很大的特点：它的使用价值是劳动，而劳动又是普通商品价值的源泉。 劳动创造商品的价值。 而劳动力商品在消费的过程，就是在劳动的过程，就能够创造新的商品的价值。 货币所有者购买到这种特殊商品，能够增值，货币也就变成了资本。 劳动力商品在消费过程中能够创造新的价值，而且这个新的价值比劳动力本身的价值更大。 正是由于这一特点，货币所有者购买到劳动力以后，在消费过程中，不仅能够收回他在购买这种商品时支付的价值，还能得到一个增值的价值即剩余价值。而一旦货币购买的劳动力带来剩余价值，货币也就变成了资本。 劳动力成为商品，货币转化为资本。 62.资本主义所有制 资本家凭借对生产资料的占有，在等价交换原则的掩盖下，雇佣工人从事劳动，占有雇佣工人的剩余价值，这就是资本主义所有制的实质。 63.剩余价值的生产 资本主义生产过程 是劳动过程和价值增殖过程的统一。 剩余价值是在资本主义的生产过程中生产出来的。 资本主义的生产过程具有两重性，一方面是生产物质资料的劳动过程，另一方面是生产剩余价值的过程，即价值增殖过程。资本主义生产过程是劳动过程和价值增殖过程的统一。 工人在劳动，资本家在获得增值价值。 样例： 有一个做包子的资本家。 情况一： 生产资料：40元的面 工人工资：20元 工人将40元的面生产为60元的包子，需要花费4个小时。 此时，剩余价值m = 0 情况二： 生产资料：80元的面 工人工资：20元 工人将80元的面生产为120元的包子，需要花费8个小时。 此时，剩余价值m = 20元 注意，工人的工资，即工人劳动力的价值是由生产、发展、维持和延续劳动力所必需的生活必需品的价值决定的，即上述1、2、3点，与工人的工作时间、强度无关。 所以工人的工资仍然为20元。 剩余价值的生产——从劳动的方面来看 劳动： 具体劳动产生商品的使用价值，抽象劳动产生商品的价值 具体劳动的任务： 转移“面粉”的价值，到包子的“价值上” 将面的80元价值，转移到包子的价值中。 生产包子的使用价值 抽象劳动的任务：生产新价值 情况二： 本来只有80元价值的面，20元价值的劳动力，经过工人的劳动，20元价值的劳动力不变，但产生了120元价值的包子。 120元的价值中有具体劳动时转移面的80元价值，还有抽象劳动生产的40元的新的价值。 剩余价值的生产——从资本的方面 全部预付资本100元： 购买面粉的80元，借助具体劳动转移到最终产品中，不会增值。称为不变资本（C）。 不变资本：以生产资料形态存在的资本。 生产资料的价值通过工人的具体劳动被转移到新产品中，其转移到价值量不会大于它原有价值量，不发生增殖。 具体劳动的任务之一，转移生产资料的价值到商品的价值中去。 购买工人的20元。由工人的劳动再创造出来，并能够增值。称之为可变资本（V）。并能够带来剩余价值（M） 可变资本：用来购买劳动力的那部分资本。 可变资本的价值在生产过程中不是被转移到新产品中去，而是由工人的劳动再生产出来。 在生产过程中，工人所创造的新价值，不仅包括相当于劳动力价值的价值，而且还包括一定量的剩余价值。 商品价值中除了生产资料转移过来的价值，还有工人抽象劳动产生的新价值。 可变成本V = 20元。 剩余价值M = 20元 剩余价值率M‘ = M/V。衡量剥削程度。 剩余价值率M’ = 100% 剩余价值的生产——从时间的方面 全天工作8小时 8个小时工人创造40元的新价值，前4个小时创造20元，即工人的工资，后4个小时创造20元，为资本家劳动。 前4个小时。为自己劳动，创造劳动力价值，即工资，称之为必要劳动时间。 必要劳动时间：用来产生生产劳动力价值或可变资本的价值的时间。 辨析：必要劳动时间和社会必要劳动时间。 社会必要劳动时间决定商品的价值量。 必要劳动时间是工人为自己劳动，创造工人劳动力价值的时间。 后4个小时。为资本家劳动，创造剩余价值，称之为剩余劳动时间。 剩余劳动时间：生产剩余价值的劳动时间。 剩余价值既不是由全部资本创造的，也不是由不变资本创造的，而是由可变资本雇佣的劳动者创造的。 雇佣劳动者的剩余劳动是剩余价值的唯一源泉。 资本家对工人的剥削程度m’ = m/v = 剩余劳动/必要劳动 = 剩余劳动时间/必要劳动时间 资本家提高对工人剥削程度的方法：绝对剩余价值的方法和相对剩余价值的生产 绝对剩余价值：必要劳动时间不变的条件下，由于延长工作日的长度和提高劳动强度而生产的剩余价值。 在相同长的劳动时间内比以前消耗更多的脑力和体力，这和延长工作日并没有本质区别，因此，由提高劳动强度而产生剩余价值的方法是绝对剩余价值生产方法。 相对剩余价值：在工作日长度不变的条件下，通过缩短必要劳动时间而相对延长剩余劳动时间所生产的剩余价值。 资本家在调整必要劳动时间与剩余劳动时间的比例上下功夫，通常缩短必要劳动时间、相对延长剩余劳动时间的方法，增加剩余价值的生产。 缩短必要劳动时间是通过全社会劳动生产率的提高实现的。 社会生产率的提高（科学技术的革新），缩短了必要劳动时间，相对延长了剩余劳动时间。 辨析：绝对剩余价值 和 相对剩余价值 都延长剩余劳动时间。 绝对剩余价值的生产没有缩短必要劳动时间，相对剩余价值的生产由缩短必要劳动时间。 绝对剩余价值没有科学技术革新，相对剩余价值有科学技术的革新。 超额剩余价值：企业由于提高劳动生产率而使商品的个别价值低于社会价值的超额。 全社会劳动生产率的提高是资本家追逐超额剩余价值的结果。 相对剩余价值是资本家追逐超额剩余价值的结果。 单个资本家改进技术、改善管理的主动动机是追求超额剩余价值，但其客观后果则是整个社会各个生产部门的劳动生产率普遍提高，导致生活资料的价值下降和补偿劳动力价值的必要劳动时间缩短，而剩余劳动时间相对延长，整个资本家阶级普遍获得更多的相对剩余价值。 资本主义条件下的生产自动化是资本家获取超额剩余价值的手段，而雇佣工人的剩余劳动仍然是这种剩余价值的唯一源泉。 生产自动化：首先是不存在绝对的无人，其次，这些机器上也蕴含着资本家对制作机器工人的剥削。 64.资本的积累 资本积累：把剩余价值转化为资本，或者说，剩余价值的资本化。 资本主义再生产的特点：扩大再生产。 资本主义扩大再生产的源泉：资本积累 资本主义简单再生产：资本家获得剩余价值后，如果将其全部用于消费，则生产就在原有规模的基础上重复进行。 资本积累的本质：资本家不断利用无偿占有的工人创造的剩余价值，来扩大自己的资本规模，进一步扩大和加强对工人的剥削和统治。 资本积累的源泉：剩余价值 资本积累规模的大小取决于：对工人的剥削程度、劳动生产率的高低、所用资本和所费资本之间的差额（投资的钱和花掉的钱的差额）以及资本家垫付资本的大小。 资本的技术构成：由生产的技术水平所决定生产资料和劳动力之间的比例。 在买包子例子中，资本的技术构成为4斤面：1个工人 资本的价值构成：资本分为不变资本和可变资本，这两部分资本价值之间的比例。 在买包子的例子中，资本的价值构成为80元：20元 资本的不变资本，用来买生产资料。 资本的可变资本，用来买劳动力。 资本的有机构成：由资本技术构成决定并反映技术构成变化的资本价值构成。通常用c: v来表示。 资本的有机构成是 资本的价值构成。 前提条件 是资本的价值构成反映资本技术构成，即资本的技术构成改变，引起资本的价值构成改变。 如果资本的技术构成不变，但由于其他原因，资本的价值构成改变了，此时资本的价值构成就不是资本的有机构成。 在资本主义生产过程中，资本有机构成呈现不断提高趋势。 c: v，资本不断积累，c增大。 失业：资本的有机构成提高，可变资本相对量减少，资本对劳动力的需求日益相对地减少，结果就不可避免地造成大批工人失业，形成相对过剩人口。 资本的积累，造成失业。 资本积累的历史趋势是资本主义制度的必然灭亡和社会主义制度的必然胜利 资本积累 -&gt; 资本有机构成提高 -&gt; 相对剩余人口过剩（失业） -&gt; 贫富差距拉大（两级分化）-&gt; 资本主义灭亡 65.剩余价值的循环 产业资本在循环过程中要经历三个不同阶段，与此联系的是资本依次执行三种不同职能： 购买阶段，即生产资料和劳动力的购买阶段：产业资本执行的是货币资本的职能。 生产阶段，生产资料与劳动者相结合在一起从事资本主义生产的阶段：产业资本执行的是生产资本的职能。 售卖阶段，商品资本向货币资本转化的阶段：产业资本执行的是商品资本的职能。 产业资本运动的两个基本条件： 产业资本的三种职能形式必须在空间上并存。 产业资本必须按照一定比例同时存在于货币资本、生产资本和商品资本三种形式中。 产业资本的三种职能形式必须在时间上继起。 产业资本循环的三种职能形式的转化必须保持时间上的依次连续性。 资本的周转：资本是在运动中增殖的，资本周而复始、不断反复的循环。 影响资本周转快慢的因素有很多，关键的因素： 资本周转的时间 生产资本中固定资本和流动资本的构成 固定资本：资本家的资本分多次转移到最终的产品中，比如机器。 流动资本：资本家的资本一次转移到最终的产品中，比如做包子的面。 总结：划分资本 考：按照不同的维度判断该资本。 内容 依据 第一次划分 不变资本、可变资本 是否能增值（是否能产生剩余价值） 第二次划分 货币资本、生产资本、货币资本 资本执行的不同职能形式 第三次划分 固定资本、流动资本 资本的周转方式 社会再生产的核心问题是社会总产品的实现问题，即社会总产品的价值补偿和实物补偿问题 价值补偿：东西卖出去后，能收回钱。 实物补偿：东西卖出去后，为了后面的生产，需要补偿原材料。 马克思将社会总产品在物质上划分为两大类，在价值上划分为三个组成部分。 马克思，真的是大智慧啊！！！！ 社会总产品：社会在一定时期（通常为一年）所生产的全部物质资料的总和。 社会总产品的物质形态上，根据其最终用途可以区分为用于生产消费的生产资料和用于生活消费的消费资料 第一部类（Ⅰ）：由生产 生产资料的部门构成，其产品进入生产领域。 第二部类（Ⅱ）：由生产 消费资料的部门构成，其产品进入生活消费领域。 社会总产品在价值形态上，又叫社会总价值(商品价值构成) = c + v + m 产品中的生产资料的转移价值(c) 凝结在产品中的由工人必要劳动时间创造的价值(v) 凝结在产品中的由工人在剩余劳动时间创造的价值(m) 第一部类的社会总价值 = Ⅰ(c) + Ⅰ(v) + Ⅰ(m) 其中Ⅰ(c)的来源可以从第一部类中自我解决 而 Ⅰ(v) + Ⅰ(m) ，一个是工人生活必需品的消耗，一个是资本家的生活必需品的消耗（家属资本家赚到的钱都去用了），Ⅰ(v) + Ⅰ(m) 需要从第二部类中来。 第二部类的社会总价值 = Ⅱ(c) + Ⅱ(v) + Ⅱ(m) 其中Ⅱ(c)的来源需要从第一部类中来 Ⅱ(v) + Ⅱ(m)，则可以从第二部类中自我解决 需要满足Ⅰ(v) + Ⅰ(m) = Ⅱ(c)的平衡，这需要两部类的交换，当Ⅰ(v) + Ⅰ(m) &gt; Ⅱ(c)，就会造成资源浪费。 这两部类的生产都是在价值规律和剩余价值规律的作用下自发进行的，具有严重的盲目性，这就导致了这两大部类生产在规模上和结构上经常处于失衡状态。 这种失衡和脱节经常表现为生产过剩，以至于社会总产品的实现，即实物替换和价值补偿难以顺利进行，最严重的就是引发经济危机。 经济危机的发生，实际上是资本主义条件下以强制的方式 解决社会再生产的实现问题的途径。 这种强制性地恢复平衡，是以社会经济生活的严重混乱以及社会资源和财富的极大浪费为代价的。 66.工资与剩余价值的分配 在资本主义制度下，工人工资：劳动力的价值或价格 资本主义工资的本质：劳动力的价值或价格 工资表现为：“劳动的价格”或工人全部劳动的报酬 这就模糊了工人必要劳动和剩余劳动的界限，掩盖了资本主义的剥削关系。 概念 生产成本（成本价格）：不变资本和可变资本构成 利润：= 剩余价值 资本家并不把剩余价值看作可变资本的产物，而是把它看作全部垫付资本的产物或增长额。 平均利润率：利润平均化形成的 社会的平均利润率（行业间的） 资本主义生产的目的是获得利润。为了得到尽可能高的利润率和尽可能多的利润，不同生产部门的资本家之间必然展开激烈的竞争，大量资本必然从利润率低的部门转投到利润率高的部门，从而导致利润率平均化。 平均利润：按照平均利润率计算和获得的利润 在利润率平均化的过程中，形成了社会的平均利润率，按照平均利润率计算和获得的利润，叫做平均利润。 生产价格：商品价值的转化形式， = 生产成本 + 平均利润 在价值转化为生产价格的条件下，价值规律作用的形式发生了变化：商品不再以价值而是以生产价格为基础进行交换，市场价格的变动不再以价值为中心，而是以生产价格为中心。 从全社会看，整个资本家阶级获得的利润总额与雇佣工人所创造的剩余价值总额是相等的；从个别部门看，商品的生产价格同价值不一致，但从全社会来看，商品的生产价格总额和价值总额相等 超额利润： = 超额剩余价值（行业内企业间的竞争） 注意： 平均利润率是社会中行业间形成的，而超额利润是行业内企业之间形成的。 平均利润率的形成，并不影响企业革新获取超额利润。 在利润平均化规律作用下，产业资本家获得产业利润，商业资本家获得商业利润，银行资本家获得银行利润，土地所有者获得地租。 67.马克思剩余价值理论的意义 马克思通过分析剩余价值的生产、积累、流通以及分配，解释了剩余价值的运动规律 ，创立了剩余价值理论。 马克思在哲学上的两大成就：创立了唯物史观；形成了唯物辩证统一 马克思理论上的两大成就：创立了唯物史观；创立了剩余价值理论 剩余价值理论深刻 揭露了资本主义生产关系的剥削本质， 阐明了资产阶级与无产阶级之间阶级斗争的经济根源， 指出了无产阶级革命的历史必然性。 剩余价值理论是马克主义经济理论的基石，是无产阶级反对资产阶级、揭示资本主义制度剥削本质的锐利武器。 由于唯物史观和剩余价值的发现，社会主义由空想变为科学。 68.资本主义的基本矛盾与经济危机 生产资料资本主义私人占有和生产社会化之间的矛盾，是资本主义的基本矛盾。 这是生产力和生产关系之间的矛盾在资本主义社会的具体形式。 生产相对过剩是资本主义经济危机的本质特征。 经济危机的可能性是由货币作为支付手段和流通手段引起的。 资本主义经济危机爆发的根本原因是：资本主义的基本矛盾 这种基本矛盾具体表现为两个方面： 表现为生产无限扩大的趋势与劳动人民有支付能力的需求相对缩小的矛盾。 表现为个别企业内部生产的有组织性和整个社会生产的无政府状态之间的矛盾 经济危机一般包括四个阶段：危机、萧条、复苏和高涨 危机是经济危机周期的基本阶段。 69.资本主义的国家、政治制度及其本质 资本主义国家的职能是以服务于资本主义制度和资产阶级利益为根本内容的，是资产阶级进行政治统治的工具。 资本主义国家的职能包括对内对外两个基本方面， 即对内实行政治统治和社会管理 对外进行国际交往和维护国家安全及利益 资本主义国家的本质是：资产阶级进行阶级统治的工具。 资本主义的民主制度：“主权在民”、“天赋人权”、“分权制衡”、“社会契约”、“自由、平等、博爱” 资本主义法制：宪法是资本主义国家法律制度的核心 依据的基本原则： 私有制原则 “主权在民”原则 分权与制衡原则 人权原则 资本主义国家政权：分权制衡的组织形式，即国家的立法权、行政区、司法权分别由三个权力主义独立行使。 资本主义政治制度的局限性： 资本主义的民主是金钱操纵下的民主，实际是资产阶级精英统治下的民主。 法律名义上的平等掩盖着事实上的不平等。 资本主义国家的政党制是一种维护资产阶级统治的政治制度。 资本主义多党制仍然是资产阶级选择自己的国家管理者、实现其内部利益平衡的政治机制。 政党恶斗相互掣（che）肘时，决策效率低下，激化社会矛盾。 70.资本主义的意识形态及其本质 资本主义国家意识形态的本质： 资本主义意识形态是资本主义社会条件下的观念上层建筑，是为资本主义社会形态的经济基础服务的。 资本主义意识形态是资产阶级的阶级意识的集中体现。 发达商品经济-垄断阶段71.资本主义从竞争到垄断 资本主义的发展经历两个阶段： 自由竞争资本主义 垄断资本主义 19世纪20世纪初，垄断取代自由竞争在资本主义经济中占据统治地位。 垄断资本主义的发展包括两种形式：私人垄断资本主义和国家垄断资本主义 垄断：少数资本主义大企业，为了获得高额利润，通过互相协议或联合，对一个或几个部门商品的生产、销售和价格进行操作和控制。 垄断的形成方式 自由竞争引起生产集中和资本集中，生产集中和资本集中发展到一定阶段必然引起垄断，这是资本主义发展的客观规律。 生产集中：生产资料、劳动力和商品的生产日益集中于少数大企业的过程，其结果是大企业所占的比重不断增加。 资本集中：大资本吞并小资本，或由许多小资本合并而成大资本的过程，其结果是越来越多的资本为少数大资本家所支配。 钱集中。 垄断的形成原因： 获得高额利润 形成竞争限制 避免两败俱伤 垄断组织的本质：通过联合实现独占和瓜分商品生产和销售市场，操作垄断价格，以攫（jue）取高额垄断利润。 垄断条件下的竞争 垄断资本主义阶段存在竞争的主要原因： 垄断没有消除产生竞争的经济条件（私有制） 垄断必须通过竞争来维持 不存在由一个垄断组织囊括一切部门、一切社会生产的绝对垄断 垄断条件下的竞争同自由竞争相比，具有的新特点 ： 竞争的目的： 自由竞争主要是为获得更多的利润或超额利润，不断扩大资本的积累 垄断条件下的竞争则是为获取高额垄断利润，并不断巩固、扩大已有的垄断地位 竞争手段： 自由竞争主要运用经济手段，如通过改进技术、提高劳动生产率、降低产品成本来战胜对手 垄断条件下的竞争不仅采取经济手段还采取非经济手段，使经济更加复杂、激烈 竞争范围： 自由竞争时期，竞争主要在经济领域，而且主要是国内市场上进行 垄断时期，国际市场上的竞争规模扩大，范围遍及各个领域和部门，并由国内扩展到国外 金融资本：工业垄断资本和银行垄断资本融合在一起而形成的一种垄断资本 金融资本形成的主要途径：金融联系、资本参与和人事参与 金融寡头：操纵国民经济命脉，并在实际上控制国家政权的少数垄断资本家或垄断资本家集团 经济领域的统治：通过”参与制“实现的。 所谓参与制，即金融寡头通过掌握一定数量的股票来层层控制企业的制度 政治上的统治（金融寡头对国家机器的控制）：通过同政府的”个人联合“来实现的。 这种联合有多种途径： 金融寡头直接出马把自己的代理人送进政府或议会，掌握政权，利用政治力量为其垄断统治服务 收买政府高官或国会议员，让他们在其政治活动中为金融寡头的利益服务 聘请曾在政府任职的高官到公司担任高级职务 建立政策咨询机构等方式对政府的政策施加影响，掌握新闻出版、广播电视、科学教育、文化体育等上层建筑的各个领域，左右国家的内政外交及社会生活。 垄断利润：垄断资本家凭借其在社会生产和流通中的垄断地位而获得的超过平均利润的高额利润。 垄断利润的来源：归根到底来自无产阶级和其他劳动人民所创造的剩余价值 来自对本国无产阶级和其他劳动人民剥削的加强 通过控制市场占有其他企业特别是非垄断企业的利润 通过加强对其他国家劳动人民对剥削和掠夺获取的国外利润 通过资本主义国家政权进行有利于垄断资本的再分配，从而将劳动人民创造的国民收入的一部分变成垄断资本的收入。 垄断利润的实现：垄断利润主要是通过垄断组织制定的垄断价格实现的 垄断价格：垄断组织在销售或购买商品时，凭借其垄断地位规定的、旨在保证获取最大限度利润的市场价格。 垄断价格 = 成本价格 + 平均利润 + 垄断利润 垄断价格包括垄断高价和垄断低价 垄断高价：垄断组织出售商品时规定的高于生产价格的价格 垄断低价：垄断组织在购买非垄断企业所生产的原材料等生产资料时规定的低于生产价格的价格 垄断价格的产生并没有否定价值规律，它是价值规律在垄断资本主义阶段作用的具体表现。 商品的价格围绕着商品的垄断价格自发波动 价值规律的基本内容不变 表现形式： 简单商品经济：商品的价格围绕着商品的价值自发波动 自由竞争的资本主义商品经济：商品的价格围绕着商品的生产价格自发波动 垄断资本主义阶段：商品的价格围绕着商品的垄断价格自发波动 72.垄断资本主义的发展 国家垄断资本主义：国家政权和私人垄断资本融合在一起的垄断资本主义 国家垄断资本主义的形成原因： 首先，（根本原因）社会生产力的发展，要求资本主义生产资料在更大范围内被支配，从而促进了国家垄断资本主义的产生 其次，经济波动和经济危机的深化，要求国家垄断资本主义的产生。 最后，缓和社会矛盾、协调利益关系，也要求国家垄断资本主义的产生。 国家垄断资本主义的主要形式： 国家所有并直接经营的企业 国家与私人共有、合营企业 国家通过多种形式参与私人垄断资本的再生产过程，包括国家作为商品和劳务的采购者，向私人垄断企业订货、提供补贴 宏观调节：国家运用财政政策、货币政策等经济手段，对社会总供给和总需求进行调节 目标：经济快速增长、充分就业、物价稳定和国际收支平衡 微观规制 ：国家运用法律手段规范市场秩序，限制垄断，保护竞争、维护社会公众的合法利益 目标：规范市场秩序，限制垄断，保护竞争、维护社会公众的合法利益 类型：其一是反托拉斯法（反垄断法）；其二是公共事业规制；其三是社会经济规制 对国家垄断资本主义的评价 国家垄断资本主义是垄断资本主义的新发展，它对资本主义经济的发展产生了积极的作用 但是，国家垄断资本主义的出现并没有改变垄断资本主义的性质。 国家垄断资本主义的出现是资本主义经济制度内的经济关系调整，并没有从根本上消除资本主义的基本矛盾。 金融垄断资本的发展 金融自由化和金融创新是金融垄断资本得以形成和壮大的重要制度条件 垄断资本主义的金融化程度不断提高：（体现在） 金融业在国民经济中的地位大幅上升 实体经济的资本利润率下降 造业就业人数严重减少 虚拟经济越来越脱离实体经济 金融垄断资本的发展，一方面促进了资本主义的发展，另一方面也造成了经济过度虚拟化，导致金融危机频繁发生，不仅给资本主义经济，也给全球经济带来灾难。 垄断资本在世界范围的扩展 垄断资本向世界范围扩展到经济动因： 将国内过剩的资本输出 将部分非要害技术转移到国外 争夺商品销售市场 确保原材料和能源的可靠来源 垄断资本向世界范围扩展到基本形式： 借贷资本输出 生产资本输出 商品资本输出 输出资本的来源：一是私人资本输出；二是国家资本输出。 经济社会后果：对资本输出国来说是有利的，对资本输入国来说是一把双刃剑。 国际垄断同盟：在经济上瓜分世界是通过垄断组织间的协议实现的，而协议的订立、瓜分的结果又以经济实力为后盾和基础。 早期的国际垄断同盟主要是国际卡特尔。 当代的国际垄断同盟的形式以跨国公司和国家垄断资本主义的国际联盟为主。 国家垄断资本主义的国际联盟：是由一些资本主义国家的政府出面缔结协定所组成的国际经济集团，如西方七国集团、欧盟。 第二次世界大战后，从事国际经济协调、维护国际经济秩序的国际性协调组织主要有三个： 国际货币基金组织 世界银行 世界贸易组织 评价： 垄断国际化条件下各种形式的国际垄断组织、国际垄断同盟和国际经济协调机构的发展，在一定程度上促进了经济全球化的发展，但它们从根本上说是为了维护资产阶级的利益、为他们攫取高额垄断利润服务的。 垄断资本主义的五个基本特征： 垄断组织在经济生活中起决定作用 在金融资本的基础上形成金融寡头的统治 资本输出有了特别重要的意义 瓜分世界的资本家国际垄断同盟已经形成 最大资本主义列强已把世界上的领土分割完毕 73.经济全球化及其后果 经济全球化的表现 国际分工进一步分化 贸易的全球化 金融的全球化（资本） 企业经营的全球化 导致经济全球化迅猛发展的因素： 科学技术的进步和生产力的发展（根本因素） 跨国公司的发展 各国经济体制的变革 经济全球化的影响：“双刃剑” 消极后果： 发达国家与发展中国家在经济全球化过程中的地位和收益不平等、不平衡 加剧了发展中国家资源短缺和环境污染恶化 一定程度上增加经济风险 科学社会主义略","link":"/2021/01/16/Marxism/"}],"tags":[{"name":"Machine-Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Gradient-Descent","slug":"Gradient-Descent","link":"/tags/Gradient-Descent/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"Backpropagation","slug":"Backpropagation","link":"/tags/Backpropagation/"},{"name":"open-classes","slug":"open-classes","link":"/tags/open-classes/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"公开课","slug":"公开课","link":"/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"Logistic Regression","slug":"Logistic-Regression","link":"/tags/Logistic-Regression/"},{"name":"Softmax","slug":"Softmax","link":"/tags/Softmax/"},{"name":"Cryptography","slug":"Cryptography","link":"/tags/Cryptography/"},{"name":"Tools","slug":"Tools","link":"/tags/Tools/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"GitHub","slug":"GitHub","link":"/tags/GitHub/"},{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"Algorithms","slug":"Algorithms","link":"/tags/Algorithms/"},{"name":"String","slug":"String","link":"/tags/String/"},{"name":"Data-Structure","slug":"Data-Structure","link":"/tags/Data-Structure/"},{"name":"Gradient","slug":"Gradient","link":"/tags/Gradient/"},{"name":"Math","slug":"Math","link":"/tags/Math/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"StreamCipher","slug":"StreamCipher","link":"/tags/StreamCipher/"},{"name":"DEEPLIZARD","slug":"DEEPLIZARD","link":"/tags/DEEPLIZARD/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"Information-Theory","slug":"Information-Theory","link":"/tags/Information-Theory/"},{"name":"error","slug":"error","link":"/tags/error/"},{"name":"Array","slug":"Array","link":"/tags/Array/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"HTML","slug":"HTML","link":"/tags/HTML/"},{"name":"CSS","slug":"CSS","link":"/tags/CSS/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"LSTM","slug":"LSTM","link":"/tags/LSTM/"},{"name":"Semi-supervised","slug":"Semi-supervised","link":"/tags/Semi-supervised/"},{"name":"Intro-to-Algorithms","slug":"Intro-to-Algorithms","link":"/tags/Intro-to-Algorithms/"},{"name":"Sort","slug":"Sort","link":"/tags/Sort/"},{"name":"DNN","slug":"DNN","link":"/tags/DNN/"},{"name":"Unsupervised","slug":"Unsupervised","link":"/tags/Unsupervised/"},{"name":"PCA","slug":"PCA","link":"/tags/PCA/"},{"name":"Unsupervised-learning","slug":"Unsupervised-learning","link":"/tags/Unsupervised-learning/"},{"name":"Word Embedding","slug":"Word-Embedding","link":"/tags/Word-Embedding/"},{"name":"政治","slug":"政治","link":"/tags/%E6%94%BF%E6%B2%BB/"},{"name":"马克思主义","slug":"马克思主义","link":"/tags/%E9%A9%AC%E5%85%8B%E6%80%9D%E4%B8%BB%E4%B9%89/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"机器学习-李宏毅","slug":"机器学习-李宏毅","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"Cryptography-Dan","slug":"Cryptography-Dan","link":"/categories/Cryptography-Dan/"},{"name":"Tools","slug":"Tools","link":"/categories/Tools/"},{"name":"LeetCode","slug":"LeetCode","link":"/categories/LeetCode/"},{"name":"PyTorch","slug":"PyTorch","link":"/categories/PyTorch/"},{"name":"Information-Theory","slug":"Information-Theory","link":"/categories/Information-Theory/"},{"name":"算法导论","slug":"算法导论","link":"/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/"},{"name":"政治","slug":"政治","link":"/categories/%E6%94%BF%E6%B2%BB/"}]}