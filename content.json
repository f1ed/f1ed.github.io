{"pages":[],"posts":[{"title":"Adagrag-demo","text":"实现这篇文章中前面两个tips。 实现了tip1 Adagrad + tip2 Stochastic Gradient Descent demo代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869################## 2020/03/06 ## Adagrad demo ##################import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model# datax_data = [[338.], [333.], [328.], [207.], [226.], [25.], [179.], [60.], [208.], [606.]]y_data = [640., 633., 619., 393., 428., 27., 193., 66., 226., 1591.]# coordinatex = np.arange(-200, -100, 1)y = np.arange(-5, 5, 0.1)Z = np.zeros((len(y), len(x)))# cal the Loss of every point(function)for i in range(len(x)): for j in range(len(y)): b = x[i] w = y[j] for k in range(len(x_data)): Z[j][i] += (y_data[k] - b - w * x_data[k][0])**2# initialb = -120w = -4lr = 1 # learning rateiteration = 100000# record the iterationb_his = [b]w_his = [w]# Adagradb_grad_sum2 = 0.0w_grad_sum2 = 0.0for i in range(iteration): for k in range(len(x_data)): b_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-1) w_grad = 2 * (y_data[k] - b - w * x_data[k][0]) * (-x_data[k][0]) b_grad_sum2 += b_grad**2 w_grad_sum2 += w_grad**2 b = b - lr / np.sqrt(b_grad_sum2) * b_grad w = w - lr / np.sqrt(w_grad_sum2) * w_grad b_his.append(b) w_his.append(w)# sklearn linear modelreg = linear_model.LinearRegression()print(reg.fit(x_data, y_data))print(reg.coef_[0])print(reg.intercept_)# display the figureplt.contourf(x, y, Z, 50, alpha=0.5, cmap=plt.get_cmap('jet'))plt.plot(reg.intercept_, reg.coef_, 'x', ms=13, lw=1.5, color='orange')plt.plot(b_his, w_his, 'o-', ms=3, lw=1.5, color='black')plt.xlim(-200, -100)plt.ylim(-5, 5)plt.xlabel('$b$', fontsize=16)plt.ylabel('$w$', fontsize=16)# plt.show()plt.savefig(\"Loss.png\") Loss 迭代图画出的图片很直观","link":"/2020/03/09/Adagrad-demo/"},{"title":"「机器学习-李宏毅」：Backpropagation","text":"这篇文章中，讲解了Deep Learning中使用的一种高效Gradient Descent的算法：BackPropagation。BackPropagation通过正向传播和反向传播两个阶段，最后能一起算出损失函数对每一个参数的gradient。 Gradient Descent在Neural Network中，参数的更新也是通过Gradient Descent。 但是当Neural Network层数很深，结构很复杂的时候，会有millions of parapmeters。 Backpropagation：To compute the gradient efficiently. Chain RuleBP中需要用到的数学知识：微积分中的链式法则。 Backpropagation 在NN中，定义损失函数 $L(\\theta)=\\sum_{n=1}^{N} C^{n}(\\theta)$ （$\\theta$ 代指NN中所有的weight 和bias） 对某一参数的gradient为 $\\frac{\\partial L(\\theta)}{\\partial w}=\\sum_{n=1}^{N} \\frac{\\partial C^{n}(\\theta)}{\\partial w}$ 在上图NN中，我们先只研究红框部分，即是以下结构： z：每个activation function的输入。 根据链式法则， $\\frac{\\partial C}{\\partial w}= \\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}$ . 要计算每个参数的 $\\frac{\\partial C}{\\partial w}$ ，分为两部分。 Forward pass: compute $\\frac{\\partial z}{\\partial w} $ for all parameters. Backward pass: compute $\\frac{\\partial C}{\\partial z} $ for all activation function inputs z. BP：Forward passCompute $\\frac{\\partial z}{\\partial w} $ for all parameters. 还是只看上图这一部分，可以轻易得出： $\\partial{z}/\\partial{w_1}=x_1\\qquad \\partial{z}/\\partial{w_2}=x_2$ 得到结论： $\\frac{\\partial z}{\\partial w} $ 等于 the value of the input connected by the weight. 【$\\frac{\\partial z}{\\partial w} $ 等于 连接w的输入的值】 那么，如何计算出NN中全部的 $\\frac{\\partial z}{\\partial w} $ ？ ：Forward pass. 用当前参数（w,b) 从hidden layer的第一层开始，计算出第一层的输出，即第二层的输入。 依次相前计算，计算出每一层的输出，即下一层的输入，即输入所连接权重的 $\\frac{\\partial z}{\\partial w}$ 。 BP：Backward passCompute $\\frac{\\partial C}{\\partial z} $ for all activation function inputs z. z：activation function的 input a：activation function的 output 这里的activation function 是 sigmod函数 $a=\\sigma(z)=\\frac{1}{1+e^{-z}}$ 要求 $\\frac{\\partial C}{\\partial z}$ ， 再根据链式法则： $\\frac{\\partial C}{\\partial z}=\\frac{\\partial a}{\\partial z}\\frac{\\partial C}{\\partial a}$ 求 $\\frac{\\partial{a}}{\\partial{z}}$ : $\\frac{\\partial{a}}{\\partial{z}}=\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$ （是其他activation function 也能轻易求出） 求 $\\frac{\\partial C}{\\partial a}$ ：根据链式法则： $\\frac{\\partial C}{\\partial a}=\\frac{\\partial z^{\\prime}}{\\partial a} \\frac{\\partial C}{\\partial z^{\\prime}}+\\frac{\\partial z^{\\prime \\prime}}{\\partial a} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}$ $\\frac{\\partial z^{\\prime}}{\\partial a} =w_3$ ， $\\frac{\\partial z^{\\prime\\prime}}{\\partial a} =w_4$ $\\frac{\\partial C}{\\partial z^{\\prime}}$ 和 $\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$ ？假设，已经通过某种方法算出这个值。 $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ 这个式子，可以画成一个反向传播的NN，见下图。 $\\frac{\\partial C}{\\partial z^{\\prime}},\\frac{\\partial C}{\\partial z^{\\prime\\prime}}$ 是这个neuron的输入， $w_3,w_4$ 仍然是 neuron的 weight（无bias）。 $\\sigma’(z)$ 是一个常数，因为在forward pass中每一个activation的输入已经被算出来了。 和forward pass中的NN的区别是，forward 中是一个activation function，输入z作用于这个函数； 而在 backward pass中，这更像一个放缩器，将他的输入变小，即乘上一个 $\\sigma’(z)$ 。 问题还是如何计算 $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ ？ 分为两种情况讨论， $z’,z’’$ 是否为输出层的输入？ Output Layer： z’,z’’：activation function的输入。 y1,y2：actiavtion function（也是NN）的输出。 C：NN输出和target的cross entropy。 根据链式法则： $\\frac{\\partial C}{\\partial z^{\\prime}}=\\frac{\\partial y_{1}}{\\partial z^{\\prime}} \\frac{\\partial C}{\\partial y_{1}} \\quad \\frac{\\partial C}{\\partial z^{\\prime \\prime}}=\\frac{\\partial y_{2}}{\\partial z^{\\prime \\prime}} \\frac{\\partial C}{\\partial y_{2}}$ 所以，已知activation function（simod或者其他），可以轻易求出 $\\frac{\\partial y_{1}}{\\partial z^{\\prime}}(=\\sigma'(z'))$ 和 $\\frac{\\partial y_{2}}{\\partial z^{\\prime\\prime}}(=\\sigma''(z''))$ 。 所以，已知损失函数，也可以轻易求出 $\\frac{\\partial C}{\\partial y_1}$ 和 $\\frac{\\partial C}{\\partial y_2}$ 。（ $C\\left(y, \\hat{y}\\right)=-\\left[\\hat{y} \\ln y+\\left(1-\\hat{y}\\right) \\ln \\left(1-y\\right)\\right]$ ) 所以，可以直接求出 $\\frac{\\partial C}{\\partial z}=\\sigma^{\\prime}(z)\\left[w_{3} \\frac{\\partial C}{\\partial z^{\\prime}}+w_{4} \\frac{\\partial C}{\\partial z^{\\prime \\prime}}\\right]$ 。 Not Output Layer: 上图中，如果我们要计算 $\\frac{\\partial C}{\\partial z’}$ ，必须要已知下一层的 $\\frac{\\partial C}{\\partial z_a}$ ，然后一直递归下去，直到到达最后的输出层，也就是上面一种情况，可以直接计算出，再递归回来，计算当前层的 $\\frac{\\partial C}{\\partial z’}$ 。 但是，这样计算每个参数的 $\\frac{\\partial{C}}{\\partial{z}}$ 都要一直递归到输出层，效率显然太低了。 计算方法如上图： 当我们已知输出层的 $\\frac{\\partial{C}}{\\partial{z'}},\\frac{\\partial{C}}{\\partial{z''}}$ 时，再通过上面的步骤3（且的确算出了 $\\frac{\\partial{C}}{\\partial{z'}},\\frac{\\partial{C}}{\\partial{z''}}$ ），画成反向的NN，计算$\\frac{\\partial{C}}{\\partial{z}}$. 再依次反向传播计算出每一个neuron的输出z （也是正向传播neuron的输入）的 $\\frac{\\partial{C}}{\\partial{z}}$ . Backforward pass 的做法： 先计算出输出层的 $\\frac{\\partial{C}}{\\partial{z}}$ （也就是上图的 $\\frac{\\partial{C}}{\\partial{z_5}}$ 和 $\\frac{\\partial{C}}{\\partial{z_6}}$ ） 用反向传播的NN，向后依次计算出每一层每一个neuron的 $\\frac{\\partial{C}}{\\partial{z}}$ 。 Summary 公式： $\\frac{\\partial z}{\\partial w} \\frac{\\partial C}{\\partial z}=\\frac{\\partial C}{\\partial w}$ 在正向传播NN中，z是neuron的activation function的输入。 在反向传播NN中，z是neuron的放缩器的输出。 通过Forward Pass计算出正向传播NN的每一个neuron的 $\\frac{\\partial z}{\\partial w}$ ，等于该层neuron的输入。 通过Backward Pass计算出反向传播NN的每一个neuron的 $\\frac{\\partial C}{\\partial z}$ 。 然后，通过相乘，计算出每个参数的 $\\frac{\\partial C}{\\partial w}$。 Reference","link":"/2020/04/18/Backpropagation/"},{"title":"「机器学习-李宏毅」:Classification-Generative Model","text":"Classification 有Generative Model和Discriminative Model。这篇文章主要讲述了用生成模型来做分类的原理及过程。 What is Classification?分类是什么呢？分类可以应用到哪些场景呢？ Credit Scoring【贷款评估】 Input: income, savings, profession, age, past financial history …… Output: accept or refuse Medical Diagnosis【医疗诊断】 Input: current symptoms, age, gender, past medical history …… Output: which kind of diseases Handwritten character recognition【手写数字辨别】 Input： Output：金 Face recognition 【人脸识别】 Input: image of a face output: person Classification：Example Application【图】 如上图，Pokemon又来啦！ Pokemon有很多属性，比如皮卡丘是电属性，杰尼龟是水属性之类。 关于Pokemon的Classification：Predict the “type” of Pokemon based on the information Input：Information of Pokemon (数值化） Output：the type Training Data: ID在前400的Pokemon Testing Data: ID在400后的Pokemon Classification as Regression?1. 简化问题，只考虑二分类：Class 1 ， Class2。 如果把分类问题当作回归问题，把类别数值化。 在Training中： Class 1 means the target is 1; Class 2 means the target is -1. 在Testing中：如果Regression的函数值接近1，说明是class 1；如果函数值接近-1，说明是class 2. Regression：输入信息只考虑两个特征。 Model：$y=w_1x_1+w_2x_2+b$ 当Training data的分布如上图所示时，得到的（最优函数）分界线感觉很合理。 但当Training data在右下角也有分布时（如右图），训练中为了减少error，训练得到的分界线会变成紫色的那一条。 所以，如果用Regression来做Classification：Penalize to the examples that are “too correct” .[1] 训练中会因为惩罚一些“过于正确”（即和我们假定的target离太远）的example，得到的最优函数反而have bad performance. 2. 此外，如果用Regression来考虑多分类。 Multiple class: Class 1 means the target is 1; Class 2 means the target is 2; Class 3 means the target is 3…… 如果用上面这种假设，可以认为Class 3和Class 2 的关系更近，和Class 1的关系更远一些。但实际中，可能这些类别have no relation。 Classification: Ideal Alternatives在上面，我们假设二元分类每一个类别都有一个target，结果不尽人意。 如上图所示，将模型改为以上形式，也可以解决分类问题。（挖坑）[2] Generative Model(生层模型)Estimate the Probabilities用概率的知识来考虑分类这个问题，如下图所示，有两个两个类别，C1和C2。 在Testing中，如果任给一个x，属于C1的概率是（贝叶斯公式） $$ P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} $$ 所以在Training中知道这些： $P(C_1),P(x|C_1),P(C_2),P(x|C_2)$ P(C1)和P(C2)很容易得知。 而P(x|C1)和P(x|C2)的概率应该如何得知呢？ 如果能假设：类别是C1中的变量x服从某种分布，如高斯分布等，即可以得到任意P(x|C1)的值。 所以Generative Model：是对examples假设一个分布模型，在training中调节分布模型的参数，使得examples出现的概率最大。（极大似然的思想） Prior Probabilities（先验概率）先只考虑Water和Normal两个类别。 先验概率：即通过过去资料分析得到的概率。 在Pokemon的例子中，Training Data是ID&lt;400的水属性和一般属性的Pokemon信息。 Training Data：79 Water，61 Normal。 得到的先验概率 P(C1)=79/(79+61)=0.56, P(C2)=61/(79+61)=0.44。 Probability from Class先只考虑Defense和SP Defense这两个feature。 如果不考虑生成分布模型，在testing中直接计算P(x|Water)的概率，如下图右下角的那只龟龟，在training data中没有出现过，那值为0吗？显然不对。 假设：上图中water type的examples是从Gaussian distribution（高斯分布）中取样出来的。 因此在training中通过training data得到最优的Gaussian distribution的参数，计算样本中没有出现过的P(x|Water)也就迎刃而解了。 Gaussian Distribution多维的高斯分布（高斯分布就是正态分布啦）的联合概率密度： $$ f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\} $$ D: 维数 $\\mu$ : mean $\\Sigma$ :covariance matrix(协方差矩阵) 协方差： $ cov(X,Y)=E[[X-E(X)][Y-E(Y)]]=E(XY)-E(X)E(Y)$ 具体协方差性质，查阅概率论课本吧。 x: vector,n维随机变量 高斯分布的性质只和 $\\mu$ 和 $\\Sigma$ 有关。 $\\Sigma$ 一定时，$\\mu$ 不同，如下图： $\\mu$ 一定， $\\Sigma$ 不同时，如下图： Maximum Likelihood（极大似然）样本分布如下图所示，假设这些样本是从Gaussian distribution中取样，那如何在训练中得到高斯分布的 $\\mu$ 和 $\\Sigma$ 呢？ 极大似然估计。 考虑Water，有79个样本，估计函数 $L(\\mu, \\Sigma)=f_{\\mu, \\Sigma}\\left(x^{1}\\right) f_{\\mu, \\Sigma}\\left(x^{2}\\right) f_{\\mu, \\Sigma}\\left(x^{3}\\right) \\ldots \\ldots f_{\\mu, \\Sigma}\\left(x^{79}\\right)$ 极大似然估计，即找到 $f_{\\mu, \\Sigma}(x)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\}$ 中的 $\\mu$ 和$\\Sigma$ 使得估计函数最大（使得取出这些样本的概率最大化）。 $\\mu^{*}, \\Sigma^{*}=\\arg \\max _{\\mu, \\Sigma} L(\\mu, \\Sigma)$ 求导计算（过于复杂，但也不是不能做是吧） 背公式[3] $\\mu^{*}=\\frac{1}{79} \\sum_{n=1}^{79} x^{n} \\qquad \\Sigma^{*}=\\frac{1}{79} \\sum_{n=1}^{79}\\left(x^{n}-\\mu^{*}\\right)\\left(x^{n}-\\mu^{*}\\right)^{T}$ 得到Water和Normal的高斯分布，如下图: Do Classification: different $\\Sigma$TestingTesting： $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ P(x|C1)由训练得出的Water的高斯分布计算出，P(x|C2)由Normal的高斯分布计算出。（如下图，过于难打） 如果P(C1|x)&gt;0.5，说明x 属于Water(Class 1)。 Results如果只考虑两个feature（Defense和SP Defense），下图是testing data的样本图，蓝色属于Water，红色属于Normal。 用训练得出的模型，Testing Data: 47% accuracy。（结果如下图） 如果考虑全部features(7个)，重新训练出的模型，结果：Testing Data：54% accuracy。（结果如下图） 结果并不好。参数过多，模型过于复杂，有些过拟合了。 Modifying Model：same $\\Sigma$模型中的参数有两个的Gaussian Distribution中的 $\\mu^$ 和 $\\Sigma^$ ，其中协方差矩阵的大小等于feature的平方，所以让不同的class share 同一个 $\\Sigma$ ，以此来减少参数，简化模型。 极大似然估计的估计函数： $$ L\\left(\\mu^{1}, \\mu^{2}, \\Sigma\\right)=f_{\\mu^{1}, \\Sigma}\\left(x^{1}\\right) f_{\\mu^{1}, \\Sigma}\\left(x^{2}\\right) \\cdots f_{\\mu^{1}, \\Sigma}\\left(x^{79}\\right)\\times f_{\\mu^{2}, \\Sigma}\\left(x^{80}\\right) f_{\\mu^{2}, \\Sigma}\\left(x^{81}\\right) \\cdots f_{\\mu^{2}, \\Sigma}\\left(x^{140}\\right) $$ 公式推导:[3] $\\mu$ 的公式不变。 $\\Sigma=\\frac{79}{140} \\Sigma^{1}+\\frac{61}{140} \\Sigma^{2}$ ,即是原 $\\Sigma^1\\ \\Sigma^2$的加权平均。 Results当只考虑两个features，用同样的协方差参数，结果如下图： 可以发现，用了同样的协方差矩阵参数后，边界变成了线性的，所以这也是一个线性模型。 再考虑7个features，用同样的协方差矩阵参数，模型也是线性模型，但由于在高维空间，人无法直接画出其boundary，这也是机器学习的魅力所在，能解决一些人无法解决的问题。 结果：从之前的54% accuracy增加到 73% accurancy. 结果明显变好了。 SummaryThree Steps： Function Set（Model）： Goodness of a function: The mean µ and convariance $\\Sigma$ that maximizing the likelihood(the probability of generating data) Find the best function:easy(公式) Appendix为什么要选择Gaussian DistributionYou can always use the distribution you like. 可以选择你喜欢的任意分布，t分布，开方分布等。 （老师说：如果我选择其他分布，你也会问这个问题，哈哈哈） Naive Bayes ClassifierIf you assume all the dimensions are independent, then you are using Naive Bayes Classifier. 如果假设features之间互相独立， $P\\left(x | C_{1}\\right)=P\\left(x_{1} | C_{1}\\right) P\\left(x_{2} | C_{1}\\right) \\quad \\ldots \\ldots \\quad P\\left(x_{k} | C_{1}\\right) $ 。 xi是x第i维度的feature。 对于每一个 P(xi|C1)，可以假设其服从一维高斯分布。如果是binary features（即feature取值只有两个），也可以假设它服从Bernoulli distribution(贝努利分布)。 Posterior Probability（后验概率）Posterior Probability后验概率，即使用贝叶斯公式，已知结果，寻找最优可能导致它发生的原因。 对 $P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}$ 进行处理。 得到： $$ \\begin{equation} \\begin{aligned} P\\left(C_{1} | x\\right)&=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\\\ &=\\frac{1}{1+\\frac{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}}\\\\&=\\frac{1}{1+\\exp (-z)} =\\sigma(z) \\qquad(z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)} \\end{aligned} \\end{equation} $$ Worning of Math $z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}=\\ln \\frac{P\\left(x | C_{1}\\right)}{P\\left(x | C_{2}\\right)}+\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}$ $\\ln \\frac{P\\left(C_{1}\\right)}{P\\left(C_{2}\\right)}=\\frac{\\frac{N_{1}}{N_{1}+N_{2}}}{\\frac{N_{2}}{N_{1}+N_{2}}}=\\frac{N_{1}}{N_{2}}$ $P\\left(x | C_{1}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\Sigma 1|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)\\right\\}$ $P\\left(x | C_{2}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{\\left|\\Sigma^{2}\\right| 1 / 2} \\exp \\left\\{-\\frac{1}{2}\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right\\}$ $\\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2}\\left[\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)-\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)\\right]$ $\\left(x-\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1}\\left(x-\\mu^{1}\\right)=x^{T}\\left(\\Sigma^{1}\\right)^{-1} x-2\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1}$ $\\left(x-\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1}\\left(x-\\mu^{2}\\right)=x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-2\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}$ $\\begin{aligned} z=& \\ln \\frac{\\left|\\Sigma^{2}\\right|^{1 / 2}}{\\left|\\Sigma^{1}\\right|^{1 / 2}}-\\frac{1}{2} x^{T}\\left(\\Sigma^{1}\\right)^{-1} x+\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T}\\left(\\Sigma^{1}\\right)^{-1} \\mu^{1} \\\\ &+\\frac{1}{2} x^{T}\\left(\\Sigma^{2}\\right)^{-1} x-\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} x+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T}\\left(\\Sigma^{2}\\right)^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} \\end{aligned}$ 简化模型后， $\\Sigma^1=\\Sigma^2=\\Sigma$ : $$ z=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} $$ 令 $w^T=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} \\qquad b=-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}}$ 当简化模型后，z是线性的，这也是为什么在之前的结果中边界是线性的原因。 最后模型变成这样： $P\\left(C_{1} | x\\right)=\\sigma(w \\cdot x+b)$ . 在生成模型中，我们先估计出 $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ 的值，也就得到了 $w\\ b$ 的值。 那，我们能不能跳过 $\\mu_1\\ \\mu_2\\ N_1\\ N_2\\ \\Sigma$ ，直接估计 $w\\ b$ 呢？ 在下一篇博客[4]中会继续Classification。 Reference Classification as Regression: Bishop, P186. 挖坑：Classification：Perceptron，SVM. Maximum likelihood solution：Bishop chapter4.2.2","link":"/2020/03/21/Classification1/"},{"title":"「机器学习-李宏毅」：Deep Learning-Introduction","text":"这篇文章中，介绍了Deep Learning的一般步骤。 Up and downs of Deep Learning 1958: Perceptron (linear model) 1969: Perceptron has limitation 1980s: Multi-layer perceptron ​ Do not have significant difference from DNN today 1986: Backpropagation ​ Usually more than 3 hidden layers is not helpful 1989: 1 hidden layer is “good enough”, why deep? 2006: RBM initialization (breakthrough) 2009: GPU 2011: Start to be popular in speech recognition【语音辨识】 2012: win ILSVRC image competition 【图像识别】 Step 1: Neural Network在将Regression 和 Classification时，Step 1 是确定一个function set。 在Deep Learning中，也是相同的，只是这里的function set就是一个neural network的结构。 上图中，一个Neuron就是如上图所示的一个unit，neuron之间不同的连接方式构成不同的Neural Network。 Fully Connect Feedforward Network 这是一个Fully Connect Feedforward Network【全连接反馈网络】，其中每个neuron的activation function都是一个sigmod函数。 为什么说neural network其实就是一个function呢？上面两张图中，输入是一个vector，输出也是一个vector，可以用下面函数来表示。 $$ f\\left(\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]\\right)=\\left[\\begin{array}{c}0.62 \\\\ 0.83\\end{array}\\right] f\\left(\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right]\\right)=\\left[\\begin{array}{l}0.51 \\\\ 0.85\\end{array}\\right] $$ 上图为全连接网络的一般形式，第一层是Input Layer，最后一层是Output Layer，中间的其他层称为Hidden Layer。 而Deep Learning中的Deep的含义就是Many hidden layers的意思。 Matrix Operation 上图的全连接网络中，第一个hidden layer的输出可以写成矩阵和向量的形式： $$ \\sigma\\left(\\left[\\begin{array}{cc}1 & -2 \\\\ -1 & 1\\end{array}\\right]\\left[\\begin{array}{c}1 \\\\ -1\\end{array}\\right]+\\left[\\begin{array}{c}1 \\\\ 0\\end{array}\\right]\\right)=\\left[\\begin{array}{c}0.98 \\\\ 0.12\\end{array}\\right] $$ 更为一般的公式，用W表示权重，b代表bias，a表示hidden layer的输出。输出vector y可以写成 $y = f(x)$ 的形式，即： $y= f(x)=$ 转换为矩阵运算的形式，就可以使用并行计算的硬件技术（GPU）来加速矩阵运算，这也是为什么用GPU来训练Neural Network 更快的原因。 Output Layer在 Logistic Regression中第4节讲到Logistic Regression有局限，消除局限的一种方法是Feature Transformation。 但是Feature Transformation需要人工设计，不太“机器学习”。 在下图全连接图中，把Output Layer换成一个Multi-class Classifier（SoftMax），而其中Hidden Layers的作用就是Feature extractor，从feature x提取出新的feature，也就是 output layer的输入。 这样就不需要人工设计Feature Transformation/Feature engineering，可以让机器自己学习：如何将原来的feature转换为更好分类的feature。 Handwriting Digit Recognition 在手写数字辨别中，输出是一个16*16的image（256维的vector），输出是一个10维的vector，每一维表示是该image是某个数字的概率。 在手写数字辨别中，需要设计neural network的结构来提取输入的256维feature。 Step 2: Goodness of function之前我们已经使用过的最小二乘法和交叉熵作为损失函数。 一般在Neural Network中，使用output vector 和target vector的交叉熵作为Loss。 Step 3: Pick the best function在NN中，也使用Gradient Descent。 但是，Deep Neural Network中，参数太多了，计算结构也很复杂。 Backpropagation：an efficient way to compute $\\partial{L}/\\partial{w}$ in neural network. Backpropagation本质也是Gradient Descent，只是一种更高效进行Gradient Descent的算法。 在很多 toolkit（TensorFlow，PyTorch ，Caffe等）中都实现了Backpropgation。 Backpropagation部分，见下一篇博客。 Reference","link":"/2020/04/18/DL-introdunction/"},{"title":"「机器学习-李宏毅」:Classification-Logistic Regression","text":"在上篇文章中，讲解了怎么用Generative Model做分类问题。这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。文章主要有两部分：第一部分讲解了Logistic Regression的三个步骤。第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。 Logistic RegressionStep1: Function Set在文章末尾，我们得出 $P_{w, b}\\left(C_{1} | x\\right)=\\sigma(w\\cdot x+b)$ 的形式，想跳过找 $\\mu_1,\\mu_2,\\Sigma$ 的过程，直接找 $w,b$ 。 因此Function Set: $f_{w, b}(x)=P_{w, b}\\left(C_{1} | x\\right)$ 。值大于0.5，则属于C1类，否则属于C2类。 Step2: Goodness of a Function使用极大似然的思想（在前一篇机率模型/生成模型中有讲） 估计函数是 ：$L(w, b)=f_{w, b}\\left(x^{1}\\right) f_{w, b}\\left(x^{2}\\right)\\left(1-f_{w, b}\\left(x^{3}\\right)\\right) \\cdots f_{w, b}\\left(x^{N}\\right)$ 目标： $ w^{*}, b^{*}=\\arg \\max _{w, b} L(w, b)$ 由于在之前的Regression中，我们都是找极小值点，为了方便处理，将估计函数转换为如下形式的损失函数： $$ \\begin{equation} \\begin{aligned} Loss &= -\\ln L(w, b)=\\ln f_{w, b}\\left(x^{1}\\right)+\\ln f_{w, b}\\left(x^{2}\\right)+\\ln \\left(1-f_{w, b}\\left(x^{3}\\right)\\right) \\cdots \\\\ &=\\sum_{n}-\\left[\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)\\right] \\end{aligned} \\end{equation} $$ 目标 ： $w^{*}, b^{*}=\\arg \\min _{w, b} L(w, b)$ Cross entropy（交叉熵） 上式中的 $\\left[\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)\\right]$ 其实是两个Bernoulli distribution的交叉熵。 交叉熵是什么？ 简单来说，交叉熵是评估两个distribution 有多接近。所以当两个分布的交叉熵为0时，表明这两个分布一模一样。 对于 $\\hat{y}^{n} \\ln f_{w, b}\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right)$ ： Distribution p: p(x = 1) = $\\hat{y}^n$ ; p( x = 0 ) = 1 - $\\hat{y}^n$ Distribution q: q(x = 1 ) = $f(x^n)$ ; q(x = 0 ) = 1 - $f(x^n)$ 交叉熵 $H(p,q)=-\\Sigma_xp(x)\\ln(q(x))$ 因此，这个损失函数的表达式其实也是输出分布和target分布的交叉熵，即： $L(f)=\\sum_{n} C\\left(f\\left(x^{n}\\right), \\hat{y}^{n}\\right)$ （ $C\\left(f\\left(x^{n}\\right), \\hat{y}^{n}\\right)=-\\left[\\hat{y}^{n} \\ln f\\left(x^{n}\\right)+\\left(1-\\hat{y}^{n}\\right) \\ln \\left(1-f\\left(x^{n}\\right)\\right)\\right]$ ） 和Linear Regression不同，为什么Logistic Regression不用square error，而要使用cross entropy。 在1.4小节会给出解释。 Step3: Find the best function在第三步，同样使用Gradient来寻找最优函数。 推导过程： $\\left.\\frac{-\\ln L(w, b)}{\\partial w_{i}}=\\sum_{n}-\\left[\\hat{y}^{n} \\frac{\\ln f_{w, b}\\left(x^{n}\\right)}{\\partial w_{i}}+\\left(1-\\hat{y}^{n}\\right) \\frac{\\ln \\left(1-f_{w, b}\\left(x^{n}\\right)\\right.}{\\partial w_{i}}\\right)\\right]$ $\\frac{\\partial \\ln f_{w, b}(x)}{\\partial w_{i}}=\\frac{\\operatorname{\\partial\\ln} f_{w, b}(x)}{\\partial z} \\frac{\\partial z}{\\partial w_{i}}$ $\\frac{\\partial \\ln \\sigma(z)}{\\partial z}=\\frac{1}{\\sigma(z)} \\frac{\\partial \\sigma(z)}{\\partial z}=\\frac{1}{\\sigma(z)} \\sigma(z)(1-\\sigma(z))$ $\\frac{\\partial z}{\\partial w_{i}}=x_{i}$ $\\frac{\\partial \\ln \\left(1-f_{w, b}(x)\\right)}{\\partial w_{i}}=\\frac{\\operatorname{\\partial\\ln}\\left(1-f_{w, b}(x)\\right)}{\\partial z} \\frac{\\partial z}{\\partial w_{i}}$ $\\frac{\\partial \\ln (1-\\sigma(z))}{\\partial z}=-\\frac{1}{1-\\sigma(z)} \\frac{\\partial \\sigma(z)}{\\partial z}=-\\frac{1}{1-\\partial(z)} \\sigma(z)(1-\\sigma(z))$ $\\frac{\\partial z}{\\partial w_{i}}=x_{i}$ $\\frac{\\partial \\sigma(z)}{\\partial z}=\\sigma(z)\\cdot(1-\\sigma(z))$ 注：$f_{w, b}(x)=\\sigma(z)$ ; $z=w \\cdot x+b=\\sum_{i} w_{i} x_{i}+b$ $$ \\begin{equation} \\begin{aligned} \\frac{-\\ln L(w, b)}{\\partial w_{i}}&=\\sum_{n}-\\left[\\hat{y}^{n}\\left(1-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n}-\\left(1-\\hat{y}^{n}\\right) f_{w, b}\\left(x^{n}\\right) x_{i}^{n}\\right] \\\\&=\\sum_{n}-\\left(\\hat{y}^{n}-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n} \\end{aligned} \\end{equation} $$ 因此Logistic Regression的损失函数的导数和Linear Regression的一样。 迭代更新： $w_{i} \\leftarrow w_{i}-\\eta \\sum_{n}-\\left(\\hat{y}^{n}-f_{w, b}\\left(x^{n}\\right)\\right) x_{i}^{n}$ 与Linear Regression 的对比 如图所示。 If : Logistic + Square Error前面一小节我们提到，在Logistic Regression中使用cross entropy判别一个函数的好坏,那为什么不使用square error来judge the goodness？ 如果使用 Square Error的方法，步骤如下： 来看Step 3: 损失函数的导数是 $2\\left(f_{w, b}(x)-\\hat{y}\\right) f_{w, b}(x)\\left(1-f_{w, b}(x)\\right) x_{i}$ 考虑 $\\hat{y}^n=1$ （即我们的target是1）： 如果 $f_{w,b}(x^n)=1$ , 即预测值接近 target, 算出来的 $\\partial{L}/\\partial{w_i}=0$ 是期望的。 如果 $f_{w,b}(x^n)=0$ , 即预测值原理 target, 算出来的 $\\partial{L}/\\partial{w_i}=0$ 是不期望的。 同理，当考虑 $\\hat{y}^n=0$ 情况时，也是如此。 更直观的看： 上图中，画出了两种损失函数的平面，中心的最低点是我们的target。 但在Square Error中，远离target的蓝色点，也处在很平坦的位置，其导数小，参数的更新会很慢。 因此在Cross Entropy中，离target越远，其导数更大，更新更快。 所以Cross Entropy的效果比Square Error更快，效果更好。 Discriminative V.S. Generative这篇文章中的Logistic Regression是Discriminative Model。 上篇文章中Classification是Generative Model。 有什么区别呢？ 上图中，Generative Model做了假设（脑补），假设它是 Gaussian Distribution，假设它是Bernoulli Distribution。然后去找这些分布的参数，在求出 $w,b$。 而在Discriminative Model中，没有做任何假设，直接找 $w,b$ 参数。 所以，这两种Model经过training找出来的参数一样吗？ 答案是不一样的。 The same model(function set), but different function is selected by the same training data. 在上篇Pokemon的例子中，比较两种方法的结果差异。 可见，在Pokemon的例子总，Discriminative的效果比Generative的效果好一些。 但是Generative Model就不好吗？ Benefit of generative model With the assumption of probability distribution, less training data is needed. 【训练生成模型所需数据更少】 With the assumption of probability distribution, more robust to the noise. 【生成模型对noise data更兼容】 Priors and class-dependent probabilities can be estimated from different sources. 【生成模型中的 先验概率Priors 和 基于类别的分布概率不同】 比如，做语音辨识系统，整个系统是generative的。 因为Prior（某一句话的概率）并不需要从data中知道，可以直接在网络上爬虫统计。 而class-dependent probabilities（这段语音是这句话的概率）需要data进行训练才能得知。 Multi-class classificationsoftmax 假设有三个类别：C1、C2、C3 。模型已经得到，参数分别是 w、b。 对于输入x, 判断x属于哪一个类别。 通过每个类别的 w、b求出 $z^i=w^i\\cdot x+b_i$ Softmax的步骤： exponential：每个z值得到 $=e^z$ . sum：将指数化后的值加起来$=\\Sigma_{j=1}^3e^{z_j}$ output: 每个类别的输出 $y_i=e^{z_1}/\\Sigma_{j=1}^3e^{z_j}$ ，即x属于类别i的概率。 求出的 $1&gt;y_i&gt;0$ 且 $\\Sigma_iy_i=1$ 。 通过Softmax，得到 $y_i=P(C_i|x)$ 。 Steps（手写笔记，略倾斜，原来不切一切还不知道自己歪的这么厉害 泪） Step 1: Step 2: Step 3: 使用Stochastic Gradient（即每个样本更新一次）的话： data: [x, $\\hat{y}$ ] , $\\hat{y}_i=1$ 更新 $w^j$ : $j=i$ : $w^j \\leftarrow w^j-\\eta\\cdot (y_i-1)\\cdot x$ $j\\neq i$ : $w^j \\leftarrow w^j-\\eta\\cdot y_i\\cdot x$ (下次一定，笔记写直一点！) 更为规范的推导见[1] Limitation of Logistic Regression 对于如上情况，Logistic Regression并不能进行分类，因为他的boundary 应该是线性的。 Feature Transforming如果对feature做转换后，就可以用Logistic Regression处理。 重定义feature， $x_1’$ :定义为到[0,0]的距离， $x_2’$ :定义为到[1,1]的距离。 于是图变成下图，即可用Logistic Regression进行分类。 但这样的做法，就不像人工智能了，因为Feature Transformation需要人来设计，而且较难设计。 Cascading logistic regression models另一种做法是，将logistic regression连接起来。 上图中，左边部分的两个logistic regression就相当于在做Feature Transformation，右边部分相当于在做Classification。 而通过这种形式，将多个model连接起来，也就是大热的Neural Network。 Reference Multi-class Classification推导：Bishop，P209-210","link":"/2020/04/01/Classification2/"},{"title":"「机器学习-李宏毅」：Gradient","text":"总结「李宏毅老师-机器学习」的Gradient，主要从以下三个方面展开：调节learning rate；加快训练速度；对数据进行Feature Scaling。 阅读完三个tips，具体实现可demo Tip 1: Tuning your learning rates carefullyVisualize 损失函数随着参数变化的函数图 左图是Loss Function的函数图，红色是最好的Step，当Step过小（蓝色），会花费很多时间，当Step过大（绿色、黄色），会发现Loss越来越大，找不到最低点。 所以在Training中，尽可能的visualize loss值的变化。 但是当参数大于等于三个时， $loss function$的函数图就不能visualize了。 因此，在右图中，visualize Loss随着参数更新的变化，横轴即迭代次数，当图像呈现蓝色（small）时，就可以把learning rate 调大一些。 Adaptive Learning Rates(Adagrad)但是手动调节 $\\eta$是低效的，我们更希望能自动地调节。 直观上的原则是： $\\eta$ 的大小应该随着迭代次数的增加而变小。 最开始，初始点离minima很远，那step应该大一些，所以learning rate也应该大一些。 随着迭代次数的增加，离minima越来越近，就应该减小 learning rate。 E.g. 1/t decay： $\\eta^t=\\eta/ \\sqrt{t+1}$ 不同参数的 $\\eta$应该不同（cannot be one-size-fits-all)。 AdagradAdagrad 的主要思想是：Divide the learning rate of each parameter by the root mean squear of its previous derivatives.(通过除这个参数的 计算出的所有导数 的均方根) root mean squar : $ \\sqrt{\\frac{1}{n}(x_1^2+x_2^2+...+x_n^2)} $ Vanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $w^{t+1} \\leftarrow w^{t}-\\frac{\\eta^{t}}{\\sigma^{t}} g^{t} $ $\\eta^t$：第t次迭代的leaning rate $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}}$ $g^{t}=\\frac{\\partial L\\left(\\theta^{t}\\right)}{\\partial w} $ $\\sigma^t$：root mean squar of previous derivatives of w $\\tau^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}} $ 对比上面两种Adaptive Gradient，Adagrade的优势是learning rate 是和parameter dependent（参数相关的）。 Adagrad步骤简化步骤： 简化公式： $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ ( $ \\eta^{t}=\\frac{\\eta}{\\sqrt{t+1}} $ , $ \\sigma^{t}=\\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}$ ,约掉共同项即可) Adagrad Contradiction? ——Adagrad原理解释Vanilla Gradient descent $w^{t+1} \\leftarrow w^{t}-\\eta^{t} g^{t}$ Adagrad $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 在Vanilla Gradient descent中， $g^t$越大，也就是当前梯度大，也就有更大的step。 而在Adagrad中，当 $g^t$越大，有更大的step,而当 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 越大，反而有更小step。 Contradiction？ 「Intuitive Reason（直观上解释）」 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 是为了造成反差的效果。 类比一下，如果一个一直很凶的人，突然温柔了一些，你会觉得他特别温柔。所以同样是 $0.1$,第一行中，你会觉得特别大，第二行中，你会觉得特别小。 因此 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 这一项的存在就能体现 $g^t$的变化有多surprise。 「数学一些的解释」1. Larger Gradient,larger steps?在前面我们都深信不疑这一点，但这样的描述真的是正确的吗？ 在这张图中，只有一个参数，认为当该点的导数越大，离minima越远，这样看来，Larger Gradient,larger steps是正确的。 在上图中的 $x_0$点，该点迭代更新的best step 应该正比于 $|x_0+\\frac{b}{2a}|$ ，即 $\\frac{|2,a, x_0+b|}{2a}$。 而 $\\frac{|2,a, x_0+b|}{2a}$的分子也就是该点的一阶导数的绝对值。 上图中，有 $w_1,w_2$两个参数。 横着用蓝色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较a、b两点，a点导数大，离minima远。 竖着用绿色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较c、d两点，c点导数大，离minima远。 但是，如果比较a、c两点呢？ a点对 $w_1$ 的偏导数和c点对 $w_2$的偏导数比较？ 比较出来，c点点偏导数更大，离minima更远吗？ 再看左图的图像，横着的弧度更平滑，竖着的弧度更尖一些，直观上看应该c点离minima更近一些。 所以Larger Gradient,larger steps点比较方法不能（cross parameters)跨参数比较。 所以最好的step $\\propto$ 一阶导数（Do not cross parameters)。 2.** Second Derivative** 前面讨论best step $\\frac{|2,a, x_0+b|}{2a}$的分子是该点一阶导数，那么其分母呢？ 当对一阶导数再求导时，可以发现其二阶导数就是best step的分母。 得出结论：the best step $\\propto$ |First dertivative| / Second derivative。 因此，再来看两个参数的情况，比较a点和c点，a点的一阶导数更小，二阶导数也更小；c点点一阶导数更大，二阶导数也更大。 所以如果要比较a、c两点，谁离minima更远，应该比较其一阶导数的绝对值除以其二阶导数的大小。 回到 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 上一部分得出的结论是：the best step $\\propto$ |First dertivative| / Second derivative。 所以我们的learning rate 也应该和 |First dertivative| / Second derivative相关。 $g^t$也就是一阶导数，但为什么 $\\sqrt{\\sum_{i=0}^t (g^i)^2} $ 能代表二阶导数呢？ 上图中，蓝色的函数图有更小的二阶导数，绿色的函数图有更大的二阶导数。 在复杂函数中，求二阶导数是一个很复杂的计算。 所以我们想用一阶导数来反映二阶导数的大小。 在一阶导数的函数图中，认为一阶导数值更小的，二阶导数也更小，但是取一个点显然是片面的，所以考虑取多个点。 也就是用 $ \\sqrt{\\text{(first derivative)}^2}$ 来代表best step中的二阶导数。 总结一下Adagrad的为了找寻最好的learning rate，从找寻best step下手，用简单的二次函数为例，得出 best step $\\propto$ |First dertivative| / Second derivative。 但是复杂函数的二阶导数是难计算的，因此考虑用多个点的一阶导数来反映其二阶导数。 得出 $ w^{t+1} \\leftarrow w^{t}-\\frac{\\eta}{\\sqrt{\\sum_{i=0}^{t}\\left(g^{i}\\right)^{2}}} g^{t}$ 。 直观来解释公式中的一阶导数的root mean square，即来为该次迭代的一阶导数造成反差效果。 其他文献中的Adaptive Gradient理应都是为了调节learning rate使之有best step。(待补充的其他Gradient)[1] Tip 2:Stochastic Gradient DescentStochastic Gradient Descent在linear model中，我们这样计算Loss function： $L=\\sum_{n}\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ 每求一次Loss function，L都对所有training examples的 $\\text{error}^2$求和，因此每一次的loss function的计算，都是一重循环。 在Stochastic Gradient Descent中，每一次求loss function，只取一个example $x^n$，减少一重循环，无疑更快。 Stochastic Gradient Descent Pick an example $x^n$ $L=\\left(\\hat{y}^{n}-\\left(b+\\sum w_{i} x_{i}^{n}\\right)\\right)^{2} $ 上图中，传统的Gradient Descent看完一次所有的examples，离minima还很远；而Stochastic Gradient Descent ，看完一次，已经离minima较近了。 Tip 3:Feature ScalingWhat is Feature Scaling 如上图所示，希望能让不同的feature能有相同的scale（定义域/规模） Why Feature Scaling假设model都是 $y = b+ w_1 x_1 +w_2 x_2$。 上图中，左边 $x_2$的规模更大，可以认为 $x_1$ 对loss 的影响更小， $ x_2$对loss的影响更大。 即当 $w_1,w_2$轻微扰动时，同时加上相同的 $\\Delta w$时，$x_2$ 使 $y$的取值更大，那么对loss 的影响也更大。 如图中下方的函数图 $w_1$方向的L更平滑， $w_2$ 方向更陡峭些，Gradient descent的步骤如图所示。 但当对 $x_2$进行feature scaling后，图像会更像正圆，Gradient descent使，参数更新向着圆心走，更新会更有效率。 How Feature Scaling概率论知识：标准化。 概率论： 随机变量 $X$ 的期望和方差均存在，且 $ D(X)>0$,令 $X^*=\\frac{X-E(X)}{\\sqrt{D(X)}}$ 那么 $E(X^*)=0,D(X)=1 $ , $ X^* $ 称为X的标准化随机变量。 对所有向量的每一维度，进行标准化处理： $x_{i}^{r} \\leftarrow \\frac{x_{i}^{r}-m_{i}}{\\sigma_{i}} $ （ $m_i$是该维度变量的均值， $\\sigma_i$ 是该维度变量的方差） 标准化后，每一个feature的期望都是0，方差都是1。 Gradient Descent Theory(公式推导)当用Gradient Descent解决 $\\theta^*=\\arg \\min_\\theta L(\\theta)$时，我们希望每次更新 $\\theta $ 都能得到 $L(\\theta^0)&gt;L(\\theta^1)&gt;L(\\theta^2)&gt;…$ 这样的理论结果，但是不总能得到这样的结果。 上图中，我们虽然不能一下知道minima的方向，但是我们希望：当给一个点 $\\theta^0$ 时，我们能很容易的知道他附近（极小的附近）的最小的loss 是哪个方向。 所以怎么做呢？ Tylor Series微积分知识：Taylor Series（泰勒公式）。 Tylor Series:函数 $h(x)$ 在 $x_0$ 无限可导，那么 $\\begin{aligned} \\mathrm{h}(\\mathrm{x}) &=\\sum_{k=0}^{\\infty} \\frac{\\mathrm{h}^{(k)}\\left(x_{0}\\right)}{k !}\\left(x-x_{0}\\right)^{k} \\\\ &=h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right)+\\frac{h^{\\prime \\prime}\\left(x_{0}\\right)}{2 !}\\left(x-x_{0}\\right)^{2}+\\ldots \\end{aligned}$ 当 x 无限接近 $x_0$ 时，忽略后面无穷小的高次项， $h(x) \\approx h\\left(x_{0}\\right)+h^{\\prime}\\left(x_{0}\\right)\\left(x-x_{0}\\right) $ 上图中，用 $\\pi/4$ 处的一阶泰勒展示来表达 $\\sin(x)$ ,图像是直线，和 $\\sin(x)$ 图像相差很大，但当 x无限接近 $\\pi/4$ 是，函数值估算很好。 Multivariable Taylor Series $h(x, y)=h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right) +\\text{something raleted to} (x-x_x^0)^2 \\text{and} (y-y_0)^2+…$ 当 $(x,y)$ 接近 $(x_0,y_0)$ 时， $h(x,y)$ 用 $(x_0,y_0)$ 处的一阶泰勒展开式估计。 $ h(x, y) \\approx h\\left(x_{0}, y_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial x}\\left(x-x_{0}\\right)+\\frac{\\partial h\\left(x_{0}, y_{0}\\right)}{\\partial y}\\left(y-y_{0}\\right)$ Back to Formal Derivation 当图中的红色圆圈足够小时，红色圆圈中的loss 值就可以用 $(a,b)$ 处的一阶泰勒展开式来表示。 $ \\mathrm{L}(\\theta) \\approx \\mathrm{L}(a, b)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}\\left(\\theta_{1}-a\\right)+\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}}\\left(\\theta_{2}-b\\right) $ $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$ ,d 足够小。 用 $s=L(a,b)$ , $ u=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}}, v=\\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} $ 表示。 最后问题变成： $L(\\theta)\\approx s+u(\\theta_1-a)+v(\\theta_2-b)$ 找 $(\\theta_1,\\theta_2)$，且满足 $(\\theta_1-a)^2+(\\theta_2-b)^2 \\leq d^2$，使 $L(\\theta)$ 最小。 变成了一个简单的最优化问题。 令 $\\Delta \\theta_1=\\theta_1-a$ , $\\Delta\\theta_2=\\theta_2-b$ 问题简化为： $\\text{min}:u \\Delta \\theta_1+v\\Delta\\theta_2$ $\\text{subject to}:{\\Delta\\theta_1}^2+{\\Delta\\theta_2}^2\\leq d^2$ 画出图，就是初中数学了。更新的方向应该是 $(u,v)$ 向量反向的方向。 所以： $\\left[\\begin{array}{l} \\Delta \\theta_{1} \\\\ \\Delta \\theta_{2} \\end{array}\\right]=-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $\\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] $ $ \\left[\\begin{array}{l} \\theta_{1} \\\\ \\theta_{2} \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right]=\\left[\\begin{array}{l} a \\\\ b \\end{array}\\right]-\\eta\\left[\\begin{array}{l} \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{1}} \\\\ \\frac{\\partial \\mathrm{L}(a, b)}{\\partial \\theta_{2}} \\end{array}\\right] $ Limitation of Gradient Descent Gradient Descent 可能会卡在local minima或者saddle point（鞍点：一个方向是极大值，一个方向是极小值，导数为0） 实践中，我们往往会在导数无穷接近0的时候停下来（&lt; 1e-7)，Gradient Descent 可能会停在plateau(高原；增长后的稳定) Reference[1] 待补充的其他Gradient","link":"/2020/03/01/Gradient/"},{"title":"「Cryptography-Dan」：Introduction","text":"本系列是学习Dan Boneh教授的Online Cryptography Course。 这是Dan教授的第一讲：对密码学的一些Introduction。 What is cryptography?Crypto core：安全通信 Secret key establishment (密钥的建立)： Alice 和 Bob 会得到一个shared secret key，而且Alice 知道她是在和Bob通信，Bob也知道他是在和Alice通信。而attacker不能从通信中获取key。 Secure communicati （安全通信）： 在通信中，Alice、Bob用k将信息加密，保证了通信的confidentiality（机密性）；同时attacker也无法篡改通信的信息，保证了通信的integrity（完整性）。 Crypto can do much more密码学除了能保证安全通信，密码学还能做很多其他的事。 Digital signature &amp; Anonymous Digital signatures（数字签名）： 现实中，人们对不同的文档进行签名，虽然是不同的文档，但是签名的字是相同的。 如果这应用在网络的文档签名中，这将是很危险的。攻击者只需要将签名进行复制、粘贴，就可以将你的签名签在你并不想签的文档中。 数字签名的主要思想：数字签名其实是代签内容的函数值，所以如果攻击者只是复制数字签名（原签名的函数值），那么攻击者得到的数字签名也是无效的（函数值不同）。 在后面的课程系列中会详细讲这部分的内容。[1] Anonymous communication（匿名通信）： 匿名通信的实现，有Mix network （wiki详细介绍）协议，这是一种路由协议，通过使用混合的代理服务器链来实现难以追踪的通信。 通过这些代理的不断加密解密可以实现： Bob不知道与之通信的是Alice。 代理也不知道是Alice和Bob在通信。 双向通信：虽然Bob不知与之通信的是Alice，但也能respond。 Anonymous digital cash（匿名数字现金）： 现实中，我们可以去超市花掉一元钱，而超市不知道我是谁。 在网络中，如果Alice想去网上商店花掉数字现金一元钱，网上商店可以不知道是谁花掉的这一元钱吗？ 这就是匿名数字现金需要解决的问题： 可以在匿名的情况下花掉数字现金吗？ 如果可以，当Alice将这一元钱复制多次（数字现金都是数据串），得到了三元钱，再去把它花掉，由于匿名的原因，没人知道是谁花掉的这三元钱，商店找不到责任人。 这是匿名数字现金需要解决的第二个问题： 如何防止 double spending情况的发生？ 可以用这样的机制去实现匿名数字现金：当Alice花费这一块 once时，系统保证Alice的匿名性；但当Alice花费这一块 more than once ,系统立即揭露Alice的全部信息。 Protocols在介绍什么是Protocols之前，先介绍两种应用场景。 Elections 有5个人要进行投票选举0和1号候选人，但是需要保证：每个人除了知道自己的投票结果，互相不知道其他人的投票情况。在这种情况下怎么知道最后的winner是谁吗？ 如上图，可以引入一个第三方——election center，第三方验证每一个人只能投一次，最后统计票数决策出最后的winner。 Private auctions 介绍一种拍卖机制，Vickery auction：对一个拍卖品，每个投标者在不知道其他人投标价格的情况下进行投标，最后的acution winner： highest bidder &amp; pays 2nd highers bid。即是标价最高者得标，但他只需要付第二高的标价。 所以public知道的信息只有：中标者和第二高投标者的标价。 需要实现这种机制，也可以引入一个第三方——auction center。 但是引入第三方真的安全吗？安全第三方也不安全。 再看上面那个Election的例子，如果把上面四个人的投票情况作为输入，第三方的任务其实是输出一个函数 $f(x_1,x_2,x_3,x_4)$ 而不公开其他信息。 因为安全第三方也许并不安全，所以如果去掉第三方，上面四个人遵从某种协议，相互通信，最后能否得出这个 $f(x_1,x_2,x_3,x_4)$ 这个结果函数，而不透露其投票信息？ 答案是 “Yes”。 有一个惊人的定理：任何能通过第三方做到的事，也能不通过第三方做到。 Thm: anythong that can done with trusted auth. can also be done without. 怎么做到？答案是 Secure multi-party computation（安全多方计算）。 挖坑博文：姚氏百万富翁问题[2] Crypto magic Privately outsourcing computation (安全外包计算) Alice想要在Google服务器查询信息，为了不让别人知道她查询的是什么，她把search query进行加密。 Google服务器接收到加密的查询请求，虽然Google不知道她实际想查询什么信息，但是服务器能根据E[query]返回E[results]。 最后Alice将收到的E[results]解密，得到真正的results。 这就是安全外包计算的简单过程：Encryption、Search、Decryption。 Zero knowl（proof of knowledge) (零知识证明)： Alice 知道p、q(两个1000位的质数)相乘等于N。 Bob只知道N的值，不知道具体的p、q值。 Alice 给 Bob说她能够分解数N，但她不用告诉Bob N的具体因子是什么，只需要证明我能分解N，证明这是我的知识。 最后Bob知道Alice能够分解N，但他不知道怎么分解（也就是不知道N的因子到底是什么）。 A rigorous science在密码学的研究中，通常是这样的步骤： Precisely specify threat model. 准确描述其威胁模型或为达到的目的。比如签名的目的：unforgeable（不可伪造）。 Propose a construction. Prove that breaking construction under threat mode will solve an underlying hard problem. 证明攻击者攻击这个系统必须解决一个很难的问题（大整数分解问题之类的NP问题）。 这样也就证明了这个系统是安全的。 HistorySubstitution cipher（替换）what is it 替换密码很好理解，如上图的这种替换表（key）。 比较historic的替换密码——Caesar Cipher（凯撒密码），凯撒密码是一种替换规则：向后移三位，因此也可以说凯撒密码没有key。 the size of key space用$\\mathcal{K}$ （花体的K）来表示密钥空间。 英语字母的替换密码，易得密钥空间的大小是 $|\\mathcal{K}|=26!\\approx2^{88}$ （即26个字母的全排列）。 这是一个就现在而言也就比较perfect的密钥空间。 但替换密码也很容易被破解。 how to break it问：英语文本中最commom的字母是什么？ 答：“E” 在英语文本（大量）中，每个字母出现的频率并不是均匀分布，我们可以利用一些最common的字母和字母组合来破解替换密码。 Use frequency of English letters. Dan教授统计了标准文献中字母频率： “e”: 12.7% , “t”: 9.1% , “a” : 8.1%. 统计密文中（大量）出现频率最高、次高、第三高的字母，他们的明文也就是e、t、a。 Use frequency of pairs of letters (diagrams).（二合字母） 频率出现较高的二合字母：”he”, “an”, “in” , “th” 也能将h, n,i等破解出。 trigrams（继续使用三合字母） ……直至全部破解 因此substitution cipher是CT only attack！（惟密文攻击：仅凭密文就可以还原出原文） Vigener cipherEncryption 加密过程如上图所示： 密钥是 “CRYPTO”, 长度为6，将密钥重复书写直至覆盖整个明文长度。 将密钥的字母和对应的明文相加模26，得到密文。 Decryption解密只需要将密文减去密钥字母，再模26即可。 How to break it破解方法和替换密码类似，思想也是使用字母频率来破解。 这里分两种情况讨论： 第一种：已知密钥长度 破解过程： 将密文按照密钥长度分组，按照图中的话，6个一组。 统计每组的的第一个位置的字母出现频率。 假设密文中第一个位置最common的是”H” 密钥的第一个字母是：”H”-“E”=”C” 统计剩下位置的字母频率，直至完全破解密钥。 第二种：未知密钥长度 未知密钥长度，只需要依次假设密钥长度是1、2、3…，再按照第一种情况破解，直至破解情况合理。 Rotor MachinesRotor: 轴轮。 所以这种密码的加密核心是：输入明文字母，轴轮旋转一定角度，映射为另一字母。 single rotor 早期的是单轴轮，rotor machine的密钥其实是图右中间那个圆圆的可以旋转的柱子。 图左是变动的密钥映射表。 变动过程： 第一次输入A，密文是K。 轴轮旋转一个字母位：看图中E，从最下到最上（一个圈，只相隔一位）。 所以第二次再输入A，密文是E。 …… Most famous ：the Enigma Enigma machine是二战时期纳粹德国使用的加密机器，因此完全破解了Enigma是盟军提前胜利的关键。 左图中可以看出Enigma机器中是有4个轴轮，每个轴轮都有自己的旋转字母位大小，因此密钥空间大小是 $|\\mathcal{K}|=26^4\\approx2^{18}$ (在plugboard中，实际是 $2^{36}$)。 密钥空间很小，放在现在很容易被暴力破解。 plugboard 允许操作员重新配置可变接线，实现两个字母的交换。plugboard比额外的rotor提供了更多的加密强度。 对于Enigma machine的更多的具体介绍可以戳Enigma machine 的wiki链接。 Data Encryption StandardDES：#keys = $2^{56}$ ,block siez = 64bits，一次可以加密8个字母。 Today：AES（2001）、Salsa20（2008）…… 这里只是简单介绍。 Discrete Probability这个segment比较简单，概率论基本完全cover了，这里只讲一些重点。 Randomized algorithms随机算法有两种，一种是Deterministic algorithm（也就是伪随机），另一种是Randomized algorithm。 Deterministic algorithm $ y\\longleftarrow A(m)$ ，这是一个确定的函数，输入映射到唯一输出。 Randomized algorithm $y\\longleftarrow A(m ; r) \\quad \\text { where } r \\stackrel{R}{\\longleftarrow}{0,1}^{n}$ output： $y \\stackrel{R}{\\longleftarrow} A(m)$ ，y is a random variable. $ r \\stackrel{R}\\longleftarrow { 0,1 }^n $ :意思是r是n位01序列中的任意一个取值。R，random。变量r服从在 ${0,1}^n$ 取值的均匀分布。 由于随机变量r，对于给定m，$A(m;r)$ 是 ${0,1}^n$ 中的一个子集。 所以，对m的加密结果y，也是一个的随机变量，而且，y在 $A(m,r)$ 也是服从均匀分布。 因此，由于r的影响，对于给定m，加密结果不会映射到同一个值。（如上图所示） XORXOR有两种理解：（ $x \\oplus y $ ） 一种是：x,y的bit位相比较，相同则为0，相异为1. 另一种是：x,y的bit位相加 mod2. 异或在密码学中被频繁使用，主要是因为异或有一个重要的性质。 异或的重要性质：有两个在 ${0,1}^n$ （n位01串）取值的随机变量X、Y。X、Y相互独立，X服从任意某种分布，随机变量Y服从均匀分布。那么 $Z=Y\\oplus X$ ，Z在 ${0,1}^n$ 取值，且Z服从均分分布。 Thm: Y a rand. var. over ${0,1}^n$ , X an index. uniform var. on ${0,1}^n$ ​ Then Z := Y $\\oplus$ X is uniform var. on ${0,1}^n$ . Proof： 当n=1 画出联合分布 Pr[ Z=0 ]=Pr[ (x,y)=(0,0)] + Pr[(x,y)=(1,1)]=1/2 每一bit位都服从均匀分布，可以容易得出 Z是服从难过均匀分布。 The birthday paradox（生日悖论）更具体的分析见 Birthday problem 。 问题前提：一个人的生日在365天的任意一天是均匀分布的（实际当然不是，貌似更多集中在9月）。 根据信鸽理论（有N个鸽子，M个隔间，如果N&gt;M，那么一定有一个隔间有两只鸽子），所以367个人中，以100%的概率有两个人的生日相同。但是，当只有70个人时，就有99.9%的概率，其中两人生日相同；当只有23人，这个概率可以达到50%。 其实这并不是一个悖论，只是直觉误导，理性和感性认识的矛盾。当只有一个人，概率为0，当有367人时，为100%，所以我们直觉认为，这是线性增长的，其实不然。 概率论知识： 设事件A：23个人中，有两个人生日相等。 $P\\left(A^{\\prime}\\right)=\\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\frac{362}{365} \\times \\cdots \\times \\frac{343}{365}$ $P\\left(A^{\\prime}\\right)=\\left(\\frac{1}{365}\\right)^{23} \\times(365 \\times 364 \\times 363 \\times \\cdots \\times 343)$ $P\\left(A^{\\prime}\\right) \\approx 0.492703$ $P(A) \\approx 1-0.492703=0.507297 \\quad(50.7297 \\%)$ 推广到一般情况，n个人(n","link":"/2020/03/04/Dan-Intro/"},{"title":"「机器学习-李宏毅」：Regression","text":"在YouTube上看台大李宏毅老师的课，看完Regression讲座的感受就是： 好想去抓Pokemon！！！ 这篇文章将总结李宏毅老师Regression的讲座，并尝试实现其demo。 Regression（回归）DefineRegression：是找到一个$function$，进行预测。对输入的feature，输出一个$Scalar$(数值，标量)。 Example Application Look for a $function$ Stock Market Forecast（股票预测） $input$：过去的股价信息 $output$：明天的股价平均值（$Scalar$) Self-Driving Car(自动驾驶) $input$：路况信息 $output$：方向盘角度（$Scalar$) Recommendation（推荐系统） $input$：使用者A、商品B $output$：使用者A购买商品B的可能性 可见，$input$都是一些特征信息，$output$都是一个标量数值，这就是Regression。 Regression Case: Pokenmon 看完这节课，感想：好想去抓宝可梦QAQ 预测一个pokemon进化后的CP（Combat Power，战斗力）值。 为什么要预测呐？ 如果进化后的CP值高，就进化他，不然就把他当糖果，因为宝可梦很难抓的。（？没玩过，我也不懂o r z） 上图妙蛙种子的信息(可能的$input$)： $x_{cp}$：CP值 $x_s$:物种 $x_{hp}$:生命值 $x_w$:重量 $x_h$:高度 output：进化后的CP值。 $x_{cp}$：用下标表示一个object的component。 $x^1$：用上标表示一个完整的object。 Step 1: 找一个Model（function set）Model ：$y = b + w \\cdot x_{cp}$ 假设用上式作为我们的Model，那么这些函数： $ \\begin{aligned} &\\mathrm{f}_{1}: \\mathrm{y}=10.0+9.0 \\cdot \\mathrm{x}_{\\mathrm{cp}}\\\\ &f_{2}: y=9.8+9.2 \\cdot x_{c p}\\\\ &f_{3}: y=-0.8-1.2 \\cdot x_{c p} \\end{aligned} $ 等都属于这个集合，但是显然像$f_3$这种函数是bad，CP值不可能是负数。bad functions 很多，所以在下面的步骤，会说明如何判别一个函数的好坏，自动的选出最好的那个 $function$。 把Model 1一般化，得到线代中的 Linear Model：$y = b+\\sum w_ix_i$ $x_i$：x的feature $b$：bias,偏置值 $w_i$：weight，权重 Step 2: 判别Goodness of Function(Training Data)Training Data假定使用Model ：$y = b + w \\cdot x_{cp}$ Training Data：十只宝可梦，用向量的形式表示。 使用Training data来judge the goodness of function.。 Loss Function(损失函数)概率论：做线性回归，一般使用最小二乘法。一般回归，大多使用极大似然估计。 Loss function $L$ ：$L(f)=L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ 其中的 $\\hat{y}^n-(b+w\\cdot x_{cp}^n)$是Estimation error(估测误差) Loss Function的意义：它的 $input$是一个 $function$，它的 $output$体现了how bad it is,这个函数有多糟/好。 Figure the Result 上图横纵坐标是函数 $L$的参数 $w 、b$，图中的每一个point都是一个 $function $。 color：体现函数的输出，越红越大，说明选择的函数越bad。 所以我们要选择紫色区域结果最小的函数。 而这个得到best function的过程是可以通过无数次迭代实现的。（重复的迭代当时是交给计算机做了） Step 3:迭代找出Best Function$L(w,b)=\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ 找到Best Function: $f^{*}=\\arg \\min _{f} L(f)$ 也就是找到参数 $w^{*},b^{*}=\\arg \\min_{w,b} L(w,b)=\\arg \\min_{w,b}\\sum_{n=1}^{10}(\\hat{y}^n-(b+w\\cdot x_{cp}^n))^2$ arg ：argument,变元 arg min：使之最小的变元 arg max：使之最大的变元 据悉，线性回归的参数可以用线性代数的知识，解出closed-form solution（解析解），我先挖个坑QAQ，以后来填这块知识。[1] 在机器学习中，只要$L$函数可微分， 即可用Gradient Descent（梯度下降）的方法来求解。 Gradient Decent（梯度下降）和概率论中的梯度下降估计参数的原理相同，只是计算机不能直接解出方程的解，所以计算机的方法是迭代。 考虑一个参数w*$w^*=\\arg \\min_w L(w)$ 步骤： 随机选取一个初始值 $w^0$ 计算 $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp; &nbsp; $\\begin{equation} w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{0}} \\end{equation}$ 计算 $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^0}$ &nbsp;&nbsp; $\\begin{equation} w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{1}} \\end{equation}$ …until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$ 上图迭代过程的几点说明 $\\begin{equation}\\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}}\\end{equation}$的正负 如果是negative，也就是该点切线斜率是负的，那应该Increse w，以找到最低点。 Negative $\\rightarrow$ Increase w Positive $\\rightarrow$ Decrease w $-\\left.\\eta \\frac{d L}{d w}\\right|_{w=w^{i}}$：步长 $\\eta$：learning rate（学习速度），事先设好的值。 $-$(负号)：如果 $\\begin{equation}\\left.\\frac{\\mathrm{d} L}{\\mathrm{d} w}\\right|_{w=w^{i}}\\end{equation}$是负的，应该增加w。 Local optimal：局部最优和全局最优 如果是以上图像，则得到的w不是全局最优。 但线性回归的损失函数是凸函数，存在一个全局最优，没有局部最优。 考虑多个参数 $w^{*},b^{*}$ 微积分知识：gradient（梯度，向量)： $\\nabla L=\\left[\\begin{array}{l}\\frac{\\partial L}{\\partial w} \\\\frac{\\partial L}{\\partial b}\\end{array}\\right]$ 考虑多个参数和考虑一个参数思路相同，每次迭代，迭代两个参数。 $w^{*}, b^{*}=\\arg \\min _{w, b} L(w, b)$ 步骤： 随机选取初值 $w^0,b^0$ 计算 $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ &nbsp; &nbsp; $w^{1} \\leftarrow w^{0}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{0}, b=b^{0}} \\quad b^{1} \\leftarrow b^{0}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{0}, b=b^{0}}$ 计算 $\\left.\\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1},} \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ &nbsp; &nbsp; &nbsp; $w^{2} \\leftarrow w^{1}-\\left.\\eta \\frac{\\partial L}{\\partial w}\\right|_{w=w^{1}, b=b^{1}} \\quad b^{2} \\leftarrow b^{1}-\\left.\\eta \\frac{\\partial L}{\\partial b}\\right|_{w=w^{1}, b=b^{1}}$ …until $ \\frac{{\\rm d}L}{{\\rm d}w}|_{w=w^n}=0$, $ \\frac{{\\rm d}L}{{\\rm d}b}|_{b=b^n}=0$ 上图，坐标为 $L(w,b)$函数的参数，Color代表 $L$的大小，越紫值越小。 每一个点都是一个 $function$，沿着梯度方向（图中法线方向）迭代，找到全局最优点。 再次说明：线性回归中，损失函数是convex（凸函数），没有局部最优解。 $\\frac{\\partial L}{\\partial w}$和 $\\frac{\\partial L}{\\partial b}$的公式推导$L(w, b)=\\sum_{n=1}^{10}\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)^{2}$ 微积分的知识，显然。 数学真香。———我自己 $\\frac{\\partial L}{\\partial w}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)（-x_{cp}^n)$ $\\frac{\\partial L}{\\partial b}=\\sum_{n=1}^{10}2\\left(\\hat{y}^{n}-\\left(b+w \\cdot x_{c p}^{n}\\right)\\right)(-1)$ 实际结果分析Training Data Training Data的Error=31.9，但我们真正关心的是Testing Data的error。 Testing Data 是new Data：另外的Pokemon！。 Testing DataModel 1： $y = b+w\\cdot x_{cp}$ error = 35,比Training Data error更大。 Model 2：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2$ Testing error=18.4，比Model 1 好。 Model 3：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3$ Testing error=18.1，比Model 2好。 Model 4:$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4$ Testing error =28.8,比Model3更差。 Model 5：$y = b+w_1\\cdot x_{cp}+w_2\\cdot (x_{cp})^2+w_3\\cdot(x_{cp})^3+w_4 \\cdot (x_{cp})^4+w_5\\cdot (x_{cp})^5$ Testing error = 232.1,爆炸了一样的差。 Overfiting（过拟合了）从上面5个Model中可以得出，越复杂的函数模型，在Testing data上不一定能得到更好的结果。（过拟合使Training data 的误差越来越小） 所以在选择Model时，需要选择合适的Model。 对模型进行改进如果收集更多的Training Data，可以发现他好像不是一个Linear Model。 Back to step 1:Redesigh the Model从上面那张图，感觉他不是一个Linear Model,而是需要if 是伊布，模型是…，if 是…,可见是和物种有关系。 （很抱歉，我只认识右上角时伊布，QAQ，我也说不出名字） 但用 $\\delta$(微积分学的狄拉克函数)表示条件语句，可以发现，他仍然是一个线性模型。 $\\delta(x_s= \\text{Pidgey)}\\left\\{\\begin{array}{ll}=1 & \\text { If } x_{s}=\\text { Pidgey } \\\\ =0 & \\text { otherwise }\\end{array}\\right.$ $y = b_1\\cdot \\delta_1+w_1\\cdot \\delta_1+b2\\cdot \\delta_2+w_2\\cdot \\delta_2+…$是一个linear model。 拟合出来，Training Data 和Testing Data的error都蛮小的。 如果想让拟合误差更小，还可以考虑其他的feature，重量、高度、HP等。 但同样的，如果函数过于复杂，也会出现Overfitting的情况。 Back to Step 2:Regularization（正则化）对于Linear Model :$y = b+\\sum w_i x_i$ 为什么要正则化？我们希望得到的函数是较平滑的，这样测试时，函数的输出对输入的noise不sensitive，即输入x的细微扰动，并不太会影响输出的结果。 所以当参数越接近0，函数越平滑。因此在原本的loss function后加入 $\\lambda \\sum(w_i)^2$项（ $\\lambda$需手调），可以保证函数较平滑。 正则化： $L = \\sum_n(\\hat{y}^n-(b+\\sum w_i x_i))^2 + \\lambda\\sum(w_i)^2$ $\\lambda $大小的选择 可以得出结论： $\\lambda $越大，Training Error变大了。 当 $\\lambda$更大，损失函数更考虑w参数的取值，更关心函数的平滑程度，而更少的关心拟合的error。 $\\lambda $越大，Testing Error变小了，当 $\\lambda$过大时，又变大。 $\\lambda $较小时，$\\lambda $增大，函数更平滑，能良好适应数据的扰动。 $\\lambda $较大时，函数过于平滑，宛如直线，这显然不能准确预测。 因此，在调节$\\lambda $大小时，也要适当选择。 正则化的一个注意点在regularization中，我们只考虑了w参数，没有考虑bias偏置值参数。 因为正则化是寻找较平滑拟合，而偏置参数只是让函数平移，与平滑无关。 Again：Regularization不考虑bias Fllowing Gradient descent[2] Overfitting and regularization[3] Validation[4] 由于博主也是在学习阶段，学习后，会po上下面内容的链接。 希望能在学习、写博客的过程中，锻炼自己的表达能力，尽量让文风言简意赅又科学严谨。 写博客是为了记录与分享，感谢指正。 Reference[1] “周志华西瓜书p55,待补充” [2] [3] [4]","link":"/2020/02/29/Regression/"},{"title":"「Cryptography-Dan」:Stream Cipher 1","text":"Stream Cipher的第一部分：介绍了One Time Pad和Stream Cipher中的PRG。其中OTP部分叙述了什么是Perfect Secrecy？为什么OTP很难在实践中应用？Stream Cipher部分中，本文主要阐述了什么是PRG？Stream Cipher的另一种安全的定义（依靠PRG的unpredictable)。本文后半部分，详细阐述了一种weak PRG——线性同余生成器，它是如何工作的？它为什么不安全？如何attack it elegantly? The One Time PadSymmetric Ciphers: difinitionDef :a cipher difined over $\\mathcal{(K,M,C)}$ is a paire of “efiicient “ algorithms $(E,D)$ where $$ E :\\mathcal{K \\times M \\longrightarrow \\mathcal{C}} \\quad ,\\quad D:\\mathcal{K\\times\\mathcal{C}\\longrightarrow\\mathcal{M}} \\\\ s.t. \\quad \\forall m\\in \\mathcal{M},k\\in \\mathcal{K}:D(k,E(k,m))=m $$ $\\mathcal{(K,M,C)}$ 分别是密钥空间、明文空间、密文空间。 对称加密其实是定义在$\\mathcal{(K,M,C)}$ 的两个有效算法 $(E,D)$ ，这两个算法满足consistence equation(一致性方程)：$D(k,E(k,m))=m$ 。 一些说明： $E$ is ofen randomized. 即加密算法E总是随机生成一些bits，用来加密明文。 $D$ is always deterministic. 即当确定密钥和明文时，解密算法的输出总是唯一的。 “efficient” 的含义 对于理论派：efficient表示 in polynomial time（多项式时间） 对于实践派：efficient表示 in a certain time One Time Pad(OTP)Definition of OTPThe one time pad(OTP) 又叫一次一密。 用对称加密的定义来表示OTP： $\\mathcal{M=C=}{0,1}^n\\quad \\mathcal{K}={0,1}^n$ $E：\\quad c = E(k,m)=k\\oplus m \\quad$ $D:\\quad m = D(k,c)=k\\oplus c$ 明文空间和密文空间相同，密钥空间也是n位01串集合。 而且，在OTP中，密钥key的长度和明文message长度一样长。 加密过程如上图所示。 证明其一致性方程 Indeed ： $D(k,E(k,m))=D(k,k\\oplus m)=k\\oplus (k\\oplus m)=0\\oplus m=m$ 但是OTP加密安全吗？ 如果已知明文(m)和他的OTP密文(c)，可以算出用来加密m的OTP key吗？ ：当然，根据异或的性质，key $k=m\\oplus c$ 所以什么是安全呢？ Information Theoretic Security根据Shannon 1949发表的论文，Shannon’s basic idea: CT(Ciphertext) should reveal no “info” about PT(Plaintext)，即密文不应该泄露明文的任何信息。 Perfect Security Def:A cipher $(E,D)$ over $\\mathcal{(K,M,C)}$ has perfect security if $\\forall m_0,m_1 \\in \\mathcal{M}\\ (|m_0|=|m_1|) \\quad \\text{and} \\quad \\forall c\\in \\mathcal{C} $$$Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$$ $k \\overset{R}\\longleftarrow \\mathcal{K}$ 的意思是 $k$ 是 从$\\mathcal{K}$ 中随机取的，即随机变量 $k$ 的取值是均匀分布。 对任意 $m_0,m_1$ （并且message长度相同），那么在密钥空间任意取 $k$ , $k$ 将 $m_0,m_1$ 加密为相同密文的概率相同。 对attacker来说 ：攻击者截取一段密文c，那么c是由 $m_0,m_1$ 加密而来的概率是相同的，即攻击者也不知道明文到底是 $m_0$ 还是 $m_1$ （因为概率相同）。 $\\Rightarrow$ Given CT can’t tell if msg is $m_0 \\ \\text{or}\\ m_1 $ (for all $m_i$ ) . 【攻击者不能区分明文到底是 $m_?$ 】 $\\Rightarrow$ most powerful adv.(adversary) learns nothing about PT from CT. 【不管攻击者多聪明，都不能从密文中得到密文的信息】 $\\Rightarrow$ no CT only attack!! (but other attackers possible). 【惟密文攻击对OTP无效】 OTP has perfect secrecyLemma : OTP has perfect secrecy. 用上一小节的perfect securecy的定义来证明这个引理。 Proof： 要证明： $Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \\qquad \\text{where} \\ k\\overset{R}{\\longleftarrow}\\mathcal{K}$ 表达式： $\\forall m, c: \\quad \\operatorname{Pr}_{k}[E(k,m)=c]=\\frac{\\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c}{|\\mathcal{K}|}$ 对于任意m,c, $\\operatorname{Pr}_{k}[E(k,m)=c]$ 等于能将m加密为c的密钥个数除以密钥空间的大小。 $\\because |\\mathcal{K}|$ 是相同的，所以即证 ： $\\{ \\# \\text{keys} \\ k \\in \\mathcal{K} \\quad s.t.\\; E(k,m)=c \\}=\\text{const}$ 对于任意 m,c，能将m加密为c的OTP key只有一个： $k=m\\oplus c$ $\\therefore$ OTP has perfect secrecy. key-len $\\geq$ msg-len Perfect Secrecy的性质带来了一个bad news。 Thm: perfect secrecy $\\Rightarrow$ $|\\mathcal{K}|\\geq|\\mathcal{M}|$ 如果一个cipher满足perfect secrecy,那么其密钥的长度必须大于等于明文长度。这也是perfect secrecy的必要条件。 所以OTP是perfect secrecy的最优情况，$|\\mathcal{K}|=|\\mathcal{M}|$ ，密钥长度等于明文长度。 为什么说是一个bad news呢？ 如果Alice用OTP给Bob发一段msg，在她发之前，她需要先发一个和msg等长的key，这个key只有Alice和Bob知道。 所以如果Alice有能保密传输key的方法，那她何不直接用这个方法传输msg呢？ 所以OTP : hard to use in practice! (long key-len) 因此，我们需要key-len短的cipher。 Pseudorandom Generators（伪随机数生成器）Stream Ciphers: making OTP practicalStream Ciphers（流密码）的思想就是：用PRG（pseudorandom Generators） key 代替 “random” key。 PRG其实就是一个function G：${ 0,1 }^s\\longrightarrow { 0,1 }^n \\quad, n&gt;&gt;s$ 。 通过函数将较小的seed space映射到大得多的output space。 注意： function G is eff. computable by a deterministic algorithm. 函数G是确定的，随机的只有s，s也是G的输入。 PRG的输出应该是 “look random”（下文会提到的PRG必须是unpredictable） Stream Ciphers的过程如上图所示：通过PRG，将长度较短的k映射为足够长的G(k)，G(k)异或m得到密文。 有两个问题？ 第一，Stream Cipher安全吗？为什么安全？ 第二，Stream Cipher have perfect secrecy? 现在，只能回答第二个问题。 ：流密码没有perfect secrecy。因为它不满足key-len $\\geq$ msg-len，流密码的密钥长度远小于明文长度。 流密码没有perfect secrecy，所以我们还需要引入另一种安全，这种安全和PRG有关。 PRG must be unpredictablePRG如果predictable，流密码安全吗？ Suppose predictable假设PRG是可预测的，即： $ \\exists:\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1,...,n} $ 已知G(k)输出的前i bis，存在一种算法，能计算G(k)的后面剩余的bits。 攻击如上图所示： 如果attacker has prior knowledge：已知一段密文前缀的对应明文（m斜线字段）（比如在SMTP协议中，报文的开头总是”from”） attacker将该密文字段与已知明文字段异或，得到G(k)的前缀。 因为PRG是可预测的，所以可以通过G(k)的前缀计算出G(k)的剩下部分。 得到的G(K)就可以recover m。 即使，G(k)只能预测后一位，即 $\\quad G(k)|_{1,2,...,i}\\quad \\overset{alg.}\\longrightarrow \\quad G(k)|_{i+1}$ ，也不安全，当预测出下一位时，又得到了新的前缀，最终得到完整的G(k)。 所以当PRG可预测时，流密码就不安全了。 所以用Stream Cipher时，PRG必须unpredictable! Predictable: difinitionPredictable Def : $ \\exists $ \"eff\" alg. A and $\\exists$ $0\\leq i\\leq n-1$ ， s.t. $Pr_{k \\overset{R}\\leftarrow \\mathcal{K} } {[A(G(k)|_{1,2,...,i})=G(k)|_{i+1}]}>1/2 +\\epsilon$ for non-negligible $\\epsilon$ (e.g. $\\epsilon=1/2^{30}$) 可预测：即存在算法，通过G(k)的前i位可以计算出第i+1位的概率大于1/2 + $\\epsilon$ (不可忽略的值) Unpredictable Def : 即predictable的反面， $\\forall i$ : no “eff.” adv. can predict bit(i+1) for “non-neg” $\\epsilon$ . Q：假设 $\\mathrm{G}: \\mathrm{K} \\rightarrow{0,1}^{\\mathrm{n}} $ ，满足XOR(G(k))=1，G可预测吗？ W：G可预测，存在i = n-1,因为当已知前n-1位,可以预测第n位。 Weak PRGsLinear Congruential Generators一种应该永远不在安全应用中使用PRG——LCG（linear congruential generators）(线性同余随机生成器)。 虽然他们在应用中使用很快，而且其输出还有良好的统计性质（比如0的个数和1的个数基本相等等），但他们应该never be used for cryptographic。 因为在实践中，给出LCG输出的一些连续序列，很容易计算出输出的剩余序列。 Basic LCGDefinitionBasic LCG has four public system parameters: an integer q, two constants a,b $\\in { 0,…,q-1}$ , and a positive integer $w\\leq q$ . The constant a is taken to be relatively prime to q. 【有四个公开参数：整数q，两个q剩余系下的常数a,b，（a与q互素）一个小于等于q的正整数w。】 We use $\\mathcal{S}_q$ and $\\mathcal{R}$ to denote the sets: $\\mathcal{S}_{q}:=\\{0, \\ldots, q-1\\} ; \\quad \\mathcal{R}:=\\{0, \\ldots,\\lfloor(q-1) / w\\rfloor\\}$ Now, the generators $G_{\\mathrm{lcg}}: \\mathcal{S}_{q} \\rightarrow \\mathcal{R} \\times \\mathcal{S}_{q}$ with seed $s\\in\\mathcal{S}_{q}$ defined as follows: $G_{\\operatorname{lcg}}(s):=(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ 【LCG的输出是一对数，$(\\lfloor s / w\\rfloor, \\quad a s+b \\bmod q)$ 】 当 $w=2^t$ 时，$\\lfloor s / w\\rfloor$ simpky erases the t lease significant bits of s【向右平移t位】。 Insecure当已知 $s^{\\prime}:=a s+b \\bmod q$ ，即可直接求出s，也就求出了所谓的随机数 $\\lfloor s/w\\rfloor$ . Variant: Blum-Micali constructionDefinition 如上图所示，变体的LCG是一个迭代，输出不包括 $s_i$ ，把 $r_1,…,r_n$ 作为一次迭代的输出。 不同的应用系统使用不同的 $q,a,b,w$ 参数，在Java 8 Development Kit（JDKv8）中，$q=2^{48}$ , $w=2^{22}$ ,constant $a=\\text{0x5DEECE66D}$ , $b=\\text{0x0B}$ 。 所以在JDKv8中, LCG的输出其实是 $s_i$（48bits） 的前48-22=26 bits 。 显然JDKv8中的参数大小应用在安全系统中，还是太不安全了。 how to attack in JDKv8 在迭代的第一次输出中，LCG就 reveal 26bits of the seed s。 对于s剩下的后22个bits，attacker can easily recover them by exhausitive search(穷举)： 对于每个可能的取值，attacker都能得到一个候选seed $\\hat{s}$ 用 $\\hat{s}$ 来验证我们所直接得到的LCG的输出。 如果 $\\hat{s}$ 验证失败，则到第三步继续穷举。直至验证成功。 当穷举至正确的s时，就可以直接预测LCG的剩余输出。 在现代处理器中，穷举 $2^{22}$ (4 million) 只需要1秒。所以LCG的参数较小时，是很容易attack。 当 $q=2^{512}$ 时，这种穷举的攻击方法就失效了。但是有一种对于LCG的著名攻击方法[1]，即使每次迭代，LCG只输出较少的bits，也能从这些较少的但连续的输出序列中预测出整个LCG输出序列。 Cryptanalysis ：elegant attackWarning of MathSupposeSuppose : q is large (e.g. $q=2^{512}$ ), and $G_{lcg}^{(n)}$ outputs about half the bits of the state s per iteration. 【q很大， $G_{lcg}^{(n)}$ 每次输出s的一半左右的bits】 More precisely, suppose: $w&lt;\\sqrt{q}/c$ for fixed c（e.g. $c=32$ ） 【保证输出s前一半左右bits的这个条件】 Suppose the attacker is given two consecutive outputs of the gnerator $r_i,r_{i+1}\\in \\mathcal{R}$ . 【已知两个连续输出 $r_i,r_{i+1}\\in \\mathcal{R}$ 】 Attacker Knows $r_{i}=\\left\\lfloor s_{i} / w\\right\\rfloor \\quad \\text { and } \\quad r_{i+1}=\\left\\lfloor s_{i+1} / w\\right\\rfloor=\\left\\lfloor\\left(a s_{i}+b \\bmod q\\right) / w\\right\\rfloor$ 【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i$ 】 $r_{i} \\cdot w+e_{0}=s \\quad \\text { and } \\quad r_{i+1} \\cdot w+e_{1}=a s+b+q x \\qquad (0\\leq e_0,e_1&lt;w&lt;\\sqrt{q}/c)$ 【 去掉floor符号和mod：$e_0,e_1$ 是 $s_i,s_{i+1}$ 除 $w$ 的余数】 【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i,e_0,e_1,x$ 】 re-arranging: put $x$ and $s$ on the left $s=r_{i} \\cdot w+e_{0} \\quad \\text { and } \\quad a s+q x=r_{i+1} w-b+e_{1}$ 【把未知参数s，x放在等式左边，方便写成矩阵形式】 $s \\cdot\\left(\\begin{array}{l}1 \\ a\\end{array}\\right)+x \\cdot\\left(\\begin{array}{l}0 \\ q\\end{array}\\right)=\\boldsymbol{g}+\\boldsymbol{e} \\quad \\text { where } \\quad \\boldsymbol{g}:=\\left(\\begin{array}{c}r_{i} w \\ r_{i+1} w-b\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{e}:=\\left(\\begin{array}{c}e_{0} \\ e_{1}\\end{array}\\right)$ 【已知： $\\boldsymbol{g},a,q$ ，未知：$\\boldsymbol{e},s,x$ 】 to break the generator it suffices to find the vector $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}$ . 【令 $u\\in {Z}^2$ , $\\boldsymbol{u}:=\\boldsymbol{g}+\\boldsymbol{e}=s \\cdot(1, a)^{\\mathrm{T}}+x \\cdot(0, q)^{\\mathrm{T}}$ 】 【如果我们求出了 $\\boldsymbol{u}$ ，那可以用线性代数的知识解出 $s$ 和 $x$ ,再用 $s$ 来预测PRG的剩下输出】 konws $\\boldsymbol{g}$ , knows $\\boldsymbol{e}$ is shorter, and $|\\boldsymbol{e} |_{\\infty}$ is at most $\\sqrt{q}/c$ , knows that $\\boldsymbol{u}$ is “close” to $\\boldsymbol{g}$ . 【e向量很小，$|\\boldsymbol{e} |_{\\infty}$ 上界是$\\sqrt{q}/c$ ，u离g很近】 Taxicab norm or Manhattan(1-norm) ${\\|}A{\\|}_1=\\max \\{ \\sum|a_{i1}|,\\sum|a_{i2}|,...,\\sum|a_{in}| \\}$ （列和范数，A矩阵每一列元素绝对值之和的最大值） Euclidean norm(2-norm) $\\|\\mathbf{x}\\|=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}\\right)^{1 / 2}$ $\\infty$-范数 $\\|A\\|_{\\infty}=\\max \\{ \\sum|a_{1j}|,\\sum|a_{2j}|,...,\\sum|a_{mj}| \\}$ （行和范数，A矩阵每一行元素绝对值之和的最大值） attack can figure the lattice with attacking LCG. the lattice is generated by the vectors $(1,5)^T$ and $(0,29)^T$ , the attacker has a vector $\\boldsymbol{g}=(9,7)^T$ and wishes to find the closest lattice vector $\\boldsymbol{u}$ . 【上图是 $(1,5)^T$ 和 $(0,29)^T$ 两个向量生成的的格点，希望能从以上格点找到离已知 $\\boldsymbol{g}$ 向量最近的格点】 $\\mathcal{L}_a$ :由 $(1, a)^{\\mathrm{T}},(0, q)^{\\mathrm{T}}$ 作为基向量生成的点集合。 The problem is a special case of a general problem call the closest vector problem: given a lattice $\\mathcal{L}$ and a vector $\\boldsymbol{g}$ ,find a vector in $\\mathcal{L}$ that is closest to $\\mathcal{g}$ . There is an efficient polynomial time algorithm for this problem.[2] 【问题归结于 closest vector problem问题，在已知栅格点集合中找离某一向量最近的点，此问题已有多项式时间算法】 step 8 aboveLemma : * For at least $(1-16/c^2)\\cdot q $ of the a in $\\mathcal{S}_q$ , the lattice $\\mathcal{L}_a\\sub Z_2$ has the following property: for every $\\boldsymbol{g} \\in Z^2$ there is at most one vector $\\boldsymbol{u}\\in \\mathcal{L}_a$ such that $\\|\\boldsymbol{g}-\\boldsymbol{u}\\|_{\\infty}","link":"/2020/03/15/StreamCipher1/"},{"title":"「Cryptography-Dan」:Stream Cipher 2","text":"作为Stream Cipher的第二篇文章。第一部分分析了基于Stream Cipher的两种攻击：第一种是Two time pad,第二种是对与其完整性的攻击，即流密码是可被篡改的。第二部分具体说明了一些使用流密码加密的例子。包括分析基于软件的RC4流密码、基于硬件的CSS流密码和现代的安全流密码:eStream中的Salsa20。 Attack on OTP and stream ciphersAttack1: two time pad is insecureNever use strame cipher key more than once!! why insecure使用相同的PRG(k)加密不同明文时：$$C_1 \\leftarrow m_1 \\oplus \\text{PRG(k)}\\C_2 \\leftarrow m_2 \\oplus \\text{PRG(k)}$$Eavesdropper（窃听者）截获这两段密文 $C_1\\ C_2$ ，对密文进行疑惑操作，可得： $C_1 \\oplus C_2\\rightarrow m_1\\oplus m_2$ 。 在传输中，英语字母是用ASCII编码后再传输，所以这样的编码会带来很多redundancy（冗余），即根据 $m_1\\oplus m_2$ ，可以得到 $m_1\\ m_2$ 。 因此，当一个密钥会被使用多次时，就不应该直接用stream cipher，后面的章节会介绍multi-use ciphers。 Examples: Project Venona(1941-1946)我们已经知道：加密应该用OTP，即一次性密钥。 但是，当时是通过人工掷骰子并记录得到密钥，工作费时费力。因此不得不用生成的密钥加密多条消息。 最后仅凭截获密文，就破译了3000多条消息。 Examples: MS-PPTP(Windows NT)微软在Windows NT的PPTP协议（point to point transfer protocol）中使用的流密码是：RC4。 在这个协议中允许一个端系统向另一个端系统发送加密后的信息。 过程如下： 在一次对话连接中：主机想发送$m_1\\ m_1\\ m_3$ 三条消息进行查询，服务器想发送 $s_1\\ s_1\\ s_3$ 三条消息进行响应。 主机和服务器hava a shared key:k。 知道密钥不能加密多次，于是主机将三条消息进行concatenation（联结）： $m_1||m_2||m_3$ 。 主机用k作为密钥，得到G(k)，进行加密 $[m_1||m_2||m_3]\\oplus\\text{G(k)}$ 。 同样，服务器也将响应消息进行联结： $s_1||s_2||s_3$ 。 服务器也用k作为密钥，得到相同的G(k)，对响应消息进行加密 $[s_1||s_2||s_3]\\oplus\\text{G(k)}$ 。 因此，在一次对话中，主机和服务器都使用了相同的 G(k)进行加密，也就是 two time pad。 如何改进主机和服务器have a shared pair of key, 即主机和服务器都使用不同的key进行加密。 Examples: 802.11b WEPHow it worksWEP(Wired Equivalent Privacy)，有效等效加密，是一种用于IEEE 802.11b的一种安全算法。这个算法设计的很糟糕，现已被WPA所淘汰。 WEP用于Wi-Fi通信，是他的的加密层。 WEP的算法过程如下： 主机和路由器进行通信： 主机和路由 have a shared key。 主机想要发送一段消息，包括明文m和其校验码CRC(m)。 PRG’s seed： IV||k, k is a long term key，IV is a counter. Length of IV: 24 bits. IV的作用：每一次传送数据包时，用IV来改变每次的密钥。 用(IV||k作为密钥，得到PRG(IV||k),使用流密码进行加密传输。 主机直接发送IV和密文。 路由器用收到的IV和k连接，用PRG(IV||k)，对密文解密。 Problems of IV IV 导致的问题1: two time pad Length of IV: 24 bits Related IV after $2^{24}$ (16M frames) 【当发送16百万的帧后，PRG又会重复】 On some 802.11 cards: IV rests to 0 after power cycle. 【在有些单片机上，重启后IV会变成0：每次重启都会使用PRG(0||k)加密】 IV 导致的问题2: related keys 在PRG中，key for frame is related。(IV||k)，k是104 bits, IV 是24 bits，所以key的后104位总是相同的，不同密钥之间的差异也很小。 对RC4 PRG的攻击： Fluhrer, Mantin, and Shamir在2001年:只需要监听 $10^6$ 帧即可破译密钥[1]。 Recent attacks：只需要监听4000帧，即可破译WEP网络中的密钥。 所以，密钥关联是灾难性的。 Avoid related keys！ A better construction对于WEP，一种更好的做法是：将多个要发送的帧联结起来，得到 $m_1||m_2||…||m_n$ 长流，再用PRG对这个长流加密。 如上图所示，k扩展后，被分成很多段，用第一段加密第一帧，第二段加密第二帧……。 这样，加密每一帧的密钥都是一个伪随机密钥。(no relationship, and looks like random)。 当然，更好的解决方法是使用更强的加密算法（WPA2）。 Examples: disk encryption另一个例子是硬盘加密，在这个例子中，你会发现：使用流密码对硬盘加密不是一个好的举措。 如果使用流密码： Alice 想要给Bob写一封邮件，如上图所示。 邮件经过硬盘加密（流密码）后，存入内存块。 Alice 想要对存在这个硬盘中的邮件进行修改。 Alice只把Bob改成了Eve，其他部分都没有变，如上图所示。 保存后，邮件再次经过硬盘加密（流密码）后，存入内存块。 Attacker：他得到了硬盘上最初的密文和修改后的密文。 通过分析，他发现两段密文只有前小部分不同。（用相同的流密码密钥加密，修改后，密文很容易看出变化） 虽然Attacker不知道Alice是怎么修改的，但是他知道了Alice修改的具体位置。 $\\Rightarrow$ attacker得到了他不应该知道的信息，即修改的具体位置。 在硬盘加密中，对于文本内容的修改前后，也使用了相同的密钥段加密不同的消息，即two time pad。 因此在硬盘加密中，不推荐使用流密码。 Two time pad: SummaryNever use stream cipher key more than once!! Network traffic: negotiate new key for every session. Disk encryption: typically do not use a stream cipher. Attack2: no integrity(OTP is malleable)OPTP和Stream Cipher一样，不提供完整性的保证，只提供机密性的保证。 如上图所示： 如果attacke截获：密文 $m\\oplus k$ 。 并用sub-permutation key（子置换密钥）来对密文进行修改，得到新的密文：$(m\\oplus k)\\oplus p$ 新的密文最后解密得到的明文是 $m\\oplus p$ 。 所以对于有修改密文能力的攻击者来说，攻击者很容易修改密文，并且修改后的密文，对原本解密后的明文也有很大的影响。 具体攻击如下： Bob想要发送一封邮件：消息开头是From: Bob，使用OTP加密后，发送密文。 Attacker：截获了这段密文 假设：attacker知道这封邮件是来自Bob。 attacker想修改这封密文邮件，使得它来自others。 于是它用一个子置换密钥对原密文的特定位置（即Bob密文位置）进行操作，得到新的密文：From： Eve。 这个子置换密钥是什么呢？ 如上图所示，Bob的ASCII码是 42 6F 62，Eve的ASCII码是 45 76 65。 那么Bob $\\oplus$ Eve的ASCII码是 07 19 07。 因此这个子置换密钥是07 19 07。 最后收件人进行解密，得到的是明文：From：Eve。 对attacker来说，虽然他不能创建来自Eve的密文，但是他可以通过修改原本的密文，达到相同的目的。 Conclusion: Modifications to ciphertext are undertected and have predictable impact on plaintext. Real-world Stream CiphersOld example(SW): RC4RC4流密码，是Ron RivestRC4在1987年设计的。曾用于secure Web traffic(in the SSL/TLS protocol) 和wireless traffic (in the 802.11b WEP protocol). It is designed to operate on 8-bit processors with little internal memory. At the heart of the RC4 cipher is a PRG, called the RC4 PRG. The PRG maintains an internal state consisting of an array S of 256 bytes plus two additional bytes i,j used as pointers into S. 【RC4 cipher的核心是一个PRG，called the RC4 PRG。这个PRG的内部状态是一个256字节的数组S和两个指向S数组的指针】 RC4 stream cipher generator setup algorithms: 对S数组进行初始化，0-255都只出现一次入下图所示： 伪代码 stream generator: Once tha array S is initialized, the PRG generates pseudo-random output one byte at atime, using the following stream generator: The procedure runs for as long as necessary. Again, the index i runs linearly through the array while the index j jumps around. Security of RC4具体参见「A Graduate Course in Applied Cryptography」的p76-78 挖坑，有空填坑 cryptanalysis of RC4[2] Weakness of RC4 Bias in initial output: Pr[2^nd^ byte=0]=2/256. 如果PRG是随机的，其概率应该是1/256。 而Pr[2^nd^ byte=0]=2/256的结果是：消息的第二个字节不被加密的概率比正常情况多一倍。（0异或不变） 统计的真实情况是，不止第二个字节，前面很多字节的概率很不都均匀。 因此，如果要使用RC4 PRG，从其输出的257个字节开始使用。 Prob. of (0,0) is 1/256^2^ +1/256^3^ . 如果PRG是随机的，00串出现的概率应该是(1/256)^2^ . Related key attackes. 在上小节中「Examples: 802.11b WEP」，WEP使用的RC4流密码，related key对安全通信也是灾难性的。 Old example(HW): CSS (badly broken)The Content Scrambling System (CSS) is a system used for protecting movies on DVD disks. It uses a stream cipher, called the CSS stream cipher, to encrypt movie contents. CSS was designed in the 1980’s when exportable encryption was restricted to 40-bits keys. As a result, CSS encrypts movies using a 40-bits key. 【1980的美国出口法，限制出口的加密算法只能是40位，于是CSS的密钥是40位】[amazing.jpg] While ciphers using 40-bit keys are woefully insecure, we show that the CSS stream cipher is particularly weak and can be broken in far less time than an exhaustive search over all 2^40^ keys. 【虽然40位的密钥本来就不够安全，但是我们能用远小于穷举时间的方法破解CSS】 因为博主也是第一次学，很多东西也不了解。 所以概述性语言，我用教科书的原文记录，附注一些中文。望海涵～ Linear feedback shift register(LFSR)CSS 流密码是由两个LFSR（线性反馈移位寄存器）组成的，除了CSS，还有很多硬件系统是基于LFSR进行加密操作，但无一例外，都被破解了。 DVD encryption (CSS)：2 LFSRs GSM encryption (A5/,2): 3 LFSRs 【全球通信系统】 Bluetooth(E0): 4LFSRs LFSR can be implemented in hardware with few transistors. And a result, stream ciphers built from LFSR are attractive for low-cost consumer electronics such as DVD players, cell phones, and Bluetooth devices. 【LFSR在硬件上运行很快，也很省电，所以虽然基于LFSR的算法都被破解了，但是改硬件有点困难，所以还是有很多系统在使用】 上图是一个8位LFSR{4,3,2,0}。 LFSR是由一组寄存器组成，LFSR每个周期输出一位（即0位）。 有一些位（如上图的4，3，2，0）称为tap positions(出头)，通过这些位计算出feedback bit(反馈位)。 反馈位和寄存器组的未输出位组成新的寄存器组。 伪代码如下： 所以基于LFSR的PGR的seed是寄存器组的初始状态。 how CSS worksCSS的seed=5 bytes=40 bits。 CSS由两个LFSR组成，如下图所示，一个17-bit LFSR，一个25-bit LFSR。 seed of LFSR: 17-bit LFSR: 1||first 2 bytes ，即17位。 25-bit LFSR: 1||last 3 bytes，即25位。 CSS过程： 两个LFSR分别运行8轮，输出8 bits。 两个8bits 相加mod 256（还有加上前一块的进位）即是CSS一轮的输出：one byte at a time. Cryptanalysis of CSS (2^16 time attack)当已知CSS PRG的输出时，我们通过穷举（2^40^ time）破解得到CSS的seed。 但还有一种更快的破解算法，只需要最多2^16^ 的尝试。 破解过程如下： 影片文件一般是MPEG文件，假设我们已知明文MPEG文件的前20个字节。（已知明文的prefix） CSS是流密码，可以通过已知prefix还原出CSS的prefix，即CSS PRG的前20个字节的输出。 For all possible initial settings of 17-bit LFSR do: run 17-it LFSR to get 20 bytes of output. 【先让17-bit输出20个字节】 subtract from CSS prefix $\\Rightarrow$ candidate 20 bytes output of 25-bit LFSR. 【通过还原的CSS PRG的输出，得到25-bit输出的前20个字节】 If consistent with 25-bit LFSR, found correct initial settings of both!! 【具体是如何判别这20个字节是否是一个25-bit LFSR的输出呢？】 假设前三个字节是y1, y2, y3. 那么25-bit LFSR的initial :{1, y1 , y2, y3},其中y都是8 bits. 用这个初始状态生成20个字节，如果和得到的20个字节相同，则正确，否则再次枚举。 当得到了两个LFSR的seed, 就可以得到CSS PRG的全部输出，即可破解。 Modern stream ciphers: eStreammain idea eStream PRG ： $\\{0,1\\}^s\\times R \\rightarrow \\{0,1\\}^n$ (n&gt;&gt;s) seed: ${0,1}^s$ nonce: a non-repeating value for a given key.【就对于确定的seed,nonce绝不重复】 Encryption: $\\text{E(k, m; r)}=\\text{m}\\oplus \\text{PRG(k; r)} $ The pair (k,r) is never used more than once. 【PRG的输入是(k,r), 确保OTP】 eStram: Salsa 20(SW+HW)Salsa20/12 and Salsa20/20 are fast stream ciphers designed by Dan Bernstein in 2005. Salsa 20/12 is one of four Profile 1 stream cipher selected for the eStream portfolio of stream ciphers. eStream is a project that identifies fast and secure stream ciphers that are appropriate for practicle use. Variants of Salsa20/12 and Salsa20/20, called ChaCha12 and ChaCha20 respectively, were proposed by Bernstein in 2008. These stream ciphers have been incorporated into several widely deployed protocols such as TLS and SSH. Salsa20 PRG: $\\{0,1\\}^{128 \\text { or } 256} \\times\\{0,1\\}^{64} \\longrightarrow\\{0,1\\}^{n}$ (max n = 2^73^ bits) Salsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ 通过计数器，使得输出联结，可以输出as long as you want pseudorandom segment. 算法过程如上图所示：Salsa20 PRG $(\\mathrm{k} ; \\mathrm{r}):=\\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 0)) | \\mathrm{H}(\\mathrm{k},(\\mathrm{r}, 1)) | \\ldots$ 32 bytes的{k,r,i}通过扩展得到64 bytes的{ $\\tau_0,k,\\tau_1,r,i,\\tau_2,k,\\tau_3$ }. k :16 bytes的seed. i: 8 bytes的index，计数器。 r: 8 bytes的nonce. $\\tau_{0,1,2,3}$ 都是4 bytes的常数，Salsa20算法说明书上公开确定的值。 64 bytes 通过h函数映射，十轮。 h : invertible function designed to be fast on x86(SEE2). 在x86上有SEE2指令可以直接运行h函数，所以很快。 h是逆函数（也是公开的函数），输出可以通过逆运算得到其输入。 h是一个一一映射的map，每一个64bytes的输入都有唯一对应的64 bytes的输出。 将第十轮H函数的输出和最开始的输入做加法运算，word by word(32位)，即每4 bytes相加。 为什么要有这一步？ h是可逆运算，如果直接将函数的输出作为PRG的输出，那可以通过h的逆运算得到原64 bytes，也就得到了(k; r). Is Salsa20 secure(unpredictable)前文我们通过unpredictable来定义PRG的安全（下一篇文章会给出安全PRG更好的定义），所以Salsa20 安全吗？是否是不可预测的？ Unknown：no known provably secure PRGs. 【不能严格证明是一个安全PRG（后文会继续讲解什么是安全的PRG），如果严格证明了，也就证明了P=NP】 In reality： no known attacks bertter than exhaustive search. 【现实中还没有比穷举更快的算法】 Performance通过下图的比较，如果在系统中需要使用流密码，建议使用eStream。 speed ：该算法每秒加密多少MB的数据。 Reference S. Fluhrer, I. Mantin, and A. Shamir. Weaknesses in the key scheduling algorithm of RC4. In proceedings of selected areas of cryptography (SAC), pages 1-24, 2001. 「A Graduate Course in Applied Cryptography」p76-78:挖坑 待补充","link":"/2020/03/19/StreamCipher2/"},{"title":"「机器学习-李宏毅」:HW1-Predict PM2.5","text":"在本篇文章中，用手刻Adagrad完成了「机器学习-李宏毅」的HW1-预测PM2.5的作业。其中包括对数据的处理，训练模型，预测，并使用sklearn toolkit的结果进行比较。有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub Task Descriptionkaggle link 从中央气象局网站下载的真实观测资料，必须利用linear regression或其他方法预测PM2.5的值。 观测记录被分为train set 和 test set, 前者是每个月前20天所有资料；后者是从剩下的资料中随机取样出来的。 train.csv: 每个月前20天的完整资料。 test.csv: 从剩下的10天资料中取出240笔资料，每一笔资料都有连续9小时的观测数据，必须以此观测出第十小时的PM2.5. Process Datatrain data如下图，每18行是一天24小时的数据，每个月取了前20天（时间上是连续的小时）。 test data 如下图，每18行是一笔连续9小时的数据，共240笔数据。 最大化training data size 每连续10小时的数据都是train set的data。为了得到更多的data，应该把每一天连起来。即下图这种效果： 每个月就有： $20*24-9=471$ 笔data 123456789# Dictionary: key:month value:month datamonth_data = {}# make data timeline continuousfor month in range(12): temp = np.empty(shape=(18, 20*24)) for day in range(20): temp[:, day*24: (day+1)*24] = data[(month*20+day)*18: (month*20+day+1)*18, :] month_data[month] = temp 筛选需要的Features : 这里，我就只考虑前9小时的PM2.5，当然还可以考虑和PM2.5等相关的氮氧化物等feature。 training data 1234567# x_data v1: only consider PM2.5x_data = np.empty(shape=(12*471, 9))y_data = np.empty(shape=(12*471, 1))for month in range(12): for i in range(471): x_data[month*471+i][:] = month_data[month][9][i: i+9] y_data[month*471+i] = month_data[month][9][i+9] testing data 12345# Testing data featurestest_x = np.empty(shape=(240, 9))for day in range(240): test_x[day, :] = test_data[18*day+9, :]test_x = np.concatenate((np.ones(shape=(240, 1)), test_x), axis=1) Normalization 123456789101112# feature scale: normalizationmean = np.mean(x_data, axis=0)std = np.std(x_data, axis=0)for i in range(x_data.shape[0]): for j in range(x_data.shape[1]): if std[j] != 0: x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]for i in range(test_data.shape[0]): for j in range(test_data.shape[1]): if std[j] != 0: test_data[i][j] = (test_data[i][j] - mean[j])/std[j] Training手刻Adagrad 进行training。（挖坑：RMSprop、Adam[1] Linear Pseudo code 123456Declare weight vector, initial lr ,and # of iterationfor i_th iteration : y’ = the product of train_x and weight vector Loss = y’ - train_y gradient = 2*np.dot((train_x)’, Loss ) weight vector -= learning rate * gradient 其中的矩阵操作时，注意求gradient时矩阵的维度。可参考下图。 Adagrad Pseudo code 123456789Declare weight vector, initial lr ,and # of iterationDeclare prev_gra storing gradients in every previous iterations for i_th iteration : y’ = the inner product of train_x and weight vector Loss = y’ - train_y gradient = 2*np.dot((train_x)’, Loss ) prev_gra += gra**2 ada = np.sqrt(prev_gra) weight vector -= learning rate * gradient / ada 注：代码实现时，将bias存在w[0]处，x_data的第0列全1。因为w和b可以一同更新。（当然，也可以分开更新） Adagrad training 123456789101112131415161718192021# train-adagradbatch = x_data.shape[0] # full batchepoch = 400# some parameters for trainingdim = x_data.shape[1]+1w = np.zeros(shape=(dim, 1)) # concatenate bias = w[0]lr = np.full((dim, 1), 0.8) # learning rategrad = np.empty(shape=(dim, 1)) # gradient of loss to every paragradsum = np.zeros(shape=(dim, 1)) # sum of gradient**2x_data = np.concatenate((np.ones(shape=(x_data.shape[0], 1)), x_data), axis=1)loss_his = np.empty(shape=(epoch, 1))for T in range(epoch): L = y_data - np.dot(x_data, w) loss_his[T] = np.sum(L**2) / x_data.shape[0] grad = (-2)*np.dot(np.transpose(x_data), L) gradsum = gradsum + grad**2 w = w - lr*grad/(gradsum**0.5) Testing1answer = np.dot(test_x, w) Draw and Analysis在每次迭代更新时，我将Loss的值存了下来，以便可视化Loss的变化和更新速度。 Loss的变化如下图：(红色的是sklearn toolkit的loss结果) 此外，在源代码中，使用sklearn toolkit来比较结果。 结果如下： 123456789101112131415161718192021222324252627v1: only consider PM2.5Using sklearnLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)bias= [21.37402689]w= [[ 0.00000000e+00] [-5.54801503e-01] [-4.32873874e-01] [ 3.63669814e+00] [-3.99037687e+00] [-9.07364636e-01] [ 8.83495803e+00] [-9.51785135e+00] [ 1.32734655e-02] [ 1.81886444e+01]]In our modelbias= [19.59387132]w= [[-0.14448468] [ 0.39205748] [ 0.26897134] [-1.02415371] [ 1.21151411] [ 2.21925424] [-5.48242478] [ 4.01080346] [13.56369122]] 发现参数有一定差异，于是我在testing时，也把sklearn的结果进行预测比较。 一部分结果如下： 1234567891011121314151617['id', 'value', 'sk_value']['id_0', 3.551092352912313, 5.37766865368331]['id_1', 13.916795471648756, 16.559245678900034]['id_2', 24.811333478647043, 23.5085950470451]['id_3', 5.101440436158914, 6.478306159981166]['id_4', 26.7374726797937, 27.207516152986663]['id_5', 19.43735346531517, 21.916809502961648]['id_6', 22.20460696285646, 24.751295357256392]['id_7', 29.660872382552682, 30.24344042612033]['id_8', 17.5964527734513, 16.64242443764712]['id_9', 56.58017426943178, 59.760988216575115]['id_10', 13.767504260132299, 10.808372404511037]['id_11', 11.743000466164233, 11.526958393801682]['id_12', 59.509878887026105, 64.201008247897]['id_13', 53.19824337746267, 54.3856368053018]['id_14', 21.97191108867921, 24.530720709840974]['id_15', 10.833283625735444, 14.350345549104446] Code有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137########################## Date: 2020-4-4# Author: FredLau# HW1: predict the PM2.5##########################import sysimport numpy as npimport pandas as pdimport csvfrom sklearn import linear_modelimport matplotlib.pyplot as plt###################### process data# process train dataraw_data = np.genfromtxt('data/train.csv', delimiter=',')data = raw_data[1:, 3:]data[np.isnan(data)] = 0 # process nan# Dictionary: key:month value:month datamonth_data = {}# make data timeline continuousfor month in range(12): temp = np.empty(shape=(18, 20*24)) for day in range(20): temp[:, day*24: (day+1)*24] = data[(month*20+day)*18: (month*20+day+1)*18, :] month_data[month] = temp# x_data v1: only consider PM2.5x_data = np.empty(shape=(12*471, 9))y_data = np.empty(shape=(12*471, 1))for month in range(12): for i in range(471): x_data[month*471+i][:] = month_data[month][9][i: i+9] y_data[month*471+i] = month_data[month][9][i+9]# process test datatest_raw_data = np.genfromtxt('data/test.csv', delimiter=',')test_data = test_raw_data[:, 2:]test_data[np.isnan(test_data)] = 0# feature scale: normalizationmean = np.mean(x_data, axis=0)std = np.std(x_data, axis=0)for i in range(x_data.shape[0]): for j in range(x_data.shape[1]): if std[j] != 0: x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]for i in range(test_data.shape[0]): for j in range(test_data.shape[1]): if std[j] != 0: test_data[i][j] = (test_data[i][j] - mean[j])/std[j]# Testing data featurestest_x = np.empty(shape=(240, 9))for day in range(240): test_x[day, :] = test_data[18*day+9, :]test_x = np.concatenate((np.ones(shape=(240, 1)), test_x), axis=1)################################# train-adagradbatch = x_data.shape[0] # full batchepoch = 400# some parameters for trainingdim = x_data.shape[1]+1w = np.zeros(shape=(dim, 1)) # concatenate bias = w[0]lr = np.full((dim, 1), 0.8) # learning rategrad = np.empty(shape=(dim, 1)) # gradient of loss to every paragradsum = np.zeros(shape=(dim, 1)) # sum of gradient**2x_data = np.concatenate((np.ones(shape=(x_data.shape[0], 1)), x_data), axis=1)loss_his = np.empty(shape=(epoch, 1))for T in range(epoch): L = y_data - np.dot(x_data, w) loss_his[T] = np.sum(L**2) / x_data.shape[0] grad = (-2)*np.dot(np.transpose(x_data), L) gradsum = gradsum + grad**2 w = w - lr*grad/(gradsum**0.5)f = open('output/v1.csv', 'w')sys.stdout = fprint('v1: only consider PM2.5\\n')################################ train by sklearn linear modelprint('Using sklearn')reg = linear_model.LinearRegression()print(reg.fit(x_data, y_data))print('bias=', reg.intercept_)print('w=', reg.coef_.transpose())print('\\n')# In our modelprint('In our model')print('bias=', w[0])print('w=', w[1:])############################ draw change of lossplt.xlim(0, epoch)plt.ylim(0, 10)plt.xlabel('$iteration$', fontsize=16)plt.ylabel('$Loss$', fontsize=16)iteration = np.arange(0, epoch)plt.plot(iteration, loss_his/100, '-', ms=3, lw=2, color='black')sk_w = reg.coef_.transpose()sk_w[0] = reg.intercept_sk_loss = np.sum((y_data - np.dot(x_data, sk_w))**2) / x_data.shape[0]plt.hlines(sk_loss/100, 0, epoch, colors='red', linestyles='solid')plt.legend(['adagrad', 'sklearn'])plt.show()# plt.savefig('output/v1.png')f.close()############### test (sklearn vs our adagradf = open('output/v1test.csv', 'w')sys.stdout = ftitle = ['id', 'value', 'sk_value']answer = np.dot(test_x, w)sk_answer = np.dot(test_x, sk_w)print(title)for i in range(test_x.shape[0]): content = ['id_'+str(i), answer[i][0], sk_answer[i][0]] print(content)f.close() Reference 待完成","link":"/2020/04/06/ml-lee-hw1/"},{"title":"「机器学习-李宏毅」：Error","text":"这篇文章叙述了进行regression时，where dose the error come from?这篇文章除了解释了error为什么来自bias和variance，还给出了当error产生时应该怎么办？如何让模型在实践应用中也能表现地和测试时几乎一样的好？ Error在中的2.4节，我们比较了不同的Model。下图为不同Model下，testing data error的变化。 可以发现，随着模型越来越复杂，testing data的error变小一些后，爆炸增大。 越复杂的模型在testing data上不一定能得到好的performance。 所以，where dose the error come from? ：bias and variance Bias and Variance of Estimator用打靶作比，如果你的准心，没有对准靶心，那打出的很多发子弹的中心应该离靶心有一段距离，这就是bias。 但把准心对准靶心，你也不一定能打中靶心，可能会有风速等一系列原因，让子弹落在靶心周围，这就是variance。 上图中，可以直观体现出bias 和 variance的影响。 概率论中 ： 一个通过样本值得到了估计量，有三个评判准则：无偏性、有效性和相和性。 这里的无偏性的偏也就是bias。 概率论中定义：设 $\\hat{\\theta}(X_1,X_2,…,X_n)$ 是未知参数 $\\theta$ 的估计量，若 $E(\\hat{\\theta})=\\theta$ ，则称 $\\hat{\\theta}$ 是 $\\theta$ 的无偏估计。 变量 $x$ ，假设他的期望是 $\\mu$ ，他的方差是 $\\sigma^2$. 对于样本： $x^1,x^2,…,x^N$ ，估计他的期望和方差。 概率论的知识： $m=\\frac{1}{N} \\sum_{n} x^{n} \\quad s^{2}=\\frac{1}{N} \\sum_{n}\\left(x^{n}-m\\right)^{2}$ $E(m)=\\mu$ ，所以用 $m$ 是 $\\mu$ 的无偏估计。(unbiased) 但是 $E\\left[s^{2}\\right]=\\frac{N-1}{N} \\sigma^{2} \\quad \\neq \\sigma^{2}$ ，所以这样的估计是有偏差的。(biased) 因此统计学中用样本估算总体方差都进行了修正。 而在机器学习中，Bias和Variance通常与模型相关。 上图中，假设黑色的线是 true function，红色的线是训练得到的函数，蓝色的线是，训练函数的平均函数。 可见，随着函数模型越来越复杂，bias在变小，但variance也在增大。 右下角图中，红色的线接近铺满了，variance已经很大了，模型过拟合了。 对机器学习中模型对bias影响的直观解释 左图的model简单，右图的model复杂。 简单的model，包含的函数集较小，可能集合圈根本没有包括target（true function），因此在这个model下，无论怎么训练，得到的函数都有 large bias。 而右图中，因为函数非常复杂，所以大概率包含了target，因此训练出的函数可能variacne很大，但有 small bias。 what to do with large bias/variance 上图中，红色的线表示bias的误差，绿色的线表示variance的误差，蓝色的线表示观测的误差。 当模型过于简单时：来自bias的误差会较大，来自vaiance的误差较小，也就是 Large Bias Small Variance 当模型过雨复杂时：来自bias的误差会较小，来自variance的误差会很大，也就是 Small Bias Large Variance 2 case : Underfitting ：If your model cannot even fit the training examples, then you have large bias. Overfitting : If you can fit the traning data, but large error on testing data , then you probably have large variance. With Large BiasFor bias, redesign your model. Add more features as input. A more complex model. 考虑更多的feature；使用稍微复杂些的模型。 With Large Variance More data Regularization (在这篇2.5.2文章中有叙述什么是regularization) Model Selection There is usually a trade-off beween bias and variance. Select a model that balances two kinds of error to minimize total error. 选择模型需要在bias和variance中平衡，尽量使得总error最小。 What you should NOT do: 以上，描述的是这样的一个情形：在traning data中，得到了三个自认不错的模型，kaggle的公开的testing data测试，分别得到三个模型的error，认为第三个模型最好！ 但是，当把kaggle用private的testing data 进行测试时，error肯定是大于0.5的，最好的model也不一定是第三个。 同理，当把我们训练出的model拿来实际应用时，可能会发现情况很糟，并且，这个model可能选的是测试中最好的，但在应用中并不是最好的。 Cross Validation什么是Cross Validation(交叉验证)？ 在机器学习中，就是下图过程： 把Traning Set 分成两个部分：Training Set和Validation Set。 在Training Set部分选出模型。 用Validation Set来判断哪个模型好：计算模型在Validate Set的error。 再用模型预测Testing Set(public)，得到的error一定是比Validation Set中大的。 Not recommend : Not用public testing data的误差结果去调整你的模型。 这样会让模型在public的performance比private的好。 但模型在private testing data的performance才是我们真正关注的。 那么当模型预测private testing set时（投入应用时），能尽最大可能的保证模型和在预测public testing data相近。 N-fold Cross ValidationN-fold Cross Validation（N-折交叉验证）的过程如下： 把Training Set 分为3（3-fold）份，每一次拿其中一份当Validation Set，另外两份当作Training Set。 每一次用Train Set来训练。得到了三个Model。 要判断哪一个Model好？ 每一个Model都计算出不同Validation Set的error。 得到一个Average Error。 最后选这个average error最小的model。 最后应用在public traning set，来评估模型应用在private training set的performance。","link":"/2020/03/15/error/"},{"title":"「机器学习-李宏毅」：HW2-Binary Income Predicting","text":"这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。包括对数据集的处理，训练模型，可视化，预测等。有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的GitHub Task introduction and Dataset Kaggle competition: link Task: Binary Classification Predict whether the income of an individual exceeds $50000 or not ? *Dataset: * Census-Income (KDD) Dataset (Remove unnecessary attributes and balance the ratio between positively and negatively labeled data) Feature Format train.csv, test_no_label.csv【都是没有处理过的数据，可作为数据参考和优化参考】 text-based raw data unnecessary attributes removed, positive/negative ratio balanced. X_train, Y_train, X_test【已经处理过的数据，可以直接使用】 discrete features in train.csv =&gt; one-hot encoding in X_train (education, martial state…) continuous features in train.csv =&gt; remain the same in X_train (age, capital losses…). X_train, X_test : each row contains one 510-dim feature represents a sample. Y_train: label = 0 means “&lt;= 50K” 、 label = 1 means “ &gt;50K ” 注：数据集超大，用notepad查看比较舒服；调试时，也可以先调试小一点的数据集。 Logistic RegressionLogistic Regression 原理部分见这篇博客。 Prepare data本文直接使用X_train Y_train X_test 已经处理好的数据集。 1234567891011121314# prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_Train'X_test_fpath = './data/X_test'output_fpath = './logistic_output/output_logistic.csv'fpath = './logistic_output/logistic'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:] 统计一下数据集： 1234567891011train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In logistic model:\\n') f.write('Size of Training set: {}\\n'.format(train_size)) f.write('Size of development set: {}\\n'.format(dev_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n'.format(data_dim)) 结果如下： 12345In logistic model:Size of Training set: 48830Size of development set: 5426Size of test set: 27622Dimension of data: 510 normalizenormalize data. 对于train data，计算出每个feature的mean和std，保存下来用来normalize test data。 代码如下： 123456789101112131415def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X. # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std # Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std) Development set split在logistic regression中使用的gradient，没有closed-form解，所以在train set中划出一部分作为development set 优化参数。 12345678def _train_dev_split(X, Y, dev_ratio=0.25): # This function splits data into training set and development set. train_size = int(X.shape[0] * (1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]# Split data into train data and development datadev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio) Useful function_shuffle(X, Y)本文使用mini-batch gradient。 所以在每次epoch时，以相同顺序同时打乱X_train,Y_train数组，再mini-batch。 1234567np.random.seed(0)def _shuffle(X, Y): # This function shuffles two two list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize] _sigmod(z)计算 $\\frac{1}{1+e^{-z}}$ ，注意：防止溢出，给函数返回值规定上界和下界。 1234def _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8)) _f(X, w, b)是sigmod函数的输入，linear part。 输入： X：shape = [size, data_dimension] w：weight vector, shape = [data_dimension, 1] b: bias, scalar 输出： 属于Class 1的概率（Label=0，即收入小于$50k的概率） 123456789def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b) _predict(X, w, b)预测Label=0？（0或者1，不是概率） 123def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int) _accuracy(Y_pred, Y_label)计算预测出的结果（0或者1）和真实结果的正确率。 这里使用 $1-\\overline{error}$ 来表示正确率。 12345def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc _cross_entropy_loss(y_pred, Y_label)计算预测出的概率（是sigmod的函数输出）和真实结果的交叉熵。 计算公式为： $\\sum_n {C(y_{pred},Y_{label})}=-\\sum[Y_{label}\\ln{y_{pred}}+(1-Y_{label})\\ln(1-{y_{pred}})]$ 12345678def _cross_entropy_loss(y_pred, Y_label): # This function calculates the cross entropy of Y_pred and Y_label # # Argument: # y_pred: predictions, float vector # Y_label: truth labels, bool vector cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((1 - Y_label).T, np.log(1 - y_pred)) return cross_entropy[0][0] _gradient(X, Y_label, w, b)和Regression的最小二乘一样。（严谨的说，最多一个系数不同） 12345678def _gradient(X, Y_label, w, b): # This function calculates the gradient of cross entropy # X, Y_label, shape = [batch_size, ] y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = - np.dot(X.T, pred_error) b_grad = - np.sum(pred_error) return w_grad, float(b_grad) Training (Adagrad)初始化一些参数。 这里特别注意 : 由于adagrad的参数更新是 $w \\longleftarrow w-\\eta \\frac{gradient}{ \\sqrt{gradsum}}$ . 防止除0，初始化gradsum的值为一个较小值。 123456789101112# training by logistic model# Initial weights and biasw = np.zeros((data_dim, 1))b = np.float(0.)w_grad_sum = np.full((data_dim, 1), 1e-8) # avoid divided by zerosb_grad_sum = np.float(1e-8)# Some parameters for trainingepoch = 20batch_size = 2**3learning_rate = 0.2 AdagradAagrad具体原理见这篇博客的1.2节。 迭代更新时，每次epoch计算一次loss和accuracy，以便可视化更新过程，调整参数。 12345678910111213141516171819202122232425262728293031323334353637# Keep the loss and accuracy history at every epoch for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Iterative trainingfor it in range(epoch): # Random shuffle at every epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for id in range(int(np.floor(train_size / batch_size))): X = X_train[id*batch_size: (id+1)*batch_size] Y = Y_train[id*batch_size: (id+1)*batch_size] # calculate gradient w_grad, b_grad = _gradient(X, Y, w, b) # adagrad gradient update w_grad_sum = w_grad_sum + w_grad**2 b_grad_sum = b_grad_sum + b_grad**2 w_ada = np.sqrt(w_grad_sum) b_ada = np.sqrt(b_grad_sum) w = w - learning_rate * w_grad / np.sqrt(w_grad_sum) b = b - learning_rate * b_grad / np.sqrt(b_grad_sum) # compute loss and accuracy of training set and development set at every epoch y_train_pred = _f(X_train, w, b) Y_train_pred = np.around(y_train_pred) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size) train_acc.append(_accuracy(Y_train_pred, Y_train)) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.around(y_dev_pred) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size) dev_acc.append(_accuracy(y_dev_pred, Y_dev)) Loss &amp; accuracy输出最后一次迭代的loss和accuracy。 结果如下： 1234Training loss: 0.2933570286596322Training accuracy: 0.8839238173254147Development loss: 0.31029505347634456Development accuracy: 0.8336166253549906 画出loss 和 accuracy的更新过程： loss： accuracy： 由于Feature数量较大，将权重影响最大的feature输出看看： 12345678910Other Rel &lt;18 spouse of subfamily RP: [7.11323764] Grandchild &lt;18 ever marr not in subfamily: [6.8321061] Child &lt;18 ever marr RP of subfamily: [6.77322397] Other Rel &lt;18 ever marr RP of subfamily: [6.76688406] Other Rel &lt;18 never married RP of subfamily: [6.37488958] Child &lt;18 spouse of subfamily RP: [5.97717831] United-States: [5.53932651] Grandchild 18+ spouse of subfamily RP: [5.42948497] United-States: [5.41543809] Mexico: [4.79920763] Code完整数据集、代码等，欢迎光临小透明GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222################## Data:2020-04-05# Author: Fred Lau# ML-Lee: HW2 : Binary Classification###########################################################import numpy as npimport csvimport sysimport matplotlib.pyplot as plt########################################################### prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_Train'X_test_fpath = './data/X_test'output_fpath = './logistic_output/output_logistic.csv'fpath = './logistic_output/logistic'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X. # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_stddef _train_dev_split(X, Y, dev_ratio=0.25): # This function splits data into training set and development set. train_size = int(X.shape[0] * (1 - dev_ratio)) return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)# Split data into train data and development datadev_ratio = 0.1X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)train_size = X_train.shape[0]dev_size = X_dev.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In logistic model:\\n') f.write('Size of Training set: {}\\n'.format(train_size)) f.write('Size of development set: {}\\n'.format(dev_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n'.format(data_dim))np.random.seed(0)################################################################ useful functiondef _shuffle(X, Y): # This function shuffles two two list/array, X and Y, together. randomize = np.arange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize]def _sigmod(z): # Sigmod function can be used to calculate probability # To avoid overflow return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))def _f(X, w, b): # This is the logistic function, parameterized by w and b # # Arguments: # X: input data, shape = [batch_size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probability of each row of X being positively labeled, shape = [batch_size, 1] return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This fucntion returns a truth value prediction for each row of X by logistic regression return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function calculates prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return accdef _cross_entropy_loss(y_pred, Y_label): # This function calculates the cross entropy of Y_pred and Y_label # # Argument: # y_pred: predictions, float vector # Y_label: truth labels, bool vector cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((1 - Y_label).T, np.log(1 - y_pred)) return cross_entropy[0][0]def _gradient(X, Y_label, w, b): # This function calculates the gradient of cross entropy # X, Y_label, shape = [batch_size, ] y_pred = _f(X, w, b) pred_error = Y_label - y_pred w_grad = - np.dot(X.T, pred_error) b_grad = - np.sum(pred_error) return w_grad, float(b_grad)######################################## training by logistic model# Initial weights and biasw = np.zeros((data_dim, 1))b = np.float(0.)w_grad_sum = np.full((data_dim, 1), 1e-8) # avoid divided by zerosb_grad_sum = np.float(1e-8)# Some parameters for trainingepoch = 20batch_size = 2**3learning_rate = 0.2# Keep the loss and accuracy history at every epoch for plottingtrain_loss = []dev_loss = []train_acc = []dev_acc = []# Iterative trainingfor it in range(epoch): # Random shuffle at every epoch X_train, Y_train = _shuffle(X_train, Y_train) # Mini-batch training for id in range(int(np.floor(train_size / batch_size))): X = X_train[id*batch_size: (id+1)*batch_size] Y = Y_train[id*batch_size: (id+1)*batch_size] # calculate gradient w_grad, b_grad = _gradient(X, Y, w, b) # adagrad gradient update w_grad_sum = w_grad_sum + w_grad**2 b_grad_sum = b_grad_sum + b_grad**2 w_ada = np.sqrt(w_grad_sum) b_ada = np.sqrt(b_grad_sum) w = w - learning_rate * w_grad / np.sqrt(w_grad_sum) b = b - learning_rate * b_grad / np.sqrt(b_grad_sum) # compute loss and accuracy of training set and development set at every epoch y_train_pred = _f(X_train, w, b) Y_train_pred = np.around(y_train_pred) train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size) train_acc.append(_accuracy(Y_train_pred, Y_train)) y_dev_pred = _f(X_dev, w, b) Y_dev_pred = np.around(y_dev_pred) dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size) dev_acc.append(_accuracy(y_dev_pred, Y_dev))with open(fpath, 'a') as f: f.write('Training loss: {}\\n'.format(train_loss[-1])) f.write('Training accuracy: {}\\n'.format(train_acc[-1])) f.write('Development loss: {}\\n'.format(dev_loss[-1])) f.write('Development accuracy: {}\\n'.format(dev_acc[-1]))#################### Plotting Loss and accuracy curve# Loss curveplt.plot(train_loss, label='train')plt.plot(dev_loss, label='dev')plt.title('Loss')plt.legend()plt.savefig('./logistic_output/loss.png')plt.show()plt.plot(train_acc, label='train')plt.plot(dev_acc, label='dev')plt.title('Accuracy')plt.legend()plt.savefig('./logistic_output/acc.png')plt.show()################################## Predictpredictions = _predict(X_test, w, b)with open(output_fpath, 'w') as f: f.write('id, label\\n') for id, label in enumerate(predictions): f.write('{}, {}\\n'.format(id, label[0]))################################ Output the weights and biasind = (np.argsort(np.abs(w), axis=0)[::-1]).reshape(1, -1)with open(X_test_fpath) as f: content = f.readline().strip('\\n').split(',')content = content[1:]with open(fpath, 'a') as f: for i in ind[0, 0: 10]: f.write('{}: {}\\n'.format(content[i], w[i])) Generative ModelGenerative Model 原理部分见 这篇博客 Prepare data这部分和Logistic regression一样。 只是，因为generative model有closed-form solution，不需要划分development set。 12345678910111213141516171819202122232425262728293031323334353637383940# Prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './generative_output/output_{}.csv'fpath = './generative_output/generative'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)train_size = X_train.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In generative model:\\n') f.write('Size of training data: {}\\n'.format(train_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n\\n'.format(data_dim)) Useful functions12345678910111213141516171819202122232425# Useful functionsdef _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8))def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function computes prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc Training公式再推导计算公式： $$ \\begin{equation}\\begin{aligned}P\\left(C_{1} | x\\right)&=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\\\&=\\frac{1}{1+\\frac{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}}\\\\&=\\frac{1}{1+\\exp (-z)} =\\sigma(z)\\qquad(z=\\ln \\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{2}\\right) P\\left(C_{2}\\right)}\\end{aligned}\\end{equation} $$ 计算z的过程： 首先计算Prior Probability。 假设模型是Gaussian的，算出 $\\mu_1,\\mu_2 ,\\Sigma$ 的closed-form solution 。 根据 $\\mu_1,\\mu_2,\\Sigma$ 计算出 $w,b$ 。 计算Prior Probability。 程序中用list comprehension处理较简单。 123# compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1]) 计算 $\\mu_1,\\mu_2 ,\\Sigma$ （Gaussian） $\\mu_0=\\frac{1}{C0} \\sum_{n=1}^{C0} x^{n} $ (Label=0) $\\mu_1=\\frac{1}{C1} \\sum_{n=1}^{C1} x^{n} $ (Label=0) $\\Sigma_0=\\frac{1}{C0} \\sum_{n=1}^{C0}\\left(x^{n}-\\mu^{}\\right)^{T}\\left(x^{n}-\\mu^{}\\right)$ (注意 ：这里的 $x^n,\\mu$ 都是行向量，注意转置的位置） $\\Sigma_1=\\frac{1}{C1} \\sum_{n=1}^{C1}\\left(x^{n}-\\mu^{}\\right)^{T}\\left(x^{n}-\\mu^{}\\right)$ $\\Sigma=(C0 \\times\\Sigma_0+C1\\times\\Sigma_1)/(C0+C1)$ (shared covariance) 123456789101112131415mean_0 = np.mean(X_train_0, axis=0)mean_1 = np.mean(X_train_1, axis=0)# compute in-class covariancecov_0 = np.zeros(shape=(data_dim, data_dim))cov_1 = np.zeros(shape=(data_dim, data_dim))for x in X_train_0: # (D,1)@(1,D) np.matmul(np.transpose([x]), x) cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# shared covariancecov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train.shape[0]) 计算 $w,b$ 在 这篇博客中的第2小节中的公式推导中， $x^n,\\mu$ 都是列向量，公式如下： $$ z=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} x-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}} $$ $w^T=\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\Sigma^{-1} \\qquad b=-\\frac{1}{2}\\left(\\mu^{1}\\right)^{T} \\Sigma^{-1} \\mu^{1}+\\frac{1}{2}\\left(\\mu^{2}\\right)^{T} \\Sigma^{-1} \\mu^{2}+\\ln \\frac{N_{1}}{N_{2}}$ 但是 ，一般我们在处理的数据集，$x^n,\\mu$ 都是行向量。推导过程相同，公式如下： （主要注意转置和矩阵乘积顺序） $$ z=x\\cdot \\Sigma^{-1}\\left(\\mu^{1}-\\mu^{2}\\right)^{T} -\\frac{1}{2} \\mu^{1}\\Sigma^{-1}\\left(\\mu^{1}\\right)^{T}+\\frac{1}{2}\\mu^{2}\\Sigma^{-1} \\left(\\mu^{2}\\right)^{T} +\\ln \\frac{N_{1}}{N_{2}} $$ $w=\\Sigma^{-1}\\left(\\mu^{1}-\\mu^{2}\\right)^{T} \\qquad b=-\\frac{1}{2} \\mu^{1}\\Sigma^{-1}\\left(\\mu^{1}\\right)^{T}+\\frac{1}{2}\\mu^{2}\\Sigma^{-1} \\left(\\mu^{2}\\right)^{T} +\\ln \\frac{N_{1}}{N_{2}}$ 但是，协方差矩阵的逆怎么求呢？ numpy中有直接求逆矩阵的方法(np.linalg.inv)，但当该矩阵是nearly singular，是奇异矩阵时，就会报错。 而我们的协方差矩阵（510*510）很大，很难保证他不是奇异矩阵。 于是，有一个 牛逼 强大的数学方法，叫SVD(singular value decomposition, 奇异值分解) 。 原理步骤我……还没有完全搞清楚QAQ（先挖个坑）[1] 利用SVD，可以将任何一个矩阵（即使是奇异矩阵），分界成 $A=u s v^T$ 的形式：其中u,v都是标准正交矩阵，s是对角矩阵。（numpy.linalg.svd方法实现了SVD） 可以利用SVD求矩阵的伪逆 $A=u s v^T$ u,v是标准正交矩阵，其逆矩阵等于其转置矩阵 s是对角矩阵，其”逆矩阵“（注意s矩阵的对角也可能有0元素） 将非0元素取倒数即可。 $A^{-1}=v s^{-1} u$ 计算 $w,b$ 的代码如下： 123456789101112131415# compute weights and bias# Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.# cov = u@s@vh# cov_inv = dot(vh.T * 1 / s, u.T)u, s, vh = np.linalg.svd(cov, full_matrices=False)s_inv = s # s_inv avoid &lt;1e-8for i in range(s.shape[0]): if s[i] &lt; (1e-8): break s_inv[i] = 1./s[i]cov_inv = np.matmul(vh.T * s_inv, u.T)w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))b = (-0.5) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (0.5) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) Accuracy accuracy结果： 1Training accuracy: 0.8756450899439694 也将权重较大的feature输出看看： 12345678910age: [-0.51867291] Masters degree(MA MS MEng MEd MSW MBA): [-0.49912643] Spouse of householder: [0.49786805]weeks worked in year: [-0.44710924] Spouse of householder: [-0.43305697]capital gains: [-0.42608727]dividends from stocks: [-0.41994666] Doctorate degree(PhD EdD): [-0.39310961]num persons worked for employer: [-0.37345994] Prof school degree (MD DDS DVM LLB JD): [-0.35594107] Code具体数据集和代码，欢迎光临小透明GitHub 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133import numpy as npnp.random.seed(0)############################################### Prepare dataX_train_fpath = './data/X_train'Y_train_fpath = './data/Y_train'X_test_fpath = './data/X_test'output_fpath = './generative_output/output_{}.csv'fpath = './generative_output/generative'X_train = np.genfromtxt(X_train_fpath, delimiter=',')Y_train = np.genfromtxt(Y_train_fpath, delimiter=',')X_test = np.genfromtxt(X_test_fpath, delimiter=',')X_train = X_train[1:, 1:]Y_train = Y_train[1:, 1:]X_test = X_test[1:, 1:]def _normalization(X, train=True, X_mean=None, X_std=None): # This function normalize columns of X # Output: # X: normalized data # X_mean, X_std if train: X_mean = np.mean(X, axis=0) X_std = np.std(X, axis=0) for j in range(X.shape[1]): X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + 1e-8) # avoid X_std==0 return X, X_mean, X_std# Normalize train_data and test_dataX_train, X_mean, X_std = _normalization(X_train, train=True)X_test, _, _ = _normalization(X_test, train=False, X_mean=X_mean, X_std=X_std)train_size = X_train.shape[0]test_size = X_test.shape[0]data_dim = X_train.shape[1]with open(fpath, 'w') as f: f.write('In generative model:\\n') f.write('Size of training data: {}\\n'.format(train_size)) f.write('Size of test set: {}\\n'.format(test_size)) f.write('Dimension of data: {}\\n\\n'.format(data_dim))######################### Useful functionsdef _sigmod(z): # Sigmod function can be used to compute probability # To avoid overflow return np.clip(1/(1.0 + np.exp(-z)), 1e-8, 1-(1e-8))def _f(X, w, b): # This function is the linear part of sigmod function # Arguments: # X: input data, shape = [size, data_dimension] # w: weight vector, shape = [data_dimension, 1] # b: bias, scalar # Output: # predict probabilities return _sigmod(np.dot(X, w) + b)def _predict(X, w, b): # This function returns a truth value prediction for each row of X belonging to class1(label=0) return np.around(_f(X, w, b)).astype(np.int)def _accuracy(Y_pred, Y_label): # This function computes prediction accuracy # Y_pred: 0 or 1 acc = 1 - np.mean(np.abs(Y_pred - Y_label)) return acc######################## Generative Model: closed-form solution, can be computed directly# compute in-class meanX_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])mean_0 = np.mean(X_train_0, axis=0)mean_1 = np.mean(X_train_1, axis=0)# compute in-class covariancecov_0 = np.zeros(shape=(data_dim, data_dim))cov_1 = np.zeros(shape=(data_dim, data_dim))for x in X_train_0: # (D,1)@(1,D) np.matmul(np.transpose([x]), x) cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]for x in X_train_1: cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]# shared covariancecov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train.shape[0])# compute weights and bias# Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.# Via SVD decomposition, one can get matrix inverse efficiently and accurately.# cov = u@s@vh# cov_inv = dot(vh.T * 1 / s, u.T)u, s, vh = np.linalg.svd(cov, full_matrices=False)s_inv = s # s_inv avoid &lt;1e-8for i in range(s.shape[0]): if s[i] &lt; (1e-8): break s_inv[i] = 1./s[i]cov_inv = np.matmul(vh.T * s_inv, u.T)w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))b = (-0.5) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (0.5) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0])# compute accuracy on training setY_train_pred = 1 - _predict(X_train, w, b)with open(fpath, 'a') as f: f.write('\\nTraining accuracy: {}\\n'.format(_accuracy(Y_train_pred, Y_train)))# Predictpredictions = 1 - _predict(X_test, w, b)with open(output_fpath.format('generative'), 'w') as f: f.write('id, label\\n') for i, label in enumerate(predictions): f.write('{}, {}\\n'.format(i, label))# Output the most significant weightwith open(X_test_fpath) as f: content = f.readline().strip('\\n').split(',')content = content[1:]ind = np.argsort(np.abs(np.concatenate(w)))[::-1]with open(fpath, 'a')as f: for i in ind[0:10]: f.write('{}: {}\\n'.format(content[i], w[i])) Reference SVD原理，待补充","link":"/2020/04/15/ml-lee-hw2/"},{"title":"「Python」：Module & Method","text":"长期记录帖：关于遇到过的那些Python 的Packets &amp; Module &amp; Method &amp; Attribute。中英记录。 Trickylist comprehension List comprehension provides a concise way to create lists. e.g. : squares = [x**2 for x in range(10)] A list comprehension consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result will be a new list resulting from evaluating the expression in the context of the for and if clauses which follow it. e.g. : Double loop: [(x,y) for x in [1,2,3] for y in [3,1,4] if x != y] 输出7个 e.g. : (Using zip() to loop together): [(x, y) for x,y in zip([1,2,3], [3,1,4]) if x!=y] 输出2个 Python-Build functionprint print(*objects, sep=’ ‘, end=’\\n’, file=sys.stdout) len Return the length(the number of items) of an object. str.format() 字符串格式化 eg: “{} {}”.format(“Hello”,”World) ‘Hello World’ zip(*iterables) Make an iterator that aggregates【聚集】 elements from each of the iterales. Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. 【返回一个元组的迭代器】 使用zip可以同时对多个迭代器进行迭代 enumerate Enumerate is a built-in funciton of Python. It allows us to loop over something and have an automatic counter. e.g. for counter, value in enumerate(some_list): print(counter, value) e.g. : an optional argument: tell enumerate from where to start the index. for c, value in enumerate(my_list, 1): print(c, value) with open(path) as f: 由于读写文件都可能产生IOError，一旦出错，后面的f.close()就不会调用。 用try……finally来实现，比较麻烦。 try: ​ f = open(path, ‘r’) ​ print(f.read()) finally: ​ if f: ​ f.close() 用with as 简化 with open(path, ‘r’) as f: ​ print(f.read()) numpynumpy.argsort numpy.argsort(a, axis=-1, kind=None, order=None) Returns the indices that would sort an array. Perform an indirect sort along the given axis using the algorithm specified by the kind keyword. It returns an array of indices of the same shape as a that index data along the given axis in sorted order. Parameters: a :array_like. axis : int or None, optional Axis along which to sort. The default is -1(the last axis). 【默认按照最后一个维度】 2-D: axis = 0按列排序 2-D: axis = 1 按行排序 kind :{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, optional The default is ‘quicksort’ Return: index_array: ndarray, int.【返回的是降序排列的索引数组】 e.g.: x = np.array([5, 1, 2]) np.argsort(x) # 降序 array([1,2,0]) np.argsort(-x) # 升序 array([0,2,1]) Linear algebra(numpy.linalg)numpy.dot numpy.dot(a,b) Dot product of two arrays. If both a and b are 1-D arrays, it is inner porduct of vectors. If both a and b are 2-D arrays, it is matrix multiplication, but using matmul is preferred. Id either a or b is 0-D(scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a*b is preferred. …… numpy.matmul Matrix product of two arrays. numpy.matmul(x1, x2) numpy.linalg.inv(a) Compute the inverse of a matrix. Given a square matrix a, return the matrix ainv satisfying dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])/ numpy.linalg.inv(a) Parameters: a :(…, M, M) array_like. Matrix to e inverted. Return: ainv. numpy.linalg.svd numpy.linalg.svd(a, full_matrices=True, compute_uv=True, hermitian=False) Singular Value Decomposition 矩阵的奇异值分解 A = u @ s @ vh u, vh是标准正交矩阵, inv(u) = uh s是对角矩阵 Parameters: a :array_like, a real or complex array with a.ndim &gt;=2 full_matrices :bool, optional. True(default) If True, u and vh have the shapes(…, M, M) and (…, N, N), respectively. Otherwise, the shapes are(…, M, K) and (…, K, N), respectively, where K = min(M,N) compute_uv : bool, optional.True(default) Whether or not to compute u and vh in addition to s.【注，vh就是v的转置】 Return： u: array s: array vh:array numpy.zeros numpy.zeros(shape, dtype=float, order=’C’) Return a new array of given shape and type, filled with zeros. parameters: shape: int or truple of ints. e.g.,(2,3) or 2 dtype: data-type, optional.(Defaults is numpy.float64) oder: optional Returns: out ndarray numpy.full numpy.full(shape, fill_value, dtype=None) Return a new array of given shape and type, filled with fill_value. parameters: shape :int or sequence of ints (2,3) or 2 fill_value :scalar dtype :data-type, optional numpy.arange numpy.arange([start, ]stop, [step, ]dtype=None) Return evenly spaced values within a given interval. parameters: start: number, optional. (Defaults is 0) stop :number. [start,stop) step :number, optional(Defaults is 1) dtype : Returns :ndarray differ with built-in range function numpy.arange returnan ndarray rathan than a list. numpy.arange’s step can be float. numpy.meshgrid numpy.meshgrid(x, y) 生成用x向量为行，y向量为列的矩阵（坐标系） 返回 X矩阵和Y矩阵 X矩阵：网格上所有点的x值 Y矩阵：网格上所有点的y值 e.g., X, Y = np.meshgrid(x, y) 【X,Y 都是网格点坐标矩阵】 numpy.genfromtext numpy.genfromtxt (fname, delimiter=None) Load data from a text file, with missing values handled as specified. Each line past the first skip_header lines is split at the delimiter character, and characters following the comments characters are discarded. Parameters: fname :file, str, list of str, generator. dtype :dtype, optional. delimiter :str, int, or sequence, optional. (default = whitespace) The strin used to separate values. Python的列表读取处理数据很慢，numpy.genfromtext就很棒。 numpy.isnan numpy.isnan(x) Test element-wise for NaN(Not a number) and return result as a boolean array. Parameters: x :array_like Returns: y:ndarray or bool. True where x is NaN, false otherwise. numpy.empty nmpy.empty(shape, dtype=float, order=’C’) Return a new arry of given shape and type, without initializing entries. Parameters: shape :int or tuple of int dtype :data-type,optional Default is numpy.float64. Returns: out: ndarray numpy.reshape numpy.reshape(a, newshape, order=’C’) Gives a new shape to an array without changing its data.【改变张量的shape，不改变张量的数据】 Parameters: a : array-like newshape : int or tuple of ints One shape dimension can be -1. The value is inferred from the length of the array and remaning dimensions. Returns: reshaped_array:ndarray numpy.mean numpy.mean(a, axis=None) Compute the arithmetic mean along the specifiied axis.(the average of the array elements) Parameters: a : array_like axis ：None or int or tuple of ints, optional Axis or axes along which the means are computed. axis=0 ：沿行的垂直往下（列） axis=1 ：沿列的方向水平向右（行） numpy.std numpy.std(a, axis=None,) Compute the standard deviation along the specified axis.【标准差】 Parameters: a :array_like axis :Axis or axes along which the means are computed. numpy.shape attribute Tuple of array dimensions. numpy.concatenate numpy.concatenate((a1, a2, …), axis=0) Join a sequence of arrays along an existing axis. Parameters: a1, a2, … :sequence of array_like The arrays must have the same shape, excepting in the dimension corresponding to axis.【除了axia方向，其他维度的shape要相同】 If axis is None, arrays are flattened before use.【值为None，就先将向量变成一维的】 Default=0 numpy.ndarray.astype method numpy.ndarray.astype(dtype) Copy of the array cast to a specified type.【强制转换数据类型】 Parameters: dtype : str or dtype numpy.ones numpy.ones(shape, dtype=None) Return a new array of given shape and type, filled with ones. Parameters: shape : int or sequence of ints. dtype : data-type, optional numpy.array numpy.array(object, dtype = none) Create an array Parameters: object :array_like An array, any object exposing the array interface, an object whose array method returns an array, or any(nested) sequence. numpy ndarray 运算 [[1]]*3 = [[1],[1],[1]] A * B 元素相乘 numpy.dot(A, B) 矩阵相乘 numpy.power numpy.power(x1, x2) First array elements raised to powers from second array. Parameters: x1 :array_like . The bases. x2 :array_like The exponents. numpy.sum numpy.sum(a, axis=None, dtype=None) Sum of arrays elements over a given axis. Parameters: a :array_like Elements to sum. axis :None or int or tuple of ints, optional Axis or axes along which a sum is perfomed. The default, None, will sum all of the elementsof the input array. numpy.transpose numpy.transpose(a, axes=None) Permute the dimensions of the array.【tensor的维度换位】 Parameters: a : array_like axes : list of ints, optinal Default, reverse the dimensions. Otherwise permute the axes according to the values given. Returns : ndarray 张量a的shape是(10,2,15), numpy.transport(2,0,1)的shape就是(15,10,2) 对于一维：行向量变成列向量 对于二维：矩阵的转置 numpy.save numpy.save(file, arr) Save an array to a binary file in Numpy .npy format. Parameters: file :file, str, or pathlib arr :array_like Array data to be saved. numpy.clip Clip(limit) the values in an array numpy.clip(a, a_min, a_max) Parameters: a :array_like a_min : scalar or array_like a_max :scalar or array_like numpy.around Evenly round to the given number of decimals(十进制) numpy.around(a) Parameters： a :array_like Notes: For values exactly halfway between rounded decimal values, Numpy rounds to the nearest even values. 【这什么意思呢？ 就是说对于0.5的这种，为了统计上平衡，不会全部向上取整或者向下取整，会向最近的偶数取整，around（2.5）=2】 numpy.log The natural logarithm log is the inverse of exponential functions, so that log(exp(x))=x. numpy.log(x) Parameters: x : array_like numpy.ndarray.T attribute, the transpose array. ndarray.T numpy.random.shuffle Modify a sequence in-space by shufflng its contents. This function only shuffles the array along the first axis of a multi-diensional array. The order of sub-arrays is changed but their contents remains the same. numpy.random.shuffle(x) Parameters: x : array_like e.g. shuffle two list, X and Y, together. 【以相同的顺序打乱两个array】 np.random.seed(0) randomize = np.arrange(len(X)) np.random.shuffle(randomize) return X[randomize], Y[randomize] sklearnskelearn.linear_model.LinearRegression class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None) Ordinary least squares Linear Regress(最小二乘法回归运算) Parameters: fit_intercept :bool, optional, defalut True【True：需要bias的截距项】 normalize :bool, optional, default False 【True：对样本做feature scaling】 Attributes： coef_ :array of shape(n_features) 【权重】 intercept_ :bias Methods： fit(self,X,y[,sample_weight]) :Fit linear model e.g. , LinearRegression().fit(x_data, y_data) matplotlib.pyplotmatplotlib.pyplot.contourf contour and contourf draw contour lines and filled contours, respectively.【一个画等高线，一个填充等高线/轮廓】 contour([X, Y, ] Z, [levels], **kwargs) Parameters X, Y: The coordinates of the values in Z. A and Y must both be 2-D with the sanme shape as Z(e.g. created via numpy.meshgrid), or they must both be 1-D such that len(X) == M is the number of columns in Z and len(Y) = N is the number of rows in Z. 【X，Y要么是由像numpy.mershgrid(x, y) 生成的网格点坐标矩阵，要么X，Y是（基）向量，X向量是x轴的，对应到Z矩阵，是Z矩阵的列数，Y向量同理】 Z ：array-like(N,M) levels : int or array-like, optional. Determines the number and positions of contour lines / religions.【划分多少块等高区域】 alpha :float, optional. Between 0(transparent) and 1(opaque).【透明度】 cmap :str or Colormap, optional. e.g., pyplot.contourf(x, y, Z, 50, alpha=0.5, cmap=pyplot.get_cmap(‘jet’))【‘jet’是常用的那种红橙黄绿青蓝紫】 matplotlib.pyplot.plot plot([x], y, [fmt], , data=None, *kwargs) The coordinates of the points or line nodes are given by x, y. Parameters: x, y :array-like or scalar. fmt :str, optional. A format string. e.g., ‘.’, point marker. ‘-‘, solid line style. ‘–’,dashed line style. ‘b’, blue. ms/markersize : float lw/linewidth :float color : matplotlib.pyplot.xlim xlim(args, *kwargs) Get or set the x limits of the current axes. e.g. left, right = xlim() :get xlim(left, right) :set matplotlib.pyplot.show show(args, *kwargs) display a figure. matplotlib.pyplot.vlines Plot vertical lines. vlines(x, ymin, ymax, color=’k’, linestyles=’solid’) Parameters: x :scalar or 1D array_like ymin, ymax :scalar or 1D array_like matplotlib.pyplot.hlines Plot horizontal lines. vlines(y, xmin, xmax, color=’k’, linestyles=’solid’) Parameters: y :scalar or 1D array_like xmin, xmax :scalar or 1D array_like matplotlib.pyplot.savefig Save the current figure. savefig(fname) Parameters: fname :str ot Pathlike matplotlib.pyplot.legend Place a lengend on the axes. e.g. : legend() Labeling exisiting plot elements plt.plot(train_loss) plt.plot(dev_loss) plt.legend([‘train’, ‘dev’]) e.g. : le syssys.argv[] python a.py data.csv sys.argv = [‘a.py’, ‘data.csv’] 重定向到文件 f = open(‘out.csv’, ‘w’) sys.stdout = f print(‘此时print掉用的就是文件对象的write方法’)","link":"/2020/03/07/python/"}],"tags":[{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"Backpropagation","slug":"Backpropagation","link":"/tags/Backpropagation/"},{"name":"公开课","slug":"公开课","link":"/tags/%E5%85%AC%E5%BC%80%E8%AF%BE/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"Logistic Regression","slug":"Logistic-Regression","link":"/tags/Logistic-Regression/"},{"name":"Softmax","slug":"Softmax","link":"/tags/Softmax/"},{"name":"Gradient","slug":"Gradient","link":"/tags/Gradient/"},{"name":"Cryptography","slug":"Cryptography","link":"/tags/Cryptography/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"StreamCipher","slug":"StreamCipher","link":"/tags/StreamCipher/"},{"name":"Python","slug":"Python","link":"/tags/Python/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"机器学习-李宏毅","slug":"机器学习-李宏毅","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"Cryptography-Dan","slug":"Cryptography-Dan","link":"/categories/Cryptography-Dan/"}]}