<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>fred&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://f1ed.github.io/"/>
  <updated>2020-07-03T11:32:00.705Z</updated>
  <id>https://f1ed.github.io/</id>
  
  <author>
    <name>f1ed</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>「机器学习-李宏毅」:Semi-supervised Learning</title>
    <link href="https://f1ed.github.io/2020/07/03/semi-supervised/"/>
    <id>https://f1ed.github.io/2020/07/03/semi-supervised/</id>
    <published>2020-07-02T16:00:00.000Z</published>
    <updated>2020-07-03T11:32:00.705Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？</p><p>再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。</p><p>对于Generative Model，文章重点讲述了如何用EM算法来训练模型。</p><p>对于Low-density，文章重点讲述了如何让模型进行Self-training，并且在训练中引入Entropy-based Regularization term来尽可能low-density的假设。</p><p>对于Smoothness Assumption，文章重点讲述了Graph-based Approach（基于图的方法），并且在训练中引入Smoothness Regularization term来尽可能满足Smoothness Assumption的假设。</p><p>对于Better Representation，本篇文章只是简单阐述了其思想，具体介绍见这篇博客。</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>什么是Semi-supervised learning(半监督学习)？和Supervised learning（监督式学习）的区别在哪？</p><p><strong>Supervised learning（监督式学习）</strong>：</p><p>用来训练的数据集 $R$  中的数据labeled data，即 ${(x^r,\hat{y}^r)}_{r=1}^R$ .</p><p>比如在图像分类数据集中： $x^r$ 是image，对应的target output $y^r$ 是分类的label。</p><p>而<strong>Semi-supervised learning（半监督式学习）</strong>：</p><p>用来的训练的数据集由两部分组成 $\{(x^r,\hat{y}^r)\}_{r=1}^R$   ,    $\{x^u\}_{u=R}^{R+U}$   ，即labeled data和unlabeled data，而且通常情况下，unlabeled data的数量远远高于labeled data是数量，即 $U&gt;&gt;R$ .</p><p>Semi-supervised learning 又分为两种，Transductive learning （转导/推论推导）和 Inductive learning（归纳推理）</p><ul><li>Transductive learing: unlabeled data is the testing data. 即测试数据在训练中用过。</li><li>Inductive learning: unlabeled data is not the testing data.测试数据是训练中没有用过的数据。</li><li>这里的使用testing data是指用testing data的feature，而不是使用testing data的label。</li></ul><hr><p>为什么会有semi-supervised learning？</p><ul><li><p>Collecting data is easy, but collecting “labelled” data is expensive.</p><p>【收集数据很简单，但收集有label的数据很难】</p></li><li><p>We do semi-supervised learning in our lives</p><p>【在生活中，更多的也是半监督式学习，我们能明白少量看到的事物，但看到了更多我们不懂的，即unlabeled data】</p></li></ul><h2 id="Why-Semi-supervised-learning-helps"><a href="#Why-Semi-supervised-learning-helps" class="headerlink" title="Why Semi-supervised learning helps"></a>Why Semi-supervised learning helps</h2><p>为什么半监督学习能帮助解决一些问题？</p><p>如上图所示，如果只有labeled data，分类所画的boundary可能是一条竖线。</p><img src="https://s1.ax1x.com/2020/07/03/NXRNz6.md.png" alt="NXRNz6.md.png" style="zoom:75%;" /><p>但如果有一些unlabeled data（如灰色的点），分类所画的boundary可能是一条斜线。</p><p>The distribution of the unlabeled data tell us something.</p><p>半监督式学习之所以有用，是因为这些unlabeled data的分布能告诉我们一些东西。</p><p>通常这也伴随着一些假设，所以半监督式学习是否有用往往取决于这些假设是否合理。</p><h1 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h1><h2 id="Supervised-Generative-Model"><a href="#Supervised-Generative-Model" class="headerlink" title="Supervised Generative Model"></a>Supervised Generative Model</h2><p>在<a href="/2020/03/21/Classification1/" title="这篇">这篇</a>文章中，有详细讲述分类问题中的generative model。</p><p>给定一个labelled training data $x^r\in C_1,C_2$ 训练集。</p><p>prior probability（先验概率）有 $P(C_i)$ 和 $P(x|C_i)$ ，假设是Gaussian模型，则 $P(x|C_i)$ 由Gaussian模型中的 $\mu^i,\Sigma$ 参数决定。</p><p>根据已有的labeled data，计算出假设的Gaussian模型的参数（如下图），从而得出prior probability。</p><img src="https://s1.ax1x.com/2020/07/03/NXonAS.md.png" alt="NXonAS.md.png" style="zoom:75%;" /><p>即可算出posterior probability  $P\left(C_{1} \mid x\right)=\frac{P\left(x \mid C_{1}\right) P\left(C_{1}\right)}{P\left(x \mid C_{1}\right) P\left(C_{1}\right)+P\left(x \mid C_{2}\right) P\left(C_{2}\right)}$ </p><h2 id="Semi-supervised-Generative-Model"><a href="#Semi-supervised-Generative-Model" class="headerlink" title="Semi-supervised Generative Model"></a>Semi-supervised Generative Model</h2><p>在只有labeled data的图中，算出来的 $\mu,\Sigma$ 参数如下图所示：</p><img src="https://s1.ax1x.com/2020/07/03/NXonAS.md.png" alt="NXonAS.md.png" style="zoom:75%;" /><p>但如果有unlabeled data（绿色点），会发现分布的模型参数更可能是是下图：</p><img src="https://s1.ax1x.com/2020/07/03/NXRtRx.md.png" alt="NXRtRx.md.png" style="zoom:75%;" /><p>The unlabeled data $x^u$ help re-estimate $P(C_1),P(C_2),\mu^1,\mu^2,\Sigma$ .</p><p>因此，unlabeled data会影响分布，从而影响prior probability，posterior probability，最终影响 boundary。</p><h3 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h3><p>所以有unlabeled data, 这个Semi-supervised 的算法怎么做呢？</p><p>其实就是<strong>EM</strong>（Expected-maximization algorithm，期望最大化算法。）</p><ol><li><p>Initialization : $\theta={P(C_1),P(C_2),\mu^1,\mu^2,\Sigma}$ .</p><p>初始化Gaussian模型参数，可以随机初始，也可以通过labeled data得出。</p><p>虽然这个算法最终会收敛，但是初始化的参数影响收敛结果，就像gradient descent一样。</p></li><li><p>E：Step 1: compute the posterior probability of unlabeled data $P_\theta(C_1|x^u)$ (depending on model $\theta$ )</p><p>根据当前model的参数，计算出unlabeled data的posterior probability $P(C_1|x^u)$ .(以$P(C_1|x^u)$ 为例) </p></li><li><p>M：Step 2: update model. Back to step1 until the algorithm converges enventually.</p><p>用E步得到unlabeled data的posterior probability来最大化极大似然函数，更新得到新的模型参数，公式很直觉。(以 $C_1$ 为例)</p><p>（$N$ ：data 的总数，包括unlabeled data; $N_1$ :label= $C_1$ 的data数）</p><ul><li><p>$P(C_1)=\frac{N_1+\Sigma_{x^u}P(C_1|x^u)}{N}$  </p><p>对比没有unlabeled data之前的式子， $P(C_1)=\frac{N_1}{N}$ ，除了已有label= $C_1$ ，还多了一部分，即unlabeled data中属于 $C_1$ 的概率和。</p></li><li>$\mu^{1}=\frac{1}{N_{1}} \sum_{x^{r} \in C_{1}} x^{r}+\frac{1}{\sum_{x^{u}} P\left(C_{1} \mid x^{u}\right)} \sum_{x^{u}} P\left(C_{1} \mid x^{u}\right) x^{u}$  <p>对比没有unlabeled data的式子 ，$\mu^{1}=\frac{1}{N_{1}} \sum_{x^{r} \in C_{1}} x^{r}$ ，除了已有的label= $C_1$ ，还多了一部分，即unlabeled data的 $x^u$ 的加权平均（权重为 $P(C_1\mid x^u)$ ，即属于 $C_1$ 的概率）。</p></li><li><p>$\Sigma$ 公式也包括了unlabeled data.</p></li></ul></li></ol><p>所以这个算法的Step 1就是EM算法的Expected期望部分，根据已有的labeled data得出极大似然函数的估计值；</p><p>Step 2就是EM算法的Maximum部分，利用unlabeled data（通过已有模型的参数）最大化E步的极大似然函数，更新模型参数。</p><p>最后反复迭代Step 1和Step 2，直至收敛。</p><h3 id="Why-EM"><a href="#Why-EM" class="headerlink" title="Why EM"></a>Why EM</h3><p>[1]挖坑EM详解。</p><p>为什么可以用EM算法来解决Semi-supervised?</p><ul><li><p>只有labeled data</p><p>极大似然函数 $\log{L(\theta)}=\sum_{x^r}\log{P_\theta(x^r,\hat{y}^r)}$ , 其中 $P_\theta(x^r,\hat{y}^r)=P_\theta(x^r\mid \hat{y}^r)P(\hat{y}^r)$ .</p><p>对上式子求导是有closed-form solution的。</p></li><li><p>有labeled data和unlabeled data</p><p>极大似然函数增加了一部分  $\log L(\theta)=\sum_{x^{r}} \log P_{\theta}\left(x^{r}, \hat{y}^{r}\right)+\sum_{x^{u}} \log P_{\theta}\left(x^{u}\right)$ .</p><p>将后部分用全概率展开， $P_{\theta}\left(x^{u}\right)=P_{\theta}\left(x^{u} \mid C_{1}\right) P\left(C_{1}\right)+P_{\theta}\left(x^{u} \mid C_{2}\right) P\left(C_{2}\right)$  .</p><p>如果要求后部分，因为是unlabeled data, 所以模型 $\theta$ 需要得知unlabeled data的label，即 $P(C_1\mid x^u)$ ,而求这个式子，也需要得到 prior probability $P(x^u\mid C_1)$ ,但这个式子需要事先得知模型 $\theta$ ，因此陷入了死循环。</p><p>因此这个极大似然函数不是convex（凸），不能直接求解，因此用迭代的EM算法逐步maximum极大似然函数。</p></li></ul><h1 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h1><p>另一种假设是Low-density Separation的假设，即这个世界是非黑即白的”Black-or-white”。</p><p>两种类别之间是low-density，交界处有明显的鸿沟，因此要么是类别1，要么是类别2，没有第三种情况。</p><h2 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h2><p>对于Low-density Separation Assumption的假设，使用Self-training的方法。</p><p>Given：labeled data set $={(x^r,\hat{y}^r}<em>{r=1}^R$ ,unlabeled data set $={x^u}</em>{u=l}^R+U$ .</p><p><strong>Repeat：</strong> </p><ol><li><p>Train model $f^<em>$ from labeled data set. ($f^</em>$ is independent to the model)</p><p>从labeled data set中训练出一个模型</p></li><li><p>Apply $f^*$ to the unlabeled data set. Obtain pseudo-label ${(x^u,y^u}_{u=l}^{R+U}$ .</p><p>用这个模型 $f^*$ 来预测unlabeled data set， 获得伪label</p></li><li><p>Remove a set of data from unlabeled data set, and add them into the labeled data set.</p><p>拿出一些unlabeled data(pseudo-label)，放到labeled data set中，回到步骤1，再训练。</p><ul><li><p>how to choose the data set remains open</p><p>如何选择unlabeled data 是自设计的</p></li><li><p>you can also provide a weight to each data.</p><p>训练中可以对unlabeled data(pseudo-label)和labeled data 赋予不同的权重.</p></li></ul></li></ol><p><strong>注意：</strong> Regression模型是不能self-training的，因为unlabeled data和其pseudo-label放在模型中的loss为0，无法再minimize。</p><h2 id="Hard-Label"><a href="#Hard-Label" class="headerlink" title="Hard Label"></a>Hard Label</h2><p><strong>V.S.  semi-supervised learning for generative model</strong> </p><p>Semi-supervised learning for generative model和Low-density Separation的区别其实是soft label 和soft label的区别。</p><p>generative model是利用来unlabeled data的 $P(C_1|x^u)$ posterior probability来计算新的prior probability，迭代更新模型。</p><p>而low-density是计算出unlabeled data的pseudo-label，选择性扩大labeled data set(即加入部分由pseudo-label的unlabeled data)来迭代训练模型。</p><p>因此，如果考虑Neural Network：</p><p>($\theta^*$ 是labeled data计算所得的network parameters)</p><p>如下图，unlabeled data $x^u$ 放入模型中预测，得到 $\begin{bmatrix} 0.7 \ 0.3\end{bmatrix}$ .</p><img src="https://s1.ax1x.com/2020/07/03/NXRYJ1.md.png" alt="NXRYJ1.md.png" style="zoom:75%;" /><p>如果是使用hard label，则 $x^u$ 的target是 $\begin{bmatrix} 1 \ 0\end{bmatrix}$ .</p><p>如果是使用soft label，则 $x^u$ 的target是 $\begin{bmatrix} 0.7 \ 0.3\end{bmatrix}$ .</p><p>如果是使用soft label，则self-training不会有效，因为新的data加进去，不会增大模型的loss，也就无法再minimize.</p><p><strong>所以基于Low-density Separation的假设，是非黑即白的，需要使用hard label来self-training。</strong> </p><h2 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h2><p>在训练模型中，我们需要尽量保证unlabeled data在模型中的分布是low-density separation。</p><p>即下图中，unlabeled data得到的pseudo-label的分布应该尽量集中，而不应该太分散。</p><img src="https://s1.ax1x.com/2020/07/03/NXRJiR.md.png" alt="NXRJiR.md.png" style="zoom:50%;" /> <p>所以，在训练中，<strong>如何评估 $y^u$ 的分布的集中度？</strong></p><p>根据信息学，使用 $y^u$ 的entropy，即  $E\left(y^{u}\right)=-\sum_{m=1}^{5} y_{m}^{u} \ln \left(y_{m}^{u}\right)$ </p><p>(注：这里的 $y^u_m$ 应该是  $y^u=m$ 的概率)</p><p>当 $E(y^u)$ 越小，说明 $y^u$ 分布越集中，如下图。</p><img src="https://s1.ax1x.com/2020/07/03/NXR3dJ.md.png" alt="NXR3dJ.md.png" style="zoom:50%;" /><hr><p>因此，在self-training中：</p><p>$L=\sum_{y^r} C(x^r,\hat{y}^r)+\lambda\sum_{x^u}E(y^u)$ </p><p>Loss function的前一项（cross entropy）minimize保证分类的正确性，后一项（entropy of  $y^u$ ) minimize保证 unlabeled data分布尽量集中，最大可能满足low-density separation的假设。</p><p>training：gradient decent.</p><p>因为这样的形式很像之前提到过的regularization(具体见<a href="/2020/04/21/tips-for-DL/" title="这篇文章的3.2">这篇文章的3.2</a>)，所以又叫entropy-based regularization.</p><h2 id="Outlook-Semi-supervised-SVM"><a href="#Outlook-Semi-supervised-SVM" class="headerlink" title="Outlook: Semi-supervised SVM"></a>Outlook: Semi-supervised SVM</h2><p>SVM也是解决semi-supervised learning的方法.</p><img src="https://s1.ax1x.com/2020/07/03/NXRaQK.md.png" alt="NXRaQK.md.png" style="zoom:50%;" /><p>上图中，在有unlabeled data的情况下，希望boundary 分的越开越好（largest margin）和有更小的error.</p><p>因此枚举unlabeled data所有可能的情况，但枚举在计算量上是巨大的，因此SVM（Support Vector Machines）可以实现枚举的目标，但不需要这么大的枚举量。</p><h1 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h1><p>Smoothness Assumption的思想可以用以下话归纳：</p><p>“You are known by the company you keep”</p><p>近朱者赤，近墨者黑。</p><p>蓬生麻中，不扶而直。白沙在涅，与之俱黑。</p><p>Assumption：“similar” $x$ has the same $\hat{y}$ .</p><p>【意思就是说：相近的 $x$ 有相同的label $\hat{y}$ .】</p><p><strong>More precise assumption：</strong></p><ul><li>x is not uniform</li><li>if $x^1$ and $x^2$ are close in a hign density region, $\hat{y}^1$ and $\hat{y}^2$ are the same.</li></ul><p>Smoothness Assumption假设更准确的表述是：</p><p> x不是均匀分布，如果 $x^1$ 和 $x^2$ 通过一个high density region的区域连在一起，且离得很近，则 $\hat{y}^1$ 和 $\hat{y}^2$ 相同。</p><p>如下图， $x^1$ 和 $x^2$ 通过high density region连接在一起，有相同的label，而 $x^2$ 和 $x^3$ 有不同的label.</p><img src="https://s1.ax1x.com/2020/07/03/NXR1Z4.md.png" alt="NXR1Z4.md.png" style="zoom:50%;" /><hr><p>Smoothness Assumption通过观察大量unlabeled data，可以得到一些信息。</p><p>比如下图中的两张人的左脸和右脸图片，都是unlabeled，但如果给大量的过渡形态（左脸转向右脸）unlabeled data，可以得出这两张图片是相似的结论.</p><p><a href="https://imgchr.com/i/NXoQpj"><img src="https://s1.ax1x.com/2020/07/03/NXoQpj.md.png" alt="NXoQpj.md.png"></a> </p><p>Smoothness Assumption还可以用在文章分类中，比如分类天文学和旅游学的文章。</p><p>如下图， 文章 d1和d3有overlap word（重叠单词），所以d1和d3是同一类，同理 d4和d2是一类。</p><img src="https://s1.ax1x.com/2020/07/03/NXoutg.md.png" alt="NXoutg.md.png" style="zoom:50%;" /><p>如果，下图中，d1和d3没有overlap word，就无法说明d1和d3是同一类。</p><img src="https://s1.ax1x.com/2020/07/03/NXoKhQ.md.png" alt="NXoKhQ.md.png" style="zoom:50%;" /><p>但是，如果我们收集到足够多但unlabeled data，如下图，通过high density region的连接和传递，也可以得出d1和d3一类，d2和d4一类。</p><img src="https://s1.ax1x.com/2020/07/03/NXol1s.md.png" alt="NXol1s.md.png" style="zoom:80%;" /><h2 id="Cluster-and-then-Label"><a href="#Cluster-and-then-Label" class="headerlink" title="Cluster and then Label"></a>Cluster and then Label</h2><p>在Smoothness Assumption假设下，直观的可以用cluster and then label，先用所有的data训练一个classifier。</p><p>直接聚类标记(比较难训练）。</p><h2 id="Graph-based-Approach"><a href="#Graph-based-Approach" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h2><p>另一种方法是利用图的结构（Graph structure）来得知 $x^1$ and $x^2$ are close in a high density region (connected by a high density path).</p><p>Represent the data points as a graph.</p><p>【把这些数据点看作一个图】</p><img src="https://s1.ax1x.com/2020/07/03/NXRKMT.md.png" alt="NXRKMT.md.png" style="zoom:50%;" /><p>建图有些时候是很直观的，比如网页中的超链接，论文中的引用。</p><p>但有的时候也需要自己建图。</p><p>注意：</p><p>如果是影像类，base on pixel，performance就不太好，一般会base on autoencoder，将feature抽象出来，效果更好。</p><h3 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h3><p>建图过程如下：</p><ol><li><p>Define the similarity $s(x^i, x^j)$ between $x^i$ and $x^j$ .</p><p>【定义data $x^i$ 和 $x^j$ 的相似度】</p></li><li><p>Add edge【定义数据点中加边（连通）的条件】</p><ul><li><p>K Nearest Neighbor【和该点最近的k个点相连接】</p><img src="https://s1.ax1x.com/2020/07/03/NXReGq.png" alt="NXReGq.png" style="zoom:50%;" /></li><li><p>e-Neighborhood【与离该点距离小于等于e的点相连接】</p><img src="https://s1.ax1x.com/2020/07/03/NXRZin.png" alt="NXRZin.png" style="zoom:50%;" /></li></ul></li><li><p>Edge weight is proportional to $s(x^i, x^j)$ 【边点权重就是步骤1定义的连接两点的相似度】</p><p>Gaussian Radial Basis Function： $s\left(x^{i}, x^{j}\right)=\exp \left(-\gamma\left\|x^{i}-x^{j}\right\|^{2}\right)$ </p><p>一般采用如上公式（经验上取得较好的performance）。</p><p>因为利用指数化后（指数内是两点的Euclidean distance），函数下降的很快，只有当两点离的很近时，该相似度 $s(x^i,x^j)$  才大，其他时候都趋近于0.</p></li></ol><h3 id="Graph-based-Approach-1"><a href="#Graph-based-Approach-1" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h3><p>图建好后：</p><p>The labeled data influence their neighbors.</p><p>Propagate through the graph.</p><p>【label data 不仅会影响他们的邻居，还会一直传播下去】</p><img src="https://s1.ax1x.com/2020/07/03/NXRmR0.md.png" alt="NXRmR0.md.png" style="zoom:50%;" /><p>如果data points够多，图建的好，就会像下图这样：</p><img src="https://s1.ax1x.com/2020/07/03/NXREIs.png" alt="NXREIs.png" style="zoom:50%;" /><p>但是，如果data较少，就可能出现下图这种label传不到unlabeled data的情况：</p><img src="https://s1.ax1x.com/2020/07/03/NXRPsS.png" alt="NXRPsS.png" style="zoom:50%;" /><h3 id="Smoothness-Definition"><a href="#Smoothness-Definition" class="headerlink" title="Smoothness Definition"></a>Smoothness Definition</h3><p>因为是基于Smoothness Assumption，所以最后训练出的模型应让得到的图尽可能满足smoothness的假设。</p><p><strong>注意：</strong> 这里的因果关系是，unlabeled data作为NN的输入，得到label $y$ ，该label $y$ 和labeled data的 label $\hat{y}$  一起得到的图是尽最大可能满足Smoothness Assumption的。</p><p>（<strong>而不是</strong>建好图，然后unlabeled data的label $y$ 是labeled data原有的 $\hat{y}$ 直接传播过来的，不然训练NN干嘛）</p><p>把unlabeled data作为NN的输入，得到label ，对labeled data和”unlabeled data” 建图。</p><p>为了在训练中使得最后的图尽可能满足假设，定义<strong>smoothness of the labels on the graph</strong>.</p>$S=\frac{1}{2} \sum_{i,j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}$  <p>（对于所有的labeled data 和 “unlabeled data”（作为NN输入后，有label））</p><p>按照上式计算，得到的Smoothness如下图所示：</p><p><a href="https://imgchr.com/i/NXRiqg"><img src="https://s1.ax1x.com/2020/07/03/NXRiqg.md.png" alt="NXRiqg.md.png"></a> </p><p><strong>Smaller means smoother.</strong> </p><p>【Smoothness $S$ 越小，表示图越满足这个假设】</p><hr><p>计算smoothness $S$ 有一种简便的方法：</p>$S=\frac{1}{2} \sum_{i, j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}=y^{T} L y$  (这里的1/2只是为了计算方便)<ul><li><p>$y$ : (R+U)-dim vector，是所有label data和”unlabeled data” 的label，所以是R+U维。</p><p>$y=\begin{bmatrix}…y^i…y^j…\end{bmatrix}^T$ </p></li><li><p>$L$ :(R+U) $\times$ (R+U) matrix，也叫Graph Laplacian（调和矩阵，拉普拉斯矩阵）</p><p>$L$ 的计算方法：$L=D-W$ </p><p>其中 $W$ 矩阵算是图的邻接矩阵（区别是无直接可达边的值是0）</p><p>$D$ 矩阵是一个对角矩阵，对角元素的值等于 $W$ 矩阵对应行的元素和</p><p>矩阵表示如下图所示：</p><img src="https://s1.ax1x.com/2020/07/03/NXRkZQ.md.png" alt="NXRkZQ.md.png" style="zoom:50%;" /></li></ul><p>（证明据说很枯燥，暂时略[2])</p><h3 id="Smoothness-Regularization"><a href="#Smoothness-Regularization" class="headerlink" title="Smoothness Regularization"></a>Smoothness Regularization</h3>$S=\frac{1}{2} \sum_{i, j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}=y^{T} L y$  <p> $S$ 中的 $y$ 其实是和network parameters有关的（unlabeled data的label），所以把 $S$ 也放进损失函数中minimize，以求尽可能满足smoothness assumption.</p><p>以满足smoothness assumption的损失函数： $L=\sum_{x^r} C\left(y^{r}, \hat{y}^{r}\right)+\lambda S$ </p><p>损失函数的前部分使labeled data的输出更贴近其label，后部分 $\lambda S$ 作为regularization term，使得labeled data和unlabeled data尽可能满足smoothness assumption.</p><p>除了让NN的output满足smoothness的假设，还可以让NN的任何一层的输出满足smoothness assumption，或者让某层外接一层embedding layer，使其满足smoothness assumption，如下图：</p><img src="https://s1.ax1x.com/2020/07/03/NXRAaj.png" alt="NXRAaj.png" style="zoom:50%;" /><h1 id="Better-Representation"><a href="#Better-Representation" class="headerlink" title="Better Representation"></a>Better Representation</h1><p>Better Presentation的思想就是：去芜存菁，化繁为简。</p><ul><li>Find the latent(潜在的) factors behind the observation.</li><li>The latent factors (usually simpler) are better representation.</li></ul><p>【找到所观察事物的潜在特征，即该事物的better representation】</p><p>该部分后续见这篇博客。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>挖坑：EM算法详解</p></li><li><p>挖坑：Graph Laplacian in smoothness.</p></li><li><p>Olivier Chapelle：Semi-Supervised Learning </p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？&lt;/p&gt;
&lt;p&gt;再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。&lt;/p&gt;
&lt;p&gt;对于Generative Model，文章重点讲述了如何用EM算法来训练模型。&lt;/p&gt;
&lt;p&gt;对于Low-density，文章重点讲述了如何让模型进行Self-training，并且在训练中引入Entropy-based Regularization term来尽可能low-density的假设。&lt;/p&gt;
&lt;p&gt;对于Smoothness Assumption，文章重点讲述了Graph-based Approach（基于图的方法），并且在训练中引入Smoothness Regularization term来尽可能满足Smoothness Assumption的假设。&lt;/p&gt;
&lt;p&gt;对于Better Representation，本篇文章只是简单阐述了其思想，具体介绍见这篇博客。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="Semi-supervised" scheme="https://f1ed.github.io/tags/Semi-supervised/"/>
    
  </entry>
  
  <entry>
    <title>「算法导论」:排序-总结</title>
    <link href="https://f1ed.github.io/2020/06/29/sort-preview/"/>
    <id>https://f1ed.github.io/2020/06/29/sort-preview/</id>
    <published>2020-06-28T16:00:00.000Z</published>
    <updated>2020-07-03T08:50:09.474Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章review了算法中的排序算法，包括冒泡排序、插入排序、归并排序、堆排序（以及用堆实现优先队列）、快速排序和计数排序。</p><p>分别从算法思路、算法伪代码实现、算法流程、算法时间复杂度四个方面阐述每个算法。</p><a id="more"></a><h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><p><strong>排序问题：</strong></p><p>输入：一个 $n$个数的序列 $&lt;a_1,a_1,…,a_n&gt;$ </p><p>输出：输入序列的一个重拍 $&lt;a_1’,a_2’,…,a_n’&gt;$ ，使得 $a_1’\leq a_2’ \leq…\leq a_n’$ .</p><p>在实际中，待排序的数很少是单独的数值，它们通常是一组数据，称为记录(record)。每个记录中包含一个关键字(key)，这就是需要排序的值。记录剩下的数据部分称为卫星数据(satellite data)，通常和关键字一同存取。</p><p><strong>原址排序：</strong>输入数组中仅有常数个元素需要在排序过程中存储在数组之外。</p><p>典型的原址排序有：插入排序、堆排序、快速排序。</p><hr><p><strong>符号说明：</strong> </p><ul><li><p>$\Theta$ 记号：</p><p>$\Theta$ 记号渐进给出一个函数的上界和下界。</p>  $\Theta(g(n))=\left\{f(n): \text { there exist positive constants } c_{1}, c_{2}, \text { and } n_{0}\text { such that } \right.  \left.0 \leq c_{1} g(n) \leq f(n) \leq c_{2} g(n) \text { for all } n \geq n_{0}\right\}$<p>$g(n)$ 称为 $f(n)$ 的一个渐进紧确界(asymptotically tight bound)</p></li><li><p>$O$ 记号</p><p>$O$ 记号只给出了函数的渐进上界。</p>  $O(g(n))=\left\{f(n): \text { there exist positive constants } c \text { and } n_{0}\text { such that } \right.  \left.0 \leq f(n) \leq c g(n) \text { for all } n \geq n_{0}\right\}$ </li><li><p>$\Omega$ 记号</p><p>$\Omega$ 记号给出了函数的渐进下界。</p>  $\Omega(g(n))=\left\{f(n): \text { there exist positive constants } c \text { and } n_{0}\text { such that } \right. \left.0 \leq c g(n) \leq f(n) \text { for all } n \geq n_{0}\right\}$ <p>符号比较如下图：</p></li></ul><p><a href="https://imgchr.com/i/Nh2if1"><img src="https://s1.ax1x.com/2020/06/29/Nh2if1.md.png" alt="Nh2if1.md.png"></a> </p><p><strong>排序算法运行时间一览</strong> ：</p><table><thead><tr><th>算法</th><th>最坏情况运行时间</th><th>平均情况/期望运行时间</th></tr></thead><tbody><tr><td>插入排序</td><td>$\Theta (n^2)$</td><td>$\Theta(n^2)$</td></tr><tr><td>归并排序</td><td>$\Theta(n\lg{n})$</td><td>$\Theta(n\lg{n})$</td></tr><tr><td>堆排序</td><td>$O(n\lg{n})$</td><td>-</td></tr><tr><td>快速排序</td><td>$\Theta(n^2)$</td><td>$\Theta(n\lg{n})$ (expected)</td></tr><tr><td>计数排序</td><td>$\Theta(k+n)$</td><td>$\Theta(k+n)$</td></tr><tr><td>基数排序</td><td>$\Theta(d(n+k))$</td><td>$\Theta(d(n+k))$</td></tr><tr><td>桶排序</td><td>$\Theta(n^2)$</td><td>$\Theta(n)$ (average-case)</td></tr></tbody></table><h1 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h1><p>反复交互相邻未按次序排列的元素。</p><p><strong>BUBBLESORT(A)</strong></p><ul><li>参数：A待排序数组</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to A.lengh<span class="literal">-1</span></span><br><span class="line"><span class="keyword">for</span> j = A.length downto i+<span class="number">1</span> //每次迭代找出A[<span class="type">i..j</span>]中最小的元素放在A[<span class="type">i</span>]位置</span><br><span class="line"><span class="keyword">if</span> A[<span class="type">j</span>] &lt; A[<span class="type">j</span>-<span class="number">1</span>]</span><br><span class="line">exchange A[<span class="type">j</span>] with A[<span class="type">j</span>-<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>冒泡排序是原址排序，流行但低效。</p><h1 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h1><p>如下图所示，插入排序就像打牌时排序一手扑克牌。</p><p><a href="https://imgchr.com/i/Nh2m0e"><img src="https://s1.ax1x.com/2020/06/29/Nh2m0e.png" alt="Nh2m0e.png"></a> </p><ol><li>开始时，我们的左手为空，桌子上的牌面向下。</li><li>然后，我们每次从桌子上拿走一张牌，想把它放在左手中的正确位置。</li><li>为了找到这张牌的正确位置，我们从右到左将这张牌和左手里的牌依次比较，放入正确的位置。</li><li>左手都是已排序好的牌。</li></ol><p><strong>INSERTION-SORT(A)</strong> </p><ul><li>A：待排序数组</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j = <span class="number">2</span> to A.length</span><br><span class="line">key = A[<span class="type">j</span>]</span><br><span class="line">//将key插入到已排序好的A[<span class="number">1</span><span class="type">..j</span>-<span class="number">1</span>]</span><br><span class="line">i = j - <span class="number">1</span> //pointer <span class="keyword">for</span> sorted sequence A[<span class="number">1</span><span class="type">..j</span>-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">while</span> i &gt; <span class="number">0</span> and A[<span class="type">i</span>] &gt; key</span><br><span class="line">A[<span class="type">i</span>+<span class="number">1</span>] = A[<span class="type">i</span>]</span><br><span class="line">i--</span><br><span class="line">A[<span class="type">i</span>+<span class="number">1</span>] = key</span><br></pre></td></tr></table></figure><p>插入排序是原址排序，对于少量元素是一个有效的算法。</p><p>最坏情况的运行时间： $\Theta(n^2)$ .</p><h1 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h1><h2 id="分治"><a href="#分治" class="headerlink" title="分治"></a>分治</h2><p><strong>分治</strong>： 将原问题分解为几个规模较小但类似于原问题的子问题，递归地求解这些子问题，然后再合并这些子问题的解来建立原问题的解。</p><p><strong>分治的步骤</strong>：</p><ol><li>分解：分解原问题为若干规模较小的子问题。</li><li>解决：递归地求解各子问题，规模较小，可直接求解。</li><li>合并：合并这些子问题的解成原问题的解。</li></ol><h2 id="算法核心"><a href="#算法核心" class="headerlink" title="算法核心"></a>算法核心</h2><p><strong>归并排序中的分治</strong> ：</p><ol><li>分解：分解待排序的n个元素序列成各n/2个元素序列的两个子序列。</li><li>解决：使用归并排序递归地排序两个子序列，当序列长度为1时，递归到达尽头。</li><li>合并：合并两个已经排序好的子序列以产生排序好的原序列。</li></ol><p><strong>核心</strong> ：合并两个已经排序好的子序列——MERGE(A, p, q, r)</p><ul><li>A: 待排序原数组。</li><li>p, q, r: 数组下标，满足 $p\leq q&lt;r$ 。</li><li>假设子数组 A[p..q] 和A[q+1..r]都已经排好序，合并这两个数组代替原来的A[p..r]子数组。</li></ul><hr><p><strong>MERGE算法理解：</strong> </p><ol><li>牌桌上有两堆牌面朝上，每堆都已排好序，最小的牌在顶上。希望将两堆牌合并成排序好的输出牌堆，且牌面朝下。</li><li>比较两堆牌顶顶牌，选取较小的那张，牌面朝下的放在输出牌堆。</li><li>重复步骤2直至某一牌堆为空。</li><li>将剩下的另一堆牌面朝下放在输出堆。</li></ol><p>MERGE合并的过程如下图所示：</p><p><a href="https://imgchr.com/i/Nh2E6K"><img src="https://s1.ax1x.com/2020/06/29/Nh2E6K.md.png" alt="Nh2E6K.md.png"></a> </p><p><strong>MERGE算法分析：</strong> </p><p>在上述过程中，n个元素，我们最多执行n个步骤，所以MERGE合并需要 $\Theta(n)$ 的时间。</p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><strong>MERGE(A, p, q, r)</strong> </p><ul><li>功能：合并已排序好的子数组A[p..q]和A[q+1..r]</li><li>参数：A为待排序数组，p, q, r为数组下标，且满足 $p\leq q&lt;r$ </li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Let S[<span class="type">p..r</span>] be new arrays</span><br><span class="line">k = p //pointer <span class="keyword">for</span> S[]</span><br><span class="line">i = p, j = q+<span class="number">1</span> //pointer <span class="keyword">for</span> subarray</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> k &lt;= r </span><br><span class="line"><span class="keyword">while</span> ( i &lt;= q and j &lt;= r and A[<span class="type">i</span>] &lt;= A[<span class="type">j</span>] ) or j &gt; r  // 取A[<span class="type">p..q</span>]牌堆</span><br><span class="line">S[<span class="type">k</span>++] = A[<span class="type">i</span>++]</span><br><span class="line"><span class="keyword">while</span> ( i &lt;= q and j &lt;= r and A[<span class="type">i</span>] &gt;= A[<span class="type">j</span>] ) or i &gt; q //取A[<span class="type">q</span>+<span class="number">1</span><span class="type">..r</span>]牌堆</span><br><span class="line"></span><br><span class="line">A[<span class="type">p..r</span>] = S[<span class="type">p..r</span>]</span><br></pre></td></tr></table></figure><hr><p><strong>MERGE-SORT(A, p, r)</strong> </p><ul><li>功能：排序子数组A[p..r]</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> p &lt; r</span><br><span class="line">q = (p+r)/<span class="number">2</span></span><br><span class="line"><span class="built_in">MERGE-SORT</span>(A, p, q)</span><br><span class="line"><span class="built_in">MERGE-SORT</span>(A, q+<span class="number">1</span>, r)</span><br><span class="line">MERGE(A, p, q, r)</span><br></pre></td></tr></table></figure><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><h3 id="分治算法运行时间分析"><a href="#分治算法运行时间分析" class="headerlink" title="分治算法运行时间分析"></a>分治算法运行时间分析</h3><p>分治算法运行时间递归式来自三个部分。</p><p>假设 $T(n)$ 是规模为 $n$ 的一个问题的运行时间。若规模问题足够小，则直接求解需要常量时间，将其写作 $\Theta(1)$ 。</p><p>假设把原问题分解成 $a$ 个子问题，每个子问题的规模是原问题的 $1/b$ (在归并排序中， $a$ 和 $b$ 都为2，但很多分治算法中 $a\neq b$ )。为了求解一个规模为 $n/b$ 规模的子问题，需要 $T(n/b)$ 的时间，所以需要 $aT(n/b)$ 的时间求解 $a$ 个子问题。</p><p>如果分解子问题需要 $D(n)$ 时间，合并子问题需要 $C(n)$ 时间。</p><p>递归式：</p>$$T(n)=\left\{\begin{array}{ll}\Theta(1) & \text { if } n \leq c \\ a T(n / b)+D(n)+C(n) & \text { otherwise }\end{array}\right.$$<h3 id="归并排序分析"><a href="#归并排序分析" class="headerlink" title="归并排序分析"></a>归并排序分析</h3><p>前文分析了MERGE(A, p, q, r) 合并两个子数组的时间复杂度是 $\Theta(n)$ ，即 $C(n)=\Theta(n)$ ，且 $D(n)=\Theta(n)$ .</p><p>归并排序的最坏情况运行时间 $T(n)$ :</p>$$T(n)=\left\{\begin{array}{ll}\Theta(1) & \text { if } n=1 \\ 2 T(n / 2)+\Theta(n) & \text { if } n>1\end{array}\right.$$<p>用递归树的思想求解递归式：</p><p><a href="https://imgchr.com/i/Nh2Al6"><img src="https://s1.ax1x.com/2020/06/29/Nh2Al6.md.png" alt="Nh2Al6.md.png"></a> </p><p>即递归树每层的代价为 $\Theta(n)=cn$ ，共有 $\lg{n}+1$ 层，所以归并排序的运行时间结果是 $\Theta(n\lg{n})$  .</p><h1 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h1><h2 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h2><p><strong>自由树</strong> ：连通的、无环的无向图。</p><p><strong>有根数</strong> ：是一棵自由树，其顶点存在一个与其他顶点不同的顶点，称为树的根。</p><p><strong>度</strong> ：有根树中一个结点 $x$ 孩子的数目称为 $x$ 的度。</p><p><strong>深度</strong> :从根 $r$ 到结点 $x$ 的简单路径。</p><p><strong>二叉树</strong> ：不包括任何结点，或者包括三个不相交的结点集合：一个根结点，一棵称为左子树的二叉树和一棵称为右子树的二叉树。</p><p><strong>完全k叉树</strong> ：所有叶结点深度相同，且所有内部结点度为k的k叉树。</p><hr><p><strong>（二叉）堆</strong> ：是一个数组，它可以被看成一个近似的完全二叉树，树上的每个结点对应数组中的一个元素。除了最底层外，该树被完全填满，并且是从左到右填充。如下图所示。</p><p><a href="https://imgchr.com/i/Nh2Ck9"><img src="https://s1.ax1x.com/2020/06/29/Nh2Ck9.md.png" alt="Nh2Ck9.md.png"></a> </p><p>堆的数组$A$ 有两个属性：</p><ul><li><p>$A.length$ ：数组元素的个数，A[1..A.length]中都存有值。</p></li><li><p>$A.heap-size$ ：有多少个堆元素在数组，A[1..heap-size]中存放的是堆的有效元素。</p><p>（$0\leq A.heap-size\leq A.lengh$ )</p></li></ul><p><strong>堆的性质：</strong> </p><ul><li><p>$A[1]$ :存放的是树的根结点。</p></li><li><p>对于给定的一个结点 $i$ ，很容易计算他的父结点、左孩子和右孩子的下标。</p><ul><li><p><strong>PARENT(i)</strong> </p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> i/<span class="number">2</span> //i&gt;&gt;&gt;<span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p><strong>LEFT(i)</strong> </p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> <span class="number">2</span>*i //i&lt;&lt;&lt;<span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p><strong>RIGHT(i)</strong> </p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> <span class="number">2</span>*i+<span class="number">1</span> //i&lt;&lt;&lt;<span class="number">1</span> | <span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>包含$n$ 个元素的堆的高度为 $\Theta(\lg{n})$ </p></li><li><p>堆结构上的基本操作的运行时间至多和堆的高度成正比，即时间复杂度为 $O(\lg{n})$ .</p></li><li><p>叶子结点：n/2+1 , n/2+2 , … , n</p></li></ul><p><strong>堆的分类：</strong> </p><ul><li><p>最大堆：</p><p>除了根以外的结点 $i$ 都满足 $A[\text{PARENT}(i)]\geq A[i]$ .</p><p>某个结点最多和其父结点一样大。</p><p>堆的最大元素存放在根结点中。</p></li><li><p>最小堆：</p><p>除了根以外的结点 $i$ 都满足 $A[\text{PARENT}(i)]\leq A[i]$ .</p><p>堆的最小元素存放在根结点中。</p></li></ul><p><strong>堆的基本过程</strong> :</p><ul><li>MAX-HEAPIFY：维护最大堆的过程，时间复杂度为 $O(\lg{n})$ </li><li>BUILD-MAX-HEAP：将无序的输入数据数组构造一个最大堆，具有线性时间复杂度 $O(n\lg{n})$ 。</li><li>HEAPSORT：对一个数组进行原址排序，时间复杂度为 $O(n\lg{n})$ </li><li>MAX-HEAP-INSERT、HEAP-EXTRACT-MAX、HEAP-INCREASE-KEY和HEAP-MAXIMUM：利用堆实现一个优先队列，时间复杂度为 $O(\lg{n})$ .</li></ul><h2 id="维护：MAX-HEAPIFY"><a href="#维护：MAX-HEAPIFY" class="headerlink" title="维护：MAX-HEAPIFY"></a>维护：MAX-HEAPIFY</h2><p>调用MAX-HEAPIFY的时候，假定根结点LEFT(i)和RIGHT(i)的二叉树都是最大堆，但A[i]可能小于其左右孩子，因此违背了堆的性质。</p><p>MAX-HEAPIFY通过让 A[i]“逐级下降”，从而使下标为i的根结点的子树满足最大堆的性质。</p><p><strong>MAX-HEAPIFY(A, i)</strong> </p><ul><li>功能：维护下标为i的根结点的子树，使其满足最大堆的性质。</li><li>参数：i 是该子树的根结点，其左子树右子树均满足最大堆的性质。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">l = LEFT(i)</span><br><span class="line">r = RIGHT(i)</span><br><span class="line"><span class="keyword">if</span> l &lt;= A.heap<span class="literal">-size</span> and A[<span class="type">l</span>] &gt; A[<span class="type">i</span>]</span><br><span class="line">largest = l</span><br><span class="line"><span class="keyword">else</span> largest = i</span><br><span class="line"><span class="keyword">if</span> r &lt;= A.heap<span class="literal">-size</span> and A[<span class="type">r</span>] &gt; A[<span class="type">i</span>]</span><br><span class="line">largest = r</span><br><span class="line"><span class="keyword">if</span> largest != i</span><br><span class="line">exchange A[<span class="type">i</span>] with A[<span class="type">largest</span>]</span><br><span class="line">MAX<span class="literal">-HEAPIFY</span>(A, largest)</span><br></pre></td></tr></table></figure><p>下图是执行 MAX-HEAPIFY(A, 2)的执行过程。A.heap-size=10, 图(a)(b)(c)依次体现了值为4的结点依次下降的过程。</p><p><a href="https://imgchr.com/i/Nh2PYR"><img src="https://s1.ax1x.com/2020/06/29/Nh2PYR.md.png" alt="Nh2PYR.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：</p><p>MAX-HEAPIFY的时间复杂度为 $O(lg{n})$.</p><h2 id="建堆：BUILD-MAX-HEAP"><a href="#建堆：BUILD-MAX-HEAP" class="headerlink" title="建堆：BUILD-MAX-HEAP"></a>建堆：BUILD-MAX-HEAP</h2><p>堆的性质：</p><p>子数组A[n/2+1..n]中的元素都是树的叶子结点。因为下标最大的父结点是n/2，所以n/2以后的结点都没有孩子。</p><p><strong>建堆</strong> ：每个叶结点都可以看成只包含一个元素的堆，利用自底向上的方法，对树中其他结点都调用一次MAX-HEAPIFY，把一个大小为n = A.length的数组A[1..n]转换为最大堆。</p><p><strong>BUILD-MAX-HEAP(A)</strong> </p><ul><li>功能：把A[1..n]数组转换为最大堆</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A.heap<span class="literal">-size</span> = A.length</span><br><span class="line"><span class="keyword">for</span> i = A.length/<span class="number">2</span> downto <span class="number">1</span></span><br><span class="line">MAX<span class="literal">-HEAPIFY</span>(A, i)</span><br></pre></td></tr></table></figure><p>下图是把A数组构造成最大堆的过程：</p><p><a href="https://imgchr.com/i/Nh2kSx"><img src="https://s1.ax1x.com/2020/06/29/Nh2kSx.md.png" alt="Nh2kSx.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：</p><p>BUILD-MAX-HEAP需要 $O(n)$ 次调用MAX-HEAPIFY，因此构造最大堆的时间复杂度是 $O(n\lg{n})$ .</p><h2 id="排序：HEAPSORT"><a href="#排序：HEAPSORT" class="headerlink" title="排序：HEAPSORT"></a>排序：HEAPSORT</h2><p><strong>算法思路</strong>： </p><ol><li>初始化时，调用BUILD-MAX-HEAP将输入数组A[1..n]建成最大堆，其中 n = A.length。</li><li>调用后，最大的元素在A[1]，将A[1]和A[n]互换，可以把元素放在正确的位置。</li><li>将n结点从堆中去掉(通过减少A.heap-size实现)，剩余结点中，原来根的孩子仍是最大堆，但根结点可能会违背堆的性质，调用MAX-HEAPIFY(A, 1)，从而构造一个新的最大堆。</li><li>重复步骤3，直到堆的大小从n-1降为2.</li></ol><p><strong>HEAPSORT(A)</strong> </p><ul><li>功能：利用堆对数组排序</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BUILD<span class="literal">-MAX</span><span class="literal">-HEAP</span>(A)</span><br><span class="line"><span class="keyword">for</span> i = A.length downto <span class="number">2</span></span><br><span class="line">exchange A[<span class="number">1</span>] with A[<span class="type">i</span>]</span><br><span class="line">A.heap<span class="literal">-size</span> = A.heap<span class="literal">-size</span> - <span class="number">1</span></span><br><span class="line">MAX<span class="literal">-HEAPIFY</span>(A, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下图为调用HEAPSORT的过程图：</p><p><a href="https://imgchr.com/i/NhgjyT"><img src="https://s1.ax1x.com/2020/06/29/NhgjyT.md.png" alt="NhgjyT.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：</p><p>建堆BUILD-MAX-HEAP的时间复杂度为 $O(n\lg{n})$ ，n-1次调用MAX-HEAPIFY的时间复杂度为 $O(n\lg{n})$ ，所以堆排序的时间复杂度为 $O(n\lg{n})$ .</p><h2 id="堆的应用：优先队列"><a href="#堆的应用：优先队列" class="headerlink" title="堆的应用：优先队列"></a>堆的应用：优先队列</h2><p>这里关注如何用最大堆实现最大优先队列。</p><p><strong>优先队列(priority queue)：</strong> </p><p>一种用来维护由一组元素构成的集合S的数据结构，其中每一个元素都有一个相关的值，称为关键字(key)。</p><p><strong>（最大）优先队列支持的操作</strong> ：</p><ul><li>INSERT(S, x)：把元素 $x$ 插入集合S中，时间复杂度为 $O(\lg{n})$ 。</li><li>MAXIMUM(S)：返回S中具有最大关键字的元素，时间复杂度为 $O(1)$ 。</li><li>EXTRACT-MAX(S)：去掉并返回S中的具有最大关键字的元素，时间复杂度为 $O(\lg{n})$ 。</li><li>INCREASE-KEY(S, x, k)：将元素 $x$ 的关键字值增加到k，这里假设k的大小不小于元素 $x$ 的原关键字值，时间复杂度为 $O(\lg{n})$ 。</li></ul><h3 id="MAXIMUM"><a href="#MAXIMUM" class="headerlink" title="MAXIMUM"></a>MAXIMUM</h3><p>将集合S已建立最大堆的前提下，调用HEAP-MAXIMUM在 $\Theta(1)$ 实现MAXIMUM的操作。</p><p><strong>HEAP-MAXIMUM(A)</strong> </p><ul><li>功能：实现最大优先队列MAXIMUM的操作，即返回集合中最大关键字的元素。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> A[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><strong>时间复杂度分析</strong> ：$\Theta(1)$ </p><h3 id="EXTRACT-MAX"><a href="#EXTRACT-MAX" class="headerlink" title="EXTRACT-MAX"></a>EXTRACT-MAX</h3><p>类似于HEAPSORT的过程。</p><ul><li><p>A[1]为最大的元素，A[1]的孩子都是最大堆。</p></li><li><p>将A[1]和A[heap-size]交换，减少堆的大小(heap-size)。</p></li><li><p>此时根结点的孩子满足最大堆，而根不一定满足最大堆性质，维护一下当前堆。</p></li></ul><p><strong>HEAP-EXTRACT-MAX(A)</strong> </p><ul><li>功能：实现最大优先队列EXTRACT-MAX的操作，即去掉并返回集合中最大关键字的元素。</li></ul><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> A.heap-<span class="keyword">size</span> &lt; <span class="number">1</span></span><br><span class="line"><span class="keyword">error</span> <span class="string">"heap underflow"</span></span><br><span class="line"><span class="keyword">max</span> = A[<span class="number">1</span>]</span><br><span class="line">A[<span class="number">1</span>] = A[A.heap-<span class="keyword">size</span>]</span><br><span class="line">A.heap-<span class="keyword">size</span> = A.heap-<span class="keyword">size</span> - <span class="number">1</span></span><br><span class="line">MAX-HEAPIFY(A, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">max</span></span><br></pre></td></tr></table></figure><p><strong>时间复杂度分析</strong> ：$O(\lg{n})$ .</p><h3 id="INCREASE-KEY"><a href="#INCREASE-KEY" class="headerlink" title="INCREASE-KEY"></a>INCREASE-KEY</h3><p>如果增加A[i]的关键词，可能会违反最大堆的性质，所以实现HEAP-INCREASE-KEY的过程类似插入排序：从当前i结点到根结点的路径上为新增的关键词寻找恰当的插入位置。</p><ol><li><p>当前元素不断和其父结点比较，如果当前元素的关键字更大，则和父结点进行交换。</p></li><li><p>步骤1不断重复，直至当前元素的关键字比父结点小。</p></li></ol><p><strong>HEAP-INCREASE-KEY(A, i, key)</strong> </p><ul><li>功能：实现最大优先队列INCREASE-KEY的功能，即将A[i]的关键字值增加为key.</li><li>参数：i为待增加元素的下标，key为新关键字值。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> key &lt; A[<span class="type">i</span>]</span><br><span class="line">error <span class="string">"new key is smaller than current key"</span></span><br><span class="line">A[<span class="type">i</span>] = key</span><br><span class="line"><span class="keyword">while</span> i &gt; <span class="number">1</span> and A[<span class="type">PARENT</span>(<span class="type">i</span>)] &lt; A[<span class="type">i</span>]</span><br><span class="line">exchange A[<span class="type">i</span>] with A[<span class="type">PARENT</span>(<span class="type">i</span>)]</span><br><span class="line">i = PARENT(i)</span><br></pre></td></tr></table></figure><p>下图展示了HEAP-INCREASE-KEY的过程：</p><p><a href="https://imgchr.com/i/Nh2Sw4"><img src="https://s1.ax1x.com/2020/06/29/Nh2Sw4.md.png" alt="Nh2Sw4.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：$O(\lg{n})$ </p><h3 id="INSERT"><a href="#INSERT" class="headerlink" title="INSERT"></a>INSERT</h3><p>如何插入一个元素扩展最大堆？</p><ol><li>先通过增加一个关键字值为 $-\infin$ 的叶子结点扩展最大堆。</li><li>再调用HEAP-INCREASE-KEY过程为新的结点设置对应的关键字值。</li></ol><p><strong>MAX-HEAP-INSERT(A, key)</strong></p><ul><li>功能：实现最大优先队列的INSERT功能，即将关键字值为key的新元素插入到最大堆中。</li><li>参数：key是待插入元素的关键字值。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A.heap<span class="literal">-size</span> = A.heap<span class="literal">-size</span> + <span class="number">1</span></span><br><span class="line">A[<span class="type">A.heap</span>-<span class="type">size</span>] = -∞</span><br><span class="line">HEAP<span class="literal">-INCREASE</span><span class="literal">-KEY</span>(A, A.heap<span class="literal">-size</span>, key)</span><br></pre></td></tr></table></figure><p><strong>时间复杂度分析</strong> ：$O(\lg{n})$ .</p><h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><p>对于包含 $n$个数的输入数组来说，快速排序是一个最坏情况时间复杂度为 $\Theta(n^2)$ 的排序算法。</p><p>虽然最坏情况时间复杂度很差，但是快速排序通常是实际排序应用中最好的选择，因为他的平均性能非常好：他的期望时间复杂度为 $\Theta(n\lg{n})$ ，而且 $\Theta(n\lg{n})$ 中隐含的常数因子非常小。</p><p>另外，它还能进行原址排序。</p><h3 id="分治-1"><a href="#分治-1" class="headerlink" title="分治"></a>分治</h3><p>对A[p..r]子数组进行快速排序的分治过程：</p><ul><li><p>分解：</p><p>数组A[p..r]被划分为两个（可能为空）的子数组A[p..q-1]和A[q+1..r]。</p><p>使得A[p..q-1]中的每个元素都小于等于A[q]，A[q+1..r]中的每个元素都大于等于A[q]。</p><p>其中计算下标q也是分解过程的一部分。</p></li><li><p>解决：通过递归调用快速排序，对子数组A[p..q-1]和A[q+1..r]进行排序。</p></li><li><p>合并：因为子数组都是原址排序的，所以不需要合并操作，A[p..r]已经排好序。</p></li></ul><h3 id="快速排序：QUICKSORT"><a href="#快速排序：QUICKSORT" class="headerlink" title="快速排序：QUICKSORT"></a>快速排序：QUICKSORT</h3><p>按照分治的过程。</p><p><strong>QUICKSORT(A, p, r)</strong> </p><ul><li>功能：快速排序子数组A[p..r]</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> p &lt; r</span><br><span class="line">q = PARTITION(A, p, r)</span><br><span class="line">QUICKSORT(A, p, q<span class="literal">-1</span>)</span><br><span class="line">QUICKSORT(A, q+<span class="number">1</span>, r)</span><br></pre></td></tr></table></figure><h3 id="数组的划分：PARTITION"><a href="#数组的划分：PARTITION" class="headerlink" title="数组的划分：PARTITION"></a>数组的划分：PARTITION</h3><p>快速排序的关键部分就在于如何对数组A[p..r]进行划分，即找到位置q。</p><p><strong>PARTITION(A, p, r)</strong> </p><ul><li>功能：对子数组A[p..r] 划分为两个子数组A[p..q-1]和子数组A[q+1..r]，其中A[p..q-1] 小于等于A[q]小于等于A[q+1..r]</li><li>返回：数组的划分下标q</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = A[<span class="type">r</span>]</span><br><span class="line">i = p - <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> j = p to r - <span class="number">1</span> // j is pointer <span class="keyword">for</span> comparation</span><br><span class="line"><span class="keyword">if</span> A[<span class="type">j</span>] &lt;= x</span><br><span class="line">i = i+<span class="number">1</span></span><br><span class="line">exchange A[<span class="type">i</span>] with A[<span class="type">j</span>]</span><br><span class="line">exchange A[<span class="type">i</span>+<span class="number">1</span>] with A[<span class="type">r</span>]</span><br><span class="line"><span class="keyword">return</span> i+<span class="number">1</span></span><br></pre></td></tr></table></figure><p>PARTITION总是选择一个 $x=A[r]$ 作为主元(pivot element)，并围绕它来划分子数组A[p..r]。</p><p>在循环中，数组被划分为下图四个（可能为空的）区域：</p><p><a href="https://imgchr.com/i/NhgvOU"><img src="https://s1.ax1x.com/2020/06/29/NhgvOU.md.png" alt="NhgvOU.md.png"></a> </p><ol><li>$p\leq k\leq i$ ，则 $A[k]\leq x$ .</li><li>$i+1\leq k \leq j-1$ ，则 $A[k]&gt;x$.</li><li>$k = r$ ，则 $A[k]=x$ .</li><li>$j\leq k\leq r-1$ ，则 $A[k]$ 与 $x$ 无关。</li></ol><p>下图是将样例数组PARTITION的过程：</p><p><a href="https://imgchr.com/i/Nh2pTJ"><img src="https://s1.ax1x.com/2020/06/29/Nh2pTJ.md.png" alt="Nh2pTJ.md.png"></a> </p><h3 id="快速排序的性能"><a href="#快速排序的性能" class="headerlink" title="快速排序的性能"></a>快速排序的性能</h3><p>[*]待补充</p><h3 id="快速排序的随机化版本"><a href="#快速排序的随机化版本" class="headerlink" title="快速排序的随机化版本"></a>快速排序的随机化版本</h3><p>与始终采用 $A[r]$ 作为主元的方法不同，随机抽样是从子数组A[p..r]随机选择一个元素作为主元。</p><p>加入随机抽样，在平均情况下，对子数组A[p..r]的划分是比较均匀的。</p><p><strong>RANDOMIZED-PEARTITION(A, p, r)</strong> </p><ul><li>功能：数组划分PARTITION的随机化主元版本</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i = RANDOM(p, r)</span><br><span class="line">exchange A[<span class="type">r</span>] with A[<span class="type">i</span>]</span><br><span class="line"><span class="keyword">return</span> PARTITION(A, p, r)</span><br></pre></td></tr></table></figure><p><strong>RANDOMIZED-QUICKSORT(A, p, r)</strong> </p><ul><li>功能：使用随机化主元的快速排序</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> p &lt; r</span><br><span class="line">q = RANDOMIZED<span class="literal">-PARTITION</span>(A, p, r)</span><br><span class="line">RANDOMIZED<span class="literal">-QUICKSORT</span>(A, p, q<span class="literal">-1</span>)</span><br><span class="line">RANDOMIZED<span class="literal">-QUICKSORT</span>(A, q+<span class="number">1</span>, r)</span><br></pre></td></tr></table></figure><h1 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h1><p><strong>计数排序</strong> ：</p><p>假设 $n$ 个输入元素中的每一个都是在 0到 $k$ 区间内到一个整数，其中 $k$ 为某个整数。当 $k = O(n)$ 时，排序的运行时间为 $\Theta(n)$ .</p><p><strong>计数排序的思想</strong> ：</p><p>对每一个输入元素 $x$，确定小于 $x$ 的元素个数。利用这个信息，就可以直接把 $x$ 放在输出数组正确的位置了。</p><p><strong>COUNTING-SORT(A, B, k)</strong> </p><ul><li>功能：计数排序</li><li>参数：<ul><li>A[1..n]输入的待排序数组，A.length = n</li><li>B[1..n] 存放排序后的输出数组；</li><li>临时存储空间 C[0..k] ，A[1..n]中的元素大小不大于k.</li></ul></li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">let C[<span class="number">0</span><span class="type">..k</span>] be a new array</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">0</span> to k</span><br><span class="line">C[<span class="type">i</span>] = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j = <span class="number">1</span> to A.length</span><br><span class="line">C[<span class="type">A</span>[<span class="type">j</span>]] = C[<span class="type">A</span>[<span class="type">j</span>]] + <span class="number">1</span></span><br><span class="line">//C[<span class="type">i</span>] now contains the number of elements equal to i.</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to k</span><br><span class="line">C[<span class="type">i</span>] = C[<span class="type">i</span>] + C[<span class="type">i</span>-<span class="number">1</span>]</span><br><span class="line">//C[<span class="type">i</span>] now contains the number of elements less than or equal to i.</span><br><span class="line"><span class="keyword">for</span> j = A.length downto <span class="number">1</span></span><br><span class="line">B[<span class="type">C</span>[<span class="type">A</span>[<span class="type">j</span>]]] = A[<span class="type">j</span>]</span><br><span class="line">C[<span class="type">A</span>[<span class="type">j</span>]] = C[<span class="type">A</span>[<span class="type">j</span>]] - <span class="number">1</span></span><br></pre></td></tr></table></figure><p>下图是计数排序的过程：</p><p><a href="https://imgchr.com/i/NhgzmF"><img src="https://s1.ax1x.com/2020/06/29/NhgzmF.md.png" alt="NhgzmF.md.png"></a> </p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Introduction to Algorithms.</li><li>算法导论</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章review了算法中的排序算法，包括冒泡排序、插入排序、归并排序、堆排序（以及用堆实现优先队列）、快速排序和计数排序。&lt;/p&gt;
&lt;p&gt;分别从算法思路、算法伪代码实现、算法流程、算法时间复杂度四个方面阐述每个算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法导论" scheme="https://f1ed.github.io/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/"/>
    
    
      <category term="Intro-to-Algorithms" scheme="https://f1ed.github.io/tags/Intro-to-Algorithms/"/>
    
      <category term="Sort" scheme="https://f1ed.github.io/tags/Sort/"/>
    
      <category term="Algorithms" scheme="https://f1ed.github.io/tags/Algorithms/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-Dan」:Stream Cipher 3</title>
    <link href="https://f1ed.github.io/2020/06/26/StreamCipher3/"/>
    <id>https://f1ed.github.io/2020/06/26/StreamCipher3/</id>
    <published>2020-06-25T16:00:00.000Z</published>
    <updated>2020-07-03T08:45:16.978Z</updated>
    
    <content type="html"><![CDATA[<p>Stream Cipher的第三篇文章。</p><p>文章主要分为两部分，前部分逐步定义Secure PRG的定义，通过引入statistical test（统计测试）和Advantage（优势）得出当且仅当PRG is unpredictable,PRG is secure的结论。</p><p>后部分介绍了密码学中的一个重要概念Semantic Security的定义，通过引入 computationally indistinguishable(计算上不可区分)的概念给出定义，并证明了OTP的语意安全和在安全PRG条件下的流密码的语意安全，得出如果流密码中使用的PRG is secure,那么流密码就具备semantic security。</p><p>文章开头，也简单介绍了密码学中negligible和non-negligible的含义。</p><a id="more"></a><h1 id="Negligible-and-non-negligible"><a href="#Negligible-and-non-negligible" class="headerlink" title="Negligible and non-negligible"></a>Negligible and non-negligible</h1><p><strong>In practice:</strong> $\epsilon$ is a scalar and </p><ul><li>$\epsilon$ non-neg: $\epsilon \geq 1/2^{30}$ (likely to happen over 1GB of data)</li><li>$\epsilon$ negligible: $\epsilon \leq 1/2^{80}$ (won’t happen over life of key)</li></ul><p>在实践中，$\epsilon$ 是一个数值，如果是non-neg不可忽略的话，大约在1GB的数据下就会发生，如果是可忽略的值，在密钥的生存周期内基本不会发生。</p><p><strong>In theory:</strong> $\epsilon$ is a function  $\varepsilon: Z^{\geq 0} \rightarrow R^{\geq 0}$  and</p><ul><li>$\epsilon$ non-neg:  $\exists d: \epsilon(\lambda)\geq 1/\lambda^d$ ($\epsilon \geq 1/\text{poly} $, for many $\lambda$ )</li><li>$\epsilon$ negligible:  $\forall d, \lambda \geq \lambda_{d}:  \varepsilon(\lambda) \leq 1 / \lambda^{d}$  ( $\epsilon \leq 1/\text{poly}$, for large $\lambda$ )</li></ul><p>[0]（理论中，这个还不太理解，待补充。）</p><h1 id="PRG-Security-Defs"><a href="#PRG-Security-Defs" class="headerlink" title="PRG Security Defs"></a>PRG Security Defs</h1><p>Let  $G:K\longrightarrow \{0,1\}^n$  be a PRG.</p><p><u>Goal:</u> define what it means that </p><p>[  $k\stackrel{R}{\longleftarrow} \mathcal{K}$   , output G(k) ] is “indistinguishable” from [  $r\stackrel{R}{\longleftarrow} \{0,1\}^n$   , output r] .</p><p>【使得PGR的输出和真随机是不可区分的】（ $\stackrel{R}{\longleftarrow}$ 的意思是在均匀分布中随机取）</p><p>下图中，红色的圈是全部的 ${0,1}^n$ 串，按照定义是均匀分布。而粉色G()是PRG的输出，由于seed很小，相对于全部的 ${0,1}^n$ ，所以G()的输出范围也很小。</p><p><a href="https://imgchr.com/i/NsuU4s"><img src="https://s1.ax1x.com/2020/06/26/NsuU4s.png" alt="NsuU4s.png"></a> </p><p>因此，attacker观测G(k)的输出，是不能和uniform distribution（均匀分布）的输出区分开。</p><p>这也就是我们所想构造的安全PGR的目标。</p><h2 id="Statistical-Tests"><a href="#Statistical-Tests" class="headerlink" title="Statistical Tests"></a>Statistical Tests</h2><p>Statistical test on ${0,1}^n$ ：有一个算法A，$x\in{0,1}^n$ ,A(x) 根据算法定义输出”0”或”1”.</p><p>统计测试是自定义的。</p><p>Example：</p><ol><li><p>A(x)=1 if  $| \#0(x)-\#1(x)|\leq 10\cdot\sqrt{n}$   </p><p>【期望串中0、1的数目差不多，这样look random】</p></li><li><p>A(x)=1 if  $|\#00(x)-\frac{n}{4}\leq10\cdot\sqrt{n}$   </p><p>【期望Pr(00)=1/4 ,串中00出现的概率为1/4，认为是look random】</p></li><li><p>A(x)=1if max-run-of-0(x) &lt; 10·log(n)</p><p>【期望串中0的最大游程不要超过规定值】</p></li></ol><p>上面的第三个例子，如果输出为全1，满足上述的统计条件输出1，但全1串看起来并不random。</p><p>统计测试也由于是自定义的，所以通过统计测试的也不一定是random，其PRG也不一定是安全的。</p><p>所以，如何评估一个统计测试的好坏？</p><p>下面引入一个重要的定义advantage，优势。</p><h2 id="Advantage"><a href="#Advantage" class="headerlink" title="Advantage"></a>Advantage</h2><p>引入Advantage（优势）来评估一个统计测试的好坏。</p><p>Let G: $k \rightarrow \{0,1\}^n$  be a PRG and A a stat. test on ${0,1}^n$ </p><p>【G是一个PRG，A是一个对01串的统计测试】</p><p>Define: the advantage of statisticaltest A relative to PRG G Adv$_\text{PRG}[A,G]$ </p>  $\text{Adv}_\text{PRG}[A,G]=|Pr[A(G(k))=1]-Pr[A(r)=1]|\in[0,1]$   ,  $k\stackrel{R}{\longleftarrow} \mathcal{K}, r\stackrel{R}{\longleftarrow}\{0,1\}^n$  <p>【定义：Adv$_\text{PRG}[A,G]$ 为统计测试A对于PRG G的优势为统计测试以PRG作为输入输出为1的概率 减去 统计测试以truly random string 作为输入输出为1的概率】</p><ul><li>Adv close to 1 : 统计测试能区分PRG的输出和truly random string，即adversary可以利用区分PRG的输出和random的这一点从而破解系统。</li><li>Adv close to 0 : 统计测试不能区分PRG的输出和truly random string, 即adversary认为PRG的输出和random别无二致。</li></ul><blockquote><p><a href="https://en.wikipedia.org/wiki/Advantage_(cryptography)">Advantage</a> 优势[1]</p><p>In cryptography, an adversary’s advantage is a measure of how successfully it can attack a cryptographic algorithm, by distinguishing it from an idealized version of that type of algorithm.Note that in this context, the “adversary” is itself an algorithm and not a person. A cryptographic algorithm is considered secure if no adversary has a non-negligible advantage, subject to specified bounds on the adversary’s computational resources (see concrete security). </p><p>在密码学中，adversary的优势是评估它通过某种理想算法破解一个加密算法的成功尺度。</p><p>这里的adversary是一种破解算法而不是指攻击者这个人。</p><p>当没有 adversary对该加密算法有不可忽略的优势时，该加密算法被认为是安全的，因为adversary只有有限的计算资源。</p></blockquote><p>e.g.1 : A(x) = 0，统计测试A对PRG的任何输出都输出0，则Adv[A,G] = 0.</p><p>e.g.2 : </p><p>G: $k \rightarrow {0,1}^n$ satisfies msb(G(k))=1 for 2/3 of keys in K.</p><p>Define statistical test A(x) as : if[ msb(x)=1 ] output “1” else output “0”</p><p>【PRG G(k)的2/3的输出的最高有效位是1，定义统计测试A(x),输入的最高有效位为1输出1，否则输出0】</p><blockquote><p>msb: most significant bit,最高有效位。</p></blockquote><p>则 Adv[A,G] = | Pr[A(G(k))] - Pr[A(r)] | = 2/3 - 1/2 = 1/6</p><p>即 A can break the generator G with advantage 1/6, PRG G is not good. </p><h2 id="Secure-PRGs-crypto-definition"><a href="#Secure-PRGs-crypto-definition" class="headerlink" title="Secure PRGs: crypto definition"></a>Secure PRGs: crypto definition</h2><p><u>Def:</u> We say that G: $k \rightarrow {0,1}^n$ is <strong>secure PRG</strong></p><p> if $\forall$ “eff” stat. tests A : Adv$_\text{PRG}$ [A,G]  is “negligible”.</p><p>这里的”eff”,指多项式时间内。</p><p>这个定义，“所有的统计测试”，这一点难以达到，因此没有provably secure PRGs。</p><p>但我们有heuristic candidates.（有限的stat. test 能满足）</p><h3 id="Easy-fact-a-secure-PRG-is-unpredictable"><a href="#Easy-fact-a-secure-PRG-is-unpredictable" class="headerlink" title="Easy fact: a secure PRG is unpredictable"></a>Easy fact: a secure PRG is unpredictable</h3><p>证明命题： a secure PRG is unpredictable.</p><p>即证明其逆否命题： PRG is predictable is insecure。</p><p>suppose A is an eddicient algorithm s.t. $\text{Pr}_{k\stackrel{R}{\longleftarrow}\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\frac{1}{2} + \epsilon$    for non-negligible $\epsilon$ .</p><p>【 算法A是一个有效的预测算法， $\text{Pr}_{k\stackrel{R}{\longleftarrow}\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\frac{1}{2} + \epsilon$   , $\epsilon$ 是不可忽略的值，即A能以大于1/2的概率推测下一位。】</p><p>Adversary能利用算法A来区分这个PRG和random依次来破解系统。</p><p>Define statistical test B as: B(x)=1 if $A(x|<em>{1,…,i})=x</em>{i+1}$ , else B(x)=0.</p><p>【定义一个统计测试B，如果预测算法A预测下一位正确输出1，否则输出0】</p><hr><p>计算Adv$_\text{PRG}$ :</p><ul><li> $r\stackrel{R}{\longleftarrow}\{0,1\}^n$ : Pr[B(r) = 1] = 1/2</li><li> $r\stackrel{R}{\longleftarrow}\{0,1\}^n$  : Pr[B(G(k)) = 1] = 1/2+ $\epsilon$ </li><li>Adv$_\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | &gt; $\epsilon$ </li></ul><p>Adversary 能以 $\epsilon$ 的优势区分PRG和random，因此PRG 不安全。</p><p>所以，如果A是一个好的预测算法，那么B就是一个好的统计算法，Adversary就能通过B以 $\epsilon$ 的优势破解系统。</p><p>在此，证明了 if A can predict PRG, PRG is insecure $\Rightarrow$ <strong>A secure PRG is unpredictable.</strong> </p><h3 id="Thm-Yao’82-an-unpredictable-PRG-is-secure"><a href="#Thm-Yao’82-an-unpredictable-PRG-is-secure" class="headerlink" title="Thm(Yao’82): an unpredictable PRG is secure"></a>Thm(Yao’82): an unpredictable PRG is secure</h3><p>上节证明了A secure PRG is unpredictable.</p><p>在1982 Yao 的论文[2]中证明了其逆命题： an unpredictable PRG is secure.</p><p>G: $k \rightarrow {0,1}^n$ be PRG</p><p><strong>“Thm</strong>“ : <strong>if $\forall i \in$ {0,…, n-1} PRG G is unpredictable at pos. i then G is a secure PRG.</strong></p><p>【定理：如果在任何位置PRG都是不可预测的，那么PRG就是安全的】</p><p><strong>Proof：</strong> </p><ul><li>A: 预测算法： $\forall i \quad\text{Pr}_{k\stackrel{R}{\longleftarrow}\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]=\frac{1}{2} $  </li><li>B:统计测试： B(x)=1 if $A(x|<em>{1,…,i})=x</em>{i+1}$ , else B(x)=0.</li><li>Adv$_\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | = 0 </li></ul><p>If next-bit predictors cannot distinguish G from random then no statistial test can!</p><p>【next-bit predictor指用预测算法的统计测试】</p><p>e.g.</p><p> Let G: $k \rightarrow {0,1}^n$ be PRG such that from the last n/2 bits of G(k) it is easy to compute the first n/2 bits.</p><p>Is G predictable for som i $\in$ {0, …, n-1} ?</p><p>: Yes.当n=2时，可以预测出第一位。 </p><p><strong>在此，可以得出结论：</strong></p><p><strong>Adversary不可区分PRG的输出和truly random string时被认为是安全的。</strong></p><p><strong>当且仅当PRG在任意位置不可预测时，PRG是安全的。</strong> </p><h2 id="More-generally-computationally-indistinguishable"><a href="#More-generally-computationally-indistinguishable" class="headerlink" title="More generally: computationally indistinguishable"></a>More generally: computationally indistinguishable</h2><p>Let P1 and P2 be two distributions over ${0,1}^n$ </p><p><u>Def</u> : We say that P1 and P2 are <strong>computationally indistinguishable</strong> (denoted $P_1\approx_p P_2$) </p><p>If $\forall$ “eff” stat. tests A  $\left|\text{Pr}_{x\leftarrow_{P_1}}[A(x)=1]-\text{Pr}_{x\leftarrow_{P_2}}[A(x)=1]\right|<\text{negligible}$   .</p><p>【P1,P2都是01串上的两个分布，且对任意的有效统计测试满足上式，则认为P1和P2两个分布在计算上不可区分，记做$P_1\approx_p P_2$】</p><p>所以，安全的PRG等价于G(k)的分布和均匀分布计算上不可区分，即 $\{\mathrm{k} \stackrel{\mathrm{R}}{\longleftarrow}{\mathrm{K}}: \mathrm{G}(\mathrm{k})\} \approx_{\mathrm{p}} \text { uniform }\left(\{0,1\}^{\mathrm{n}}\right)$  </p><h1 id="Semantic-Security"><a href="#Semantic-Security" class="headerlink" title="Semantic Security"></a>Semantic Security</h1><p>上文详细了定义secure PRGs 的过程，本节的目标是定义”secure” stream cipher,安全流密码。</p><p><strong>What is a secure cipher?</strong></p><p>attacker abilities: obtains one ciphertext(唯密文攻击)</p><p>下面这些可能的安全需求是否能定义是安全的加密解密算法？</p><p>Possible security requirements：</p><ol><li><p>attacker cannot recover secret key.</p><p>attacker不能恢复密钥。</p><p>E(k,m)=m直接输出明文，这个算法满足条件，但显然不安全。</p></li><li><p>attacker cannot recover all of plaintext.</p><p>attacker不能恢复所有的明文。</p><p>E(k, m0||m1) = m0||k $\oplus$ m1,该算法泄漏了一半的明文，不安全。</p></li><li><p>Shannon’s idea: CT should reveal no “info” about PT.</p></li></ol><h2 id="Recall-Shannon‘s-perfect-secrecy"><a href="#Recall-Shannon‘s-perfect-secrecy" class="headerlink" title="Recall Shannon‘s perfect secrecy"></a>Recall Shannon‘s perfect secrecy</h2><p>在<a href="/2020/03/15/StreamCipher1/" title="这篇文章1.2.2">这篇文章1.2.2</a> 定义了perfect secrecy。</p><p>A cipher $(E,D)$ over  $\mathcal{(K,M,C)}$ has <strong>perfect security</strong> if  $\forall m_0,m_1 \in \mathcal{M}\ (|m_0|=|m_1|) \quad \text{and} \quad \forall c\in \mathcal{C} $ </p>$$Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \qquad \text{where} \ k\overset{R}{\longleftarrow}\mathcal{K}$$<p>下面根据perfect security的定义逐步放宽条件定义：</p><p>Let (E,D) be a cipher over (K,M,C)</p><ul><li><p><a href="https://imgchr.com/i/NsutEQ"><img src="https://s1.ax1x.com/2020/06/26/NsutEQ.md.png" alt="NsutEQ.md.png"></a> </p><p>上图中的定义就是perfect security的定义，密钥加密m0和m1的分布完全相同。</p><p>引入上文中的computationally indistinguishable（计算不可区分）的概率放宽条件，得到下面的定义。</p></li><li><p><a href="https://imgchr.com/i/NsuGDS"><img src="https://s1.ax1x.com/2020/06/26/NsuGDS.md.png" alt="NsuGDS.md.png"></a></p><p>密钥加密m0和m1的分布计算上不可区分。</p></li><li><p>但枚举所有的m0和m1的定义也太强了，因此希望能让adversary列举计算上可列举的m0和m1.</p></li></ul><h2 id="Semantic-Security-for-OTP"><a href="#Semantic-Security-for-OTP" class="headerlink" title="Semantic Security for OTP"></a>Semantic Security for OTP</h2><h3 id="attack-game"><a href="#attack-game" class="headerlink" title="attack game"></a>attack game</h3><p>在定义OTP的semantic security （语意安全）之前，定义一个attack game。</p><p><strong>Attack Game</strong>[3]: For a given cipher $\mathcal{E}$ = (E, D), deﬁned over (K, M , C), and for a given adversary $\mathcal{A}$ , we deﬁne two experiments, Experiment 0 and Experiment 1. For b = 0, 1, we deﬁne</p><p>【定义一个adversary $\mathcal{A}$，两个实验也对应两个challenger，实验定义如下】</p><p><strong>Experiment b:</strong></p><ul><li><p>The adversary computes m0 , m1 ∈ M , of the same length, and sends them to the challenger.</p><p>【adversary向challenger发m0和m1，他们长度相同】</p></li><li><p>The challenger computes $k\stackrel{R}{\leftarrow}\mathcal{K}$, $c\stackrel{R}{\leftarrow}E(k,m_b)$, and sends c to the adversary.</p><p>【实验b中challenger随机选取一个密钥k,用密钥加密mb,再将结果c返回给adversary】</p></li><li><p>The adversary outputs a bit $\hat{b}\in$  { 0, 1 } .</p><p>【adversary收到c，猜测c是加密m0还是m1的，输出 $\hat{b}$ 】</p></li></ul><p>过程如下图所示，对于adversary $\mathcal{A}$ 来说，就像只有一个challenger，没有EXP(0)和EXP(1)的区别，因为它本来就是猜测challenger发送的c是加密m0所的还是m1所的。而对于challenger来说，才有EXP(0)和EXP(1)，在EXP(0)中，加密算法随机选择密钥加密m0返回，在EXP(1)中，加密算法随机选择密钥加密m1返回。</p><p><a href="https://imgchr.com/i/Nsuu4A"><img src="https://s1.ax1x.com/2020/06/26/Nsuu4A.md.png" alt="Nsuu4A.md.png"></a> </p><p>for b=0,1 $W_b$ = [event that $\mathcal{A}$  output 1 in experiment b].</p><p>【定义事件 $W_b$ 为在实验EXP(b)中$\mathcal{A}$ 输出1】</p><p><u>Def</u>： $\mathcal{A}$‘s <strong>semantic security advantage</strong> with respect to $\mathcal{E}$ as </p>$$\operatorname{Adv_{SS}}[\mathcal{A}, \mathcal{E}]:=\left|\operatorname{Pr}\left[W_{0}\right]-\operatorname{Pr}\left[W_{1}\right]\right|$$<p>【定义adversary $\mathcal{A}$ 对于cipher $\mathcal{E}$ 的语意安全优势为上式】</p><p>Note that in the above game, the events W0 and W1 are deﬁned with respect to the probability space determined by the random choice of k, the random choices made (if any) by the encryption algorithm, and the random choices made (if any) by the adversary. The value Advss[ A , E] is a number between 0 and 1.</p><p>【注意：上面的游戏中，W0和W1的概率空间分布是与密钥的随机选择，加密算法的随机选择（<strong>该加密算法选择的是challenger0加密的m0还是challenger1加密的m1，adversary并不知道</strong>）和adversary的”随机”输出（按照adversary的判断方式的)有关的】</p><p>当challenger用随机密钥加密m0/m1时，观测adversary的行为是否相同，如果   $\left|\operatorname{Pr}\left[W_{0}\right]-\operatorname{Pr}\left[W_{1}\right]\right|$   等于0，说明adversary无法区分这两个实验，如果  $\left|\operatorname{Pr}\left[W_{0}\right]-\operatorname{Pr}\left[W_{1}\right]\right|$   等于1，则adversary可以区分这两个实验。</p><h3 id="semantic-security"><a href="#semantic-security" class="headerlink" title="semantic security"></a>semantic security</h3><p><u>Def</u>：<strong>semantic security</strong>：A cipher $\mathcal{E}$ is semantially secure if for all efficient $\mathcal{A}$ , the value  $\operatorname{Adv_{SS}}[\mathcal{A}, \mathcal{E}]$  is negligible.</p><p>当所有的有效的攻击算法 $\mathcal{A}$ 对于加密算法 $\mathcal{E}$ 的语意安全优势是可忽略的，则认为加密算法 $\mathcal{E}$ 是语意安全的。</p><p>即没有有效的攻击算法能区分对m0和m1对加密结果。</p><p>即任意密钥加密m0 m1的分布是计算上不可区分的，for all explicit m0, m1 $\in M:$   $\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{0}\right)\right\} \approx_{\mathrm{p}}\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{1}\right)\right\}$ .</p><h3 id="example-1"><a href="#example-1" class="headerlink" title="example 1"></a>example 1</h3><p>Suppose efficient A can always deduce LSB of PT from CT.</p><p>【假设算法A总能从CT中推断出PT的最低有效位】</p><p>在下图cipher系统中，Adversary B算法向challenger发送的m0和m1，其发送的m0 m1特点是LSB(m0)=0,LSB(m1)=1。</p><p><a href="https://imgchr.com/i/NsuJHg"><img src="https://s1.ax1x.com/2020/06/26/NsuJHg.md.png" alt="NsuJHg.md.png"></a> </p><p>当adversary B收到密文c时，利用有效的算法A推断出对应明文的最低有效位LSB，如果是0，则输出0，即认为是实验0中challenger加密的m0，反之输出1。</p><p>adversary B对于cipher $\mathcal{E}$ 的优势是：  $\operatorname{Adv_{SS}}[\mathcal{B}, \mathcal{E}]:=\left|\operatorname{Pr}\left[\text{EXP}(0)=1\right]-\operatorname{Pr}\left[\text{EXP}(1)=1\right]\right|=|0-1|=1$ </p><p> 所以该cipher $\mathcal{E}$ 不具有语意安全。</p><p>不仅是LSB，<strong>如果adversary能从密文中学到明文的任何一位，则该cipher都不具有semantic security。</strong></p><h3 id="example-2-OTP"><a href="#example-2-OTP" class="headerlink" title="example 2: OTP"></a>example 2: OTP</h3><p>OTP虽然不实用，但是具备perfect security，下面来证明OTP也具有semantic security。</p><p><a href="https://imgchr.com/i/NsunNd"><img src="https://s1.ax1x.com/2020/06/26/NsunNd.md.png" alt="NsunNd.md.png"></a> </p><p>OTP的perfect security的性质是  $Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \qquad \text{where} \ k\overset{R}{\longleftarrow}\mathcal{K}$  ，k和任意m异或都是均匀分布，因此上图中的  $\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{0}\right)\right\} =\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{1}\right)\right\}$  是同分布。</p><p>For all A :  $\operatorname{Adv}_{\operatorname{ss}}[A, \text { OTP }]=\mid \operatorname{Pr}\left[A\left(\mathbf{k} \oplus m_{0}\right)=1\right]-\operatorname{Pr}\left[A\left(\mathbf{k} \oplus m_{1}\right)=1\right]|=0$   </p><h2 id="Stream-Ciphers-are-semantically-secure"><a href="#Stream-Ciphers-are-semantically-secure" class="headerlink" title="Stream Ciphers are semantically  secure"></a>Stream Ciphers are semantically  secure</h2><p>前面介绍了什么是安全的PRG，什么是语意安全，而流密码是否具有语意安全呢？</p><p><strong>Thm:  Let G: $k \rightarrow {0,1}^n$ is a secure PRG $\Rightarrow$ stream cipher E derived from G is semantic secure.</strong></p><p> 要证明以上定理：即要证明 $\forall$ sem. sec. adversary A $\operatorname{Adv}_{\operatorname{SS}}[A,E]$  is negligible.</p><p>如果 $\forall$ sem. sec. adversary A, $\exist$ a PRG adversay B s.t.  $A d v_{s s}[A, E] \leq 2 \cdot A d v_{P R G}[B, G]$ 不等式成立，则定理得证。</p><p>因为G is a secure PRG, 根据定义，Adv$<em>\text{PRG}$ [A,G]  is negligible，所以左边$\operatorname{Adv}</em>{\operatorname{SS}}[A,E]$  is negligible.</p><h3 id="Proof-intuition"><a href="#Proof-intuition" class="headerlink" title="Proof: intuition"></a>Proof: intuition</h3><p>如果直观上证明Adv$_\text{PRG}$ [A,G]  is negligible，如下图：</p><p><a href="https://imgchr.com/i/Nsu1jf"><img src="https://s1.ax1x.com/2020/06/26/Nsu1jf.md.png" alt="Nsu1jf.md.png"></a> </p><p>（上左上右下左下右：图1234，G(k):PRG 的输出， r: truly random string）</p><p>图1和图2中，由于G is a secure PRG，因此A计算上无法区分用G(k)和 random对m0加密的结果， 即$\ E(m_0,\mathrm{G}(\mathrm{k}))\} \approx_{\mathrm{p}} \{E(m_0,r)\}$  。同理图三图四中，A计算上也无法区分用G(k)和random 对m1加密的结果，即  $\ E(m_1,\mathrm{G}(\mathrm{k}))\} \approx_{\mathrm{p}} \{E(m_1,r)\}$  。</p><p>而图2图4中，是OPT的，即r和任意m异或都是均匀分布，满足 $\left\{\mathrm{E}\left(\mathrm{r}, \mathrm{m}_{0}\right)\right\} =\left\{\mathrm{E}\left(\mathrm{r}, \mathrm{m}_{1}\right)\right\}$  是同分布.(图中应该可以写严格的等号)</p><p>所以可以直观得到  $\ E(m_0,\mathrm{G}(\mathrm{k}))\} \approx_{\mathrm{p}} E(m_1,\mathrm{G}(\mathrm{k}))\}$ ，即$\forall$ sem. sec. adversary A $\operatorname{Adv}_{\operatorname{SS}}[A,E]$  is negligible.</p><h3 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h3><p>Let A be a sem. sec. adversary.</p><p>使用PRG，即$E(m,k) = m\oplus G(k)$ 时，如下图：</p><p><a href="https://imgchr.com/i/NsuEnO"><img src="https://s1.ax1x.com/2020/06/26/NsuEnO.md.png" alt="NsuEnO.md.png"></a> </p><p> for b = 0,1 Wb = [event that b’=1]</p><p>$\operatorname{Adv}_{\operatorname{SS}}[A,E]=|\text{Pr}[W_0]-\text{Pr}[W_1]|$  </p><hr><p>使用OTP时，即 $E(m,r)=m\oplus r$ 时，如下图：</p><p><a href="https://imgchr.com/i/NsuVBD"><img src="https://s1.ax1x.com/2020/06/26/NsuVBD.md.png" alt="NsuVBD.md.png"></a> </p><p>for b = 0,1 Rb = [event that b’=1]</p><p>$\operatorname{Adv}_{\operatorname{SS}}[A,OTP]=|\text{Pr}[R_0]-\text{Pr}[R_1]|=0$ </p><hr><p>如果把Pr[]的关系画在数轴上，如下图：</p><p><a href="https://imgchr.com/i/NsuZHe"><img src="https://s1.ax1x.com/2020/06/26/NsuZHe.md.png" alt="NsuZHe.md.png"></a> </p><ol><li><p>根据OTP的semantic security ，已经得到 $|\text{Pr}[R_0]-\text{Pr}[R_1]|=\operatorname{Adv}_{\operatorname{SS}}[A,OTP]=0$ ，所以Pr[R0]=Pr[R1]。</p></li><li><p>因为G是secure PRG，所以 $\exists$ adversary B满足 $|\text{Pr}[W_b]-\text{Pr}[R_b]|=\operatorname{Adv}_{\operatorname{PRG}}[B,G]$ ,如下图：</p><p><a href="https://imgchr.com/i/Nsu8u8"><img src="https://s1.ax1x.com/2020/06/26/Nsu8u8.md.png" alt="Nsu8u8.md.png"></a> </p></li></ol><p>所以根据数轴的关系，Pr[W0]和Pr[W1]的距离最大为 $2 \cdot \mathrm{Adv}_{\mathrm{PRG}}[\mathrm{B}, \mathrm{G}]$ .</p><p>即  $\Rightarrow\mathrm{Adv}<em>{\mathrm{SS}}[\mathrm{A}, \mathrm{E}]=\left|\operatorname{Pr}\left[\mathrm{W}</em>{0}\right]-\operatorname{Pr}\left[\mathrm{W}<em>{1}\right]\right| \leq 2 \cdot \mathrm{Adv}</em>{\mathrm{PRG}}[\mathrm{B}, \mathrm{G}]$  </p><p>证毕。</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>secure PRG 就是计算上无法区分G(k)的输出和truly random string。</p><p>semantic secure 就是计算上无法区分m0和m1的加密结果。</p><p>而使用secure PRG的流密码是具有semantic security的。</p><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ol start="0"><li><p>Negligible, super-poly, and poly-bounded functions.(Book p28)</p></li><li><p>Advantage Wiki定义：<a href="https://en.wikipedia.org/wiki/Advantage_(cryptography)">https://en.wikipedia.org/wiki/Advantage_(cryptography)</a></p></li><li><p>Yao’82: Theory and application of trapdoor functions:<a href="https://ieeexplore.ieee.org/document/4568378">https://ieeexplore.ieee.org/document/4568378</a></p></li><li><p>Dan Boneh and Victor Shoup: A Graduate Course in Applied Cryptography</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Stream Cipher的第三篇文章。&lt;/p&gt;
&lt;p&gt;文章主要分为两部分，前部分逐步定义Secure PRG的定义，通过引入statistical test（统计测试）和Advantage（优势）得出当且仅当PRG is unpredictable,PRG is secure的结论。&lt;/p&gt;
&lt;p&gt;后部分介绍了密码学中的一个重要概念Semantic Security的定义，通过引入 computationally indistinguishable(计算上不可区分)的概念给出定义，并证明了OTP的语意安全和在安全PRG条件下的流密码的语意安全，得出如果流密码中使用的PRG is secure,那么流密码就具备semantic security。&lt;/p&gt;
&lt;p&gt;文章开头，也简单介绍了密码学中negligible和non-negligible的含义。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cryptography-Dan" scheme="https://f1ed.github.io/categories/Cryptography-Dan/"/>
    
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="Cryptography" scheme="https://f1ed.github.io/tags/Cryptography/"/>
    
      <category term="StreamCipher" scheme="https://f1ed.github.io/tags/StreamCipher/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Recurrent Neural Network（RNN）</title>
    <link href="https://f1ed.github.io/2020/06/11/rnn/"/>
    <id>https://f1ed.github.io/2020/06/11/rnn/</id>
    <published>2020-06-10T16:00:00.000Z</published>
    <updated>2020-07-03T08:43:33.524Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中首先从RNN能解决什么问题入手，分析了RNN与一般NN的区别。<br>然后着重讲解了基于RNN的LSTM模型，包括LSTM的细节、和一般NN的区别，以及如何训练LSTM模型。<br>具体阐述了在模型（RNN类模型）训练过程中为什么会遇到剧烈抖动问题和如何解决抖动的工程解决方法。</p><a id="more"></a><h1 id="Example-application"><a href="#Example-application" class="headerlink" title="Example application"></a>Example application</h1><h2 id="Solt-filling"><a href="#Solt-filling" class="headerlink" title="Solt filling"></a>Solt filling</h2><p>先从RNN的应用说起，RNN能做什么？</p><p>RNN可以做智慧系统：</p><p>如下图中，用户告诉订票系统：”I would like to arrive Taipei on November 2nd”.</p><p>订票系统能从这句话中得到Destination: Taipei，time of arrival: November 2nd.</p><p><a href="https://imgchr.com/i/tqZaQJ"><img src="https://s1.ax1x.com/2020/06/11/tqZaQJ.md.png" alt="tqZaQJ.md.png"></a> </p><p>这个过程也就是<strong>Solt Filling</strong> （槽位填充）。</p><p>如果用Feedforward network来解决solt filling问题，输入就是单词，输出是每个槽位（slot）的单词，如下图。</p><p><a href="https://imgchr.com/i/tqZ8oV"><img src="https://s1.ax1x.com/2020/06/11/tqZ8oV.md.png" alt="tqZ8oV.md.png"></a></p><p>上图中，如何将word表示为一个vector？</p><h2 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h2><p>How to represent each word as a vector?</p><h3 id="1-of-N-encoding"><a href="#1-of-N-encoding" class="headerlink" title="1-of-N encoding"></a>1-of-N encoding</h3><p>最简单的方式是1-of-N encoding方式（独热方式）。</p><p>向量维度大小是整个词汇表的大小，每一个维度代表词汇表中的一个单词，如果该维度置1，表示这个维度代表的单词。</p><h3 id="Beyond-1-of-N-encoding"><a href="#Beyond-1-of-N-encoding" class="headerlink" title="Beyond 1-of-N encoding"></a>Beyond 1-of-N encoding</h3><p>对1-of-N encoding方式改进。</p><p>第一种：<u>Dimension for “Other”</u> </p><p><a href="https://imgchr.com/i/tqZNz4"><img src="https://s1.ax1x.com/2020/06/11/tqZNz4.md.png" alt="tqZNz4.md.png"></a> </p><p>在1-of-N的基础上增加一维度——‘other’维度，即当单词不在系统词汇表中，将other维度置1代表该单词。</p><p>第二种：<u>Word hashing</u> </p><p><a href="https://imgchr.com/i/tqZtWF"><img src="https://s1.ax1x.com/2020/06/11/tqZtWF.md.png" alt="tqZtWF.md.png"></a> </p><p>即便是增加了”other”维度，编码vector的维度也很大，用word hashing的方式将大幅减少维度。</p><p>以apple为例，拆成app, ppl, ple三个部分，如上图所示，vector中表示这三个部分的维度置1。</p><p>用这样的word hashing方式，vector的维度只有 $26\times 26\times26$ ，大幅减少词向量的维度。</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>通过encoding的方式，单词用vector来表示，用前馈神经网络来解决solt filling问题。</p><p>如下图.</p><p>input:一个单词（encoding为vector）</p><p>output: input单词中属于该槽位(solts)的概率分布(vector)。</p><p><a href="https://imgchr.com/i/tqZ8oV"><img src="https://s1.ax1x.com/2020/06/11/tqZ8oV.md.png" alt="tqZ8oV.md.png"></a> </p><hr><p>但用普通的前馈神经网络处理solt filling问题会出现下图问题：</p><p><a href="https://imgchr.com/i/tqZJiT"><img src="https://s1.ax1x.com/2020/06/11/tqZJiT.md.png" alt="tqZJiT.md.png"></a> </p><p>上图中，arrive Taipei on November 2nd 和 leave Taipei on November 2nd，将这两句话的每个单词（vector）放入前馈神经网络，得出的dest槽位都应该是Taipei。</p><p>但，通过之前的语意，arrive Taipei的Taipei应该是终点，而leave Taipei的Taipei是起点。</p><p>因此，在处理这种问题时，我们的神经网络应该需要memory，对该输入的上下文有一定的记忆存储。</p><h1 id="Recurrent-Neural-Network-RNN"><a href="#Recurrent-Neural-Network-RNN" class="headerlink" title="Recurrent Neural Network(RNN)"></a>Recurrent Neural Network(RNN)</h1><h2 id="Basic-structure"><a href="#Basic-structure" class="headerlink" title="Basic structure"></a>Basic structure</h2><p>因此，我们对一般的前馈神经网络加入记忆元件a, a 存储hidden layer的输出，同时a也作为下一次计算的输入部分,下图就是最基础的RNN模型。</p><p><a href="https://imgchr.com/i/tqZYJU"><img src="https://s1.ax1x.com/2020/06/11/tqZYJU.md.png" alt="tqZYJU.md.png"></a> </p><p>举一个例子来说明该过程：</p><p>Input sequence: $\begin{bmatrix}1 \ 1 \end{bmatrix}$  $\begin{bmatrix}1 \ 1 \end{bmatrix}$  $\begin{bmatrix}2 \ 2 \end{bmatrix}$ …</p><p>RNN模型如下图所示：所有的weight都是1，没有bias; 所有的神经元的activation function 都是线性的。</p><ol><li><p>input : $\begin{bmatrix}1 \ 1 \end{bmatrix}$, 记忆元件初值 a1=0 a2=0.</p><p><a href="https://imgchr.com/i/tqZ3d0"><img src="https://s1.ax1x.com/2020/06/11/tqZ3d0.md.png" alt="tqZ3d0.md.png"></a> </p><p>记忆元件也作为输入的一部分，hidden layer的输出为 2 2, 更新记忆元件的值.</p><p>output: $\begin{bmatrix}4 \ 4 \end{bmatrix}$ , 记忆元件存储值 a1=2 a2=2.</p></li><li><p>input : $\begin{bmatrix}1 \ 1 \end{bmatrix}$  , 记忆元件存储值 a1=2 a2=2.</p><p><a href="https://sbimg.cn/image/0000D"><img src="https://wx2.sbimg.cn/2020/06/11/2020-06-11-8.42.06.md.png" alt="2020-06-11-8.42.06.md.png"></a> </p></li></ol><p>   记忆元件也作为输入的一部分，hidden layer 的输出为6 6,更新记忆元件的值。</p><p>   output: $\begin{bmatrix}12 \ 12 \end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6.</p><p>   这里可以发现，第一次和第二次的输入相同，但是由于有记忆元件的缘故，两次输出不同。</p><ol start="3"><li><p>input : $\begin{bmatrix}2 \ 2 \end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6.</p><p><a href="https://imgchr.com/i/tqZQLn"><img src="https://s1.ax1x.com/2020/06/11/tqZQLn.md.png" alt="tqZQLn.md.png"></a> </p><p>记忆元件也作为输入的一部分，hidden layer 的输出为16 16,更新记忆元件的值。</p><p>output: $\begin{bmatrix}32 \ 32 \end{bmatrix}$ , 记忆元件存储值 a1=16 a2=16.</p></li></ol><p>RNN中，由于有memory，会和一般前馈模型有两个不同的地方：一是输入相同的vector，输出可能是不同的；二是将一个sequence连续放进RNN模型中，如果sequence中改变顺序，输出也大多不同。</p><hr><p>用这个RNN模型来解决之前的solt filling问题，就可以解决上下文语意不同影响solt的问题。</p><p>将arrive Taipei on November 2nd的每个单词都放入同样的模型中。</p><p><a href="https://imgchr.com/i/tqZAdP"><img src="https://s1.ax1x.com/2020/06/11/tqZAdP.md.png" alt="tqZAdP.md.png"></a> </p><p>因此将RNN展开，如上图，像不同时间点的模型，但其实是不同时间点循环使用同一个模型。</p><p><a href="https://imgchr.com/i/tqZKMj"><img src="https://s1.ax1x.com/2020/06/11/tqZKMj.md.png" alt="tqZKMj.md.png"></a> </p><p>由于左边的前文是arrive，右边的前文是leave，所以存储在memory中的值不同，Taipei作为input的输出（槽位的概率分布）也不同。</p><h2 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h2><p>上文中只是RNN模型中的一种，即Elman Network，记忆元件存储的是上一个时间点hidden layer的输出。</p><p>而Jordan Network模型中,他的记忆元件存储的是上一时间点的output。</p><p>（据说，记忆元件中存储output的值会有较好的performance，因为output是有target vector的，因此能具象的体现放进memory的是什么）</p><p><a href="https://imgchr.com/i/tqZEIf"><img src="https://s1.ax1x.com/2020/06/11/tqZEIf.md.png" alt="tqZEIf.md.png"></a> </p><h2 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h2><p>上文中的RNN模型，记忆元件中存储的都是上文的信息，如果要同时考虑上下文信息，即是bidirectional RNN(双向RNN)。</p><p>模型如下图。</p><p><a href="https://sbimg.cn/image/002sN"><img src="https://wx1.sbimg.cn/2020/06/11/2020-06-11-8.45.55.md.png" alt="2020-06-11-8.45.55.md.png"></a> </p><p>双向RNN的好处是看的范围比较广，当计算输出 $y^t$ 时，上下文的内容都有考虑到。</p><h2 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory(LSTM)"></a>Long Short-term Memory(LSTM)</h2><p>现在最常用的RNN模型是LSTM，Long Short-term Memory，这里的long是相当于上文中的RNN模型，因为上文提到的RNN模型都是short-term,即每一个时间点，都会把memory中的值洗掉，LSTM的long，就是会把memory的值保留的相对于久一些。</p><p>LSTM如下图，与一般NN不同的地方是，他有4个inputs,一个outputs。</p><p><a href="https://imgchr.com/i/tqZmRg"><img src="https://s1.ax1x.com/2020/06/11/tqZmRg.md.png" alt="tqZmRg.md.png"></a> </p><p>LSTM主要有四部分组成：</p><ul><li>Input Gate：输入门，下方箭头是输入，左方箭头是输入信号控制输入门的打开程度，完全打开LSTM才能将输入值完全读入，打开的程度也是NN自己学。</li><li>Output Gate：输出门，上方箭头是输出，左方箭头是输入信号控制输出门的打开程度，同理，打开程度也是NN自己学习。</li><li>Memory Cell：记忆元件。</li><li>Forget Gate：遗忘门，右边的箭头是输入信号控制遗忘门的打开程度，控制将memory cell洗掉的程度。</li></ul><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>更详细的阐述LSTM的内部机制：</p><p>注意：</p><ul><li>$z_o,z_i,z_f$ 是门的signal control,其实就等同于一般NN中neuron的输入z，是scalar。</li><li>gate其实就是一个neuron，通常gate neuron 的activation function f取 sigmod,因为值域在0到1之间，即对应门的打开程度。</li><li>input/forget/output gate的neuron的activation function是f(sigmod function), input neuron的activation function是g。</li></ul><p><a href="https://imgchr.com/i/tqZZi8"><img src="https://s1.ax1x.com/2020/06/11/tqZZi8.md.png" alt="tqZZi8.md.png"></a> </p><ol><li>input gate控制输入:$g(z)f(z_i)$ <ul><li>input:  z $\rightarrow$  $g(z)$ </li><li>input gate signal control:  $z_i \rightarrow f(z_i)$ </li><li>multiply：$g(z)f(z_i)$ </li></ul></li><li>forget gate 控制memory：$cf(z_f)$ <ul><li>forget gate signal control: $z_f\rightarrow f(z_f)$ </li><li>如果 $f(z_f)=1$ ,说明memory里的值保留；如果 $f(z_f)=0$ ,说明memory里的值洗掉。</li></ul></li><li>更新当前时间点的memory(输入+旧的memory值) ：$c’=g(z)f(z_i)+cf(z_f)$ </li><li>output gate 控制输出：$h(c’)f(z_o)$ <ul><li>output: $c’ \rightarrow h(c’)$ </li><li>output gare signal control:  $z_o \rightarrow f(z_o)$ </li><li>multiply: $h(c’)f(z_o)$ </li></ul></li></ol><hr><p>LSTM模型（trained）如下图：</p><p>输入序列为 $\begin{bmatrix}3 \ 1 \ 0 \end{bmatrix}$$\begin{bmatrix}4 \ 1 \ 0 \end{bmatrix}$ $\begin{bmatrix}2 \ 0 \ 0 \end{bmatrix}$ $\begin{bmatrix}1 \ 0 \ 1 \end{bmatrix}$ $\begin{bmatrix}3 \ -1 \ 0 \end{bmatrix}$ </p><p><a href="https://imgchr.com/i/tqZkZt"><img src="https://s1.ax1x.com/2020/06/11/tqZkZt.md.png" alt="tqZkZt.md.png"></a> </p><p>该LSTM activation function: g、h都为linear function（即输出等于输入），f为sigmod.</p><p>通过该LSTM的输出序列为： 0 0 0 7 0 0</p><p>（建议手算一遍）</p><h2 id="Compared-with-Original-Network"><a href="#Compared-with-Original-Network" class="headerlink" title="Compared with Original Network"></a>Compared with Original Network</h2><p>original network如下图：</p><p><a href="https://imgchr.com/i/tqVXa6"><img src="https://s1.ax1x.com/2020/06/11/tqVXa6.md.png" alt="tqVXa6.md.png"></a> </p><p>LSTM 的NN即用LSTM替换原来的neuron，这个neuron有四个inputs，相对于original network也有4倍的参数，如下图：</p><p><a href="https://imgchr.com/i/tqZSRe"><img src="https://s1.ax1x.com/2020/06/11/tqZSRe.md.png" alt="tqZSRe.md.png"></a> </p><hr><p>所以原来RNN的neuron换为LSTM，就是下图：</p><p><a href="https://imgchr.com/i/tqVjIK"><img src="https://s1.ax1x.com/2020/06/11/tqVjIK.md.png" alt="tqVjIK.md.png"></a> </p><p>上图中：</p><p>这里的 $z^f,z^u,z,z^o$ 都是 $x^t \begin{bmatrix} \quad\end{bmatrix}$ 矩阵运算得到的vector, 因为上图中有多个LSTM，因此 $z^i$ 的第k个元素，就是控制第k个LSTM的input signal control scalar。所以，$z^f,z^u,z,z^o$ 的维度等于下一层neuron/LSTM的个数。</p><p>所以这里memory（cell）$c^t$ 也是一个vector，第k个元素是第k个LSTM中cell存储的值。</p><p>向量运算和scalar一样，LSTM细节如下图：</p><p><a href="https://imgchr.com/i/tqVxPO"><img src="https://s1.ax1x.com/2020/06/11/tqVxPO.md.png" alt="tqVxPO.md.png"></a> </p><h2 id="Extension：“peephole”"><a href="#Extension：“peephole”" class="headerlink" title="Extension：“peephole”"></a>Extension：“peephole”</h2><p>上小节的LSTM是simplified，将LSTM hidden layer的输出 $h^t$ 和cell中存储的值 $c^t$  和下一时间点的输入 $x^{t+1}$ 一同作为下一时间点的输入，就是LSTM的扩展版”peephole”。</p><p>如下图：</p><p><a href="https://imgchr.com/i/tqVqq1"><img src="https://s1.ax1x.com/2020/06/11/tqVqq1.md.png" alt="tqVqq1.md.png"></a> </p><h2 id="Multi-layer-LSTM"><a href="#Multi-layer-LSTM" class="headerlink" title="Multi-layer LSTM"></a>Multi-layer LSTM</h2><p>多层的peephole LSTM如下图：</p><p><img src="https://i.loli.net/2020/06/11/sD46OVQpxokjiw8.png" alt="1截屏2020-04-19 下午4.41.50.png"> </p><p>（：wtf 我到底看到了什么</p><p>不要怕：Keras PyTorch等套件都有 “LSTM”，“GUR，”SimpleRNN“ 已实现好的layers.</p><h2 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h2><p>训练RNN时，输入与target如下所示：</p><p><img src="https://i.loli.net/2020/06/11/AK1kSPltUDaqVmR.png" alt="截屏2020-06-11 下午8.52.09.png"> </p><p>估测模型的好坏，计算RNN的Loss时，需要看作一个整体，计算每个时间点RNN输出与target的crossentropy的和。</p><p>训练也可同样用Backpropagation，但考虑到时间点，有一个进阶版的”Backpropogation through time(BPTT)”[1]。</p><p>RNN一般就用BPTT训练。</p><h2 id="How-to-train-well"><a href="#How-to-train-well" class="headerlink" title="How to train well"></a>How to train well</h2><h3 id="not-easy-to-train"><a href="#not-easy-to-train" class="headerlink" title="not easy to train"></a>not easy to train</h3><p>RNN-based network is not always easy to learn.</p><p>但基于RNN的模型往往不太好训练，总是会出现下图中的绿色线情况（即抖动）。</p><p><a href="https://imgchr.com/i/tqVbrR"><img src="https://s1.ax1x.com/2020/06/11/tqVbrR.md.png" alt="tqVbrR.md.png"></a> </p><h3 id="error-surface-is-rough"><a href="#error-surface-is-rough" class="headerlink" title="error surface is rough"></a>error surface is rough</h3><p>error surface，即total loss在参数变化时的函数图。</p><p>会发现基于RNN的模型的error surface会长下图这个样子：有时很平坦(flat)有时很陡峭(steep)</p><p><a href="https://imgchr.com/i/tqZiqI"><img src="https://s1.ax1x.com/2020/06/11/tqZiqI.md.png" alt="tqZiqI.md.png"></a> </p><ul><li><p>橙色点出发：</p><ul><li>起初处在flat的位置。</li><li>随着一次次更新，gradient在变小，learning rate即会变大。</li><li>可能稍微不幸，就会出现跨过悬崖，即出现了剧烈震荡的问题。</li><li>如果刚好当前处在悬崖低，这时的gradient很大，learning rate也很大，step就会很大，飞出去，极可能出现segment fault(NaN).</li></ul></li><li><p>Thomas Mikolv 用工程师的角度来解决这个问题，即当此时的gradient大于某个阈值(threshold)时，就不要让当前的gradient超过这个阈值（通常取15）。</p></li><li><p>这样处在悬崖低的橙色点，（Clipping路线），更新就会到绿色的，继续更新。</p></li></ul><h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><p>为什么RNN模型会出现抖动的情况呢？</p><p>用下图这个简单例子说明（一般activation function用sigmod,而ReLu的performance一般较差）：</p><p><a href="https://imgchr.com/i/tqVh5T"><img src="https://s1.ax1x.com/2020/06/11/tqVh5T.md.png" alt="tqVh5T.md.png"></a> </p><p>上图中，输入序列是1 0 0 0 …，memory连接下一个时间点的权重是w，可以轻易得到最后一个时间点的输出 $y^{1000}=w^{999}$ 。</p><p>上图中，循环输出1000次，如果w变化 $\Delta w$  ，看输出 $y^{1000}$ 的变化，来直观体现gradient 的变化：</p><p><a href="https://imgchr.com/i/tqV5PU"><img src="https://s1.ax1x.com/2020/06/11/tqV5PU.md.png" alt="tqV5PU.md.png"></a> </p><p>上图中，可以看出：</p><ul><li>绿色部分：当w从1变化为1.01时， $y^{1000}$ 的输出变化即大，既有较大的gradient，理应有小的learning rate。</li><li>黄色部分：当w从0.99变化为0.01时， $y^{1000}$ 的输出几乎不变化，即有较小的gradient，理应有大大learning rate.</li><li>在很小的地方（0.01 到 1.01），他的gradient就变化即大，即抖动的出现。</li></ul><p><strong>Reason</strong>：RNN，虽然可以看作不同时间点的展开计算，但始终是同一个NN的权重计算（cell连接到下一个时间点的权重），在不同时间中，反复叠乘，因此会出现这种情况。</p><h3 id="Helpful-Techniques"><a href="#Helpful-Techniques" class="headerlink" title="Helpful Techniques"></a>Helpful Techniques</h3><ol><li>LSTM几乎已经算RNN的一个标准了，为什么LSTM的performance比较好呢。</li></ol><ul><li><p>为什么用LSTM替换为RNN？</p><p>:Can deal with gradient vanishing(not gradient explode).</p><p>可以解决gradient vanish的问题（gradient vanish problem 具体见 <a href="/2020/04/21/tips-for-DL/" title="这篇文章2.1.1">这篇文章2.1.1</a>）</p></li><li><p>为什么LSTM可以解决gradient vanish问题</p><p>：memory and input are added.（LSTM的的输出与输入和memory有关）</p><p>: The influence never disappears unless forget gate is closed.（memory的影响可以很持久）</p></li></ul><ol start="2"><li><p>GRU[2]（Gated Recurrent Unit）：是只有两个Gate，比LSTM简单，参数更少，不容易overfitting</p></li><li><p>玄学了叭</p><p><a href="https://imgchr.com/i/tqVHM9"><img src="https://s1.ax1x.com/2020/06/11/tqVHM9.md.png" alt="tqVHM9.md.png"></a> </p></li></ol><h1 id="More-Applications"><a href="#More-Applications" class="headerlink" title="More Applications"></a>More Applications</h1><p>【待更新】</p><h2 id="Many-to-One"><a href="#Many-to-One" class="headerlink" title="Many to One"></a>Many to One</h2><h2 id="Many-to-Many"><a href="#Many-to-Many" class="headerlink" title="Many to Many"></a>Many to Many</h2><h2 id="Beyond-Sequence"><a href="#Beyond-Sequence" class="headerlink" title="Beyond Sequence"></a>Beyond Sequence</h2><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><h3 id="Auto-encoder-Text"><a href="#Auto-encoder-Text" class="headerlink" title="Auto-encoder-Text"></a>Auto-encoder-Text</h3><h3 id="Auto-encoder-Speech"><a href="#Auto-encoder-Speech" class="headerlink" title="Auto-encoder-Speech"></a>Auto-encoder-Speech</h3><h2 id="Chat-bot"><a href="#Chat-bot" class="headerlink" title="Chat-bot"></a>Chat-bot</h2><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>BPTT</li><li>GRU</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中首先从RNN能解决什么问题入手，分析了RNN与一般NN的区别。&lt;br&gt;然后着重讲解了基于RNN的LSTM模型，包括LSTM的细节、和一般NN的区别，以及如何训练LSTM模型。&lt;br&gt;具体阐述了在模型（RNN类模型）训练过程中为什么会遇到剧烈抖动问题和如何解决抖动的工程解决方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f1ed.github.io/tags/DeepLearning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="RNN" scheme="https://f1ed.github.io/tags/RNN/"/>
    
      <category term="LSTM" scheme="https://f1ed.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Convolution Neural Network（CNN）</title>
    <link href="https://f1ed.github.io/2020/04/25/CNN/"/>
    <id>https://f1ed.github.io/2020/04/25/CNN/</id>
    <published>2020-04-24T16:00:00.000Z</published>
    <updated>2020-07-03T08:40:42.768Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中首先介绍了为什么要用CNN做图像识别，或者说图像识别问题的特点是什么？<br>文章中也详细介绍了CNN的具体架构，主要包括Convolution、Max Pooling、Flatten。<br>文章最后简要介绍了CNN在诸多领域的应用。</p><a id="more"></a><h1 id="Why-CNN-for-Image"><a href="#Why-CNN-for-Image" class="headerlink" title="Why CNN for Image?"></a>Why CNN for Image?</h1><p>图片本质都是pixels。</p><p>在做图像识别时，本质是对图片中的某些特征像素（properities)识别。</p><p><strong>So Why CNN for image?</strong></p><ol><li><p>Some patterns are much smaller than the whole image.</p><p>A neuron does <strong>not have to see the whole image</strong> to discover the pattern.</p><p>Connecting to small region with <strong>less parameters.</strong></p><p>【很多特征图案的大小远小于整张图片的大小，因此一个neuron不需要为了识别某个pattern而看完整张图片。并且，如果只识别某个小的region，会减少大量参数的数目。】</p><p>如下图，用一个neuron识别红框中的beak，即能大概率认为图片中有bird。</p><p><a href="https://imgchr.com/i/JyitKJ"><img src="https://s1.ax1x.com/2020/04/25/JyitKJ.md.png" alt="JyitKJ.md.png"></a> </p></li><li><p>The same patterns appear in different regions. They can <strong>use the same set of parameters.</strong></p><p>【同样的pattern可能出现在图片的不同位置。pattern几乎相同，因此可以用同一组参数。】</p><p>如下图，两个neuron识别两个不同位置的beak。被识别的beak几乎无差别，因此neuron的参数可以是相同的。</p><p><a href="https://imgchr.com/i/JyiNr9"><img src="https://s1.ax1x.com/2020/04/25/JyiNr9.md.png" alt="JyiNr9.md.png"></a> </p></li><li><p><strong>Subsampling</strong> the pixels will not change the object.</p><p>【一张图片是由许多pixel组成的，如下图，如果去掉图片的所有奇数行偶数列的pixel，图片内容几乎无差别。并且，Subsample pixels，即减少了输入的size，也可以减少NN的参数数量。】</p><p><a href="https://imgchr.com/i/JyiJv4"><img src="https://s1.ax1x.com/2020/04/25/JyiJv4.md.png" alt="JyiJv4.md.png"></a> </p></li></ol><h1 id="The-whole-CNN"><a href="#The-whole-CNN" class="headerlink" title="The whole CNN"></a>The whole CNN</h1><p>CNN的架构如下图。</p><p><a href="https://imgchr.com/i/JyiG2F"><img src="https://s1.ax1x.com/2020/04/25/JyiG2F.md.png" alt="JyiG2F.md.png"></a> </p><p>一张图片经过多次Convolution、Max Pooling得到新的image，再将新的image Flatten（拉直）得到一组提取好的features，将这组features放入前馈神经网络。</p><p>Convolution满足图片识别的：</p><ul><li>Property 1 : Some patterns are much smaller than the whole image.</li><li>Property 2 : The same patterns appear in different regions.</li></ul><p>Max Pooling满足图片识别的：</p><ul><li>Property 3 : Subsamplingthe pixels will not change the object.</li></ul><h2 id="CNN-Convolution"><a href="#CNN-Convolution" class="headerlink" title="CNN-Convolution"></a>CNN-Convolution</h2><p>一张简单的黑白图片如下图，0为白色，1为黑色。</p><p><a href="https://imgchr.com/i/Jyi88U"><img src="https://s1.ax1x.com/2020/04/25/Jyi88U.md.png" alt="Jyi88U.md.png"></a> </p><p>如果图片是彩色的，即用RGB三原色来表示，用三个matrix分别表示R、G、B的值，如下图：</p><p><a href="https://imgchr.com/i/JyiMEq"><img src="https://s1.ax1x.com/2020/04/25/JyiMEq.md.png" alt="JyiMEq.md.png"></a> </p><p>下文中，以黑白图举例。</p><h3 id="Property-1"><a href="#Property-1" class="headerlink" title="Property 1"></a>Property 1</h3><p>设计Filer matrix满足Property 1，如下图：</p><p><a href="https://imgchr.com/i/Jyi3CT"><img src="https://s1.ax1x.com/2020/04/25/Jyi3CT.png" alt="Jyi3CT.png"></a> </p><p>上图中，filter的大小是3*3，可以检测到小区域的某个pattern。</p><p>每个filter的参数都是NN中的参数，需要learned。</p><p>如果是彩色图片，filter应该是3张3*3matrix组成的，分别代表R、G、B的filter。</p><h3 id="Property-2"><a href="#Property-2" class="headerlink" title="Property 2"></a>Property 2</h3><p>为了满足Property 2，filter可以在图片中移动。设置stride，即每次filter移动的步长。</p><p>filter与覆盖图片的位置做内积，需要走完整张图片，最后得到一张feature map。</p><p>下图为stride=1的convolution结果：</p><p><a href="https://imgchr.com/i/Jyil5V"><img src="https://s1.ax1x.com/2020/04/25/Jyil5V.md.png" alt="Jyil5V.md.png"></a> </p><p>Convolution layer（卷积层）有几个filter，就会得到几张feature maps。</p><h3 id="Convolution-v-s-Fully-Connected"><a href="#Convolution-v-s-Fully-Connected" class="headerlink" title="Convolution v.s. Fully Connected"></a>Convolution v.s. Fully Connected</h3><p>*<em>Fully Connected： *</em> </p><p>如果用全连接的方式做图片识别，图片的每一个pixel都要都第一层layer的每个neuron，则需要大量参数连接。</p><p>如下图：</p><p><a href="https://imgchr.com/i/JyinDs"><img src="https://s1.ax1x.com/2020/04/25/JyinDs.md.png" alt="JyinDs.md.png"></a> </p><hr><p>*<em>Convolution： *</em> </p><p>而在Convolution中，把feature map中的每一个值作为neuron的输出，对于一个3*3的filter，一个neuron的连接如下：</p><p><a href="https://imgchr.com/i/Jyiubn"><img src="https://s1.ax1x.com/2020/04/25/Jyiubn.md.png" alt="Jyiubn.md.png"></a> </p><p>filter中的值是连接参数，则每一个neuron只需要与3*3个input连接，与全连接相比减少了大量参数。</p><p><strong>shared weights</strong> </p><p>filter在图中移动时，filter的参数不变，即第二个neuron的连接参数和第一个neuron的连接参数是相同的，连接图如下：</p><p><a href="https://imgchr.com/i/Jyimuj"><img src="https://s1.ax1x.com/2020/04/25/Jyimuj.md.png" alt="Jyimuj.md.png"></a> </p><p>通过filter实现了shared weights（参数共享），更大幅度减少了参数数量。</p><h2 id="CNN-Max-Pooling"><a href="#CNN-Max-Pooling" class="headerlink" title="CNN-Max Pooling"></a>CNN-Max Pooling</h2><p>Max Pooling：将convolution layer的neuron作为输入，neuron的activation function其实就是Maxout（Maxout介绍见 <a href="/2020/04/21/tips-for-DL/" title="这篇">这篇</a> 2.1.4的介绍）。</p><p>将convolution layer得到的feature map做Max pooling（池化），即取下图中每个框中的最大值。</p><p><a href="https://imgchr.com/i/JyiZvQ"><img src="https://s1.ax1x.com/2020/04/25/JyiZvQ.md.png" alt="JyiZvQ.md.png"></a> </p><p>如下图，6*6的image经过Convolution layer 和 Max Pooling layer后，得到了new but smaller image，新的image的由两层channel组成，每层channel都是2 * 2的image。</p><p><a href="https://imgchr.com/i/JyiAC8"><img src="https://s1.ax1x.com/2020/04/25/JyiAC8.md.png" alt="JyiAC8.md.png"></a> </p><p>一个image每经过一次Convolution layer 和 Max Pooling layer，都会得到a new image。</p><p>This new image is smaller than the origin image. And the number of channel (of the new image) is the number of filters.</p><p>举个例子：</p><p>Convolution layer有25个filters，再经过Max Pooling，得到的新的image有25 个channel。</p><p>再重复一次Convolution 和Max Pooling，新的Convolution layer也有25个filters，再经过Max Pooling，得到的新的image有多少个channel呢？</p><p>答案是25个channel。</p><p><strong>注意</strong> ：在第二次Convolution中，image有depth，depth=25。因此在convolution中，filter其实是一个cubic，也有depth，depth=image-depth=25，再做内积。</p><p>因此，新的image的channel数是等于filter数的。</p><h2 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h2><p>Flatten很好理解，将最后得到的新的image 拉直（Flatten）为一个vector。</p><p><a href="https://imgchr.com/i/JyiE8S"><img src="https://s1.ax1x.com/2020/04/25/JyiE8S.md.png" alt="JyiE8S.md.png"></a> </p><p>拉直后的vector是一组提取好的features，作为 前馈神经网络的输入。</p><h1 id="What-dose-CNN-learn"><a href="#What-dose-CNN-learn" class="headerlink" title="What dose CNN learn"></a>What dose CNN learn</h1><p>为什么CNN能够学习pattern，最终达到识别图像的目的？</p><h2 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h2><p>在下图CNN过程中，我们先分析能从Convolution layer的filter能够学到什么？</p><p><a href="https://imgchr.com/i/JyiF4f"><img src="https://s1.ax1x.com/2020/04/25/JyiF4f.md.png" alt="JyiF4f.md.png"></a> </p><p>每个filter本质上是一组shared weights 的neuron。</p><p>因此，定义这组filter的激活程度，即：</p><p> Degree of the activation of the k-th filter: $a^k=\sum_{i=1}^{11}\sum_{j=1}^{11}a_{ij}^{k}$ .</p><p>目标是找到使k-th filter激活程度最大的输入image，即</p><p>$x^{*}=\arg \max _{x} a^{k}$ ，(method :gradient descent).</p><p>部分结果如下图：</p><p><a href="https://imgchr.com/i/JyiVgg"><img src="https://s1.ax1x.com/2020/04/25/JyiVgg.md.png" alt="JyiVgg.md.png"></a> </p><p>(每一张图都代表一个filter)</p><p>上图中，找到使filter激活程度最大的image，即上图中每个filter可以检测一定的条纹，只有当图像中有该条纹，filter（一组neuron）的激活程度（即输出）才能达到最大。</p><h2 id="Neuron（Hidden-layer）"><a href="#Neuron（Hidden-layer）" class="headerlink" title="Neuron（Hidden layer）"></a>Neuron（Hidden layer）</h2><p>这里的neuron指前馈神经网络中的neuron，如下图的 $a_j$ :</p><p><a href="https://imgchr.com/i/JyiiUP"><img src="https://s1.ax1x.com/2020/04/25/JyiiUP.png" alt="JyiiUP.png"></a> </p><p>目标：找到使neuron的输出最大的输入image，即：</p><p>$x^{*}=\arg \max _{x} a^{j}$ .</p><p>部分结果如下：</p><p><a href="https://imgchr.com/i/JykouQ"><img src="https://s1.ax1x.com/2020/04/25/JykouQ.md.png" alt="JykouQ.md.png"></a> </p><p>（每一张图代表一个neuron)</p><p>在上图中，感觉输入像一个什么东西吧emmmm。</p><p>但和filter学到的相比，neuron学到的不仅是图中的小小的pattern（比如条纹、鸟喙等），neuron学的是看整张图像什么。</p><h2 id="Output（Output-layer）"><a href="#Output（Output-layer）" class="headerlink" title="Output（Output layer）"></a>Output（Output layer）</h2><p>再用同样的方法，看看输出层的neuron学到了什么，如下图的 $y_i$  ：</p><p><a href="https://imgchr.com/i/Jyk5jg"><img src="https://s1.ax1x.com/2020/04/25/Jyk5jg.png" alt="Jyk5jg.png"></a> </p><p>在手写数字辨识中 $y_i$ 是数字为 $i$ 的概率，因此目标是：找到一个使输出是数字 $i$ 概率最大的输入image，即：</p><p>$x^{*}=\arg \max _{x} y^{i}$ .</p><p>结果如下图：</p><p><a href="https://imgchr.com/i/JyiSud"><img src="https://s1.ax1x.com/2020/04/25/JyiSud.md.png" alt="JyiSud.md.png"></a>  </p><p>结果和我们期望相差甚远，根本不能辨别以上图片是某个数字。</p><p>这其实也是DNN的一个特点: Deep Neural Networks are Easily Fooled [1]，即NN学到的东西往往和人类学到的东西是不一样的。</p><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>所以CNN到底学到了什么？</p><p>上文中，output 学到的都是一团密密麻麻杂乱的像素点，根本不像数字。</p><p>但是，再考虑手写数字image的特点：图片中应该有少量模式，大片空白部分。</p><p>因此目标改进为：  $x^{*}=\arg \max <em>{x}\left(y^{i}+\sum</em>{i, j}\left|x_{i j}\right|\right)$ </p><p>结果如下：</p><p><a href="https://imgchr.com/i/Jyi9HI"><img src="https://s1.ax1x.com/2020/04/25/Jyi9HI.md.png" alt="Jyi9HI.md.png"></a> </p><p>（注：图中白色为墨水，黑色为空白）</p><h1 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h1><h2 id="Deep-Dream"><a href="#Deep-Dream" class="headerlink" title="Deep Dream"></a>Deep Dream</h2><p>CNN exaggerates what it sees.</p><p>CNN可以夸大图片中他所看到的东西。</p><p>比如：</p><p>可以把下图</p><p><a href="https://imgchr.com/i/JyiPEt"><img src="https://s1.ax1x.com/2020/04/25/JyiPEt.md.png" alt="JyiPEt.md.png"></a> </p><p>变成下图（emmmm看着有点难受）</p><p><a href="https://imgchr.com/i/JyPxjH"><img src="https://s1.ax1x.com/2020/04/25/JyPxjH.md.png" alt="JyPxjH.md.png"></a> </p><p>附上生成deep dream image的网站<a href="http://deepdreamgenerator.com/">[2]</a> .</p><h2 id="Deep-Style-3"><a href="#Deep-Style-3" class="headerlink" title="Deep Style[3]"></a>Deep Style<a href="https://arxiv.org/abs/1508.06576">[3]</a></h2><p>Given a photo, make its style like famous paintings.</p><p><a href="https://imgchr.com/i/JyPX9O"><img src="https://s1.ax1x.com/2020/04/25/JyPX9O.md.png" alt="JyPX9O.md.png"></a> </p><p>上图中，用一个CNN学习图中的content，用另一个CNN学习风格图中的style。</p><p>再用一个CNN使得输入的图像content像原图，风格像另一张图。</p><h2 id="Playing-Go"><a href="#Playing-Go" class="headerlink" title="Playing Go"></a>Playing Go</h2><p>CNN 还可以用在下围棋中，如下图，输入是19 * 19的围棋局势（matrix/image），通过CNN，学出下一步应该走哪？</p><p><a href="https://imgchr.com/i/JyPL4K"><img src="https://s1.ax1x.com/2020/04/25/JyPL4K.md.png" alt="JyPL4K.md.png"></a> </p><h3 id="Why-CNN-playing-Go"><a href="#Why-CNN-playing-Go" class="headerlink" title="Why CNN playing Go?"></a>Why CNN playing Go?</h3><p>下围棋满足以下两个property：</p><ol><li><p>Some patterns are much smaller than the whole image.</p><p><a href="https://imgchr.com/i/JyPou9"><img src="https://s1.ax1x.com/2020/04/25/JyPou9.png" alt="JyPou9.png"></a> </p><p>（围棋新手，博主只下赢过几次hhh)</p><p>如果白棋棋手，看到上图的pattern，上图的白子只有一口气了，被堵住就会被吃掉，那白棋棋手大概率会救那个白子，下在白棋的下方。</p><p>Alpha Go uese 5 * 5 for first layer.</p></li><li><p>The same patterns appear in different regions.</p><p><a href="https://imgchr.com/i/JyP7H1"><img src="https://s1.ax1x.com/2020/04/25/JyP7H1.md.png" alt="JyP7H1.md.png"></a> </p></li></ol><hr><p>但如何解释CNN的另一结构——Max Pooling？</p><p>因为围棋的棋谱matrix不像image的pixel，subsample后，围棋的棋谱就和原棋谱完全不像了。</p><p>Alpha Go的论文中：Alpha Go并没有用Max Pooling。</p><p><a href="https://imgchr.com/i/JyPbAx"><img src="https://s1.ax1x.com/2020/04/25/JyPbAx.md.png" alt="JyPbAx.md.png"></a> </p><p>所以，可以根据要训练的东西调整CNN模型。</p><h2 id="Speech"><a href="#Speech" class="headerlink" title="Speech"></a>Speech</h2><p>可以用CNN学习<a href="[https://zh.wikipedia.org/wiki/%E6%97%B6%E9%A2%91%E8%B0%B1](https://zh.wikipedia.org/wiki/时频谱)">Spectrogram</a> ，即识别出这一时段说的是什么话。</p><p><a href="https://imgchr.com/i/JyPqN6"><img src="https://s1.ax1x.com/2020/04/25/JyPqN6.md.png" alt="JyPqN6.md.png"></a> </p><h2 id="Text"><a href="#Text" class="headerlink" title="Text"></a>Text</h2><p>CNN还可以用在文本的情感分析中，对句子中每个word embedding后，通过CNN，学习sentence表达的是negative 还是positive还是neutral的情绪。</p><p><a href="https://imgchr.com/i/JyPTBR"><img src="https://s1.ax1x.com/2020/04/25/JyPTBR.md.png" alt="JyPTBR.md.png"></a> </p><h3 id="More"><a href="#More" class="headerlink" title="More"></a>More</h3><p>（挖坑…生命很漫长，学无止境QAQ）</p><ul><li><p>The methods of visualization in these slides：</p><p> <a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html">https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html</a></p></li><li><p>More about visualization：</p><p><a href="http://cs231n.github.io/understanding-cnn/">http://cs231n.github.io/understanding-cnn/</a></p></li><li><p>Very cool CNN visualization toolkit</p><p><a href="http://yosinski.com/deepvis">http://yosinski.com/deepvis</a></p><p><a href="http://scs.ryerson.ca/~aharley/vis/conv/">http://scs.ryerson.ca/~aharley/vis/conv/</a></p></li><li><p>The 9 Deep Learning Papers You Need To Know About</p><p><a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html</a></p></li><li><p>How to let machine draw an image</p><ul><li><p>PixelRNN</p><p><a href="https://arxiv.org/abs/1601.06759">https://arxiv.org/abs/1601.06759</a></p></li><li><p>Variation Autoencoder (VAE)</p><p><a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></p></li><li><p>Generative Adversarial Network (GAN)</p><p><a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a></p></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>Deep Neural Networks are Easily Fooled： <a href="https://www.youtube.com/watch?v=M2IebCN9Ht4">https://www.youtube.com/watch?v=M2IebCN9Ht4</a></p></li><li><p>deep dream generator: <a href="http://deepdreamgenerator.com/">http://deepdreamgenerator.com/</a></p></li><li><p>A Neural Algorithm of Artistic Style: <a href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中首先介绍了为什么要用CNN做图像识别，或者说图像识别问题的特点是什么？&lt;br&gt;文章中也详细介绍了CNN的具体架构，主要包括Convolution、Max Pooling、Flatten。&lt;br&gt;文章最后简要介绍了CNN在诸多领域的应用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f1ed.github.io/tags/DeepLearning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="CNN" scheme="https://f1ed.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Tips for Deep Learning</title>
    <link href="https://f1ed.github.io/2020/04/21/tips-for-DL/"/>
    <id>https://f1ed.github.io/2020/04/21/tips-for-DL/</id>
    <published>2020-04-20T16:00:00.000Z</published>
    <updated>2020-07-03T08:45:39.300Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。<br>tips从Training和Testing两个方面展开。<br>在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有RMSprop、Momentum、Adam等）。<br>当在Training中得到好的performance，但在testing中perform bad时，即遇到了overfitting，又该怎么处理呢？文章后半部分详尽介绍了EarlyStopping、Regularization和Dropout三个solution。</p><a id="more"></a><h1 id="Recipe-of-Deep-Learning"><a href="#Recipe-of-Deep-Learning" class="headerlink" title="Recipe of Deep Learning"></a>Recipe of Deep Learning</h1><p>Deep Learning 的三个步骤：</p><p><a href="https://imgchr.com/i/JGW8js"><img src="https://s1.ax1x.com/2020/04/21/JGW8js.md.png" alt="JGW8js.md.png"></a> </p><p>如果在Training Data中没有得到好的结果，需要重新训练Neural Network。</p><p>如果在Training Data中得到好的结果，在Testing Data（这里的Testing Data是指有Label的Data，比如Kaggle的Public Data或者是从Training Data中划分出的Development Data）没有得到的好的结果，说明Overfitting了，需要重新设计Neural Network的结构。</p><h2 id="Do-not-always-blame-Overfitting"><a href="#Do-not-always-blame-Overfitting" class="headerlink" title="Do not always blame Overfitting"></a>Do not always blame Overfitting</h2><p><a href="https://imgchr.com/i/JGW3cj"><img src="https://s1.ax1x.com/2020/04/21/JGW3cj.md.png" alt="JGW3cj.md.png"></a> </p><p>如果在Testing Data中，看到上图，20-layer的error小，56-layer的error大，56-layer一定overfitting了。</p><p>No!!!不要总把原因归咎于Overfitting。</p><p><a href="https://imgchr.com/i/JGW13Q"><img src="https://s1.ax1x.com/2020/04/21/JGW13Q.md.png" alt="JGW13Q.md.png"></a> </p><p>再看Testing Data error之前，先看看Training Data的error。上图中，56-layer的DNN在Training Data的error本来就比20-layer的大，说明56-layer的DNN根本没有train好。</p><p>所以56-layer的DNN在Testing Data上的error大，原因不是overfitting，而是模型根本没有train好。</p><p><strong>注：</strong> Overfitting是在Training Data上error小，但在Testing Data上的error大。</p><p>因此，对于在Training Data上得到不好的结果和在Training Data上得到好的结果但在Testing Data上得到不好的结果这两种情况，需要不同的解决方法。</p><h1 id="Bad-Results-on-Training-Data"><a href="#Bad-Results-on-Training-Data" class="headerlink" title="Bad Results on Training Data"></a>Bad Results on Training Data</h1><p>在不重新设计DNN结构时，如果在Training Data中得到Bad Results，一般有两种方法来改进结果：</p><ul><li>New activation function【neuron换新的激活函数】</li><li>Adaptive Learning Rate</li></ul><h2 id="New-activation-function"><a href="#New-activation-function" class="headerlink" title="New activation function"></a>New activation function</h2><h3 id="Vanishing-Gradient-Problem"><a href="#Vanishing-Gradient-Problem" class="headerlink" title="Vanishing Gradient Problem"></a>Vanishing Gradient Problem</h3><p><a href="https://imgchr.com/i/JGWl9g"><img src="https://s1.ax1x.com/2020/04/21/JGWl9g.md.png" alt="JGWl9g.md.png"></a> </p><p>上图表示，在手写数字辨识中，Deeper layers并不能有好的performance。</p><p>为什么会这样呢？</p><p>因为出现了Vanishing Gradient Problem，即gradient随着deeper layer逐渐消失的问题。</p><p><a href="https://imgchr.com/i/JGWM4S"><img src="https://s1.ax1x.com/2020/04/21/JGWM4S.md.png" alt="JGWM4S.md.png"></a> </p><p>上图中，假设neuron的activation function是sigmod函数。</p><p>靠近Input layer层的参数的变化对Loss的影响很小，所以对Loss function做微分，gradient很小，参数更新慢。</p><p>而靠近Output layer层的参数的编号对Loss的影响更大，所以对Loss function做微分，gradient很大，参数更新快。</p><p>因为靠近Output Layer层的参数更新快，所以很快converge（收敛、趋于稳定）；但靠近Input Layer层的参数更新慢，几乎还处在random（随机）的状态。</p><p>当靠近Output Layer层的参数趋于稳定时，由于靠近Output Layer层的参数对Loss影响大，所以观察到的Loss的值也趋于稳定，于是，你就把training停掉了。</p><p>但是，靠近Input层的参数几乎处在random状态，所以拿模型用在Testing Data上，发现结果几乎是随机的。</p><hr><p>怎么直观理解靠近Input Layer的参数的gradient小呢？</p><p>用微分的直观含义来表示gradient $\partial{l}/\partial{w}$ : </p><p><strong>当 $w$ 增加 $\Delta{w}$ 时，如果 $l$ 的变化 $\Delta{l}$ 变化大，说明 $\partial{l}/\partial{w}$ 大，否则 $\partial{l}/\partial{w}$ 小。</strong></p><p><a href="https://imgchr.com/i/JGWKN8"><img src="https://s1.ax1x.com/2020/04/21/JGWKN8.md.png" alt="JGWKN8.md.png"></a> </p><p>我们在DNN中使用的activation function是sigmod函数，sigmod函数会把值压到0和1之间。</p><p>因此，上图中，其他值不变，只有连接 $x_N$ 的参数 $w$ 增加 $\Delta w$ 时，输入通过neuron的sigmod函数，函数的输出增加的 $\Delta$ 会变小，随着Deeper Layer，neuron的输出的 $\Delta$ 会越变越小，趋至0。</p><p>最后DNN输出的变化对 loss的影响小，即 $\Delta{l}$ 趋至0，即参数的gradient  $\partial{l}/\partial{w}$ 趋至0。（即 Vanishing Gradient）</p><h3 id="ReLu-：Rectified-Linear-Unit"><a href="#ReLu-：Rectified-Linear-Unit" class="headerlink" title="ReLu ：Rectified Linear Unit"></a>ReLu ：Rectified Linear Unit</h3><p>为了防止发生Vanishing Gradient Problem，在DNN中选择使用新的activation function。</p><p>ReLu长下面这个样子：</p><p><a href="https://imgchr.com/i/JGWuAf"><img src="https://s1.ax1x.com/2020/04/21/JGWuAf.md.png" alt="JGWuAf.md.png"></a> </p><p>z: input</p><p>a: output</p><p>当 $z\leq0$ 时， $a=0$ ；当 $z &gt;0$ 时， $a=z$ 。</p><p><u>Reason :</u> </p><ol><li>Fast to compute</li><li>Biological reason【有生物上的原因】</li><li>Infinite sigmod with different biases. 【是无穷个 有不同bias的sigmod函数 的叠加】</li><li><strong>Vanishing gradient problem</strong> 【最重要的是没有vanishing gradient problem】</li></ol><hr><p>为什么ReLu没有vanishing gradient problem</p><p><a href="https://imgchr.com/i/JGWmHP"><img src="https://s1.ax1x.com/2020/04/21/JGWmHP.md.png" alt="JGWmHP.md.png"></a> </p><p>上图DNN中，ReLu在输入是负数时，输出是0。因此这些输出是0的neuron可以去掉。</p><p>就变成了下图这个A Thinner linear network。由于ReLu函数的性质，靠近Input Layer的参数不会有smaller gradient。</p><p><a href="https://imgchr.com/i/JGWeBt"><img src="https://s1.ax1x.com/2020/04/21/JGWeBt.md.png" alt="JGWeBt.md.png"></a> </p><p>这里有一个Q&amp;A: </p><p>Q1: function变成linear的，会不会DNN就变弱了？</p><p>： 当neuron的operation region不变的话，DNN的确是linear的，但是当neuron的operation region改变后，就是unlinear的。</p><p>：即，当input的变化小，operation region不变（即输入不会从大于0变成小于0，小于0变成大于0这种），model还是linear的；但当input的变化大时，很多neuron的operation region都变化了，model其实就是unlinear的。</p><p>Q2: ReLu 怎么微分？</p><p>：ReLu在0点不可微，那就随便指定为0这样（台湾腔QAQ）。</p><h3 id="ReLu-variant"><a href="#ReLu-variant" class="headerlink" title="ReLu - variant"></a>ReLu - variant</h3><p>当 $z\leq 0$ 时，输出为0，就不能更新参数了。于是就有下图变体：</p><p><a href="https://imgchr.com/i/JGWZnI"><img src="https://s1.ax1x.com/2020/04/21/JGWZnI.md.png" alt="JGWZnI.md.png"></a> </p><p>当 $z\leq0$ 时，gradient都为0.01，为什么不能是其他值。于是就有下图变体：其中 $\alpha$ 也是一个需要学习的参数</p><h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>Maxout，如下图，在设计neural network时，会给每一层的neuron分组，成为一个新的neuron。</p><p><a href="https://imgchr.com/i/JGWk1H"><img src="https://s1.ax1x.com/2020/04/21/JGWk1H.md.png" alt="JGWk1H.md.png"></a> </p><p>Maxout也是一个Learnable activation function。</p><p><strong>ReLu是Maxout学出来的一个特例。</strong></p><p><a href="https://imgchr.com/i/JGWAcd"><img src="https://s1.ax1x.com/2020/04/21/JGWAcd.md.png" alt="JGWAcd.md.png"></a> </p><p>上图中，左图是ReLu。 ReLu的输入 $z = wx+b$ ，输出 $a$ 如上图的绿色的线。</p><p>右图是Maxout。Maxout的输入 $z_1 =wx+b,z_2=0$ ，那么输出取max，输出 $a$ 如上图中绿色的线，和左图的ReLu相同。</p><hr><p><strong>Maxout is more than ReLu。</strong> </p><p>当参数更新时，Maxout的函数图像如下图：</p><p><a href="https://imgchr.com/i/JGWPhD"><img src="https://s1.ax1x.com/2020/04/21/JGWPhD.md.png" alt="JGWPhD.md.png"></a> </p><p>DNN中的参数是learnable的，所以Maxout也是一个learnable的activation function。</p><p><u>Reason ：</u></p><ul><li><p>Learnable activation function [Ian J. Goodfellow, ICML’13]</p><ul><li><p>Activation function in maxout network can be any piecewise linear convex function.</p><p>在maxout神经网络中的激活函数可以是任意的分段凸函数。</p></li><li><p>How many pieces depending on how many elements in a group.</p><p>分段函数分几段取决于一组中有多少个元素。</p><p><a href="https://imgchr.com/i/JGWCtO"><img src="https://s1.ax1x.com/2020/04/21/JGWCtO.md.png" alt="JGWCtO.md.png"></a> </p></li></ul></li></ul><h3 id="Maxout-how-to-train"><a href="#Maxout-how-to-train" class="headerlink" title="Maxout : how to train"></a>Maxout : how to train</h3><p>Given a training data x, we know which z would be the max.</p><p>【当给出每笔training data时，我们能知道Maxout neuron中哪一个最大】</p><p><a href="https://imgchr.com/i/JGW9AK"><img src="https://s1.ax1x.com/2020/04/21/JGW9AK.md.png" alt="JGW9AK.md.png"></a> </p><p>如上图，在这笔training data x中，我们只train this thin and linear network 的参数，即max z相连的参数。</p><p>每笔不同的training data x，会得到不同的thin and linear network，最后，会train到每一个参数。</p><h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><h3 id="Review-Adagrad"><a href="#Review-Adagrad" class="headerlink" title="Review Adagrad"></a>Review Adagrad</h3><p>在这篇文章： <a href="/2020/03/01/Gradient/" title="Gradient">Gradient</a> 第一小节讲到一种adaptive learning rate的gradient 算法：Adagrad 算法。在那篇文章中，我们得出的结论是 the best step $\propto$  |First dertivative| / Second derivative.</p><p><a href="https://imgchr.com/i/JGWF9e"><img src="https://s1.ax1x.com/2020/04/21/JGWF9e.md.png" alt="JGWF9e.md.png"></a>  </p><p>在上图中，两个方向，因为蓝色方向的二阶微分更小，所以蓝色方向应该有更大的learning rate。</p><p>因此，在Adagrad中，我们用一阶微分来估量二阶微分的大小：</p>$$w^{t+1}  \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}$$<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>但是，在训练NN时，Error Surface（Total Loss对参数的变化）的图像可能会更复杂，如下图：</p><p><a href="https://imgchr.com/i/JGWS76"><img src="https://s1.ax1x.com/2020/04/21/JGWS76.md.png" alt="JGWS76.md.png"></a> </p><p>因为函数图像过于复杂，可能在同一方向的不同位置，也需要有不同的learning rate。</p><p>RMSProp是Adagrad的进阶版。</p><p><strong>RMSProp过程：</strong> </p><ol><li> $w^{1} \leftarrow w^{0}-\frac{\eta}{\sigma^{0}} g^{0} \quad \sigma^{0}=g^{0}$ </li><li> $w^{2} \leftarrow w^{1}-\frac{\eta}{\sigma^{1}} g^{1} \quad \sigma^{1}=\sqrt{\alpha (\sigma^{0})^2+(1-\alpha)(g^1)^2}$ </li><li> $w^{3} \leftarrow w^{2}-\frac{\eta}{\sigma^{2}} g^{2} \quad \sigma^{2}=\sqrt{\alpha (\sigma^{1})^2+(1-\alpha)(g^2)^2}$ </li><li><p>…</p></li><li> $w^{t+1} \leftarrow w^{t}-\frac{\eta}{\sigma^{t}} g^{t} \quad \sigma^{t}=\sqrt{\alpha (\sigma^{t-1})^2+(1-\alpha)(g^t)^2}$ <p>$\sigma^t$ 也是在算gradients的 root mean squar。</p></li></ol><p>但是在RMSProp中，加入了参数 $\alpha$ (需要手动调节大小的参数)，可以给当前算出来的gradient $g^t$ 更大的权重，即更相信现在gradient的方向，不那么相信以前gradient的方向。</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Momentum，则是引用物理中的惯性。</p><p><a href="https://imgchr.com/i/JGRxn1"><img src="https://s1.ax1x.com/2020/04/21/JGRxn1.md.png" alt="JGRxn1.md.png"></a> </p><p>上图中，当小球到达local minima时，会因为惯性继续往前更新，则有可能到达minima的位置。</p><p>这里的Momentum，就代指上一次前进（参数更新）的方向。</p><p><strong>Vanilla Gradient Descent</strong> </p><p>如果将Gradient的步骤画出图来，就是下图这样：</p><p><a href="https://imgchr.com/i/JGRXc9"><img src="https://s1.ax1x.com/2020/04/21/JGRXc9.md.png" alt="JGRXc9.md.png"></a> </p><p>过程：</p><ol><li><p>Start at position $\theta^0$</p></li><li><p>Compute gradietn at $\theta^0$</p><p>Move to  $\theta^1=\theta^0-\eta\nabla{L(\theta^0)}$ </p></li><li><p>Compute gradietn at $\theta^1$ </p><p>Move to  $\theta^2=\theta^1-\eta\nabla{L(\theta^1)}$ </p></li><li><p>… …Stop until $\nabla{L(\theta^t)}\approx0$</p></li></ol><hr><p><strong>Momentum</strong> </p><p>在Momentum中，参数更新方向是当前Gradient方向和Momentum方向（上一次更新方向）的叠加。</p><p><a href="https://imgchr.com/i/JGRjXR"><img src="https://s1.ax1x.com/2020/04/21/JGRjXR.md.png" alt="JGRjXR.md.png"></a> </p><p>Movement方向：上一次更新方向 - 当前gradient方向。</p><p>过程：</p><ol><li><p>Start at position $\theta^0$</p><p>Movement: $v^0=0$ </p></li><li><p>Compute gradient at $\theta^0$ </p><p>Movement  $v^1=\lambda v^0-\eta\nabla{L(\theta^0)}$  </p><p>Move to  $\theta^1=\theta^0+v^1$ </p></li><li><p>Compute gradient at $\theta^1$  </p><p>Movement  $v^2=\lambda v^1-\eta\nabla{L(\theta^1)}$ </p><p>Move to $\theta^2=\theta^1+v^2$  </p></li><li><p>… …Stop until $\nabla{L(\theta^t)}\approx0$ </p></li></ol><p>和Vanilla Gradient Descent比较，$v^i$ 其实是过去gradient( $\nabla{L(\theta^0)}$ 、$\nabla{L(\theta^1)}$ 、… 、 $\nabla{L(\theta^{i-1})}$  )的加权和。</p><ul><li>迭代过程：<ul><li>$v^0=0$ </li><li> $v^1=-\eta\nabla{L(\theta^0)}$ </li><li> $v^2=-\lambda\eta\nabla{L(\theta^0)}-\eta\nabla{L(\theta^1)}$ </li><li>…</li></ul></li></ul><hr><p>再用那个小球的例子来直觉的解释Momentum：</p><p><a href="https://imgchr.com/i/JGRO1J"><img src="https://s1.ax1x.com/2020/04/21/JGRO1J.md.png" alt="JGRO1J.md.png"></a> </p><p>当小球在local minima时，gradient为0，但是Momentum（即上次移动方向）是继续往前，于是小球可以继续向前更新。</p><h3 id="Adam-RMSProp-Momentum"><a href="#Adam-RMSProp-Momentum" class="headerlink" title="Adam = RMSProp + Momentum"></a>Adam = RMSProp + Momentum</h3><p><a href="https://imgchr.com/i/JGRLp4"><img src="https://s1.ax1x.com/2020/04/21/JGRLp4.md.png" alt="JGRLp4.md.png"></a> </p><p>Algorithm：Adam, our proposed algorithm for stochastic optimization. </p><p>【Adam，是为了优化stochastic gradient】（至于什么是stochastic gradient，建议戳)</p><p>$g_t^2$ indicates the elementwise square $g_t\odot g_t$ .</p><p>【$g_t^2$ 是gradient $g_t$ 向量和 $g_t$ 的元素乘】</p><p>Good default settings for the tested machine learning problems are $\alpha=0.001$ , $\beta_1=0.9$ , $\beta_2=0.999$ and $\epsilon=10^{-8}$ . All operations on vectors are element-wise. With $\beta_1^t$ and $\beta_2^t$ we denote $\beta_1$ and $\beta_2$ to the power t.</p><p>【参数说明：算法默认的参数设置是 $\alpha=0.001$ , $\beta_1=0.9$ , $\beta_2=0.999$ ， $\epsilon=10^{-8}$ 。算法中所有vector之间的操作都是对元素操作。 $\beta_1^t$ 和 $\beta_2^t$ 是 $\beta_1$ 和 $\beta_2$ 的 $t$ 次幂】</p><p><strong>Adam Pseudo Code：</strong> </p><ol start="0"><li><p><strong>Require</strong>：$\alpha$ : Stepsize 【步长/learning rate $\eta$ 】</p><p><strong>Require</strong>：$\beta_1,\beta_2\in\left[0,1\right)$ : Exponential decay rates for the moment estimates.</p><p><strong>Require</strong>：$f(\theta)$ : Stochastic objective function with parameters $\theta$ .【参数 $\theta$ 的损失函数】</p><p><strong>Require</strong>: $\theta_0$ ：Initial parameter vector 【初值】</p></li><li><p>$m_0\longleftarrow 0$ (Initial 1st moment vector) 【 $m$ 是Momentum算法中的更新参数后的方向 $v$ 】</p><p>$v_0\longleftarrow 0$ (Initial 2nd moment vector) 【 $v$ 是RMSprop算法中gradient的root mean square $\sigma$ 】</p><p>$t\longleftarrow 0$ (Initial timestep) 【更新次数】</p></li><li><p><strong>while</strong> $\theta_t$ not concerged <strong>do</strong> 【当 $\theta$ 趋于稳定，即 $\nabla{f(\theta)}\approx0$ 时】</p><ol><li><p>$t\longleftarrow t+1$ </p></li><li> $g_t\longleftarrow \nabla{f_t(\theta_{t-1})}$  (Get gradients w.r.t. stochastic objective at timestep t)<p>【算第t次时 $\theta$ 的gradient】</p></li><li> $m_{t} \leftarrow \beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t}$   (Update biased first momen t estimate)<p>【用Momentum算更新方向】</p></li><li> $v_{t} \leftarrow \beta_{2} \cdot v_{t-1}+\left(1-\beta_{2}\right) \cdot g_{t}^{2}$  (Update biased second raw moment estimate)<p>【RMSprop估测最佳步长（ 和$v$ 负相关） 】</p></li><li> $\widehat{m}_{t} \leftarrow m_{t} /\left(1-\beta_{1}^{t}\right)$ （Comppute bbi. as-corrected first momen t estima te)<p>【算出来的值有bias，论文中有具体解释为什么有。当更新次数增加时， $1-\beta_1^t$ 也趋近于1】</p></li><li> $\widehat{v}_{t} \leftarrow v_{t} /\left(1-\beta_{2}^{t}\right)$  (Compute bias-corrected second raw momen t estimate)<p>【和上同理】</p></li><li> $\theta_{t} \leftarrow \theta_{t-1}-\alpha \cdot \widehat{m}_{t} /(\sqrt{\widehat{v}_{t}}+\epsilon)$ （Update parameters）<p>【 $\widehat{m}<em>t$ 相当于是更准确的gradient的方向，$\sqrt{\widehat{v}</em>{t}}+\epsilon$ 是为了估测最好的步长，调节learning rate】</p></li></ol></li></ol><h3 id="Gradient-Descent-Limitation？"><a href="#Gradient-Descent-Limitation？" class="headerlink" title="Gradient Descent Limitation？"></a>Gradient Descent Limitation？</h3><p>在<a href="/2020/03/01/Gradient/" title="Gradient">Gradient</a>这篇文章中，讲到过Gradient有一些问题不能处理：</p><ul><li>Stuck at local minima</li><li>Stuck at saddle point</li><li>Very slow at the plateau</li></ul><p><a href="https://imgchr.com/i/JG4l9O"><img src="https://s1.ax1x.com/2020/04/21/JG4l9O.md.png" alt="JG4l9O.md.png"></a> </p><p>（李老师说的，不是我说的QAQ）：但是Andrew（吴恩达）在2017年说过，不用太担心这个问题。为什么呢？</p><p>如果要stuck at local minima，前提是每一维度都是local minima。</p><p>如果在一个维度遇到local minima的概率是p，当NN很复杂时，有很多参数时，比如1000，那么遇到local minima的概率是 $p^{1000}$ ，趋近于0了，几乎不会发生。</p><p>：所以不用太担心Gradient Descent的局限性。</p><h1 id="Bad-Results-on-Testing-Data"><a href="#Bad-Results-on-Testing-Data" class="headerlink" title="Bad Results on Testing Data"></a>Bad Results on Testing Data</h1><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p>在更新参数时，可能会出现这样曲线图：</p><p><a href="https://imgchr.com/i/JGRbhF"><img src="https://s1.ax1x.com/2020/04/21/JGRbhF.md.png" alt="JGRbhF.md.png"></a> </p><p>图中，Total Loss在training set中逐渐减小，但在validation set中逐渐增大。</p><p>而我们真正关心的其实是validation set的Loss。</p><p>所以想让参数停在validation set中loss最低时。</p><p>Keras能够实现EarlyStopping功能[1]：click <a href="https://keras.io/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore">here</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line">early_stopping = EarlyStopping(monitor=<span class="string">'val_loss'</span>, patience=<span class="number">2</span>)</span><br><span class="line">model.fit(x, y, validation_split=<span class="number">0.2</span>, callbacks=[early_stopping])</span><br></pre></td></tr></table></figure><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Regularization：Find a set of weight not only minimizing original cost but also close to zero.</p><p>构造一个新的loss function，除了最小化原来的loss function，还能使得参数趋紧0，使得function更平滑。</p><p>function的曲线更平滑，当输入有轻微扰动，不会太影响输出的结果。</p><h3 id="L2-norm-regularization"><a href="#L2-norm-regularization" class="headerlink" title="L2 norm regularization"></a>L2 norm regularization</h3><p><strong>New loss function:</strong> </p>$$\begin{equation}\begin{aligned}\mathrm{L}^{\prime}(\theta)&=L(\theta)+\lambda \frac{1}{2}\|\theta\|_{2}\\ \theta &={w_1,w_2,...}\\ \|\theta\|_2&=(w1)^2+(w_2)^2+...\end{aligned}\end{equation}$$<p>其中用第二范式 $\lambda\frac{1}{2}|\theta|_2$ 作为regularization term。做regularization是为了使函数更平滑，所以一般不考虑bias)</p><p><strong>New gradient:</strong> </p>$$\frac{\partial \mathrm{L}^{\prime}}{\partial w}=\frac{\partial \mathrm{L}}{\partial w}+\lambda w$$<p><strong>New update:</strong> </p>$$\begin{equation}\begin{aligned}w^{t+1} &\longrightarrow w^{t}-\eta \frac{\partial \mathrm{L}^{\prime}}{\partial w}\\ &=w^{t}-\eta\left(\frac{\partial \mathrm{L}}{\partial w}+\lambda w^{t}\right)\\ &=(1-\eta \lambda) w^{t}-\eta \frac{\partial \mathrm{L}}{\partial w}\end{aligned}\end{equation}$$<p>在更新参数时，先乘一个 $(1-\eta\lambda)$ ，再更新。</p><p>weight decay（权值衰减）：由于 $\eta,\lambda$ 都是很小的值，所以 $w^t$ 每次都会先乘一个小于1的数，即逐渐趋于0，实现regularization。但是，因为更新中还有gradient部分，所以不会等于0。</p><h3 id="L1-norm-regularization"><a href="#L1-norm-regularization" class="headerlink" title="L1 norm regularization"></a>L1 norm regularization</h3><p>Regularization除了用第二范式，还可以用其他的，比如第一范式 $|\theta|_1=|w_1|+|w_2|+…$ </p><p><strong>New loss function:</strong> </p>$$\begin{equation}\begin{aligned}\mathrm{L}^{\prime}(\theta)&=L(\theta)+\lambda \frac{1}{2}\|\theta\|_1\\ \theta &={w_1,w_2,...}\\ \|\theta\|_1&=|w_1|+|w_2|+...\end{aligned}\end{equation}$$<p>用sgn()符号函数来表示绝对值的求导。</p><blockquote><p>符号函数：Sgn(number)</p><p>如果number 大于0，返回1；等于0，返回0；小于0，返回-1。</p></blockquote><p><strong>New gradient:</strong> </p>$$\frac{\partial \mathrm{L}^{\prime}}{\partial w}=\frac{\partial \mathrm{L}}{\partial w}+\lambda \text{sgn}(w)$$<p><strong>New update:</strong> </p>$$\begin{equation}\begin{aligned}w^{t+1} &\longrightarrow w^{t}-\eta \frac{\partial \mathrm{L}^{\prime}}{\partial w}\\ &=w^{t}-\eta\left(\frac{\partial \mathrm{L}}{\partial w}+\lambda \text{sgn}(w^t)\right)\\ &=w^{t}-\eta \frac{\partial \mathrm{L}}{\partial w}-\eta \lambda \operatorname{sgn}\left(w^{t}\right)\end{aligned}\end{equation}$$<p>在用第一范式做regularization时，每次 $w^t$ 都要减一个值 $\eta\lambda\text{sgn}(w^t)$ ，和用第二范式做regularization比较，后者每次都要乘一个小于1的值，即使是乘0.99，w下降也很快。</p><blockquote><p>Weight decay（权值衰减）的生物意义：</p><p>Our brain prunes（修剪） out the useless link between neurons.</p><p><a href="https://imgchr.com/i/JGRHtU"><img src="https://s1.ax1x.com/2020/04/21/JGRHtU.md.png" alt="JGRHtU.md.png"></a> </p></blockquote><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Wiki: <strong>Dropout</strong>是Google提出的一种正则化技术，用以在人工神经网络中对抗过拟合。Dropout有效的原因，是它能够避免在训练数据上产生复杂的相互适应。Dropout这个术语代指在神经网络中丢弃部分神经元（包括隐藏神经元和可见神经元）。在训练阶段，dropout使得每次只有部分网络结构得到更新，因而是一种高效的神经网络模型平均化的方法。[2]</p><p>这里讲Dropout怎么做。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p><a href="https://imgchr.com/i/JG4M4K"><img src="https://s1.ax1x.com/2020/04/21/JG4M4K.md.png" alt="JG4M4K.md.png"></a> </p><ul><li><p>Each time before updating the parameters:</p><ul><li><p>Each neuron has p% to dropout. Using the new thin network for training.</p><p>【如上图，每个neuron有p的概率被dropout。于是NN就变成了下图thinner的NN】</p><p><a href="https://imgchr.com/i/JGR5mq"><img src="https://s1.ax1x.com/2020/04/21/JGR5mq.md.png" alt="JGR5mq.md.png"></a> </p></li><li><p>For each mini-batch, we resample the dropout neurons.</p><p>【每次mini-batch，都要重新dropout，更新NN的结构】</p></li></ul></li></ul><h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p><strong>Testing中不做dropout</strong> </p><ul><li><p>If the dropout rate at training is p%, all the weights times 1-p%.</p><p>【如果在training中 dropout rate是 p%，在testing是，每个参数都乘 （1-p%)】</p><p>【比如dropout rate 是0.5。如果train出来的w是 1，那么testing中 w=0.5】</p></li></ul><h3 id="Why-dropout-in-training：Intuitive-Reason"><a href="#Why-dropout-in-training：Intuitive-Reason" class="headerlink" title="Why dropout in training：Intuitive Reason"></a>Why dropout in training：Intuitive Reason</h3><ol><li><p>这是一个比较有趣的比喻：</p><p><a href="https://imgchr.com/i/JGRI00"><img src="https://s1.ax1x.com/2020/04/21/JGRI00.md.png" alt="JGRI00.md.png"></a> </p></li><li><p>这也是一个有趣的比喻hhh:</p><p><a href="https://imgchr.com/i/JGRo7V"><img src="https://s1.ax1x.com/2020/04/21/JGRo7V.md.png" alt="JGRo7V.md.png"></a> </p><p>即，团队合作的时候，如果每个人都认为队友在带我，那每个人都可能划水。</p><p>但是，（training中）如果你知道你的队友在划水，那你可能会做的更好。</p><p>但是，（testing中）发现每个人都有更好地做，都没有划水，那么结果就会很好。</p><p>（hhhh，李老师每次讲Intuitive Reason的时候，都觉得好有道理hhh，科学的直觉orz给我也整一个）</p></li></ol><h3 id="Why-multiply-1-p-in-testing-Intuitive-reason"><a href="#Why-multiply-1-p-in-testing-Intuitive-reason" class="headerlink" title="Why multiply (1-p%) in testing: Intuitive reason"></a>Why multiply (1-p%) in testing: Intuitive reason</h3><p>为什么在testing中 weights要乘（1-p%)?</p><p>用一个具体的例子来直观说明：</p><p><a href="https://imgchr.com/i/JGRf6s"><img src="https://s1.ax1x.com/2020/04/21/JGRf6s.md.png" alt="JGRf6s.md.png"></a> </p><p>上图中，如果dropout rate=0.5，假设只训练一次， $w_2,w_4$ 相连的neuron都被dropout。</p><p>在testing中，因为不对neurondropout，所以如果不改变weight，计算出的结果 $z’\approx 2z$ 。</p><p>因此将所有weight简单地和(1-p%) 相乘，能尽量保证计算出的结果 $z’\approx z$ 。</p><h3 id="Dropout-is-a-kind-of-ensemble"><a href="#Dropout-is-a-kind-of-ensemble" class="headerlink" title="Dropout is a kind of ensemble"></a>Dropout is a kind of ensemble</h3><p>Ensemble(合奏)，如下图，将testing data丢给train好的NN来估计，最后的估计值取所有NN输出的平均，如下图：</p><p><a href="https://imgchr.com/i/JGRhXn"><img src="https://s1.ax1x.com/2020/04/21/JGRhXn.md.png" alt="JGRhXn.md.png"></a> </p><p>为什么说dropout is a kind of ensemble?</p><p><a href="https://imgchr.com/i/JGRRpQ"><img src="https://s1.ax1x.com/2020/04/21/JGRRpQ.md.png" alt="JGRRpQ.md.png"></a> </p><ul><li><p>Using one mini-batch to train one network</p><p>【dropout相当于每次用一个mini-batch来训练一个network】</p></li><li><p>Some parameters in the network are shared</p><p>【有些参数可能会在很多个mini-batch都被train到】</p></li></ul><p>由于每个神经元有 p%的概率被dropout，因此理论上，如果有M个neuron，可能会训练 $2^M$ 个network。</p><p>但是在Ensemble中，将每个network存下来，testing的时候输出取平均，这样的过程太复杂了，结果也不一定会很好。</p><p>所以在testing中，no dropout，对原始network中的每个参数乘 (1-p%)，用这样简单的操作来达到ensemble的目的。</p><p><a href="https://imgchr.com/i/JGRWlj"><img src="https://s1.ax1x.com/2020/04/21/JGRWlj.md.png" alt="JGRWlj.md.png"></a> </p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Keras: <a href="https://keras.io/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore">how can i interrupt training when the validation loss isn’t decresing anymore.</a> </li><li>Dropout-wiki：<a href="https://zh.wikipedia.org/wiki/Dropout">https://zh.wikipedia.org/wiki/Dropout</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。&lt;br&gt;tips从Training和Testing两个方面展开。&lt;br&gt;在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有RMSprop、Momentum、Adam等）。&lt;br&gt;当在Training中得到好的performance，但在testing中perform bad时，即遇到了overfitting，又该怎么处理呢？文章后半部分详尽介绍了EarlyStopping、Regularization和Dropout三个solution。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f1ed.github.io/tags/DeepLearning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="DNN" scheme="https://f1ed.github.io/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Backpropagation</title>
    <link href="https://f1ed.github.io/2020/04/18/Backpropagation/"/>
    <id>https://f1ed.github.io/2020/04/18/Backpropagation/</id>
    <published>2020-04-17T16:00:00.000Z</published>
    <updated>2020-07-03T08:39:21.739Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，讲解了Deep Learning中使用的一种高效Gradient Descent的算法：BackPropagation。<br>BackPropagation通过正向传播和反向传播两个阶段，最后能一起算出损失函数对每一个参数的gradient。</p><a id="more"></a><h1 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h1><p>在Neural Network中，参数的更新也是通过Gradient Descent。</p><p>但是当Neural Network层数很深，结构很复杂的时候，会有millions of parapmeters。</p><p>Backpropagation：To compute the gradient efficiently.</p><h2 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h2><p>BP中需要用到的数学知识：微积分中的链式法则。</p><p><a href="https://imgchr.com/i/Jmc7z4"><img src="https://s1.ax1x.com/2020/04/18/Jmc7z4.md.png" alt="Jmc7z4.md.png"></a> </p><h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><p><a href="https://imgchr.com/i/JmcTWF"><img src="https://s1.ax1x.com/2020/04/18/JmcTWF.md.png" alt="JmcTWF.md.png"></a> </p><p>在NN中，定义损失函数 $L(\theta)=\sum_{n=1}^{N} C^{n}(\theta)$ （$\theta$ 代指NN中所有的weight 和bias）</p><p>对某一参数的gradient为  $\frac{\partial L(\theta)}{\partial w}=\sum_{n=1}^{N} \frac{\partial C^{n}(\theta)}{\partial w}$ </p><p><a href="https://imgchr.com/i/JmcoJU"><img src="https://s1.ax1x.com/2020/04/18/JmcoJU.md.png" alt="JmcoJU.md.png"></a> </p><p>在上图NN中，我们先只研究红框部分，即是以下结构：</p><p><a href="https://imgchr.com/i/JmcIiT"><img src="https://s1.ax1x.com/2020/04/18/JmcIiT.md.png" alt="JmcIiT.md.png"></a> </p><p>z：每个activation function的输入。</p><p>根据链式法则， $\frac{\partial C}{\partial w}= \frac{\partial z}{\partial w} \frac{\partial C}{\partial z}$  .</p><p>要计算每个参数的  $\frac{\partial C}{\partial w}$  ，分为两部分。</p><ol><li><u>Forward pass:</u>  compute $\frac{\partial z}{\partial w} $ for all parameters.</li><li><u>Backward pass:</u>  compute $\frac{\partial C}{\partial z} $ for all activation function inputs z.</li></ol><h2 id="BP：Forward-pass"><a href="#BP：Forward-pass" class="headerlink" title="BP：Forward pass"></a>BP：Forward pass</h2><p><strong>Compute $\frac{\partial z}{\partial w} $ for all parameters.</strong></p><p><a href="https://imgchr.com/i/Jmchd0"><img src="https://s1.ax1x.com/2020/04/18/Jmchd0.md.png" alt="Jmchd0.md.png"></a> </p><p>还是只看上图这一部分，可以轻易得出： $\partial{z}/\partial{w_1}=x_1\qquad \partial{z}/\partial{w_2}=x_2$  </p><p>得到结论： $\frac{\partial z}{\partial w} $  等于 the value of the input connected by the weight. </p><p>【$\frac{\partial z}{\partial w} $ 等于 连接w的输入的值】</p><hr><p>那么，如何计算出NN中全部的 $\frac{\partial z}{\partial w} $ ？</p><p><a href="https://imgchr.com/i/Jmc4oV"><img src="https://s1.ax1x.com/2020/04/18/Jmc4oV.md.png" alt="Jmc4oV.md.png"></a> </p><p>：Forward pass.</p><p>用当前参数（w,b)</p><p>从hidden layer的第一层开始，计算出第一层的输出，即第二层的输入。</p><p>依次相前计算，计算出每一层的输出，即下一层的输入，即输入所连接权重的 $\frac{\partial z}{\partial w}$ 。</p><h2 id="BP：Backward-pass"><a href="#BP：Backward-pass" class="headerlink" title="BP：Backward pass"></a>BP：Backward pass</h2><p><strong>Compute $\frac{\partial C}{\partial z} $ for all activation function inputs z.</strong> </p><p><a href="https://imgchr.com/i/JmcfZq"><img src="https://s1.ax1x.com/2020/04/18/JmcfZq.md.png" alt="JmcfZq.md.png"></a> </p><p>z：activation function的 input</p><p>a：activation function的 output</p><p>这里的activation function 是 sigmod函数  $a=\sigma(z)=\frac{1}{1+e^{-z}}$ </p><p>要求  $\frac{\partial C}{\partial z}$  ， 再根据链式法则： $\frac{\partial C}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial C}{\partial a}$ </p><ol><li><p>求  $\frac{\partial{a}}{\partial{z}}$  :   $\frac{\partial{a}}{\partial{z}}=\sigma'(z)=\sigma(z)(1-\sigma(z))$  （是其他activation function 也能轻易求出）</p></li><li><p>求 $\frac{\partial C}{\partial a}$ ：根据链式法则： $\frac{\partial C}{\partial a}=\frac{\partial z^{\prime}}{\partial a} \frac{\partial C}{\partial z^{\prime}}+\frac{\partial z^{\prime \prime}}{\partial a} \frac{\partial C}{\partial z^{\prime \prime}}$  </p><p><a href="https://imgchr.com/i/JmcfZq"><img src="https://s1.ax1x.com/2020/04/18/JmcfZq.md.png" alt="JmcfZq.md.png"></a> </p><ul><li> $\frac{\partial z^{\prime}}{\partial a} =w_3$  ， $\frac{\partial z^{\prime\prime}}{\partial a} =w_4$ </li><li> $\frac{\partial C}{\partial z^{\prime}}$  和 $\frac{\partial C}{\partial z^{\prime\prime}}$ ？假设，已经通过某种方法算出这个值。</li></ul></li><li> $\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_{3} \frac{\partial C}{\partial z^{\prime}}+w_{4} \frac{\partial C}{\partial z^{\prime \prime}}\right]$  <p>这个式子，可以画成一个反向传播的NN，见下图。</p><p><a href="https://imgchr.com/i/JmcRLn"><img src="https://s1.ax1x.com/2020/04/18/JmcRLn.md.png" alt="JmcRLn.md.png"></a> </p> $\frac{\partial C}{\partial z^{\prime}},\frac{\partial C}{\partial z^{\prime\prime}}$  是这个neuron的输入， $w_3,w_4$ 仍然是 neuron的 weight（无bias）。<p>$\sigma’(z)$ 是一个常数，因为在forward pass中每一个activation的输入已经被算出来了。</p><p>和forward pass中的NN的区别是，forward 中是一个activation function，输入z作用于这个函数；</p><p>而在 backward pass中，这更像一个放缩器，将他的输入变小，即乘上一个 $\sigma’(z)$ 。</p></li></ol><hr><p>问题还是如何计算  $\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_{3} \frac{\partial C}{\partial z^{\prime}}+w_{4} \frac{\partial C}{\partial z^{\prime \prime}}\right]$  ？</p><p>分为两种情况讨论， $z’,z’’$ 是否为输出层的输入？</p><ul><li><p>Output Layer：</p><p><a href="https://imgchr.com/i/Jmc2ss"><img src="https://s1.ax1x.com/2020/04/18/Jmc2ss.md.png" alt="Jmc2ss.md.png"></a> </p><p>z’,z’’：activation function的输入。</p><p>y1,y2：actiavtion function（也是NN）的输出。</p><p>C：NN输出和target的cross entropy。</p><p>根据链式法则： $\frac{\partial C}{\partial z^{\prime}}=\frac{\partial y_{1}}{\partial z^{\prime}} \frac{\partial C}{\partial y_{1}} \quad \frac{\partial C}{\partial z^{\prime \prime}}=\frac{\partial y_{2}}{\partial z^{\prime \prime}} \frac{\partial C}{\partial y_{2}}$  </p><p>所以，已知activation function（simod或者其他），可以轻易求出  $\frac{\partial y_{1}}{\partial z^{\prime}}(=\sigma'(z'))$ 和  $\frac{\partial y_{2}}{\partial z^{\prime\prime}}(=\sigma''(z''))$      。</p><p>所以，已知损失函数，也可以轻易求出 $\frac{\partial C}{\partial y_1}$ 和  $\frac{\partial C}{\partial y_2}$  。（  $C\left(y, \hat{y}\right)=-\left[\hat{y} \ln y+\left(1-\hat{y}\right) \ln \left(1-y\right)\right]$ )</p><p>所以，可以直接求出  $\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_{3} \frac{\partial C}{\partial z^{\prime}}+w_{4} \frac{\partial C}{\partial z^{\prime \prime}}\right]$  。</p></li><li><p>Not Output Layer:</p><p><a href="https://imgchr.com/i/JmcsJS"><img src="https://s1.ax1x.com/2020/04/18/JmcsJS.md.png" alt="JmcsJS.md.png"></a> </p><p>上图中，如果我们要计算 $\frac{\partial C}{\partial z’}$ ，必须要已知下一层的 $\frac{\partial C}{\partial z_a}$ ，然后一直递归下去，直到到达最后的输出层，也就是上面一种情况，可以直接计算出，再递归回来，计算当前层的 $\frac{\partial C}{\partial z’}$ 。</p><p>但是，这样计算每个参数的 $\frac{\partial{C}}{\partial{z}}$ 都要一直递归到输出层，效率显然太低了。</p><p><a href="https://imgchr.com/i/JmcyRg"><img src="https://s1.ax1x.com/2020/04/18/JmcyRg.md.png" alt="JmcyRg.md.png"></a>  </p><p>计算方法如上图：</p><p>当我们已知输出层的  $\frac{\partial{C}}{\partial{z'}},\frac{\partial{C}}{\partial{z''}}$  时，再通过上面的步骤3（且的确算出了    $\frac{\partial{C}}{\partial{z'}},\frac{\partial{C}}{\partial{z''}}$ ），画成反向的NN，计算$\frac{\partial{C}}{\partial{z}}$. </p><p>再依次反向传播计算出每一个neuron的输出z （也是正向传播neuron的输入）的 $\frac{\partial{C}}{\partial{z}}$ .</p></li></ul><hr><p><strong>Backforward pass 的做法：</strong></p><p><a href="https://imgchr.com/i/Jmcri8"><img src="https://s1.ax1x.com/2020/04/18/Jmcri8.md.png" alt="Jmcri8.md.png"></a> </p><ol><li>先计算出输出层的 $\frac{\partial{C}}{\partial{z}}$ （也就是上图的  $\frac{\partial{C}}{\partial{z_5}}$ 和 $\frac{\partial{C}}{\partial{z_6}}$  ）</li><li>用反向传播的NN，向后依次计算出每一层每一个neuron的 $\frac{\partial{C}}{\partial{z}}$ 。</li></ol><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><a href="https://imgchr.com/i/JmcgMj"><img src="https://s1.ax1x.com/2020/04/18/JmcgMj.md.png" alt="JmcgMj.md.png"></a> </p><p>公式：  $\frac{\partial z}{\partial w} \frac{\partial C}{\partial z}=\frac{\partial C}{\partial w}$ </p><p>在正向传播NN中，z是neuron的activation function的输入。</p><p>在反向传播NN中，z是neuron的放缩器的输出。</p><p>通过Forward Pass计算出正向传播NN的每一个neuron的 $\frac{\partial z}{\partial w}$ ，等于该层neuron的输入。</p><p>通过Backward Pass计算出反向传播NN的每一个neuron的 $\frac{\partial C}{\partial z}$ 。</p><p>然后，通过相乘，计算出每个参数的 $\frac{\partial C}{\partial w}$。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，讲解了Deep Learning中使用的一种高效Gradient Descent的算法：BackPropagation。&lt;br&gt;BackPropagation通过正向传播和反向传播两个阶段，最后能一起算出损失函数对每一个参数的gradient。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f1ed.github.io/tags/DeepLearning/"/>
    
      <category term="Backpropagation" scheme="https://f1ed.github.io/tags/Backpropagation/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Deep Learning-Introduction</title>
    <link href="https://f1ed.github.io/2020/04/18/DL-introdunction/"/>
    <id>https://f1ed.github.io/2020/04/18/DL-introdunction/</id>
    <published>2020-04-17T16:00:00.000Z</published>
    <updated>2020-07-03T08:41:15.486Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，介绍了Deep Learning的一般步骤。</p><a id="more"></a><h1 id="Up-and-downs-of-Deep-Learning"><a href="#Up-and-downs-of-Deep-Learning" class="headerlink" title="Up and downs of Deep Learning"></a>Up and downs of Deep Learning</h1><ul><li><p>1958: Perceptron (linear model)</p></li><li><p>1969: Perceptron has limitation</p></li><li><p>1980s: Multi-layer perceptron </p><p>​            Do not have significant difference from DNN today</p></li><li><p>1986: Backpropagation</p><p>​            Usually more than 3 hidden layers is not helpful</p></li><li><p>1989: 1 hidden layer is “good enough”, why deep?</p></li><li><p>2006: RBM initialization (breakthrough) </p></li><li><p>2009: GPU</p></li><li><p>2011: Start to be popular in speech recognition【语音辨识】</p></li><li><p>2012: win ILSVRC image competition 【图像识别】</p></li></ul><h1 id="Step-1-Neural-Network"><a href="#Step-1-Neural-Network" class="headerlink" title="Step 1: Neural Network"></a>Step 1: Neural Network</h1><p>在将Regression 和 Classification时，Step 1 是确定一个function set。</p><p>在Deep Learning中，也是相同的，只是这里的function set就是一个neural network的结构。</p><p><a href="https://imgchr.com/i/JmFnne"><img src="https://s1.ax1x.com/2020/04/18/JmFnne.md.png" alt="JmFnne.md.png"></a> </p><p>上图中，一个Neuron就是如上图所示的一个unit，neuron之间不同的连接方式构成不同的Neural Network。</p><h2 id="Fully-Connect-Feedforward-Network"><a href="#Fully-Connect-Feedforward-Network" class="headerlink" title="Fully Connect Feedforward Network"></a>Fully Connect Feedforward Network</h2><p><a href="https://imgchr.com/i/JmFV1K"><img src="https://s1.ax1x.com/2020/04/18/JmFV1K.md.png" alt="JmFV1K.md.png"></a> </p><p>这是一个Fully Connect Feedforward Network【全连接反馈网络】，其中每个neuron的activation function都是一个sigmod函数。</p><p><a href="https://imgchr.com/i/JmFeXD"><img src="https://s1.ax1x.com/2020/04/18/JmFeXD.md.png" alt="JmFeXD.md.png"></a>  </p><p>为什么说neural network其实就是一个function呢？上面两张图中，输入是一个vector，输出也是一个vector，可以用下面函数来表示。</p>$$f\left(\left[\begin{array}{c}1 \\ -1\end{array}\right]\right)=\left[\begin{array}{c}0.62 \\ 0.83\end{array}\right] f\left(\left[\begin{array}{l}0 \\ 0\end{array}\right]\right)=\left[\begin{array}{l}0.51 \\ 0.85\end{array}\right]$$<p><a href="https://imgchr.com/i/JmFZ6O"><img src="https://s1.ax1x.com/2020/04/18/JmFZ6O.md.png" alt="JmFZ6O.md.png"></a> </p><p>上图为全连接网络的一般形式，第一层是Input Layer，最后一层是Output Layer，中间的其他层称为Hidden Layer。</p><p>而Deep Learning中的Deep的含义就是Many hidden layers的意思。</p><h2 id="Matrix-Operation"><a href="#Matrix-Operation" class="headerlink" title="Matrix Operation"></a>Matrix Operation</h2><p><a href="https://imgchr.com/i/JmFkfx"><img src="https://s1.ax1x.com/2020/04/18/JmFkfx.md.png" alt="JmFkfx.md.png"></a> </p><p>上图的全连接网络中，第一个hidden layer的输出可以写成矩阵和向量的形式：</p>$$\sigma\left(\left[\begin{array}{cc}1 & -2 \\ -1 & 1\end{array}\right]\left[\begin{array}{c}1 \\ -1\end{array}\right]+\left[\begin{array}{c}1 \\ 0\end{array}\right]\right)=\left[\begin{array}{c}0.98 \\ 0.12\end{array}\right]$$<p><a href="https://imgchr.com/i/JmFEp6"><img src="https://s1.ax1x.com/2020/04/18/JmFEp6.md.png" alt="JmFEp6.md.png"></a> </p><p>更为一般的公式，用W表示权重，b代表bias，a表示hidden layer的输出。输出vector y可以写成 $y = f(x)$ 的形式，即： $y= f(x)=$</p><p><a href="https://imgchr.com/i/JmFFt1"><img src="https://s1.ax1x.com/2020/04/18/JmFFt1.md.png" alt="JmFFt1.md.png"></a> </p><p>转换为矩阵运算的形式，就可以使用并行计算的硬件技术（GPU）来加速矩阵运算，这也是为什么用GPU来训练Neural Network 更快的原因。</p><h2 id="Output-Layer"><a href="#Output-Layer" class="headerlink" title="Output Layer"></a>Output Layer</h2><p>在 <a href="/2020/04/01/Classification2/" title="Logistic Regression">Logistic Regression</a>中第4节讲到Logistic Regression有局限，消除局限的一种方法是Feature Transformation。</p><p>但是Feature Transformation需要人工设计，不太“机器学习”。</p><p>在下图全连接图中，把Output Layer换成一个Multi-class Classifier（SoftMax），而其中Hidden Layers的作用就是Feature extractor，从feature x提取出新的feature，也就是 output layer的输入。</p><p><a href="https://imgchr.com/i/JmFpm4"><img src="https://s1.ax1x.com/2020/04/18/JmFpm4.md.png" alt="JmFpm4.md.png"></a> </p><p>这样就不需要人工设计Feature Transformation/Feature engineering，可以让机器自己学习：如何将原来的feature转换为更好分类的feature。</p><hr><p><strong>Handwriting Digit Recognition</strong> </p><p><a href="https://imgchr.com/i/Jmix6U"><img src="https://s1.ax1x.com/2020/04/18/Jmix6U.md.png" alt="Jmix6U.md.png"></a> </p><p>在手写数字辨别中，输出是一个16*16的image（256维的vector），输出是一个10维的vector，每一维表示是该image是某个数字的概率。</p><p>在手写数字辨别中，需要设计neural network的结构来提取输入的256维feature。</p><p><a href="https://imgchr.com/i/JmFC79"><img src="https://s1.ax1x.com/2020/04/18/JmFC79.md.png" alt="JmFC79.md.png"></a> </p><h1 id="Step-2-Goodness-of-function"><a href="#Step-2-Goodness-of-function" class="headerlink" title="Step 2: Goodness of function"></a>Step 2: Goodness of function</h1><p>之前我们已经使用过的最小二乘法和交叉熵作为损失函数。</p><p>一般在Neural Network中，使用output vector 和target vector的交叉熵作为Loss。</p><h1 id="Step-3-Pick-the-best-function"><a href="#Step-3-Pick-the-best-function" class="headerlink" title="Step 3: Pick the best function"></a>Step 3: Pick the best function</h1><p>在NN中，也使用Gradient Descent。</p><p><a href="https://imgchr.com/i/JmF90J"><img src="https://s1.ax1x.com/2020/04/18/JmF90J.md.png" alt="JmF90J.md.png"></a> </p><p>但是，Deep Neural Network中，参数太多了，计算结构也很复杂。</p><p>Backpropagation：an efficient way to compute $\partial{L}/\partial{w}$ in neural network.</p><p>Backpropagation本质也是Gradient Descent，只是一种更高效进行Gradient Descent的算法。</p><p>在很多 toolkit（TensorFlow，PyTorch ，Caffe等）中都实现了Backpropgation。</p><p>Backpropagation部分，见<a href="/2020/04/18/Backpropagation/" title="下一篇博客">下一篇博客</a>。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，介绍了Deep Learning的一般步骤。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f1ed.github.io/tags/DeepLearning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：HW2-Binary Income Predicting</title>
    <link href="https://f1ed.github.io/2020/04/15/ml-lee-hw2/"/>
    <id>https://f1ed.github.io/2020/04/15/ml-lee-hw2/</id>
    <published>2020-04-14T16:00:00.000Z</published>
    <updated>2020-07-03T08:42:48.419Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。<br>包括对数据集的处理，训练模型，可视化，预测等。<br>有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的<a href="https://github.com/f1ed/ML-HW2">GitHub</a></p><a id="more"></a><h1 id="Task-introduction-and-Dataset"><a href="#Task-introduction-and-Dataset" class="headerlink" title="Task introduction and Dataset"></a>Task introduction and Dataset</h1><p> Kaggle competition: <a href="https://www.kaggle.com/c/ml2020spring-hw2">link</a> </p><p><strong>Task: Binary Classification</strong></p><p>Predict whether the income of an individual exceeds $50000 or not ?</p><p>*<em>Dataset: *</em> Census-Income (KDD) Dataset</p><p>(Remove unnecessary attributes and balance the ratio between positively and negatively labeled data)</p><h1 id="Feature-Format"><a href="#Feature-Format" class="headerlink" title="Feature Format"></a>Feature Format</h1><ul><li><p>train.csv, test_no_label.csv【都是没有处理过的数据，可作为数据参考和优化参考】</p><ul><li><p>text-based raw data</p></li><li><p>unnecessary attributes removed, positive/negative ratio balanced.</p></li></ul></li><li><p>X_train, Y_train, X_test【已经处理过的数据，可以直接使用】</p><ul><li><p>discrete features in train.csv =&gt; one-hot encoding in X_train (education, martial state…)</p></li><li><p>continuous features in train.csv =&gt; remain the same in X_train (age, capital losses…).</p></li><li><p>X_train, X_test : each row contains one 510-dim feature represents a sample.</p></li><li><p>Y_train: label = 0 means “&lt;= 50K” 、 label = 1 means “ &gt;50K ”</p></li></ul></li></ul><p>注：数据集超大，用notepad查看比较舒服；调试时，也可以先调试小一点的数据集。</p><h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><p>Logistic Regression 原理部分见<a href="/2020/04/01/Classification2/" title="这篇博客">这篇博客</a>。</p><h2 id="Prepare-data"><a href="#Prepare-data" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>本文直接使用X_train Y_train X_test 已经处理好的数据集。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_Train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./logistic_output/output_logistic.csv'</span></span><br><span class="line">fpath = <span class="string">'./logistic_output/logistic'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br></pre></td></tr></table></figure><p>统计一下数据集：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In logistic model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of Training set: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of development set: &#123;&#125;\n'</span>.format(dev_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n'</span>.format(data_dim))</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">In</span> logistic model:</span><br><span class="line">Size of Training set: <span class="number">48830</span></span><br><span class="line">Size of development set: <span class="number">5426</span></span><br><span class="line">Size of test set: <span class="number">27622</span></span><br><span class="line">Dimension of <span class="keyword">data</span>: <span class="number">510</span></span><br></pre></td></tr></table></figure><h3 id="normalize"><a href="#normalize" class="headerlink" title="normalize"></a>normalize</h3><p>normalize data.</p><p>对于train data，计算出每个feature的mean和std，保存下来用来normalize test data。</p><p>代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br></pre></td></tr></table></figure><h3 id="Development-set-split"><a href="#Development-set-split" class="headerlink" title="Development set split"></a>Development set split</h3><p>在logistic regression中使用的gradient，没有closed-form解，所以在train set中划出一部分作为development set 优化参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_dev_split</span><span class="params">(X, Y, dev_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br></pre></td></tr></table></figure><h2 id="Useful-function"><a href="#Useful-function" class="headerlink" title="Useful function"></a>Useful function</h2><h3 id="shuffle-X-Y"><a href="#shuffle-X-Y" class="headerlink" title="_shuffle(X, Y)"></a>_shuffle(X, Y)</h3><p>本文使用mini-batch gradient。</p><p>所以在每次epoch时，以相同顺序同时打乱X_train,Y_train数组，再mini-batch。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br></pre></td></tr></table></figure><h3 id="sigmod-z"><a href="#sigmod-z" class="headerlink" title="_sigmod(z)"></a>_sigmod(z)</h3><p>计算 $\frac{1}{1+e^{-z}}$ ，注意：防止溢出，给函数返回值规定上界和下界。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br></pre></td></tr></table></figure><h3 id="f-X-w-b"><a href="#f-X-w-b" class="headerlink" title="_f(X, w, b)"></a>_f(X, w, b)</h3><p>是sigmod函数的输入，linear part。</p><ul><li>输入：<ul><li>X：shape = [size, data_dimension]</li><li>w：weight vector, shape = [data_dimension, 1]</li><li>b: bias, scalar</li></ul></li><li>输出：<ul><li>属于Class 1的概率（Label=0，即收入小于$50k的概率）</li></ul></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br></pre></td></tr></table></figure><h3 id="predict-X-w-b"><a href="#predict-X-w-b" class="headerlink" title="_predict(X, w, b)"></a>_predict(X, w, b)</h3><p>预测Label=0？（0或者1，不是概率）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br></pre></td></tr></table></figure><h3 id="accuracy-Y-pred-Y-label"><a href="#accuracy-Y-pred-Y-label" class="headerlink" title="_accuracy(Y_pred, Y_label)"></a>_accuracy(Y_pred, Y_label)</h3><p>计算预测出的结果（0或者1）和真实结果的正确率。</p><p>这里使用 $1-\overline{error}$ 来表示正确率。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure><h3 id="cross-entropy-loss-y-pred-Y-label"><a href="#cross-entropy-loss-y-pred-Y-label" class="headerlink" title="_cross_entropy_loss(y_pred, Y_label)"></a>_cross_entropy_loss(y_pred, Y_label)</h3><p>计算预测出的概率（是sigmod的函数输出）和真实结果的交叉熵。</p><p>计算公式为： $\sum_n {C(y_{pred},Y_{label})}=-\sum[Y_{label}\ln{y_{pred}}+(1-Y_{label})\ln(1-{y_{pred}})]$ </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="gradient-X-Y-label-w-b"><a href="#gradient-X-Y-label-w-b" class="headerlink" title="_gradient(X, Y_label, w, b)"></a>_gradient(X, Y_label, w, b)</h3><p>和Regression的最小二乘一样。（严谨的说，最多一个系数不同）</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">def</span> <span class="string">_gradient(X, Y_label, w, b):</span></span><br><span class="line"><span class="comment">    # This function calculates the gradient of cross entropy</span></span><br><span class="line"><span class="comment">    # X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    <span class="attr">y_pred</span> = <span class="string">_f(X, w, b)</span></span><br><span class="line">    <span class="attr">pred_error</span> = <span class="string">Y_label - y_pred</span></span><br><span class="line">    <span class="attr">w_grad</span> = <span class="string">- np.dot(X.T, pred_error)</span></span><br><span class="line">    <span class="attr">b_grad</span> = <span class="string">- np.sum(pred_error)</span></span><br><span class="line">    <span class="attr">return</span> <span class="string">w_grad, float(b_grad)</span></span><br></pre></td></tr></table></figure><h2 id="Training-Adagrad"><a href="#Training-Adagrad" class="headerlink" title="Training (Adagrad)"></a>Training (Adagrad)</h2><p>初始化一些参数。</p><p><strong>这里特别注意</strong> :</p><p>由于adagrad的参数更新是 $w \longleftarrow w-\eta \frac{gradient}{ \sqrt{gradsum}}$ .</p><p><strong>防止除0</strong>，初始化gradsum的值为一个较小值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.float(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.float(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br></pre></td></tr></table></figure><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Aagrad具体原理见<a href="/2020/03/01/Gradient/" title="这篇博客">这篇博客</a>的1.2节。</p><p>迭代更新时，每次epoch计算一次loss和accuracy，以便可视化更新过程，调整参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> range(int(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br></pre></td></tr></table></figure><h3 id="Loss-amp-accuracy"><a href="#Loss-amp-accuracy" class="headerlink" title="Loss &amp; accuracy"></a>Loss &amp; accuracy</h3><p>输出最后一次迭代的loss和accuracy。</p><p>结果如下：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Training loss: <span class="number">0.2933570286596322</span></span><br><span class="line">Training accuracy: <span class="number">0.8839238173254147</span></span><br><span class="line">Development loss: <span class="number">0.31029505347634456</span></span><br><span class="line">Development accuracy: <span class="number">0.8336166253549906</span></span><br></pre></td></tr></table></figure><p>画出loss 和 accuracy的更新过程：</p><p>loss：</p><p><a href="https://imgchr.com/i/JPCjx0"><img src="https://s1.ax1x.com/2020/04/15/JPCjx0.png" alt="JPCjx0.png"></a> </p><p>accuracy：</p><p><a href="https://imgchr.com/i/JPCxMV"><img src="https://s1.ax1x.com/2020/04/15/JPCxMV.png" alt="JPCxMV.png"></a> </p><p>由于Feature数量较大，将权重影响最大的feature输出看看：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Other Rel &lt;<span class="number">18</span> spouse of subfamily RP: [<span class="number">7.11323764</span>]</span><br><span class="line"> Grandchild &lt;<span class="number">18</span> ever marr not <span class="keyword">in</span> subfamily: [<span class="number">6.8321061</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> ever marr RP of subfamily: [<span class="number">6.77322397</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> ever marr RP of subfamily: [<span class="number">6.76688406</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> never married RP of subfamily: [<span class="number">6.37488958</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> spouse of subfamily RP: [<span class="number">5.97717831</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.53932651</span>]</span><br><span class="line"> Grandchild <span class="number">18</span>+ spouse of subfamily RP: [<span class="number">5.42948497</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.41543809</span>]</span><br><span class="line"> Mexico: [<span class="number">4.79920763</span>]</span><br></pre></td></tr></table></figure><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>完整数据集、代码等，欢迎光临小透明<a href="https://github.com/f1ed/ML-HW2">GitHub</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################</span></span><br><span class="line"><span class="comment"># Data:2020-04-05</span></span><br><span class="line"><span class="comment"># Author: Fred Lau</span></span><br><span class="line"><span class="comment"># ML-Lee: HW2 : Binary Classification</span></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################</span></span><br><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_Train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./logistic_output/output_logistic.csv'</span></span><br><span class="line">fpath = <span class="string">'./logistic_output/logistic'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_dev_split</span><span class="params">(X, Y, dev_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In logistic model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of Training set: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of development set: &#123;&#125;\n'</span>.format(dev_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################</span></span><br><span class="line"><span class="comment"># useful function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to calculate probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - (<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This is the logistic function, parameterized by w and b</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [batch_size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       predict probability of each row of X being positively labeled, shape = [batch_size, 1]</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This fucntion returns a truth value prediction for each row of X by logistic regression</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradient</span><span class="params">(X, Y_label, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the gradient of cross entropy</span></span><br><span class="line">    <span class="comment"># X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    y_pred = _f(X, w, b)</span><br><span class="line">    pred_error = Y_label - y_pred</span><br><span class="line">    w_grad = - np.dot(X.T, pred_error)</span><br><span class="line">    b_grad = - np.sum(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, float(b_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#######################################</span></span><br><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.float(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.float(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> range(int(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'Training loss: &#123;&#125;\n'</span>.format(train_loss[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Training accuracy: &#123;&#125;\n'</span>.format(train_acc[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Development loss: &#123;&#125;\n'</span>.format(dev_loss[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Development accuracy: &#123;&#125;\n'</span>.format(dev_acc[<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###################</span></span><br><span class="line"><span class="comment"># Plotting Loss and accuracy curve</span></span><br><span class="line"><span class="comment"># Loss curve</span></span><br><span class="line">plt.plot(train_loss, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(dev_loss, label=<span class="string">'dev'</span>)</span><br><span class="line">plt.title(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'./logistic_output/loss.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(train_acc, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(dev_acc, label=<span class="string">'dev'</span>)</span><br><span class="line">plt.title(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'./logistic_output/acc.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id, label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> id, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;, &#123;&#125;\n'</span>.format(id, label[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line"><span class="comment"># Output the weights and bias</span></span><br><span class="line">ind = (np.argsort(np.abs(w), axis=<span class="number">0</span>)[::<span class="number">-1</span>]).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>, <span class="number">0</span>: <span class="number">10</span>]:</span><br><span class="line">       f.write(<span class="string">'&#123;&#125;: &#123;&#125;\n'</span>.format(content[i], w[i]))</span><br></pre></td></tr></table></figure><h1 id="Generative-Model"><a href="#Generative-Model" class="headerlink" title="Generative Model"></a>Generative Model</h1><p>Generative Model 原理部分见 <a href="/2020/03/21/Classification1/" title="这篇博客">这篇博客</a></p><h2 id="Prepare-data-1"><a href="#Prepare-data-1" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>这部分和Logistic regression一样。</p><p>只是，因为generative model有closed-form solution，不需要划分development set。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./generative_output/output_&#123;&#125;.csv'</span></span><br><span class="line">fpath = <span class="string">'./generative_output/generative'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In generative model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of training data: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n\n'</span>.format(data_dim))</span><br></pre></td></tr></table></figure><h2 id="Useful-functions"><a href="#Useful-functions" class="headerlink" title="Useful functions"></a>Useful functions</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="公式再推导"><a href="#公式再推导" class="headerlink" title="公式再推导"></a>公式再推导</h3><p>计算公式： </p>$$\begin{equation}\begin{aligned}P\left(C_{1} | x\right)&=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}\\&=\frac{1}{1+\frac{P\left(x | C_{2}\right) P\left(C_{2}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)}}\\&=\frac{1}{1+\exp (-z)} =\sigma(z)\qquad(z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}\end{aligned}\end{equation}$$<p>计算z的过程：</p><ol><li>首先计算Prior Probability。</li><li>假设模型是Gaussian的，算出 $\mu_1,\mu_2 ,\Sigma$  的closed-form solution 。</li><li>根据 $\mu_1,\mu_2,\Sigma$ 计算出 $w,b$ 。</li></ol><hr><ol><li><p><strong>计算Prior Probability。</strong> </p><p>程序中用list comprehension处理较简单。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br></pre></td></tr></table></figure></li><li><p>计算 $\mu_1,\mu_2 ,\Sigma$ （Gaussian）</p><p>$\mu_0=\frac{1}{C0} \sum_{n=1}^{C0} x^{n} $  (Label=0)</p><p>$\mu_1=\frac{1}{C1} \sum_{n=1}^{C1} x^{n} $  (Label=0)</p><p>$\Sigma_0=\frac{1}{C0} \sum_{n=1}^{C0}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$  (<strong>注意</strong> ：这里的 $x^n,\mu$ 都是行向量，注意转置的位置）</p><p>$\Sigma_1=\frac{1}{C1} \sum_{n=1}^{C1}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$ </p><p>$\Sigma=(C0 \times\Sigma_0+C1\times\Sigma_1)/(C0+C1)$   (shared covariance) </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>计算 $w,b$ </p><p>在 <a href="/2020/03/21/Classification1/" title="这篇博客">这篇博客</a>中的第2小节中的公式推导中， $x^n,\mu$ 都是列向量，公式如下：</p>   $$   z=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} x-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}   $$     $w^T=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} \qquad b=-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}$ <hr><p><strong>但是</strong> ，一般我们在处理的数据集，$x^n,\mu$ 都是行向量。推导过程相同，公式如下：</p><p><font color=#f00> <strong>（主要注意转置和矩阵乘积顺序）</strong> </font></p>   $$   z=x\cdot \Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  -\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}   $$     $w=\Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  \qquad b=-\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}$ </li></ol><hr><p><font color=#f00>但是，协方差矩阵的逆怎么求呢？ </font> </p><p>numpy中有直接求逆矩阵的方法(np.linalg.inv)，但当该矩阵是nearly singular，是奇异矩阵时，就会报错。</p><p>而我们的协方差矩阵（510*510）很大，很难保证他不是奇异矩阵。</p><p>于是，有一个 <del>牛逼</del> 强大的数学方法，叫SVD(singular value decomposition, 奇异值分解) 。</p><p>原理步骤我……还没有完全搞清楚QAQ（先挖个坑）[1]</p><p>利用SVD，可以将任何一个矩阵（即使是奇异矩阵），分界成 $A=u s v^T$ 的形式：其中u,v都是标准正交矩阵，s是对角矩阵。（numpy.linalg.svd方法实现了SVD）</p><p><font color=#f00>可以利用SVD求矩阵的伪逆 </font> </p><ul><li>$A=u s v^T$<ul><li>u,v是标准正交矩阵，其逆矩阵等于其转置矩阵</li><li>s是对角矩阵，其”逆矩阵“<strong>（注意s矩阵的对角也可能有0元素）</strong> 将非0元素取倒数即可。</li></ul></li><li>$A^{-1}=v s^{-1} u$</li></ul><p>计算 $w,b$ 的代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><p> accuracy结果：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">0.8756450899439694</span></span><br></pre></td></tr></table></figure><p>也将权重较大的feature输出看看：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">age: [-<span class="number">0.51867291</span>]</span><br><span class="line"> Masters degree(MA MS MEng MEd MSW MBA): [-<span class="number">0.49912643</span>]</span><br><span class="line"> Spouse of householder: [<span class="number">0.49786805</span>]</span><br><span class="line">weeks worked <span class="keyword">in</span> year: [-<span class="number">0.44710924</span>]</span><br><span class="line"> Spouse of householder: [-<span class="number">0.43305697</span>]</span><br><span class="line">capital gains: [-<span class="number">0.42608727</span>]</span><br><span class="line">dividends from stocks: [-<span class="number">0.41994666</span>]</span><br><span class="line"> Doctorate degree(PhD EdD): [-<span class="number">0.39310961</span>]</span><br><span class="line">num persons worked <span class="keyword">for</span> employer: [-<span class="number">0.37345994</span>]</span><br><span class="line"> Prof school degree (MD DDS DVM LLB JD): [-<span class="number">0.35594107</span>]</span><br></pre></td></tr></table></figure><h2 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h2><p>具体数据集和代码，欢迎光临小透明<a href="https://github.com/f1ed/ML-HW2">GitHub</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment">##############################################</span></span><br><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./generative_output/output_&#123;&#125;.csv'</span></span><br><span class="line">fpath = <span class="string">'./generative_output/generative'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In generative model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of training data: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n\n'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment">########################</span></span><br><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"><span class="comment">#######################</span></span><br><span class="line"><span class="comment"># Generative Model: closed-form solution, can be computed directly</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute accuracy on training set</span></span><br><span class="line">Y_train_pred = <span class="number">1</span> - _predict(X_train, w, b)</span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'\nTraining accuracy: &#123;&#125;\n'</span>.format(_accuracy(Y_train_pred, Y_train)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = <span class="number">1</span> - _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath.format(<span class="string">'generative'</span>), <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id, label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;, &#123;&#125;\n'</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output the most significant weight</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">ind = np.argsort(np.abs(np.concatenate(w)))[::<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>)<span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>:<span class="number">10</span>]:</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;: &#123;&#125;\n'</span>.format(content[i], w[i]))</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>SVD原理，待补充</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。&lt;br&gt;包括对数据集的处理，训练模型，可视化，预测等。&lt;br&gt;有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的&lt;a href=&quot;https://github.com/f1ed/ML-HW2&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="Classification" scheme="https://f1ed.github.io/tags/Classification/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:HW1-Predict PM2.5</title>
    <link href="https://f1ed.github.io/2020/04/06/ml-lee-hw1/"/>
    <id>https://f1ed.github.io/2020/04/06/ml-lee-hw1/</id>
    <published>2020-04-05T16:00:00.000Z</published>
    <updated>2020-07-03T08:42:35.767Z</updated>
    
    <content type="html"><![CDATA[<p>在本篇文章中，用手刻Adagrad完成了「机器学习-李宏毅」的HW1-预测PM2.5的作业。其中包括对数据的处理，训练模型，预测，并使用sklearn toolkit的结果进行比较。<br>有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的<a href="https://github.com/f1ed/ML-HW1">GitHub</a>  </p><a id="more"></a><h1 id="Task-Description"><a href="#Task-Description" class="headerlink" title="Task Description"></a>Task Description</h1><p><a href="https://www.kaggle.com/c/ml2020spring-hw1">kaggle link</a> </p><p>从中央气象局网站下载的真实观测资料，必须利用linear regression或其他方法预测PM2.5的值。</p><p>观测记录被分为train set 和 test set, 前者是每个月前20天所有资料；后者是从剩下的资料中随机取样出来的。</p><p>train.csv: 每个月前20天的完整资料。</p><p>test.csv: 从剩下的10天资料中取出240笔资料，每一笔资料都有连续9小时的观测数据，必须以此观测出第十小时的PM2.5.</p><h1 id="Process-Data"><a href="#Process-Data" class="headerlink" title="Process Data"></a>Process Data</h1><p>train data如下图，每18行是一天24小时的数据，每个月取了前20天（时间上是连续的小时）。</p><p><a href="https://imgchr.com/i/GyqsyR"><img src="https://s1.ax1x.com/2020/04/06/GyqsyR.md.png" alt="GyqsyR.md.png"></a> </p><p>test data 如下图，每18行是一笔连续9小时的数据，共240笔数据。</p><p><a href="https://imgchr.com/i/Gyqcex"><img src="https://s1.ax1x.com/2020/04/06/Gyqcex.md.png" alt="Gyqcex.md.png"></a> </p><hr><ol><li><p><strong>最大化training data size</strong></p><p>每连续10小时的数据都是train set的data。为了得到更多的data，应该把每一天连起来。即下图这种效果：</p><p><a href="https://imgchr.com/i/GyqyO1"><img src="https://s1.ax1x.com/2020/04/06/GyqyO1.md.png" alt="GyqyO1.md.png"></a> </p><p>每个月就有： $20*24-9=471$ 笔data</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dictionary: key:month value:month data</span></span><br><span class="line">month_data = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># make data timeline continuous</span></span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    temp = np.empty(shape=(<span class="number">18</span>, <span class="number">20</span>*<span class="number">24</span>))</span><br><span class="line">    <span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        temp[:, day*<span class="number">24</span>: (day+<span class="number">1</span>)*<span class="number">24</span>] = data[(month*<span class="number">20</span>+day)*<span class="number">18</span>: (month*<span class="number">20</span>+day+<span class="number">1</span>)*<span class="number">18</span>, :]</span><br><span class="line">    month_data[month] = temp</span><br></pre></td></tr></table></figure></li><li><p><strong>筛选需要的Features</strong> :</p><p>这里，我就只考虑前9小时的PM2.5，当然还可以考虑和PM2.5等相关的氮氧化物等feature。</p><p><strong>training data</strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># x_data v1: only consider PM2.5</span><br><span class="line">x_data &#x3D; np.empty(shape&#x3D;(12*471, 9))</span><br><span class="line">y_data &#x3D; np.empty(shape&#x3D;(12*471, 1))</span><br><span class="line">for month in range(12):</span><br><span class="line">    for i in range(471):</span><br><span class="line">        x_data[month*471+i][:] &#x3D; month_data[month][9][i: i+9]</span><br><span class="line">        y_data[month*471+i] &#x3D; month_data[month][9][i+9]</span><br></pre></td></tr></table></figure><p><strong>testing data</strong> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Testing data features</span></span><br><span class="line">test_x = np.empty(shape=(<span class="number">240</span>, <span class="number">9</span>))</span><br><span class="line"><span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">240</span>):</span><br><span class="line">    test_x[day, :] = test_data[<span class="number">18</span>*day+<span class="number">9</span>, :]</span><br><span class="line">test_x = np.concatenate((np.ones(shape=(<span class="number">240</span>, <span class="number">1</span>)), test_x), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Normalization</strong> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature scale: normalization</span></span><br><span class="line">mean = np.mean(x_data, axis=<span class="number">0</span>)</span><br><span class="line">std = np.std(x_data, axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x_data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(x_data.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[j] != <span class="number">0</span>:</span><br><span class="line">            x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(test_data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(test_data.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[j] != <span class="number">0</span>:</span><br><span class="line">            test_data[i][j] = (test_data[i][j] - mean[j])/std[j]</span><br></pre></td></tr></table></figure></li></ol><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><p>手刻Adagrad 进行training。（挖坑：RMSprop、Adam[1]</p><p><strong>Linear Pseudo code</strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Declare weight vector, initial lr ,and # of iteration</span><br><span class="line">for i_th iteration :</span><br><span class="line"> y’ &#x3D; the product of train_x  and weight vector</span><br><span class="line"> Loss &#x3D; y’ - train_y</span><br><span class="line"> gradient &#x3D; 2*np.dot((train_x)’, Loss )</span><br><span class="line">   weight vector -&#x3D; learning rate * gradient</span><br></pre></td></tr></table></figure><p>其中的矩阵操作时，注意求gradient时矩阵的维度。可参考下图。</p><p><a href="https://imgchr.com/i/Gyqgw6"><img src="https://s1.ax1x.com/2020/04/06/Gyqgw6.md.png" alt="Gyqgw6.md.png"></a> </p><p><strong>Adagrad Pseudo code</strong></p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Declare weight vector, initial lr ,and <span class="comment"># of iteration</span></span><br><span class="line">Declare prev_gra storing gradients <span class="keyword">in</span> every previous iterations</span><br><span class="line"> <span class="keyword">for</span> i_th iteration :</span><br><span class="line">  y’ = the inner product of train_x  and weight vector</span><br><span class="line">  Loss = y’ - train_y</span><br><span class="line">  gradient = <span class="number">2</span>*np.dot((train_x)’, Loss )</span><br><span class="line">        prev_gra += gra**<span class="number">2</span></span><br><span class="line"> ada = np.sqrt(prev_gra)</span><br><span class="line">    weight vector -= learning rate * gradient / ada</span><br></pre></td></tr></table></figure><p>注：代码实现时，将bias存在w[0]处，x_data的第0列全1。因为w和b可以一同更新。（当然，也可以分开更新）</p><p><strong>Adagrad training</strong> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train-adagrad</span></span><br><span class="line"></span><br><span class="line">batch = x_data.shape[<span class="number">0</span>]  <span class="comment"># full batch</span></span><br><span class="line">epoch = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># some parameters for training</span></span><br><span class="line">dim = x_data.shape[<span class="number">1</span>]+<span class="number">1</span></span><br><span class="line">w = np.zeros(shape=(dim, <span class="number">1</span>))  <span class="comment"># concatenate bias = w[0]</span></span><br><span class="line">lr = np.full((dim, <span class="number">1</span>), <span class="number">0.8</span>)  <span class="comment"># learning rate</span></span><br><span class="line">grad = np.empty(shape=(dim, <span class="number">1</span>))  <span class="comment"># gradient of loss to every para</span></span><br><span class="line">gradsum = np.zeros(shape=(dim, <span class="number">1</span>))  <span class="comment"># sum of gradient**2</span></span><br><span class="line"></span><br><span class="line">x_data = np.concatenate((np.ones(shape=(x_data.shape[<span class="number">0</span>], <span class="number">1</span>)), x_data), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss_his = np.empty(shape=(epoch, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> T <span class="keyword">in</span> range(epoch):</span><br><span class="line">    L = y_data - np.dot(x_data, w)</span><br><span class="line">    loss_his[T] = np.sum(L**<span class="number">2</span>) / x_data.shape[<span class="number">0</span>]</span><br><span class="line">    grad = (<span class="number">-2</span>)*np.dot(np.transpose(x_data), L)</span><br><span class="line">    gradsum = gradsum + grad**<span class="number">2</span></span><br><span class="line">    w = w - lr*grad/(gradsum**<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><h1 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">answer = np.dot(test_x, w)</span><br></pre></td></tr></table></figure><h1 id="Draw-and-Analysis"><a href="#Draw-and-Analysis" class="headerlink" title="Draw and Analysis"></a>Draw and Analysis</h1><p>在每次迭代更新时，我将Loss的值存了下来，以便可视化Loss的变化和更新速度。</p><p>Loss的变化如下图：(红色的是sklearn toolkit的loss结果)</p><p><a href="https://imgchr.com/i/Gyqrl9"><img src="https://s1.ax1x.com/2020/04/06/Gyqrl9.png" alt="Gyqrl9.png"></a> </p><p>此外，在源代码中，使用sklearn toolkit来比较结果。</p><p>结果如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">v1: only consider PM2<span class="number">.5</span></span><br><span class="line"></span><br><span class="line">Using sklearn</span><br><span class="line">LinearRegression(copy_X=<span class="literal">True</span>, fit_intercept=<span class="literal">True</span>, n_jobs=<span class="literal">None</span>, normalize=<span class="literal">False</span>)</span><br><span class="line">bias= [<span class="number">21.37402689</span>]</span><br><span class="line">w= [[ <span class="number">0.00000000e+00</span>]</span><br><span class="line"> [<span class="number">-5.54801503e-01</span>]</span><br><span class="line"> [<span class="number">-4.32873874e-01</span>]</span><br><span class="line"> [ <span class="number">3.63669814e+00</span>]</span><br><span class="line"> [<span class="number">-3.99037687e+00</span>]</span><br><span class="line"> [<span class="number">-9.07364636e-01</span>]</span><br><span class="line"> [ <span class="number">8.83495803e+00</span>]</span><br><span class="line"> [<span class="number">-9.51785135e+00</span>]</span><br><span class="line"> [ <span class="number">1.32734655e-02</span>]</span><br><span class="line"> [ <span class="number">1.81886444e+01</span>]]</span><br><span class="line"></span><br><span class="line">In our model</span><br><span class="line">bias= [<span class="number">19.59387132</span>]</span><br><span class="line">w= [[<span class="number">-0.14448468</span>]</span><br><span class="line"> [ <span class="number">0.39205748</span>]</span><br><span class="line"> [ <span class="number">0.26897134</span>]</span><br><span class="line"> [<span class="number">-1.02415371</span>]</span><br><span class="line"> [ <span class="number">1.21151411</span>]</span><br><span class="line"> [ <span class="number">2.21925424</span>]</span><br><span class="line"> [<span class="number">-5.48242478</span>]</span><br><span class="line"> [ <span class="number">4.01080346</span>]</span><br><span class="line"> [<span class="number">13.56369122</span>]]</span><br></pre></td></tr></table></figure><p>发现参数有一定差异，于是我在testing时，也把sklearn的结果进行预测比较。</p><p>一部分结果如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'id'</span>, <span class="string">'value'</span>, <span class="string">'sk_value'</span>]</span><br><span class="line">[<span class="string">'id_0'</span>, <span class="number">3.551092352912313</span>, <span class="number">5.37766865368331</span>]</span><br><span class="line">[<span class="string">'id_1'</span>, <span class="number">13.916795471648756</span>, <span class="number">16.559245678900034</span>]</span><br><span class="line">[<span class="string">'id_2'</span>, <span class="number">24.811333478647043</span>, <span class="number">23.5085950470451</span>]</span><br><span class="line">[<span class="string">'id_3'</span>, <span class="number">5.101440436158914</span>, <span class="number">6.478306159981166</span>]</span><br><span class="line">[<span class="string">'id_4'</span>, <span class="number">26.7374726797937</span>, <span class="number">27.207516152986663</span>]</span><br><span class="line">[<span class="string">'id_5'</span>, <span class="number">19.43735346531517</span>, <span class="number">21.916809502961648</span>]</span><br><span class="line">[<span class="string">'id_6'</span>, <span class="number">22.20460696285646</span>, <span class="number">24.751295357256392</span>]</span><br><span class="line">[<span class="string">'id_7'</span>, <span class="number">29.660872382552682</span>, <span class="number">30.24344042612033</span>]</span><br><span class="line">[<span class="string">'id_8'</span>, <span class="number">17.5964527734513</span>, <span class="number">16.64242443764712</span>]</span><br><span class="line">[<span class="string">'id_9'</span>, <span class="number">56.58017426943178</span>, <span class="number">59.760988216575115</span>]</span><br><span class="line">[<span class="string">'id_10'</span>, <span class="number">13.767504260132299</span>, <span class="number">10.808372404511037</span>]</span><br><span class="line">[<span class="string">'id_11'</span>, <span class="number">11.743000466164233</span>, <span class="number">11.526958393801682</span>]</span><br><span class="line">[<span class="string">'id_12'</span>, <span class="number">59.509878887026105</span>, <span class="number">64.201008247897</span>]</span><br><span class="line">[<span class="string">'id_13'</span>, <span class="number">53.19824337746267</span>, <span class="number">54.3856368053018</span>]</span><br><span class="line">[<span class="string">'id_14'</span>, <span class="number">21.97191108867921</span>, <span class="number">24.530720709840974</span>]</span><br><span class="line">[<span class="string">'id_15'</span>, <span class="number">10.833283625735444</span>, <span class="number">14.350345549104446</span>]</span><br></pre></td></tr></table></figure><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><p>有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的<a href="https://github.com/f1ed/ML-HW1">GitHub</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########################</span></span><br><span class="line"><span class="comment"># Date: 2020-4-4</span></span><br><span class="line"><span class="comment"># Author: FredLau</span></span><br><span class="line"><span class="comment"># HW1: predict the PM2.5</span></span><br><span class="line"><span class="comment">##########################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#####################</span></span><br><span class="line"><span class="comment"># process data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># process train data</span></span><br><span class="line">raw_data = np.genfromtxt(<span class="string">'data/train.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">data = raw_data[<span class="number">1</span>:, <span class="number">3</span>:]</span><br><span class="line">data[np.isnan(data)] = <span class="number">0</span>  <span class="comment"># process nan</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dictionary: key:month value:month data</span></span><br><span class="line">month_data = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># make data timeline continuous</span></span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    temp = np.empty(shape=(<span class="number">18</span>, <span class="number">20</span>*<span class="number">24</span>))</span><br><span class="line">    <span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        temp[:, day*<span class="number">24</span>: (day+<span class="number">1</span>)*<span class="number">24</span>] = data[(month*<span class="number">20</span>+day)*<span class="number">18</span>: (month*<span class="number">20</span>+day+<span class="number">1</span>)*<span class="number">18</span>, :]</span><br><span class="line">    month_data[month] = temp</span><br><span class="line"></span><br><span class="line"><span class="comment"># x_data v1: only consider PM2.5</span></span><br><span class="line">x_data = np.empty(shape=(<span class="number">12</span>*<span class="number">471</span>, <span class="number">9</span>))</span><br><span class="line">y_data = np.empty(shape=(<span class="number">12</span>*<span class="number">471</span>, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">471</span>):</span><br><span class="line">        x_data[month*<span class="number">471</span>+i][:] = month_data[month][<span class="number">9</span>][i: i+<span class="number">9</span>]</span><br><span class="line">        y_data[month*<span class="number">471</span>+i] = month_data[month][<span class="number">9</span>][i+<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># process test data</span></span><br><span class="line">test_raw_data = np.genfromtxt(<span class="string">'data/test.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">test_data = test_raw_data[:, <span class="number">2</span>:]</span><br><span class="line">test_data[np.isnan(test_data)] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># feature scale: normalization</span></span><br><span class="line">mean = np.mean(x_data, axis=<span class="number">0</span>)</span><br><span class="line">std = np.std(x_data, axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x_data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(x_data.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[j] != <span class="number">0</span>:</span><br><span class="line">            x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(test_data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(test_data.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[j] != <span class="number">0</span>:</span><br><span class="line">            test_data[i][j] = (test_data[i][j] - mean[j])/std[j]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing data features</span></span><br><span class="line">test_x = np.empty(shape=(<span class="number">240</span>, <span class="number">9</span>))</span><br><span class="line"><span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">240</span>):</span><br><span class="line">    test_x[day, :] = test_data[<span class="number">18</span>*day+<span class="number">9</span>, :]</span><br><span class="line">test_x = np.concatenate((np.ones(shape=(<span class="number">240</span>, <span class="number">1</span>)), test_x), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">################################</span></span><br><span class="line"><span class="comment"># train-adagrad</span></span><br><span class="line"></span><br><span class="line">batch = x_data.shape[<span class="number">0</span>]  <span class="comment"># full batch</span></span><br><span class="line">epoch = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># some parameters for training</span></span><br><span class="line">dim = x_data.shape[<span class="number">1</span>]+<span class="number">1</span></span><br><span class="line">w = np.zeros(shape=(dim, <span class="number">1</span>))  <span class="comment"># concatenate bias = w[0]</span></span><br><span class="line">lr = np.full((dim, <span class="number">1</span>), <span class="number">0.8</span>)  <span class="comment"># learning rate</span></span><br><span class="line">grad = np.empty(shape=(dim, <span class="number">1</span>))  <span class="comment"># gradient of loss to every para</span></span><br><span class="line">gradsum = np.zeros(shape=(dim, <span class="number">1</span>))  <span class="comment"># sum of gradient**2</span></span><br><span class="line"></span><br><span class="line">x_data = np.concatenate((np.ones(shape=(x_data.shape[<span class="number">0</span>], <span class="number">1</span>)), x_data), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss_his = np.empty(shape=(epoch, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> T <span class="keyword">in</span> range(epoch):</span><br><span class="line">    L = y_data - np.dot(x_data, w)</span><br><span class="line">    loss_his[T] = np.sum(L**<span class="number">2</span>) / x_data.shape[<span class="number">0</span>]</span><br><span class="line">    grad = (<span class="number">-2</span>)*np.dot(np.transpose(x_data), L)</span><br><span class="line">    gradsum = gradsum + grad**<span class="number">2</span></span><br><span class="line">    w = w - lr*grad/(gradsum**<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">f = open(<span class="string">'output/v1.csv'</span>, <span class="string">'w'</span>)</span><br><span class="line">sys.stdout = f</span><br><span class="line">print(<span class="string">'v1: only consider PM2.5\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line"><span class="comment"># train by sklearn linear model</span></span><br><span class="line">print(<span class="string">'Using sklearn'</span>)</span><br><span class="line">reg = linear_model.LinearRegression()</span><br><span class="line">print(reg.fit(x_data, y_data))</span><br><span class="line">print(<span class="string">'bias='</span>, reg.intercept_)</span><br><span class="line">print(<span class="string">'w='</span>, reg.coef_.transpose())</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In our model</span></span><br><span class="line">print(<span class="string">'In our model'</span>)</span><br><span class="line">print(<span class="string">'bias='</span>, w[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'w='</span>, w[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment">###########################</span></span><br><span class="line"><span class="comment"># draw change of loss</span></span><br><span class="line">plt.xlim(<span class="number">0</span>, epoch)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$iteration$'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.ylabel(<span class="string">'$Loss$'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">iteration = np.arange(<span class="number">0</span>, epoch)</span><br><span class="line">plt.plot(iteration, loss_his/<span class="number">100</span>, <span class="string">'-'</span>, ms=<span class="number">3</span>, lw=<span class="number">2</span>, color=<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sk_w = reg.coef_.transpose()</span><br><span class="line">sk_w[<span class="number">0</span>] = reg.intercept_</span><br><span class="line">sk_loss = np.sum((y_data - np.dot(x_data, sk_w))**<span class="number">2</span>) / x_data.shape[<span class="number">0</span>]</span><br><span class="line">plt.hlines(sk_loss/<span class="number">100</span>, <span class="number">0</span>, epoch, colors=<span class="string">'red'</span>, linestyles=<span class="string">'solid'</span>)</span><br><span class="line">plt.legend([<span class="string">'adagrad'</span>, <span class="string">'sklearn'</span>])</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('output/v1.png')</span></span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">##############</span></span><br><span class="line"><span class="comment"># test (sklearn vs our adagrad</span></span><br><span class="line">f = open(<span class="string">'output/v1test.csv'</span>, <span class="string">'w'</span>)</span><br><span class="line">sys.stdout = f</span><br><span class="line"></span><br><span class="line">title = [<span class="string">'id'</span>, <span class="string">'value'</span>, <span class="string">'sk_value'</span>]</span><br><span class="line">answer = np.dot(test_x, w)</span><br><span class="line">sk_answer = np.dot(test_x, sk_w)</span><br><span class="line">print(title)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(test_x.shape[<span class="number">0</span>]):</span><br><span class="line">    content = [<span class="string">'id_'</span>+str(i), answer[i][<span class="number">0</span>], sk_answer[i][<span class="number">0</span>]]</span><br><span class="line">    print(content)</span><br><span class="line"></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>待完成</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在本篇文章中，用手刻Adagrad完成了「机器学习-李宏毅」的HW1-预测PM2.5的作业。其中包括对数据的处理，训练模型，预测，并使用sklearn toolkit的结果进行比较。&lt;br&gt;有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的&lt;a href=&quot;https://github.com/f1ed/ML-HW1&quot;&gt;GitHub&lt;/a&gt;  &lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="Regression" scheme="https://f1ed.github.io/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:Classification-Logistic Regression</title>
    <link href="https://f1ed.github.io/2020/04/01/Classification2/"/>
    <id>https://f1ed.github.io/2020/04/01/Classification2/</id>
    <published>2020-03-31T16:00:00.000Z</published>
    <updated>2020-07-03T08:40:06.165Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中，讲解了怎么用Generative Model做分类问题。<br>这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。<br>文章主要有两部分：<br>第一部分讲解了Logistic Regression的三个步骤。<br>第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。</p><a id="more"></a><h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><h2 id="Step1-Function-Set"><a href="#Step1-Function-Set" class="headerlink" title="Step1: Function Set"></a>Step1: Function Set</h2><p>在文章末尾，我们得出 $P_{w, b}\left(C_{1} | x\right)=\sigma(w\cdot x+b)$ 的形式，想跳过找 $\mu_1,\mu_2,\Sigma$ 的过程，直接找 $w,b$ 。</p><p>因此Function Set: $f_{w, b}(x)=P_{w, b}\left(C_{1} | x\right)$ 。值大于0.5，则属于C1类，否则属于C2类。</p><p><a href="https://imgchr.com/i/G8TlKf"><img src="https://s1.ax1x.com/2020/04/01/G8TlKf.md.png" alt="G8TlKf.md.png"></a> </p><h2 id="Step2-Goodness-of-a-Function"><a href="#Step2-Goodness-of-a-Function" class="headerlink" title="Step2: Goodness of a Function"></a>Step2: Goodness of a Function</h2><p>使用极大似然的思想（在前一篇机率模型/生成模型中有讲）</p><p>估计函数是 ：$L(w, b)=f_{w, b}\left(x^{1}\right) f_{w, b}\left(x^{2}\right)\left(1-f_{w, b}\left(x^{3}\right)\right) \cdots f_{w, b}\left(x^{N}\right)$ </p><p>目标：  $ w^{*}, b^{*}=\arg \max _{w, b} L(w, b)$ </p><p>由于在之前的Regression中，我们都是找极小值点，为了方便处理，将估计函数转换为如下形式的<strong>损失函数：</strong> </p>$$\begin{equation}\begin{aligned}Loss &= -\ln L(w, b)=\ln f_{w, b}\left(x^{1}\right)+\ln f_{w, b}\left(x^{2}\right)+\ln \left(1-f_{w, b}\left(x^{3}\right)\right) \cdots \\ &=\sum_{n}-\left[\hat{y}^{n} \ln f_{w, b}\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f_{w, b}\left(x^{n}\right)\right)\right]\end{aligned}\end{equation}$$<p><strong>目标</strong> ： $w^{*}, b^{*}=\arg \min _{w, b} L(w, b)$ </p><blockquote><p><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy</a>（交叉熵）</p><p>上式中的 $\left[\hat{y}^{n} \ln f_{w, b}\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f_{w, b}\left(x^{n}\right)\right)\right]$ 其实是两个Bernoulli distribution的交叉熵。</p><p>交叉熵是什么？ 简单来说，交叉熵是评估两个distribution 有多接近。所以当两个分布的交叉熵为0时，表明这两个分布一模一样。</p><p>对于 $\hat{y}^{n} \ln f_{w, b}\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f_{w, b}\left(x^{n}\right)\right)$ ：</p><p> Distribution p: p(x = 1) = $\hat{y}^n$ ; p( x = 0 ) = 1 - $\hat{y}^n$ </p><p>Distribution q: q(x = 1 ) =  $f(x^n)$ ; q(x = 0 ) = 1 - $f(x^n)$ </p><p>交叉熵  $H(p,q)=-\Sigma_xp(x)\ln(q(x))$ </p></blockquote><p>因此，这个损失函数的表达式其实也是输出分布和target分布的交叉熵，即：</p><p>$L(f)=\sum_{n} C\left(f\left(x^{n}\right), \hat{y}^{n}\right)$ </p><p>（ $C\left(f\left(x^{n}\right), \hat{y}^{n}\right)=-\left[\hat{y}^{n} \ln f\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f\left(x^{n}\right)\right)\right]$  ）</p><hr><p>和Linear Regression不同，为什么Logistic Regression不用square error，而要使用cross entropy。</p><p>在1.4小节会给出解释。</p><h2 id="Step3-Find-the-best-function"><a href="#Step3-Find-the-best-function" class="headerlink" title="Step3: Find the best function"></a>Step3: Find the best function</h2><p>在第三步，同样使用Gradient来寻找最优函数。</p><p>推导过程：</p><ul><li><p>$\left.\frac{-\ln L(w, b)}{\partial w_{i}}=\sum_{n}-\left[\hat{y}^{n} \frac{\ln f_{w, b}\left(x^{n}\right)}{\partial w_{i}}+\left(1-\hat{y}^{n}\right) \frac{\ln \left(1-f_{w, b}\left(x^{n}\right)\right.}{\partial w_{i}}\right)\right]$ </p><ul><li><p>$\frac{\partial \ln f_{w, b}(x)}{\partial w_{i}}=\frac{\operatorname{\partial\ln} f_{w, b}(x)}{\partial z} \frac{\partial z}{\partial w_{i}}$ </p><ul><li>$\frac{\partial \ln \sigma(z)}{\partial z}=\frac{1}{\sigma(z)} \frac{\partial \sigma(z)}{\partial z}=\frac{1}{\sigma(z)} \sigma(z)(1-\sigma(z))$  </li><li>$\frac{\partial z}{\partial w_{i}}=x_{i}$ </li></ul></li><li><p>$\frac{\partial \ln \left(1-f_{w, b}(x)\right)}{\partial w_{i}}=\frac{\operatorname{\partial\ln}\left(1-f_{w, b}(x)\right)}{\partial z} \frac{\partial z}{\partial w_{i}}$ </p><ul><li>$\frac{\partial \ln (1-\sigma(z))}{\partial z}=-\frac{1}{1-\sigma(z)} \frac{\partial \sigma(z)}{\partial z}=-\frac{1}{1-\partial(z)} \sigma(z)(1-\sigma(z))$ </li><li>$\frac{\partial z}{\partial w_{i}}=x_{i}$ </li></ul></li><li><p>$\frac{\partial \sigma(z)}{\partial z}=\sigma(z)\cdot(1-\sigma(z))$ </p><p><a href="https://imgchr.com/i/G8TMxP"><img src="https://s1.ax1x.com/2020/04/01/G8TMxP.png" alt="G8TMxP.png"></a> </p></li></ul></li><li><p>注：$f_{w, b}(x)=\sigma(z)$   ; $z=w \cdot x+b=\sum_{i} w_{i} x_{i}+b$  </p></li><li> $$  \begin{equation}  \begin{aligned}  \frac{-\ln L(w, b)}{\partial w_{i}}&=\sum_{n}-\left[\hat{y}^{n}\left(1-f_{w, b}\left(x^{n}\right)\right) x_{i}^{n}-\left(1-\hat{y}^{n}\right) f_{w, b}\left(x^{n}\right) x_{i}^{n}\right]  \\&=\sum_{n}-\left(\hat{y}^{n}-f_{w, b}\left(x^{n}\right)\right) x_{i}^{n}  \end{aligned}  \end{equation}  $$ </li></ul><p>因此Logistic Regression的损失函数的导数和Linear Regression的一样。</p><p>迭代更新： $w_{i} \leftarrow w_{i}-\eta \sum_{n}-\left(\hat{y}^{n}-f_{w, b}\left(x^{n}\right)\right) x_{i}^{n}$ </p><h2 id="与Linear-Regression-的对比"><a href="#与Linear-Regression-的对比" class="headerlink" title="与Linear Regression 的对比"></a>与Linear Regression 的对比</h2><p><a href="https://imgchr.com/i/G8Tu8I"><img src="https://s1.ax1x.com/2020/04/01/G8Tu8I.md.png" alt="G8Tu8I.md.png"></a> </p><p>如图所示。</p><h2 id="If-Logistic-Square-Error"><a href="#If-Logistic-Square-Error" class="headerlink" title="If : Logistic + Square Error"></a>If : Logistic + Square Error</h2><p>前面一小节我们提到，在Logistic Regression中使用cross entropy判别一个函数的好坏,那为什么不使用square error来judge the goodness？</p><p>如果使用 Square Error的方法，步骤如下：</p><p><a href="https://imgchr.com/i/G8TnPA"><img src="https://s1.ax1x.com/2020/04/01/G8TnPA.md.png" alt="G8TnPA.md.png"></a> </p><p>来看Step 3: 损失函数的导数是 $2\left(f_{w, b}(x)-\hat{y}\right) f_{w, b}(x)\left(1-f_{w, b}(x)\right) x_{i}$ </p><p>考虑 $\hat{y}^n=1$ （即我们的target是1）：</p><ul><li>如果  $f_{w,b}(x^n)=1$ , 即预测值接近 target, 算出来的 $\partial{L}/\partial{w_i}=0$ 是期望的。</li><li>如果  $f_{w,b}(x^n)=0$ , 即预测值原理 target, 算出来的 $\partial{L}/\partial{w_i}=0$ 是不期望的。</li></ul><p>同理，当考虑 $\hat{y}^n=0$ 情况时，也是如此。</p><p>更直观的看：</p><p><a href="https://imgchr.com/i/G8Te5d"><img src="https://s1.ax1x.com/2020/04/01/G8Te5d.md.png" alt="G8Te5d.md.png"></a> </p><p>上图中，画出了两种损失函数的平面，中心的最低点是我们的target。</p><p>但在Square Error中，远离target的蓝色点，也处在很平坦的位置，其导数小，参数的更新会很慢。</p><p>因此在Cross Entropy中，离target越远，其导数更大，更新更快。</p><p>所以Cross Entropy的效果比Square Error更快，效果更好。</p><h1 id="Discriminative-V-S-Generative"><a href="#Discriminative-V-S-Generative" class="headerlink" title="Discriminative V.S. Generative"></a>Discriminative V.S. Generative</h1><p>这篇文章中的Logistic Regression是Discriminative Model。</p><a href="/2020/03/21/Classification1/" title="上篇文章">上篇文章</a>中Classification是Generative Model。<p>有什么区别呢？</p><p><a href="https://imgchr.com/i/G8TVVe"><img src="https://s1.ax1x.com/2020/04/01/G8TVVe.md.png" alt="G8TVVe.md.png"></a> </p><p>上图中，Generative Model做了假设（脑补），假设它是 Gaussian Distribution，假设它是Bernoulli Distribution。然后去找这些分布的参数，在求出 $w,b$。</p><p>而在Discriminative Model中，没有做任何假设，直接找 $w,b$ 参数。</p><p>所以，这两种Model经过training找出来的参数一样吗？</p><p>答案是不一样的。</p><p>The same model(function set), but different function is selected by the same training data.</p><p>在上篇Pokemon的例子中，比较两种方法的结果差异。</p><p><a href="https://imgchr.com/i/G8TAbD"><img src="https://s1.ax1x.com/2020/04/01/G8TAbD.md.png" alt="G8TAbD.md.png"></a> </p><p>可见，在Pokemon的例子总，Discriminative的效果比Generative的效果好一些。</p><hr><p>但是Generative Model就不好吗？</p><p><strong>Benefit of generative model</strong></p><ul><li><p>With the assumption of probability distribution, less training data is needed.</p><p>【训练生成模型所需数据更少】 </p></li><li><p>With the assumption of probability distribution, more robust to the noise.</p><p>【生成模型对noise data更兼容】</p></li><li><p>Priors and class-dependent probabilities can be estimated from different sources.</p><p>【生成模型中的 先验概率Priors 和 基于类别的分布概率不同】</p><p>比如，做语音辨识系统，整个系统是generative的。</p><p>因为Prior（某一句话的概率）并不需要从data中知道，可以直接在网络上爬虫统计。</p><p>而class-dependent probabilities（这段语音是这句话的概率）需要data进行训练才能得知。</p></li></ul><h1 id="Multi-class-classification"><a href="#Multi-class-classification" class="headerlink" title="Multi-class classification"></a>Multi-class classification</h1><h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2><p><a href="https://imgchr.com/i/G8TkDO"><img src="https://s1.ax1x.com/2020/04/01/G8TkDO.md.png" alt="G8TkDO.md.png"></a> </p><p>假设有三个类别：C1、C2、C3 。模型已经得到，参数分别是 w、b。</p><p>对于输入x, 判断x属于哪一个类别。</p><ol start="0"><li>通过每个类别的 w、b求出 $z^i=w^i\cdot x+b_i$  </li></ol><p>Softmax的步骤：</p><ol><li>exponential：每个z值得到 $=e^z$ .</li><li>sum：将指数化后的值加起来$=\Sigma_{j=1}^3e^{z_j}$ </li><li>output: 每个类别的输出 $y_i=e^{z_1}/\Sigma_{j=1}^3e^{z_j}$  ，即x属于类别i的概率。</li></ol><p>求出的 $1&gt;y_i&gt;0$ 且 $\Sigma_iy_i=1$ 。</p><p>通过Softmax，得到 $y_i=P(C_i|x)$ 。</p><h2 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h2><p>（手写笔记，略倾斜，原来不切一切还不知道自己歪的这么厉害 泪）</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1:"></a>Step 1:</h3><p><a href="https://imgchr.com/i/G8TFKK"><img src="https://s1.ax1x.com/2020/04/01/G8TFKK.md.png" alt="G8TFKK.md.png"></a> </p><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2:"></a>Step 2:</h3><p><a href="https://imgchr.com/i/G8TPv6"><img src="https://s1.ax1x.com/2020/04/01/G8TPv6.md.png" alt="G8TPv6.md.png"></a> </p><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3:"></a>Step 3:</h3><p><a href="https://imgchr.com/i/G8TCgx"><img src="https://s1.ax1x.com/2020/04/01/G8TCgx.md.png" alt="G8TCgx.md.png"></a> </p><p>使用Stochastic Gradient（即每个样本更新一次）的话：</p><p>data: [x, $\hat{y}$ ] , $\hat{y}_i=1$ </p><p>更新 $w^j$ :</p><ol><li><p>$j=i$ :</p><p>$w^j \leftarrow w^j-\eta\cdot (y_i-1)\cdot x$ </p></li><li><p>$j\neq i$ :</p><p>$w^j \leftarrow w^j-\eta\cdot y_i\cdot x$  </p></li></ol><p>(下次一定，笔记写直一点！)</p><p>更为规范的推导见[1]</p><h1 id="Limitation-of-Logistic-Regression"><a href="#Limitation-of-Logistic-Regression" class="headerlink" title="Limitation of Logistic Regression"></a>Limitation of Logistic Regression</h1><p><a href="https://imgchr.com/i/G8oLuT"><img src="https://s1.ax1x.com/2020/04/01/G8oLuT.md.png" alt="G8oLuT.md.png"></a> </p><p>对于如上情况，Logistic Regression并不能进行分类，因为他的boundary 应该是线性的。</p><h2 id="Feature-Transforming"><a href="#Feature-Transforming" class="headerlink" title="Feature Transforming"></a>Feature Transforming</h2><p>如果对feature做转换后，就可以用Logistic Regression处理。</p><p>重定义feature， $x_1’$ :定义为到[0,0]的距离， $x_2’$ :定义为到[1,1]的距离。</p><p>于是图变成下图，即可用Logistic Regression进行分类。</p><p><a href="https://imgchr.com/i/G8oxUJ"><img src="https://s1.ax1x.com/2020/04/01/G8oxUJ.md.png" alt="G8oxUJ.md.png"></a> </p><p>但这样的做法，就不像人工智能了，因为Feature Transformation需要人来设计，而且较难设计。</p><h2 id="Cascading-logistic-regression-models"><a href="#Cascading-logistic-regression-models" class="headerlink" title="Cascading logistic regression models"></a>Cascading logistic regression models</h2><p>另一种做法是，将logistic regression连接起来。</p><p><a href="https://imgchr.com/i/G8TpCR"><img src="https://s1.ax1x.com/2020/04/01/G8TpCR.md.png" alt="G8TpCR.md.png"></a></p><p>上图中，左边部分的两个logistic regression就相当于在做Feature Transformation，右边部分相当于在做Classification。</p><p>而通过这种形式，将多个model连接起来，也就是大热的Neural Network。</p><p><a href="https://imgchr.com/i/G8T981"><img src="https://s1.ax1x.com/2020/04/01/G8T981.md.png" alt="G8T981.md.png"></a> </p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Multi-class Classification推导：Bishop，P209-210</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上篇文章中，讲解了怎么用Generative Model做分类问题。&lt;br&gt;这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。&lt;br&gt;文章主要有两部分：&lt;br&gt;第一部分讲解了Logistic Regression的三个步骤。&lt;br&gt;第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="Classification" scheme="https://f1ed.github.io/tags/Classification/"/>
    
      <category term="Logistic Regression" scheme="https://f1ed.github.io/tags/Logistic-Regression/"/>
    
      <category term="Softmax" scheme="https://f1ed.github.io/tags/Softmax/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:Classification-Generative Model</title>
    <link href="https://f1ed.github.io/2020/03/21/Classification1/"/>
    <id>https://f1ed.github.io/2020/03/21/Classification1/</id>
    <published>2020-03-20T16:00:00.000Z</published>
    <updated>2020-07-03T08:39:45.986Z</updated>
    
    <content type="html"><![CDATA[<p>Classification 有Generative Model和Discriminative Model。<br>这篇文章主要讲述了用生成模型来做分类的原理及过程。</p><a id="more"></a><h1 id="What-is-Classification"><a href="#What-is-Classification" class="headerlink" title="What is Classification?"></a>What is Classification?</h1><p>分类是什么呢？分类可以应用到哪些场景呢？</p><ul><li><p>Credit Scoring【贷款评估】</p><ul><li><p>Input: income, savings, profession, age, past financial history ……</p></li><li><p>Output: accept or refuse</p></li></ul></li><li><p>Medical Diagnosis【医疗诊断】</p><ul><li><p>Input: current symptoms, age, gender, past medical history ……</p></li><li><p>Output: which kind of diseases</p></li></ul></li><li><p>Handwritten character recognition【手写数字辨别】</p><ul><li>Input：<a href="https://imgchr.com/i/8hsTEV"><img src="https://s1.ax1x.com/2020/03/21/8hsTEV.png" alt="8hsTEV.png"></a> </li><li>Output：金</li></ul></li><li><p>Face recognition 【人脸识别】</p><ul><li>Input: image of a face</li><li>output: person</li></ul></li></ul><h2 id="Classification：Example-Application"><a href="#Classification：Example-Application" class="headerlink" title="Classification：Example Application"></a>Classification：Example Application</h2><p>【图】</p><p>如上图，<del>Pokemon又来啦！</del> Pokemon有很多属性，比如皮卡丘是电属性，杰尼龟是水属性之类。</p><p>关于Pokemon的Classification：Predict the “type” of Pokemon based on the information</p><p>Input：Information of Pokemon (数值化）<a href="https://imgchr.com/i/8hsfjs"><img src="https://s1.ax1x.com/2020/03/21/8hsfjs.md.png" alt="8hsfjs.md.png"></a> </p><p>Output：the type</p><p>Training Data: ID在前400的Pokemon<a href="https://imgchr.com/i/8hsWcj"><img src="https://s1.ax1x.com/2020/03/21/8hsWcj.md.png" alt="8hsWcj.md.png"></a> </p><p>Testing Data: ID在400后的Pokemon</p><h3 id="Classification-as-Regression"><a href="#Classification-as-Regression" class="headerlink" title="Classification as Regression?"></a>Classification as Regression?</h3><p><strong>1. 简化问题，只考虑二分类：Class 1 ， Class2。</strong></p><p>如果把分类问题当作回归问题，把类别数值化。</p><p>在Training中： Class 1 means the target is 1; Class 2 means the target is -1.</p><p>在Testing中：如果Regression的函数值接近1，说明是class 1；如果函数值接近-1，说明是class 2.</p><hr><p>Regression：输入信息只考虑两个特征。</p><p>Model：$y=w_1x_1+w_2x_2+b$ </p><p><a href="https://imgchr.com/i/8hs29g"><img src="https://s1.ax1x.com/2020/03/21/8hs29g.md.png" alt="8hs29g.md.png"></a> </p><p>当Training data的分布如上图所示时，得到的（最优函数）分界线感觉很合理。</p><p><a href="https://imgchr.com/i/8hsR3Q"><img src="https://s1.ax1x.com/2020/03/21/8hsR3Q.md.png" alt="8hsR3Q.md.png"></a></p><p>但当Training data在右下角也有分布时（如右图），训练中为了减少error，训练得到的分界线会变成紫色的那一条。</p><p>所以，如果用Regression来做Classification：<strong>Penalize to the examples that are “too correct”</strong> .[1]</p><p>训练中会因为惩罚一些“过于正确”（即和我们假定的target离太远）的example，得到的最优函数反而have bad performance.</p><p><strong>2. 此外，如果用Regression来考虑多分类。</strong></p><p>Multiple class: Class 1 means the target is 1; Class 2 means the target is 2; Class 3 means the target is 3……</p><p>如果用上面这种假设，可以认为Class 3和Class 2 的关系更近，和Class 1的关系更远一些。但实际中，可能这些类别<strong>have no relation</strong>。</p><h3 id="Classification-Ideal-Alternatives"><a href="#Classification-Ideal-Alternatives" class="headerlink" title="Classification: Ideal Alternatives"></a>Classification: Ideal Alternatives</h3><p>在上面，我们假设二元分类每一个类别都有一个target，结果不尽人意。</p><p><a href="https://imgchr.com/i/8hs6N8"><img src="https://s1.ax1x.com/2020/03/21/8hs6N8.md.png" alt="8hs6N8.md.png"></a> </p><p>如上图所示，将模型改为以上形式，也可以解决分类问题。（挖坑）[2]</p><h1 id="Generative-Model-生层模型"><a href="#Generative-Model-生层模型" class="headerlink" title="Generative Model(生层模型)"></a>Generative Model(生层模型)</h1><h2 id="Estimate-the-Probabilities"><a href="#Estimate-the-Probabilities" class="headerlink" title="Estimate the Probabilities"></a>Estimate the Probabilities</h2><p>用<strong>概率的知识来考虑分类</strong>这个问题，如下图所示，有两个两个类别，C1和C2。</p><p><a href="https://imgchr.com/i/8hsyAf"><img src="https://s1.ax1x.com/2020/03/21/8hsyAf.md.png" alt="8hsyAf.md.png"></a> </p><p>在Testing中，如果任给一个x，属于C1的概率是（贝叶斯公式）</p>$$P\left(C_{1} | x\right)=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}$$<p>所以在Training中知道这些： $P(C_1),P(x|C_1),P(C_2),P(x|C_2)$ </p><p>P(C1)和P(C2)很容易得知。</p><p>而P(x|C1)和P(x|C2)的概率应该如何得知呢？</p><p>如果能假设：类别是C1中的变量x服从某种分布，如高斯分布等，即可以得到任意P(x|C1)的值。</p><p>所以Generative Model：是对examples假设一个分布模型，在training中调节分布模型的参数，使得examples出现的概率最大。（极大似然的思想）</p><h2 id="Prior-Probabilities（先验概率）"><a href="#Prior-Probabilities（先验概率）" class="headerlink" title="Prior Probabilities（先验概率）"></a>Prior Probabilities（先验概率）</h2><p>先只考虑Water和Normal两个类别。</p><p>先验概率：即通过过去资料分析得到的概率。</p><p>在Pokemon的例子中，Training Data是ID&lt;400的水属性和一般属性的Pokemon信息。</p><p>Training Data：79 Water，61 Normal。</p><p>得到的先验概率 P(C1)=79/(79+61)=0.56, P(C2)=61/(79+61)=0.44。</p><h2 id="Probability-from-Class"><a href="#Probability-from-Class" class="headerlink" title="Probability from Class"></a>Probability from Class</h2><p>先只考虑Defense和SP Defense这两个feature。</p><p>如果不考虑生成分布模型，在testing中直接计算P(x|Water)的概率，如下图右下角的那只龟龟，在training data中没有出现过，那值为0吗？显然不对。</p><p><a href="https://imgchr.com/i/8hsDBt"><img src="https://s1.ax1x.com/2020/03/21/8hsDBt.md.png" alt="8hsDBt.md.png"></a> </p><p>假设：上图中<strong>water type的examples是从Gaussian distribution（高斯分布）中取样</strong>出来的。</p><p>因此在training中通过training data得到最优的Gaussian distribution的参数，计算样本中没有出现过的P(x|Water)也就迎刃而解了。</p><h3 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h3><p>多维的高斯分布（高斯分布就是正态分布啦）的联合概率密度：</p>$$f_{\mu, \Sigma}(x)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right\}$$<p>D: 维数</p><p>$\mu$ : mean</p><p>$\Sigma$ :covariance matrix(协方差矩阵)</p><blockquote><p>协方差： $ cov(X,Y)=E[[X-E(X)][Y-E(Y)]]=E(XY)-E(X)E(Y)$ </p><p>具体协方差性质，查阅概率论课本吧。</p></blockquote><p>x: vector,n维随机变量</p><p><strong>高斯分布的性质只和 $\mu$ 和 $\Sigma$ 有关。</strong></p><p> $\Sigma$ 一定时，$\mu$ 不同，如下图：</p><p><a href="https://imgchr.com/i/8hsBnI"><img src="https://s1.ax1x.com/2020/03/21/8hsBnI.md.png" alt="8hsBnI.md.png"></a> </p><p>$\mu$ 一定， $\Sigma$ 不同时，如下图：</p><p><a href="https://imgchr.com/i/8hswjA"><img src="https://s1.ax1x.com/2020/03/21/8hswjA.md.png" alt="8hswjA.md.png"></a> </p><h3 id="Maximum-Likelihood（极大似然）"><a href="#Maximum-Likelihood（极大似然）" class="headerlink" title="Maximum Likelihood（极大似然）"></a>Maximum Likelihood（极大似然）</h3><p>样本分布如下图所示，假设这些样本是从Gaussian distribution中取样，那如何在训练中得到高斯分布的 $\mu$ 和 $\Sigma$ 呢？</p><p>极大似然估计。</p><p>考虑Water，有79个样本，估计函数  $L(\mu, \Sigma)=f_{\mu, \Sigma}\left(x^{1}\right) f_{\mu, \Sigma}\left(x^{2}\right) f_{\mu, \Sigma}\left(x^{3}\right) \ldots \ldots f_{\mu, \Sigma}\left(x^{79}\right)$ </p><p>极大似然估计，即找到  $f_{\mu, \Sigma}(x)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right\}$  中的 $\mu$ 和$\Sigma$ 使得估计函数最大（使得取出这些样本的概率最大化）。</p><hr>$\mu^{*}, \Sigma^{*}=\arg \max _{\mu, \Sigma} L(\mu, \Sigma)$ <ol><li><p>求导计算（过于复杂，但也不是不能做是吧）</p></li><li><p>背公式[3]</p> $\mu^{*}=\frac{1}{79} \sum_{n=1}^{79} x^{n} \qquad \Sigma^{*}=\frac{1}{79} \sum_{n=1}^{79}\left(x^{n}-\mu^{*}\right)\left(x^{n}-\mu^{*}\right)^{T}$ </li></ol><p>得到Water和Normal的高斯分布，如下图:</p><p><a href="https://imgchr.com/i/8hsa1H"><img src="https://s1.ax1x.com/2020/03/21/8hsa1H.md.png" alt="8hsa1H.md.png"></a>  </p><h2 id="Do-Classification-different-Sigma"><a href="#Do-Classification-different-Sigma" class="headerlink" title="Do Classification: different $\Sigma$"></a>Do Classification: different $\Sigma$</h2><h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p>Testing：  $P\left(C_{1} | x\right)=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}$  </p><p>P(x|C1)由训练得出的Water的高斯分布计算出，P(x|C2)由Normal的高斯分布计算出。（如下图，过于难打）</p><p><a href="https://imgchr.com/i/8hsJAK"><img src="https://s1.ax1x.com/2020/03/21/8hsJAK.md.png" alt="8hsJAK.md.png"></a> </p><p>如果P(C1|x)&gt;0.5，说明x 属于Water(Class 1)。</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>如果只考虑两个feature（Defense和SP Defense），下图是testing data的样本图，蓝色属于Water，红色属于Normal。</p><p><a href="https://imgchr.com/i/8hsthD"><img src="https://s1.ax1x.com/2020/03/21/8hsthD.md.png" alt="8hsthD.md.png"></a> </p><p>用训练得出的模型，Testing Data: 47% accuracy。（结果如下图）</p><p><a href="https://imgchr.com/i/8hcrH1"><img src="https://s1.ax1x.com/2020/03/21/8hcrH1.md.png" alt="8hcrH1.md.png"></a> </p><p>如果考虑全部features(7个)，重新训练出的模型，结果：Testing Data：54% accuracy。（结果如下图）</p><p><a href="https://imgchr.com/i/8hsYtO"><img src="https://s1.ax1x.com/2020/03/21/8hsYtO.md.png" alt="8hsYtO.md.png"></a> </p><p>结果并不好。参数过多，模型过于复杂，有些过拟合了。</p><h2 id="Modifying-Model：same-Sigma"><a href="#Modifying-Model：same-Sigma" class="headerlink" title="Modifying Model：same $\Sigma$"></a>Modifying Model：same $\Sigma$</h2><p>模型中的参数有两个的Gaussian Distribution中的 $\mu^<em>$ 和 $\Sigma^</em>$ ，其中协方差矩阵的大小等于feature的平方，所以让<strong>不同的class share 同一个 $\Sigma$ ，以此来减少参数，简化模型</strong>。</p><p><a href="https://imgchr.com/i/8hsMc9"><img src="https://s1.ax1x.com/2020/03/21/8hsMc9.md.png" alt="8hsMc9.md.png"></a> </p><p>极大似然估计的估计函数：</p>$$L\left(\mu^{1}, \mu^{2}, \Sigma\right)=f_{\mu^{1}, \Sigma}\left(x^{1}\right) f_{\mu^{1}, \Sigma}\left(x^{2}\right) \cdots f_{\mu^{1}, \Sigma}\left(x^{79}\right)\times f_{\mu^{2}, \Sigma}\left(x^{80}\right) f_{\mu^{2}, \Sigma}\left(x^{81}\right) \cdots f_{\mu^{2}, \Sigma}\left(x^{140}\right)$$<p>公式推导:[3]</p><p>$\mu$ 的公式不变。</p>$\Sigma=\frac{79}{140} \Sigma^{1}+\frac{61}{140} \Sigma^{2}$  ,即是原 $\Sigma^1\ \Sigma^2$的加权平均。<h3 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h3><p>当只考虑两个features，用同样的协方差参数，结果如下图：</p><p><a href="https://imgchr.com/i/8hsQXR"><img src="https://s1.ax1x.com/2020/03/21/8hsQXR.md.png" alt="8hsQXR.md.png"></a> </p><p>可以发现，用了同样的协方差矩阵参数后，边界变成了线性的，所以这也是一个线性模型。</p><p>再考虑7个features，用同样的协方差矩阵参数，模型也是线性模型，但由于在高维空间，人无法直接画出其boundary，这也是机器学习的魅力所在，能解决一些人无法解决的问题。</p><p>结果：从之前的54% accuracy增加到 73% accurancy.</p><p>结果明显变好了。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Three Steps：</p><ol><li><p>Function Set（Model）：<a href="https://imgchr.com/i/8hs30x"><img src="https://s1.ax1x.com/2020/03/21/8hs30x.md.png" alt="8hs30x.md.png"></a> </p></li><li><p>Goodness of a function:</p><p>The mean µ and convariance $\Sigma$ that maximizing the likelihood(the probability of generating data)</p></li><li><p>Find the best function:easy(公式)</p></li></ol><h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="为什么要选择Gaussian-Distribution"><a href="#为什么要选择Gaussian-Distribution" class="headerlink" title="为什么要选择Gaussian Distribution"></a>为什么要选择Gaussian Distribution</h2><p><strong>You can always use the distribution you like.</strong> </p><p>可以选择你喜欢的任意分布，t分布，开方分布等。</p><p>（老师说：如果我选择其他分布，你也会问这个问题，哈哈哈）</p><h2 id="Naive-Bayes-Classifier"><a href="#Naive-Bayes-Classifier" class="headerlink" title="Naive Bayes Classifier"></a>Naive Bayes Classifier</h2><p>If you assume all the dimensions are independent, then you are using <em>Naive Bayes Classifier</em>.</p><p>如果假设features之间互相独立， $P\left(x | C_{1}\right)=P\left(x_{1} | C_{1}\right) P\left(x_{2} | C_{1}\right) \quad \ldots \ldots \quad P\left(x_{k} | C_{1}\right) $   。</p><p>xi是x第i维度的feature。</p><p>对于每一个 P(xi|C1)，可以假设其服从一维高斯分布。如果是binary features（即feature取值只有两个），也可以假设它服从Bernoulli distribution(贝努利分布)。</p><h2 id="Posterior-Probability（后验概率）"><a href="#Posterior-Probability（后验概率）" class="headerlink" title="Posterior Probability（后验概率）"></a>Posterior Probability（后验概率）</h2><p>Posterior Probability后验概率，即使用贝叶斯公式，已知结果，寻找最优可能导致它发生的原因。</p><p>对  $P\left(C_{1} | x\right)=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}$ 进行处理。</p><p>得到：</p>$$\begin{equation}\begin{aligned}P\left(C_{1} | x\right)&=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}\\&=\frac{1}{1+\frac{P\left(x | C_{2}\right) P\left(C_{2}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)}}\\&=\frac{1}{1+\exp (-z)} =\sigma(z)\qquad(z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}\end{aligned}\end{equation}$$<p><strong><font color=#f00>Worning of Math</font></strong> </p><ul><li> $z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}=\ln \frac{P\left(x | C_{1}\right)}{P\left(x | C_{2}\right)}+\ln \frac{P\left(C_{1}\right)}{P\left(C_{2}\right)}$  <ul><li> $\ln \frac{P\left(C_{1}\right)}{P\left(C_{2}\right)}=\frac{\frac{N_{1}}{N_{1}+N_{2}}}{\frac{N_{2}}{N_{1}+N_{2}}}=\frac{N_{1}}{N_{2}}$  </li><li> $P\left(x | C_{1}\right)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\Sigma 1|^{1 / 2}} \exp \left\{-\frac{1}{2}\left(x-\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1}\left(x-\mu^{1}\right)\right\}$ </li><li> $P\left(x | C_{2}\right)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{\left|\Sigma^{2}\right| 1 / 2} \exp \left\{-\frac{1}{2}\left(x-\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1}\left(x-\mu^{2}\right)\right\}$ </li></ul></li><li> $\ln \frac{\left|\Sigma^{2}\right|^{1 / 2}}{\left|\Sigma^{1}\right|^{1 / 2}}-\frac{1}{2}\left[\left(x-\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1}\left(x-\mu^{1}\right)-\left(x-\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1}\left(x-\mu^{2}\right)\right]$  <ul><li> $\left(x-\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1}\left(x-\mu^{1}\right)=x^{T}\left(\Sigma^{1}\right)^{-1} x-2\left(\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1} x+\left(\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1} \mu^{1}$ </li><li> $\left(x-\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1}\left(x-\mu^{2}\right)=x^{T}\left(\Sigma^{2}\right)^{-1} x-2\left(\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1} x+\left(\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1} \mu^{2}$ </li></ul></li><li> $\begin{aligned} z=& \ln \frac{\left|\Sigma^{2}\right|^{1 / 2}}{\left|\Sigma^{1}\right|^{1 / 2}}-\frac{1}{2} x^{T}\left(\Sigma^{1}\right)^{-1} x+\left(\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1} x-\frac{1}{2}\left(\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1} \mu^{1} \\ &+\frac{1}{2} x^{T}\left(\Sigma^{2}\right)^{-1} x-\left(\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1} x+\frac{1}{2}\left(\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}} \end{aligned}$  </li></ul><hr><p>简化模型后， $\Sigma^1=\Sigma^2=\Sigma$ :</p>$$z=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} x-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}$$<p>令  $w^T=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} \qquad b=-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}$ </p><p>当简化模型后，z是线性的，这也是为什么在之前的结果中边界是线性的原因。</p><p>最后模型变成这样： $P\left(C_{1} | x\right)=\sigma(w \cdot x+b)$ .</p><p>在生成模型中，我们先估计出  $\mu_1\ \mu_2\ N_1\ N_2\ \Sigma$  的值，也就得到了 $w\ b$ 的值。</p><p>那，我们能不能跳过  $\mu_1\ \mu_2\ N_1\ N_2\ \Sigma$  ，直接估计 $w\ b$ 呢？</p><p>在下一篇博客[4]中会继续Classification。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>Classification as Regression: Bishop, P186.</p></li><li><p>挖坑：Classification：Perceptron，SVM.</p></li><li><p>Maximum likelihood solution：Bishop chapter4.2.2</p></li><li></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Classification 有Generative Model和Discriminative Model。&lt;br&gt;这篇文章主要讲述了用生成模型来做分类的原理及过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="Classification" scheme="https://f1ed.github.io/tags/Classification/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-Dan」:Stream Cipher 2</title>
    <link href="https://f1ed.github.io/2020/03/19/StreamCipher2/"/>
    <id>https://f1ed.github.io/2020/03/19/StreamCipher2/</id>
    <published>2020-03-18T16:00:00.000Z</published>
    <updated>2020-07-03T08:45:05.098Z</updated>
    
    <content type="html"><![CDATA[<p>作为Stream Cipher的第二篇文章。<br>第一部分分析了基于Stream Cipher的两种攻击：第一种是Two time pad,第二种是对与其完整性的攻击，即流密码是可被篡改的。<br>第二部分具体说明了一些使用流密码加密的例子。包括分析基于软件的RC4流密码、基于硬件的CSS流密码和现代的安全流密码:eStream中的Salsa20。</p><a id="more"></a><h1 id="Attack-on-OTP-and-stream-ciphers"><a href="#Attack-on-OTP-and-stream-ciphers" class="headerlink" title="Attack on OTP and stream ciphers"></a>Attack on OTP and stream ciphers</h1><h2 id="Attack1-two-time-pad-is-insecure"><a href="#Attack1-two-time-pad-is-insecure" class="headerlink" title="Attack1: two time pad is insecure"></a>Attack1: two time pad is insecure</h2><p><strong><font color=#f00>Never use  strame cipher key more than once!! </font></strong></p><h3 id="why-insecure"><a href="#why-insecure" class="headerlink" title="why insecure"></a>why insecure</h3><p>使用相同的PRG(k)加密不同明文时：<br>$$<br>C_1 \leftarrow m_1 \oplus \text{PRG(k)}\<br>C_2 \leftarrow m_2 \oplus \text{PRG(k)}<br>$$<br>Eavesdropper（窃听者）截获这两段密文 $C_1\ C_2$ ，对密文进行疑惑操作，可得： $C_1 \oplus C_2\rightarrow m_1\oplus m_2$ 。</p><p>在传输中，英语字母是用ASCII编码后再传输，所以这样的编码会带来很多redundancy（冗余），即根据   $m_1\oplus m_2$ ，可以得到 $m_1\ m_2$ 。</p><p>因此，当一个密钥会被使用多次时，就不应该直接用stream cipher，后面的章节会介绍multi-use ciphers。</p><h3 id="Examples-Project-Venona-1941-1946"><a href="#Examples-Project-Venona-1941-1946" class="headerlink" title="Examples: Project Venona(1941-1946)"></a>Examples: <a href="[https://zh.wikipedia.org/wiki/%E7%BB%B4%E8%AF%BA%E9%82%A3%E8%AE%A1%E5%88%92](https://zh.wikipedia.org/wiki/维诺那计划)">Project Venona</a>(1941-1946)</h3><p>我们已经知道：加密应该用OTP，即一次性密钥。</p><p>但是，当时是通过人工掷骰子并记录得到密钥，工作费时费力。因此不得不用生成的密钥加密多条消息。</p><p>最后仅凭截获密文，就破译了3000多条消息。</p><h3 id="Examples-MS-PPTP-Windows-NT"><a href="#Examples-MS-PPTP-Windows-NT" class="headerlink" title="Examples: MS-PPTP(Windows NT)"></a>Examples: MS-PPTP(Windows NT)</h3><p>微软在Windows NT的PPTP协议（point to point transfer protocol）中使用的流密码是：<strong>RC4</strong>。</p><p>在这个协议中允许一个端系统向另一个端系统发送加密后的信息。</p><p>过程如下：</p><p><a href="https://imgchr.com/i/8y4XMn"><img src="https://s1.ax1x.com/2020/03/19/8y4XMn.md.png" alt="8y4XMn.md.png"></a> </p><ol><li>在一次对话连接中：主机想发送$m_1\ m_1\ m_3$ 三条消息进行查询，服务器想发送 $s_1\ s_1\ s_3$ 三条消息进行响应。</li><li>主机和服务器hava a shared key:k。</li><li>知道密钥不能加密多次，于是主机将三条消息进行concatenation（联结）： $m_1||m_2||m_3$ 。</li><li>主机用k作为密钥，得到G(k)，进行加密 $[m_1||m_2||m_3]\oplus\text{G(k)}$ 。</li><li>同样，服务器也将响应消息进行联结： $s_1||s_2||s_3$ 。</li><li>服务器也用k作为密钥，得到相同的G(k)，对响应消息进行加密 $[s_1||s_2||s_3]\oplus\text{G(k)}$ 。</li></ol><p>因此，在一次对话中，主机和服务器都使用了相同的 G(k)进行加密，也就是 two time pad。</p><h4 id="如何改进"><a href="#如何改进" class="headerlink" title="如何改进"></a>如何改进</h4><p>主机和服务器have a shared pair of key, 即主机和服务器都使用不同的key进行加密。</p><h3 id="Examples-802-11b-WEP"><a href="#Examples-802-11b-WEP" class="headerlink" title="Examples: 802.11b WEP"></a>Examples: 802.11b WEP</h3><h4 id="How-it-works"><a href="#How-it-works" class="headerlink" title="How it works"></a><strong>How it works</strong></h4><p>WEP(Wired Equivalent Privacy)，有效等效加密，是一种用于IEEE 802.11b的一种安全算法。这个算法设计的很糟糕，现已被WPA所淘汰。</p><p>WEP用于Wi-Fi通信，是他的的加密层。</p><p>WEP的算法过程如下：</p><p><a href="https://imgchr.com/i/8y4Lxs"><img src="https://s1.ax1x.com/2020/03/19/8y4Lxs.md.png" alt="8y4Lxs.md.png"></a> </p><p>主机和路由器进行通信：</p><ol><li><p>主机和路由 have a shared key。</p></li><li><p>主机想要发送一段消息，包括明文m和其校验码CRC(m)。</p></li><li><p>PRG’s seed： IV||k, k is a long term key，IV is a counter.</p><p>Length of IV: 24 bits.</p><p>IV的作用：每一次传送数据包时，用IV来改变每次的密钥。</p></li><li><p>用(IV||k作为密钥，得到PRG(IV||k),使用流密码进行加密传输。</p></li><li><p>主机直接发送IV和密文。</p></li><li><p>路由器用收到的IV和k连接，用PRG(IV||k)，对密文解密。</p></li></ol><h4 id="Problems-of-IV"><a href="#Problems-of-IV" class="headerlink" title="Problems of IV"></a><strong>Problems of IV</strong></h4><p> <strong>IV 导致的问题1: two time pad</strong></p><p>Length of IV: 24 bits</p><ul><li><p>Related IV after $2^{24}$ (16M frames)</p><p>【当发送16百万的帧后，PRG又会重复】</p></li><li><p>On some 802.11 cards: IV rests to 0 after power cycle.</p><p>【在有些单片机上，重启后IV会变成0：每次重启都会使用PRG(0||k)加密】</p></li></ul><p><strong>IV 导致的问题2: related keys</strong></p><p>在PRG中，key for frame is related。(IV||k)，k是104 bits, IV 是24 bits，所以key的后104位总是相同的，不同密钥之间的差异也很小。</p><p>对RC4 PRG的攻击：</p><ol><li>Fluhrer, Mantin, and Shamir在2001年:只需要监听 $10^6$ 帧即可破译密钥[1]。</li><li>Recent attacks：只需要监听4000帧，即可破译WEP网络中的密钥。</li></ol><p>所以，密钥关联是灾难性的。</p><p><strong>Avoid related keys！</strong></p><h4 id="A-better-construction"><a href="#A-better-construction" class="headerlink" title="A better construction"></a><strong>A better construction</strong></h4><p>对于WEP，一种更好的做法是：将多个要发送的帧联结起来，得到 $m_1||m_2||…||m_n$ 长流，再用PRG对这个长流加密。</p><p><a href="https://imgchr.com/i/8y4q2j"><img src="https://s1.ax1x.com/2020/03/19/8y4q2j.md.png" alt="8y4q2j.md.png"></a> </p><p>如上图所示，k扩展后，被分成很多段，用第一段加密第一帧，第二段加密第二帧……。</p><p>这样，加密每一帧的密钥都是一个伪随机密钥。(no relationship, and looks like random)。</p><p>当然，更好的解决方法是使用更强的加密算法（WPA2）。</p><h3 id="Examples-disk-encryption"><a href="#Examples-disk-encryption" class="headerlink" title="Examples: disk encryption"></a>Examples: disk encryption</h3><p>另一个例子是硬盘加密，在这个例子中，你会发现：使用流密码对硬盘加密不是一个好的举措。</p><h4 id="如果使用流密码："><a href="#如果使用流密码：" class="headerlink" title="如果使用流密码："></a>如果使用流密码：</h4><p><a href="https://imgchr.com/i/8y4HPg"><img src="https://s1.ax1x.com/2020/03/19/8y4HPg.md.png" alt="8y4HPg.md.png"></a> </p><ul><li>Alice 想要给Bob写一封邮件，如上图所示。<ul><li>邮件经过硬盘加密（流密码）后，存入内存块。</li></ul></li><li>Alice 想要对存在这个硬盘中的邮件进行修改。<ul><li>Alice只把Bob改成了Eve，其他部分都没有变，如上图所示。</li><li>保存后，邮件再次经过硬盘加密（流密码）后，存入内存块。</li></ul></li><li>Attacker：他得到了硬盘上最初的密文和修改后的密文。<ul><li>通过分析，他发现两段密文只有前小部分不同。（用相同的流密码密钥加密，修改后，密文很容易看出变化）</li><li>虽然Attacker不知道Alice是怎么修改的，但是他知道了Alice修改的具体位置。</li><li>$\Rightarrow$ attacker得到了他不应该知道的信息，即修改的具体位置。</li></ul></li></ul><p>在硬盘加密中，对于文本内容的修改前后，也使用了相同的密钥段加密不同的消息，即two time pad。</p><p>因此在硬盘加密中，不推荐使用流密码。</p><h3 id="Two-time-pad-Summary"><a href="#Two-time-pad-Summary" class="headerlink" title="Two time pad: Summary"></a>Two time pad: Summary</h3><p><strong>Never use stream cipher key more than once!!</strong></p><ul><li>Network traffic: negotiate new key for every session.</li><li>Disk encryption: typically do not use a stream cipher.</li></ul><h2 id="Attack2-no-integrity-OTP-is-malleable"><a href="#Attack2-no-integrity-OTP-is-malleable" class="headerlink" title="Attack2: no integrity(OTP is malleable)"></a>Attack2: no integrity(OTP is malleable)</h2><p>OPTP和Stream Cipher一样，不提供完整性的保证，只提供机密性的保证。</p><p><a href="https://imgchr.com/i/8y4oa8"><img src="https://s1.ax1x.com/2020/03/19/8y4oa8.md.png" alt="8y4oa8.md.png"></a> </p><p>如上图所示：</p><ol><li>如果attacke截获：密文 $m\oplus k$ 。</li><li>并用sub-permutation key（子置换密钥）来对密文进行修改，得到新的密文：$(m\oplus k)\oplus p$</li><li>新的密文最后解密得到的明文是 $m\oplus p$ 。</li></ol><p>所以对于有修改密文能力的攻击者来说，攻击者很容易修改密文，并且修改后的密文，对原本解密后的明文也有很大的影响。</p><h4 id="具体攻击如下："><a href="#具体攻击如下：" class="headerlink" title="具体攻击如下："></a>具体攻击如下：</h4><p><a href="https://imgchr.com/i/8y4IVf"><img src="https://s1.ax1x.com/2020/03/19/8y4IVf.md.png" alt="8y4IVf.md.png"></a> </p><ol><li><p>Bob想要发送一封邮件：消息开头是From: Bob，使用OTP加密后，发送密文。</p></li><li><p>Attacker：截获了这段密文</p><ul><li><p>假设：attacker知道这封邮件是来自Bob。</p></li><li><p>attacker想修改这封密文邮件，使得它来自others。</p></li><li><p>于是它用一个<strong>子置换密钥</strong>对原密文的特定位置（即Bob密文位置）进行操作，得到新的密文：From： Eve。</p><ul><li><p>这个子置换密钥是什么呢？</p><p>如上图所示，Bob的ASCII码是 42 6F 62，Eve的ASCII码是 45 76 65。</p><p>那么Bob $\oplus$ Eve的ASCII码是 07 19 07。</p><p>因此这个子置换密钥是07 19 07。</p></li></ul></li></ul></li><li><p>最后收件人进行解密，得到的是明文：From：Eve。</p></li></ol><p>对attacker来说，虽然他不能创建来自Eve的密文，但是他可以通过修改原本的密文，达到相同的目的。</p><p>Conclusion: Modifications to ciphertext are undertected and have predictable impact on plaintext.</p><h1 id="Real-world-Stream-Ciphers"><a href="#Real-world-Stream-Ciphers" class="headerlink" title="Real-world Stream Ciphers"></a>Real-world Stream Ciphers</h1><h2 id="Old-example-SW-RC4"><a href="#Old-example-SW-RC4" class="headerlink" title="Old example(SW): RC4"></a>Old example(SW): RC4</h2><p>RC4流密码，是Ron RivestRC4在1987年设计的。曾用于secure  Web traffic(in the SSL/TLS protocol) 和wireless traffic (in the 802.11b WEP protocol).</p><p>It is designed to operate on 8-bit processors with little internal memory.</p><p>At the heart of the RC4 cipher is a PRG, called the RC4 PRG. The PRG maintains an internal</p><p>state consisting of an array S of 256 bytes plus two additional bytes i,j used as pointers into S.</p><p>【RC4 cipher的核心是一个PRG，called the RC4 PRG。这个PRG的内部状态是一个256字节的数组S和两个指向S数组的指针】</p><h3 id="RC4-stream-cipher-generator"><a href="#RC4-stream-cipher-generator" class="headerlink" title="RC4 stream cipher generator"></a>RC4 stream cipher generator</h3><ul><li><p>setup algorithms:</p><ul><li><p>对S数组进行初始化，0-255都只出现一次入下图所示：</p><p><a href="https://imgchr.com/i/8y4RxA"><img src="https://s1.ax1x.com/2020/03/19/8y4RxA.md.png" alt="8y4RxA.md.png"></a>    </p></li><li><p>伪代码</p><p><a href="https://imgchr.com/i/8y422d"><img src="https://s1.ax1x.com/2020/03/19/8y422d.md.png" alt="8y422d.md.png"></a>  </p></li></ul></li><li><p>stream generator:</p><ul><li><p>Once tha array S is initialized, the PRG generates pseudo-random output one byte at atime, using the following stream generator:</p><p><a href="https://imgchr.com/i/8y4g8H"><img src="https://s1.ax1x.com/2020/03/19/8y4g8H.md.png" alt="8y4g8H.md.png"></a> </p></li><li><p>The procedure runs for as long as necessary.</p><p>Again, the index i runs linearly through the array while the index j jumps around.</p></li></ul></li></ul><h3 id="Security-of-RC4"><a href="#Security-of-RC4" class="headerlink" title="Security of RC4"></a>Security of RC4</h3><p>具体参见「A Graduate Course in Applied Cryptography」的p76-78</p><p>挖坑，有空填坑 cryptanalysis of RC4[2]</p><h3 id="Weakness-of-RC4"><a href="#Weakness-of-RC4" class="headerlink" title="Weakness of RC4"></a>Weakness of RC4</h3><ol><li><p>Bias in initial output: Pr[2^nd^ byte=0]=2/256.</p><ul><li><p>如果PRG是随机的，其概率应该是1/256。</p><p>而Pr[2^nd^ byte=0]=2/256的结果是：消息的第二个字节不被加密的概率比正常情况多一倍。（0异或不变）</p></li><li><p>统计的真实情况是，不止第二个字节，前面很多字节的概率很不都均匀。</p><p>因此，如果要使用RC4 PRG，从其输出的257个字节开始使用。</p></li></ul></li><li><p>Prob. of (0,0) is 1/256^2^ +1/256^3^ .</p><ul><li>如果PRG是随机的，00串出现的概率应该是(1/256)^2^ .</li></ul></li><li><p>Related key attackes.</p><p>在上小节中「Examples: 802.11b WEP」，WEP使用的RC4流密码，related key对安全通信也是灾难性的。</p></li></ol><h2 id="Old-example-HW-CSS-badly-broken"><a href="#Old-example-HW-CSS-badly-broken" class="headerlink" title="Old example(HW): CSS (badly broken)"></a>Old example(HW): CSS (badly broken)</h2><p>The Content Scrambling System (CSS) is a system used for protecting movies on DVD disks.</p><p>It uses a stream cipher, called the <strong>CSS stream cipher</strong>, to encrypt movie contents.</p><p>CSS was designed in the 1980’s when exportable encryption was restricted to 40-bits keys. As a result, CSS encrypts movies using a 40-bits key.</p><p>【1980的美国出口法，限制出口的加密算法只能是40位，于是CSS的密钥是40位】[amazing.jpg]</p><p>While ciphers using 40-bit keys are woefully insecure, we show that the CSS stream cipher is particularly weak and can be broken in far less time than an exhaustive search over all 2^40^ keys.</p><p>【虽然40位的密钥本来就不够安全，但是我们能用远小于穷举时间的方法破解CSS】</p><blockquote><p>因为博主也是第一次学，很多东西也不了解。</p><p>所以概述性语言，我用教科书的原文记录，附注一些中文。望海涵～</p></blockquote><h3 id="Linear-feedback-shift-register-LFSR"><a href="#Linear-feedback-shift-register-LFSR" class="headerlink" title="Linear feedback shift register(LFSR)"></a>Linear feedback shift register(LFSR)</h3><p>CSS 流密码是由两个LFSR（线性反馈移位寄存器）组成的，除了CSS，还有很多硬件系统是基于LFSR进行加密操作，但无一例外，都被破解了。</p><ul><li><p>DVD encryption (CSS)：2 LFSRs</p></li><li><p>GSM encryption (A5/,2): 3 LFSRs</p><p>【全球通信系统】</p></li><li><p>Bluetooth(E0): 4LFSRs</p></li></ul><p>LFSR can be implemented in hardware with few transistors. And a result, stream ciphers built from LFSR are attractive for low-cost consumer electronics such as DVD players, cell phones, and Bluetooth devices.</p><p>【LFSR在硬件上运行很快，也很省电，所以虽然基于LFSR的算法都被破解了，但是改硬件有点困难，所以还是有很多系统在使用】</p><hr><p><a href="https://imgchr.com/i/8y4cPe"><img src="https://s1.ax1x.com/2020/03/19/8y4cPe.md.png" alt="8y4cPe.md.png"></a> </p><p>上图是一个8位LFSR{4,3,2,0}。</p><p>LFSR是由一组寄存器组成，LFSR每个周期输出一位（即0位）。</p><p>有一些位（如上图的4，3，2，0）称为tap positions(出头)，通过这些位计算出feedback bit(反馈位)。</p><p>反馈位和寄存器组的未输出位组成新的寄存器组。</p><p>伪代码如下：</p><p><a href="https://imgchr.com/i/8y4rVK"><img src="https://s1.ax1x.com/2020/03/19/8y4rVK.md.png" alt="8y4rVK.md.png"></a> </p><p>所以基于LFSR的PGR的seed是寄存器组的初始状态。</p><h3 id="how-CSS-works"><a href="#how-CSS-works" class="headerlink" title="how CSS works"></a>how CSS works</h3><p>CSS的seed=5 bytes=40 bits。</p><p>CSS由两个LFSR组成，如下图所示，一个17-bit LFSR，一个25-bit LFSR。</p><p><strong>seed of LFSR:</strong></p><ol><li>17-bit LFSR: 1||first 2 bytes ，即17位。</li><li>25-bit LFSR: 1||last 3 bytes，即25位。</li></ol><p><a href="https://imgchr.com/i/8y4N8J"><img src="https://s1.ax1x.com/2020/03/19/8y4N8J.md.png" alt="8y4N8J.md.png"></a> </p><p>CSS过程：</p><ol><li>两个LFSR分别运行8轮，输出8 bits。</li><li>两个8bits 相加mod 256（还有加上前一块的进位）即是CSS一轮的输出：one byte at a time.</li></ol><h3 id="Cryptanalysis-of-CSS-2-16-time-attack"><a href="#Cryptanalysis-of-CSS-2-16-time-attack" class="headerlink" title="Cryptanalysis of CSS (2^16 time attack)"></a>Cryptanalysis of CSS (2^16 time attack)</h3><p>当已知CSS PRG的输出时，我们通过穷举（2^40^  time）破解得到CSS的seed。</p><p>但还有一种更快的破解算法，只需要最多2^16^ 的尝试。</p><p><a href="https://imgchr.com/i/8y4avR"><img src="https://s1.ax1x.com/2020/03/19/8y4avR.md.png" alt="8y4avR.md.png"></a> </p><p>破解过程如下：</p><ol><li><p>影片文件一般是MPEG文件，假设我们已知明文MPEG文件的前20个字节。（已知明文的prefix）</p></li><li><p>CSS是流密码，可以通过已知prefix还原出CSS的prefix，即CSS PRG的前20个字节的输出。</p></li><li><p>For all possible initial settings of 17-bit LFSR do:</p><ul><li><p>run 17-it LFSR to get 20 bytes of output.</p><p>【先让17-bit输出20个字节】</p></li><li><p>subtract from CSS prefix $\Rightarrow$ candidate 20 bytes output of 25-bit LFSR.</p><p>【通过还原的CSS PRG的输出，得到25-bit输出的前20个字节】</p></li><li><p>If consistent with 25-bit LFSR, found correct initial settings of both!!</p><p>【具体是如何判别这20个字节是否是一个25-bit LFSR的输出呢？】</p><ol><li>假设前三个字节是y1, y2, y3.</li><li>那么25-bit LFSR的initial :{1, y1 , y2, y3},其中y都是8 bits.</li><li>用这个初始状态生成20个字节，如果和得到的20个字节相同，则正确，否则再次枚举。</li></ol></li></ul></li><li><p>当得到了两个LFSR的seed, 就可以得到CSS PRG的全部输出，即可破解。</p></li></ol><h2 id="Modern-stream-ciphers-eStream"><a href="#Modern-stream-ciphers-eStream" class="headerlink" title="Modern stream ciphers: eStream"></a>Modern stream ciphers: eStream</h2><h3 id="main-idea"><a href="#main-idea" class="headerlink" title="main idea"></a>main idea</h3><ul><li><p><strong>eStream PRG</strong> ： $\{0,1\}^s\times R \rightarrow \{0,1\}^n$  (n&gt;&gt;s)</p><p>seed: ${0,1}^s$ </p><p>nonce: a non-repeating value for a given key.【就对于确定的seed,nonce绝不重复】</p></li><li><p>Encryption: $\text{E(k, m; r)}=\text{m}\oplus \text{PRG(k; r)} $ </p><p>The pair (k,r) is never used more than once.</p><p>【PRG的输入是(k,r), 确保OTP】</p></li></ul><h3 id="eStram-Salsa-20-SW-HW"><a href="#eStram-Salsa-20-SW-HW" class="headerlink" title="eStram: Salsa 20(SW+HW)"></a>eStram: Salsa 20(SW+HW)</h3><p>Salsa20/12 and Salsa20/20 are fast stream ciphers designed by Dan Bernstein in 2005.</p><p>Salsa 20/12 is one of four Profile 1 stream cipher selected for the eStream portfolio of stream ciphers.</p><p>eStream is a project that identifies fast and secure stream ciphers that are appropriate for practicle use.</p><p>Variants of Salsa20/12 and Salsa20/20, called ChaCha12 and ChaCha20 respectively, were proposed by Bernstein in 2008.</p><p> These stream ciphers have been incorporated into several widely deployed protocols such as TLS and SSH.</p><hr><p>Salsa20 PRG: $\{0,1\}^{128 \text { or } 256} \times\{0,1\}^{64} \longrightarrow\{0,1\}^{n}$ (max n = 2^73^ bits)</p><p>Salsa20 PRG $(\mathrm{k} ; \mathrm{r}):=\mathrm{H}(\mathrm{k},(\mathrm{r}, 0)) | \mathrm{H}(\mathrm{k},(\mathrm{r}, 1)) | \ldots$ 通过计数器，使得输出联结，可以输出as long as you want pseudorandom segment.</p><p><a href="https://imgchr.com/i/8y4wK1"><img src="https://s1.ax1x.com/2020/03/19/8y4wK1.md.png" alt="8y4wK1.md.png"></a> </p><p>算法过程如上图所示：Salsa20 PRG $(\mathrm{k} ; \mathrm{r}):=\mathrm{H}(\mathrm{k},(\mathrm{r}, 0)) | \mathrm{H}(\mathrm{k},(\mathrm{r}, 1)) | \ldots$</p><ol><li><p>32 bytes的{k,r,i}通过扩展得到64 bytes的{ $\tau_0,k,\tau_1,r,i,\tau_2,k,\tau_3$ }.</p><ul><li>k :16 bytes的seed.</li><li>i: 8 bytes的index，计数器。</li><li>r: 8 bytes的nonce.</li><li>$\tau_{0,1,2,3}$ 都是4 bytes的常数，Salsa20算法说明书上公开确定的值。</li></ul></li><li><p>64 bytes 通过h函数映射，十轮。</p><ul><li><p>h : invertible function designed to be fast on x86(SEE2).</p><p>在x86上有SEE2指令可以直接运行h函数，所以很快。</p></li><li><p>h是逆函数（也是公开的函数），输出可以通过逆运算得到其输入。</p></li><li><p>h是一个一一映射的map，每一个64bytes的输入都有唯一对应的64 bytes的输出。</p></li></ul></li><li><p>将第十轮H函数的输出和最开始的输入做加法运算，word by word(32位)，即每4 bytes相加。</p><ul><li><p>为什么要有这一步？</p><p>h是可逆运算，如果直接将函数的输出作为PRG的输出，那可以通过h的逆运算得到原64 bytes，也就得到了(k; r).</p></li></ul></li></ol><h3 id="Is-Salsa20-secure-unpredictable"><a href="#Is-Salsa20-secure-unpredictable" class="headerlink" title="Is Salsa20 secure(unpredictable)"></a>Is Salsa20 secure(unpredictable)</h3><p>前文我们通过unpredictable来定义PRG的安全（下一篇文章会给出安全PRG更好的定义），所以Salsa20 安全吗？是否是不可预测的？</p><ul><li><p>Unknown：no known <strong>provably</strong> secure PRGs.</p><p>【不能严格证明是一个安全PRG（后文会继续讲解什么是安全的PRG），如果严格证明了，也就证明了P=NP】</p></li><li><p>In reality： no known attacks bertter than exhaustive search.</p><p>【现实中还没有比穷举更快的算法】</p></li></ul><h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><p>通过下图的比较，如果在系统中需要使用流密码，建议使用eStream。</p><p><a href="https://imgchr.com/i/8y40Dx"><img src="https://s1.ax1x.com/2020/03/19/8y40Dx.md.png" alt="8y40Dx.md.png"></a> </p><p>speed ：该算法每秒加密多少MB的数据。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>S. Fluhrer, I. Mantin, and A. Shamir. Weaknesses in the key scheduling algorithm of RC4.</p><p>In proceedings of selected areas of cryptography (SAC), pages 1-24, 2001.</p></li><li><p>「A Graduate Course in Applied Cryptography」p76-78:挖坑 待补充</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为Stream Cipher的第二篇文章。&lt;br&gt;第一部分分析了基于Stream Cipher的两种攻击：第一种是Two time pad,第二种是对与其完整性的攻击，即流密码是可被篡改的。&lt;br&gt;第二部分具体说明了一些使用流密码加密的例子。包括分析基于软件的RC4流密码、基于硬件的CSS流密码和现代的安全流密码:eStream中的Salsa20。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cryptography-Dan" scheme="https://f1ed.github.io/categories/Cryptography-Dan/"/>
    
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="Cryptography" scheme="https://f1ed.github.io/tags/Cryptography/"/>
    
      <category term="StreamCipher" scheme="https://f1ed.github.io/tags/StreamCipher/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-Dan」:Stream Cipher 1</title>
    <link href="https://f1ed.github.io/2020/03/15/StreamCipher1/"/>
    <id>https://f1ed.github.io/2020/03/15/StreamCipher1/</id>
    <published>2020-03-14T16:00:00.000Z</published>
    <updated>2020-07-03T08:44:51.408Z</updated>
    
    <content type="html"><![CDATA[<p>Stream Cipher的第一部分：介绍了One Time Pad和Stream Cipher中的PRG。<br>其中OTP部分叙述了什么是Perfect Secrecy？为什么OTP很难在实践中应用？<br>Stream Cipher部分中，本文主要阐述了什么是PRG？Stream Cipher的另一种安全的定义（依靠PRG的unpredictable)。<br>本文后半部分，详细阐述了一种weak PRG——线性同余生成器，它是如何工作的？它为什么不安全？如何attack it elegantly?</p><a id="more"></a><h1 id="The-One-Time-Pad"><a href="#The-One-Time-Pad" class="headerlink" title="The One Time Pad"></a>The One Time Pad</h1><h2 id="Symmetric-Ciphers-difinition"><a href="#Symmetric-Ciphers-difinition" class="headerlink" title="Symmetric Ciphers: difinition"></a>Symmetric Ciphers: difinition</h2><p><strong>Def</strong> <em>:a cipher difined over $\mathcal{(K,M,C)}$  is a paire of “efiicient “ algorithms $(E,D)$ where</em></p> $$E :\mathcal{K \times M \longrightarrow \mathcal{C}} \quad ,\quad D:\mathcal{K\times\mathcal{C}\longrightarrow\mathcal{M}} \\ s.t. \quad \forall m\in \mathcal{M},k\in \mathcal{K}:D(k,E(k,m))=m$$<hr><p>$\mathcal{(K,M,C)}$ 分别是密钥空间、明文空间、密文空间。</p><p>对称加密其实是定义在$\mathcal{(K,M,C)}$ 的两个有效算法 $(E,D)$ ，这两个算法满足consistence equation(一致性方程)：$D(k,E(k,m))=m$ 。</p><p>一些说明：</p><ol><li><p>$E$ is ofen randomized. 即加密算法E总是随机生成一些bits，用来加密明文。</p></li><li><p>$D$ is always deterministic. 即当确定密钥和明文时，解密算法的输出总是唯一的。</p></li><li><p>“efficient” 的含义</p><ul><li>对于理论派：efficient表示 in polynomial time（多项式时间）</li><li>对于实践派：efficient表示 in a certain time</li></ul></li></ol><h2 id="One-Time-Pad-OTP"><a href="#One-Time-Pad-OTP" class="headerlink" title="One Time Pad(OTP)"></a>One Time Pad(OTP)</h2><h3 id="Definition-of-OTP"><a href="#Definition-of-OTP" class="headerlink" title="Definition of OTP"></a>Definition of OTP</h3><p>The one time pad(OTP) 又叫一次一密。</p><p><strong>用对称加密的定义来表示OTP：</strong></p><ul><li>$\mathcal{M=C=}{0,1}^n\quad \mathcal{K}={0,1}^n$ </li><li>$E：\quad c = E(k,m)=k\oplus m \quad$</li><li>$D:\quad m = D(k,c)=k\oplus c$ </li></ul><p>明文空间和密文空间相同，密钥空间也是n位01串集合。</p><p>而且，在OTP中，密钥key的长度和明文message长度一样长。</p><p><a href="https://imgchr.com/i/81TrSs"><img src="https://s1.ax1x.com/2020/03/15/81TrSs.png" alt="81TrSs.png"></a> </p><p>加密过程如上图所示。</p><ul><li><p><strong>证明其一致性方程</strong></p><p><strong>Indeed</strong> ： </p><p>$D(k,E(k,m))=D(k,k\oplus m)=k\oplus (k\oplus m)=0\oplus m=m$ </p></li></ul><hr><p>但是OTP加密安全吗？</p><p>如果已知明文(m)和他的OTP密文(c)，可以算出用来加密m的OTP key吗？</p><p>：当然，根据异或的性质，key $k=m\oplus c$ </p><p>所以什么是安全呢？</p><h3 id="Perfect-Security-Definition"><a href="#Perfect-Security-Definition" class="headerlink" title="Perfect Security Definition"></a>Perfect Security Definition</h3><p>根据Shannon 1949发表的论文，<strong>Shannon’s basic idea: CT(Ciphertext) should reveal no “info” about PT(Plaintext)</strong>，即密文不应该泄露明文的任何信息。</p><h4 id="Perfect-Security-Def"><a href="#Perfect-Security-Def" class="headerlink" title="Perfect Security Def:"></a><strong><u>Perfect Security Def:</u></strong></h4><p>A cipher $(E,D)$ over  $\mathcal{(K,M,C)}$ has <strong>perfect security</strong> if $\forall m_0,m_1 \in \mathcal{M}\ (|m_0|=|m_1|) \quad \text{and} \quad \forall c\in \mathcal{C} $<br>$$<br>Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \qquad \text{where} \ k\overset{R}{\longleftarrow}\mathcal{K}<br>$$</p><hr><blockquote><p>$k \overset{R}\longleftarrow \mathcal{K}$  的意思是 $k$ 是 从$\mathcal{K}$ 中随机取的，即随机变量 $k$ 的取值是均匀分布。</p></blockquote><p>对任意 $m_0,m_1$ （并且message长度相同），那么在密钥空间任意取 $k$  , $k$ 将 $m_0,m_1$ 加密为相同密文的概率相同。</p><h4 id="对attacker来说-："><a href="#对attacker来说-：" class="headerlink" title="对attacker来说 ："></a><strong>对attacker来说</strong> ：</h4><p>攻击者截取一段密文c，那么c是由 $m_0,m_1$ 加密而来的概率是相同的，即攻击者也不知道明文到底是 $m_0$ 还是 $m_1$ （因为概率相同）。</p><ul><li>$\Rightarrow$ Given CT can’t tell if msg is $m_0 \ \text{or}\ m_1 $ (for all $m_i$  ) .   【攻击者不能区分明文到底是 $m_?$ 】</li><li>$\Rightarrow$ most powerful adv.(adversary) learns nothing about PT from CT.   【不管攻击者多聪明，都不能从密文中得到密文的信息】</li><li>$\Rightarrow$ no CT only attack!! (but other attackers possible).     【惟密文攻击对OTP无效】</li></ul><h3 id="OTP-has-perfect-secrecy"><a href="#OTP-has-perfect-secrecy" class="headerlink" title="OTP has perfect secrecy"></a>OTP has perfect secrecy</h3><p><strong><u>Lemma</u></strong> : <em>OTP has perfect secrecy.</em></p><p>用上一小节的perfect securecy的定义来证明这个引理。</p><p><strong>Proof</strong>：</p><ol><li><p>要证明： $Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \qquad \text{where} \ k\overset{R}{\longleftarrow}\mathcal{K}$ </p></li><li><p>表达式： $\forall m, c: \quad \operatorname{Pr}_{k}[E(k,m)=c]=\frac{\# \text{keys} \  k \in \mathcal{K} \quad s.t.\; E(k,m)=c}{|\mathcal{K}|}$ </p><p>对于任意m,c,  $\operatorname{Pr}_{k}[E(k,m)=c]$  等于能将m加密为c的密钥个数除以密钥空间的大小。</p></li><li> $\because |\mathcal{K}|$ 是相同的，所以即证 ： $\{ \# \text{keys} \  k \in \mathcal{K} \quad s.t.\; E(k,m)=c \}=\text{const}$ </li><li><p>对于任意 m,c，能将m加密为c的OTP key只有一个： $k=m\oplus c$</p></li><li><p>$\therefore$ OTP has perfect secrecy.</p><h3 id="key-len-geq-msg-len"><a href="#key-len-geq-msg-len" class="headerlink" title="key-len $\geq$ msg-len"></a>key-len $\geq$ msg-len</h3></li></ol><p>Perfect Secrecy的性质带来了一个bad news。</p><p><strong><u>Thm</u></strong>: <em>perfect secrecy $\Rightarrow$   $|\mathcal{K}|\geq|\mathcal{M}|$</em> </p><p>如果一个cipher满足perfect secrecy,那么其密钥的长度必须大于等于明文长度。这也是perfect secrecy的必要条件。</p><p>所以OTP是perfect secrecy的最优情况，$|\mathcal{K}|=|\mathcal{M}|$ ，密钥长度等于明文长度。</p><hr><p>为什么说是一个bad news呢？</p><p>如果Alice用OTP给Bob发一段msg，在她发之前，她需要先发一个和msg等长的key，这个key只有Alice和Bob知道。</p><p>所以如果Alice有能保密传输key的方法，那她何不直接用这个方法传输msg呢？</p><p>所以OTP : <strong>hard to use in practice!</strong>  (long key-len)</p><p>因此，我们需要key-len短的cipher。</p><h1 id="Pseudorandom-Generators（伪随机数生成器）"><a href="#Pseudorandom-Generators（伪随机数生成器）" class="headerlink" title="Pseudorandom Generators（伪随机数生成器）"></a>Pseudorandom Generators（伪随机数生成器）</h1><h2 id="Stream-Ciphers-making-OTP-practical"><a href="#Stream-Ciphers-making-OTP-practical" class="headerlink" title="Stream Ciphers: making OTP practical"></a>Stream Ciphers: making OTP practical</h2><p>Stream Ciphers（流密码）的思想就是：用PRG（pseudorandom Generators） key 代替 “random” key。</p><p><strong>PRG</strong>其实就是一个function  G：${ 0,1 }^s\longrightarrow { 0,1 }^n \quad, n&gt;&gt;s$ 。</p><p>通过函数将较小的seed space映射到大得多的output space。</p><p><strong>注意：</strong> function G is eff. computable by a deterministic algorithm.</p><ul><li>函数G是确定的，随机的只有s，s也是G的输入。</li><li>PRG的输出应该是 “look random”（下文会提到的PRG必须是unpredictable）</li></ul><p><a href="https://imgchr.com/i/81TUw8"><img src="https://s1.ax1x.com/2020/03/15/81TUw8.md.png" alt="81TUw8.md.png"></a> </p><p>Stream Ciphers的过程如上图所示：通过PRG，将长度较短的k映射为足够长的G(k)，G(k)异或m得到密文。</p><hr><p>有两个问题？</p><p>第一，Stream Cipher安全吗？为什么安全？</p><p>第二，Stream Cipher have perfect secrecy?</p><p>现在，只能回答第二个问题。</p><p>：流密码没有perfect secrecy。因为它不满足key-len $\geq$ msg-len，流密码的密钥长度远小于明文长度。</p><p>流密码没有perfect secrecy，所以我们还需要引入另一种安全，这种安全和PRG有关。</p><h2 id="PRG-must-be-unpredictable"><a href="#PRG-must-be-unpredictable" class="headerlink" title="PRG must be unpredictable"></a>PRG must be unpredictable</h2><p>PRG如果predictable，流密码安全吗？</p><h3 id="Suppose-predictable"><a href="#Suppose-predictable" class="headerlink" title="Suppose predictable"></a>Suppose predictable</h3><p>假设PRG是可预测的，即：</p>$ \exists:\quad G(k)|_{1,2,...,i}\quad \overset{alg.}\longrightarrow \quad G(k)|_{i+1,...,n} $<p>已知G(k)输出的前i bis，存在一种算法，能计算G(k)的后面剩余的bits。</p><p><a href="https://imgchr.com/i/81TNef"><img src="https://s1.ax1x.com/2020/03/15/81TNef.png" alt="81TNef.png"></a> </p><p>攻击如上图所示：</p><ol><li><p>如果attacker has prior knowledge：已知一段密文前缀的对应明文（m斜线字段）（比如在SMTP协议中，报文的开头总是”from”）</p></li><li><p>attacker将该密文字段与已知明文字段异或，得到G(k)的前缀。</p></li><li><p>因为PRG是可预测的，所以可以通过G(k)的前缀计算出G(k)的剩下部分。</p></li><li><p>得到的G(K)就可以recover m。</p></li></ol><p>即使，G(k)只能预测后一位，即  $\quad G(k)|_{1,2,...,i}\quad \overset{alg.}\longrightarrow \quad G(k)|_{i+1}$  ，也不安全，当预测出下一位时，又得到了新的前缀，最终得到完整的G(k)。</p><p>所以当PRG可预测时，流密码就不安全了。</p><p><strong>所以用Stream Cipher时，PRG必须unpredictable!</strong> </p><h3 id="Predictable-difinition"><a href="#Predictable-difinition" class="headerlink" title="Predictable: difinition"></a>Predictable: difinition</h3><p><strong><u>Predictable Def</u></strong> :</p>$ \exists $ "eff" alg. A and $\exists$ $0\leq i\leq n-1$ ， s.t. $Pr_{k \overset{R}\leftarrow \mathcal{K} }  {[A(G(k)|_{1,2,...,i})=G(k)|_{i+1}]}>1/2 +\epsilon$ <p><em>for non-negligible</em> $\epsilon$ (e.g. $\epsilon=1/2^{30}$)</p><p>可预测：即存在算法，通过G(k)的前i位可以计算出第i+1位的概率大于1/2 + $\epsilon$ (不可忽略的值)</p><p><strong><u>Unpredictable Def</u></strong> :</p><p>即predictable的反面， $\forall i$ : no “eff.” adv. can predict bit(i+1) for “non-neg” $\epsilon$ .</p><hr><p>Q：假设 $\mathrm{G}: \mathrm{K} \rightarrow{0,1}^{\mathrm{n}} $ ，满足XOR(G(k))=1，G可预测吗？</p><p>W：G可预测，存在i = n-1,因为当已知前n-1位,可以预测第n位。</p><h2 id="Weak-PRGs"><a href="#Weak-PRGs" class="headerlink" title="Weak PRGs"></a>Weak PRGs</h2><h3 id="Linear-Congruential-Generators"><a href="#Linear-Congruential-Generators" class="headerlink" title="Linear Congruential Generators"></a>Linear Congruential Generators</h3><p>一种应该永远不在安全应用中使用PRG——LCG（linear congruential generators）(线性同余随机生成器)。</p><p>虽然他们在应用中使用很快，而且其输出还有良好的统计性质（比如0的个数和1的个数基本相等等），但他们应该never be used for cryptographic。</p><p>因为在实践中，给出LCG输出的一些连续序列，很容易计算出输出的剩余序列。</p><h3 id="Basic-LCG"><a href="#Basic-LCG" class="headerlink" title="Basic LCG"></a>Basic LCG</h3><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>Basic LCG has four public system parameters: </p><p>an integer q, two constants a,b $\in { 0,…,q-1}$ , and a positive integer $w\leq q$ . </p><p>The constant a is taken to be relatively prime to q.</p><p>【有四个公开参数：整数q，两个q剩余系下的常数a,b，（a与q互素）一个小于等于q的正整数w。】</p>We use $\mathcal{S}_q$ and $\mathcal{R}$ to denote the sets: $\mathcal{S}_{q}:=\{0, \ldots, q-1\} ; \quad \mathcal{R}:=\{0, \ldots,\lfloor(q-1) / w\rfloor\}$Now, the generators $G_{\mathrm{lcg}}: \mathcal{S}_{q} \rightarrow \mathcal{R} \times \mathcal{S}_{q}$ with seed $s\in\mathcal{S}_{q}$  defined as follows:$G_{\operatorname{lcg}}(s):=(\lfloor s / w\rfloor, \quad a s+b \bmod q)$<p>【LCG的输出是一对数，$(\lfloor s / w\rfloor, \quad a s+b \bmod q)$ 】</p><p>当 $w=2^t$ 时，$\lfloor s / w\rfloor$  simpky erases the t lease significant bits of s【向右平移t位】。</p><h4 id="Insecure"><a href="#Insecure" class="headerlink" title="Insecure"></a>Insecure</h4><p>当已知 $s^{\prime}:=a s+b \bmod q$ ，即可直接求出s，也就求出了所谓的随机数 $\lfloor s/w\rfloor$ .</p><h3 id="Variant-Blum-Micali-construction"><a href="#Variant-Blum-Micali-construction" class="headerlink" title="Variant: Blum-Micali construction"></a>Variant: Blum-Micali construction</h3><h4 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h4><p><a href="https://imgchr.com/i/81TaTS"><img src="https://s1.ax1x.com/2020/03/15/81TaTS.md.png" alt="81TaTS.md.png"></a> </p><p>如上图所示，变体的LCG是一个迭代，输出不包括 $s_i$ ，把 $r_1,…,r_n$ 作为一次迭代的输出。</p><p>不同的应用系统使用不同的 $q,a,b,w$ 参数，在Java 8 Development Kit（JDKv8）中，$q=2^{48}$ , $w=2^{22}$ ,constant $a=\text{0x5DEECE66D}$ ,  $b=\text{0x0B}$ 。</p><p>所以在JDKv8中, LCG的输出其实是 $s_i$（48bits） 的前48-22=26 bits 。</p><p>显然JDKv8中的参数大小应用在安全系统中，还是太不安全了。</p><h4 id="how-to-attack-in-JDKv8"><a href="#how-to-attack-in-JDKv8" class="headerlink" title="how to attack in JDKv8"></a>how to attack in JDKv8</h4><ol><li><p>在迭代的第一次输出中，LCG就 reveal 26bits of the seed s。</p></li><li><p>对于s剩下的后22个bits，attacker can easily recover them by exhausitive search(穷举)：</p></li><li><p>对于每个可能的取值，attacker都能得到一个候选seed $\hat{s}$ </p></li><li><p>用 $\hat{s}$ 来验证我们所直接得到的LCG的输出。</p></li><li><p>如果 $\hat{s}$ 验证失败，则到第三步继续穷举。直至验证成功。</p></li><li><p>当穷举至正确的s时，就可以直接预测LCG的剩余输出。</p></li></ol><p>在现代处理器中，穷举 $2^{22}$ (4 million) 只需要1秒。所以LCG的参数较小时，是很容易attack。</p><p>当 $q=2^{512}$ 时，这种穷举的攻击方法就失效了。但是有一种对于LCG的著名攻击方法[1]，即使每次迭代，LCG只输出较少的bits，也能从这些较少的但连续的输出序列中预测出整个LCG输出序列。</p><h3 id="Cryptanalysis-：elegant-attack"><a href="#Cryptanalysis-：elegant-attack" class="headerlink" title="Cryptanalysis ：elegant attack"></a>Cryptanalysis ：elegant attack</h3><h4 id="Warning-of-Math"><a href="#Warning-of-Math" class="headerlink" title="Warning of Math"></a><font color=#f00>Warning of Math</font></h4><h4 id="Suppose"><a href="#Suppose" class="headerlink" title="Suppose"></a>Suppose</h4><p>Suppose : q is large (e.g. $q=2^{512}$  ), and $G_{lcg}^{(n)}$ outputs about half the bits of the state s per iteration.</p><p>【q很大， $G_{lcg}^{(n)}$ 每次输出s的一半左右的bits】</p><p>More precisely, suppose: $w&lt;\sqrt{q}/c$  for fixed c（e.g. $c=32$ ）</p><p>【保证输出s前一半左右bits的这个条件】</p><p>Suppose the attacker is given two consecutive outputs of the gnerator $r_i,r_{i+1}\in \mathcal{R}$ .</p><p>【已知两个连续输出 $r_i,r_{i+1}\in \mathcal{R}$ 】</p><h4 id="Attacker-Knows"><a href="#Attacker-Knows" class="headerlink" title="Attacker Knows"></a>Attacker Knows</h4><ol><li><p>$r_{i}=\left\lfloor s_{i} / w\right\rfloor \quad \text { and } \quad r_{i+1}=\left\lfloor s_{i+1} / w\right\rfloor=\left\lfloor\left(a s_{i}+b \bmod q\right) / w\right\rfloor$ </p><p>【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i$ 】</p></li><li><p>$r_{i} \cdot w+e_{0}=s \quad \text { and } \quad r_{i+1} \cdot w+e_{1}=a s+b+q x \qquad (0\leq e_0,e_1&lt;w&lt;\sqrt{q}/c)$  </p><p>【 去掉floor符号和mod：$e_0,e_1$  是 $s_i,s_{i+1}$ 除 $w$ 的余数】</p><p>【已知： $r_i,r_{i+1},w,a,b,q$；未知： $s_i,e_0,e_1,x$ 】</p></li><li><p>re-arranging: put $x$ and $s$ on the left</p><p>$s=r_{i} \cdot w+e_{0} \quad \text { and } \quad a s+q x=r_{i+1} w-b+e_{1}$ </p><p>【把未知参数s，x放在等式左边，方便写成矩阵形式】</p></li><li><p>$s \cdot\left(\begin{array}{l}1 \ a\end{array}\right)+x \cdot\left(\begin{array}{l}0 \ q\end{array}\right)=\boldsymbol{g}+\boldsymbol{e} \quad \text { where } \quad \boldsymbol{g}:=\left(\begin{array}{c}r_{i} w \ r_{i+1} w-b\end{array}\right) \quad \text { and } \quad \boldsymbol{e}:=\left(\begin{array}{c}e_{0} \ e_{1}\end{array}\right)$ </p><p>【已知： $\boldsymbol{g},a,q$ ，未知：$\boldsymbol{e},s,x$ 】 </p></li><li><p>to break the generator it suffices to find the vector $\boldsymbol{u}:=\boldsymbol{g}+\boldsymbol{e}$ .</p><p>【令 $u\in {Z}^2$ , $\boldsymbol{u}:=\boldsymbol{g}+\boldsymbol{e}=s \cdot(1, a)^{\mathrm{T}}+x \cdot(0, q)^{\mathrm{T}}$ 】</p><p>【如果我们求出了 $\boldsymbol{u}$ ，那可以用线性代数的知识解出 $s$ 和 $x$ ,再用 $s$ 来预测PRG的剩下输出】</p></li><li><p>konws $\boldsymbol{g}$ ,  knows $\boldsymbol{e}$ is shorter, and $|\boldsymbol{e} |_{\infty}$ is at most $\sqrt{q}/c$ , knows that $\boldsymbol{u}$ is “close” to $\boldsymbol{g}$ .</p><p>【e向量很小，$|\boldsymbol{e} |_{\infty}$ 上界是$\sqrt{q}/c$ ，u离g很近】</p></li></ol><blockquote><p><strong>Taxicab norm or Manhattan(1-norm)</strong></p>  ${\|}A{\|}_1=\max \{ \sum|a_{i1}|,\sum|a_{i2}|,...,\sum|a_{in}| \}$  （列和范数，A矩阵每一列元素绝对值之和的最大值）<p><strong>Euclidean norm(2-norm)</strong>  </p>  $\|\mathbf{x}\|=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{2}\right)^{1 / 2}$  <p><strong>$\infty$-范数</strong></p>  $\|A\|_{\infty}=\max \{ \sum|a_{1j}|,\sum|a_{2j}|,...,\sum|a_{mj}| \}$ （行和范数，A矩阵每一行元素绝对值之和的最大值）</blockquote><ol start="7"><li><p>attack can figure the lattice with attacking LCG.</p><p><a href="https://imgchr.com/i/81T0YQ"><img src="https://s1.ax1x.com/2020/03/15/81T0YQ.md.png" alt="81T0YQ.md.png"></a> </p><p>the lattice is generated by the vectors $(1,5)^T$ and $(0,29)^T$ , the attacker has a vector $\boldsymbol{g}=(9,7)^T$ and wishes to find the closest lattice vector $\boldsymbol{u}$ .</p><p>【上图是  $(1,5)^T$ 和 $(0,29)^T$ 两个向量生成的的格点，希望能从以上格点找到离已知 $\boldsymbol{g}$ 向量最近的格点】</p></li></ol><blockquote><p>$\mathcal{L}_a$ :由 $(1, a)^{\mathrm{T}},(0, q)^{\mathrm{T}}$ 作为基向量生成的点集合。</p></blockquote><ol start="8"><li><p>The problem is a special case of a general problem call the <strong>closest vector problem</strong>: given a lattice $\mathcal{L}$ and a vector $\boldsymbol{g}$ ,find a vector in $\mathcal{L}$ that is closest to $\mathcal{g}$ .</p><p>There is an efficient  polynomial time algorithm for this problem.[2]</p><p>【问题归结于 closest vector problem问题，在已知栅格点集合中找离某一向量最近的点，此问题已有多项式时间算法】</p></li></ol><h4 id="step-8-above"><a href="#step-8-above" class="headerlink" title="step 8 above"></a>step 8 above</h4><p><strong><u>Lemma</u></strong> :<br> * For at least $(1-16/c^2)\cdot q $ of the a in $\mathcal{S}_q$ , the lattice $\mathcal{L}_a\sub Z_2$  has the following property: for every $\boldsymbol{g} \in Z^2$ there is at most one vector $\boldsymbol{u}\in \mathcal{L}_a$ such that $\|\boldsymbol{g}-\boldsymbol{u}\|_{\infty}<\sqrt{q}/c$ . *<br>【假设c=32，这个定理表示的意思是：a在$\mathcal{S}_q$ 的所有取值，有98%的取值保证了 $\mathcal{L}_a$ 中离向量 $\boldsymbol{g}$ 最近的点就是我们所求的 $\boldsymbol{u}$ 】</p><hr><p>Armed with this algorithm the attacker can recover the internal state $s_i$  of the LCG generator from just two outputs $r_i,r_{i+1}$  of the generator and predict the remaining sequence. This attack works for 98% of the $a\in \mathcal{S}_q$.</p><p>【这个定理，可以保证文献中那个多项式算法[2]可以解决98%的a的取值】</p><p>For completeness we note that some example $a\in \mathcal{S}_q$ in the 2% where the attack fails are a = 1</p><p>and a = 2. For these a there may be many lattice vectors in $\mathcal{L}_a$ close to a given $\boldsymbol{g}$ .</p><p>【剩下2%的取值，是a=1,a=2这种取值很小的情况，因为Lattice中格点都离太近了】</p><h4 id="Warning-of-Math！！！（建议清醒的时候看）"><a href="#Warning-of-Math！！！（建议清醒的时候看）" class="headerlink" title="Warning of Math！！！（建议清醒的时候看）"></a><font color=#f00>Warning of Math！！！</font>（建议<del>清醒的时候</del>看）</h4><h4 id="Proof-of-lemma"><a href="#Proof-of-lemma" class="headerlink" title="Proof of lemma"></a><u>Proof of lemma</u></h4><ul><li><p>suppose:   there are two vectors $u_0,u_1$ close to $g$ , that is , $\left\|\boldsymbol{u}_{i}-\boldsymbol{g}\right\|_{\infty}<\sqrt{q} / c \text { for } i=0,1$ . </p><p>【反证：假设有两个u1,u2离向量g很近，那么u1,u2也离的很近】</p></li><li> by the triangle inequality, have $\left\|\boldsymbol{u}_{0}-\boldsymbol{u}_{1}\right\|_{\infty} \leq\left\|\boldsymbol{u}_{0}-\boldsymbol{g}\right\|_{\infty}+\left\|\boldsymbol{g}-\boldsymbol{u}_{1}\right\|_{\infty} \leq 2 \sqrt{q} / c$ .<p>【三角不等式得到， $\boldsymbol{u}_0-\boldsymbol{u}_1$ 的行和范数最大是 $\sqrt{q} / c$ 】</p></li><li>$\boldsymbol{u}:=\boldsymbol{u}_{0}-\boldsymbol{u}_{1}$ is a vector in $\mathcal{L}_a$ ,and $\mathcal{L}_a$ conclude a "short" vector of non-zero vecor of norm at most $B= 2\sqrt{q} / c$ . Let's bound the number of bad a's for which $\mathcal{L}_a$  contains such a "short vector".<p>【向量集合的加法封闭性，$\mathcal{L}_a$ 中有一种”short vector”, 他的行和范数最大是$2\sqrt{q} / c$】</p><p>【我们去找有多少个”short vector” (number of bad a的最大值)，使得有两点$ \boldsymbol{u}_0,\boldsymbol{u}_1$ 都离 $\boldsymbol{g}$ 很近】 </p></li><li><p>set $\boldsymbol{t}=(s, y)^{\mathrm{T}} \in \mathbb{Z}^{2}$ be the “short vector” such that $|t|_{\infty} \leq B$ .</p></li><li><p>【以下情况讨论可以用一般同余方程 $y=a s(\bmod q)$  求通解的推导过程来理解】</p></li><li><p>first consider the case when q is prime.</p><p>【先考虑q是素数的情况】</p><ol><li> $\boldsymbol{t}$ 是$\mathcal{L}_a$ 中的点：存在 $s_a,x_a$ 让 $s_{a} \cdot(1, a)^{\mathrm{T}}+x_{a} \cdot(0, q)^{\mathrm{T}}=\boldsymbol{t}=(s, y)^{\mathrm{T}}$ 等式成立。</li><li><p>$\Rightarrow$ $s=s_a$  和  $y=as_a\pmod{q}$ , 而且 $s\neq0$ 。</p></li><li><p>$y=as\pmod{q}$ ，由于q是素数， $a$ 可以唯一确定，得到 $a=y s^{-1} \bmod q$ 。</p></li><li><p>Hence, when q is prime, every non-zero short vector t is contained in at most one lattice $\mathcal{L}_a$ for some $a\in \mathcal{S}_q$ .</p><p>【每一个 $\boldsymbol{t}$  都最多唯一对于 $\mathcal{L}_a$ 中的一个a】</p></li><li><p>∞-范数的图如下图，所以”short vector” 的个数不超过这个正方形中的格点数，即 $=(2B)^2=16q/c^2$ </p><p><a href="https://imgchr.com/i/81TYOP"><img src="https://s1.ax1x.com/2020/03/15/81TYOP.png" alt="81TYOP.png"></a> </p></li><li><p>表述：<strong>当 $q$ 是素数时，每一个 “short vector” 对应一个 $a$ 的取值。</strong></p><p>表述： $a$ 最多有 $16q/c^2$ 个。</p></li></ol></li><li><p>second consider the case when q is not prime.</p><ol><li><p>$y=a s(\bmod q)$ ,q 不是质数，令 $d=\gcd(s,q)$ ，y肯定也是d的倍数</p></li><li><p>$\Rightarrow y/d=a\cdot s/d \pmod{q/d}$ ,且 $\gcd(s/d,q/d)=1$ </p></li><li><p>$a_0$ 是 $a$ 在 mod $q/d$ 下的逆元。【同余方程的通解推导过程可以去看看】</p></li><li><p>那么$y/d=a\cdot s/d \pmod{q/d}$ 中解 $a=a_0\cdot y/d$ </p></li><li><p>$y=a s(\bmod q)$ 的通解就是 $a=a_0\cdot y/d + k\times q/d$  ($k=0,1,…d-1$  ) ，解数是 $d$ .</p></li><li><p>表述：$y$ 最多有 $2B$ 个取值，但是只有 $2B/d$ 个 $y$ 使得方程有解。</p><p>表述：所以 $\boldsymbol{t}$向量的 $y$ 值最多只有 $2B/d$ 个。即”short vector” 只有 $2B/d \cdot2B$ 个</p><p>表述：每个 $y$ 使方程有解，但其解数是 $d$ 个。<strong>即每一个 “short vector” 对应 $d$ 个 $a$ 值。</strong></p><p>表述：同样, $a$ 最多有 $16q/c^2$ 个。</p></li></ol></li><li><p>综上， $\mathcal{S}_q$ 中最多有 $16q/c^2$ 个 bad $a$ 。</p></li><li><p>Therefore, for $(1-16/c^2)\cdot q$   of the a values in $\mathcal{S}_q$ , the lattice $\mathcal{L}_a$  contains no non-zero short vectors and the lemma follows. </p><p>【证毕】 </p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol start="0"><li><p>LCG的数学部分来自Dan教授密码学的公开书:「A Graduate Course in Applied Cryptography」by D. Boneh and V.shoup</p></li><li><p>A. M. Frieze, R. Kannan, and J. C. Lagarias. Linear congruential generators do not produce random sequences. In FOCS, pages 480{484, 1984.</p></li><li><p>P. C. van Oorschot and M. J. Wiener. Parallel collision search with application to hash functions and discrete logarithms. In Proceedings of the 2nd ACM Conference on Computer and Communications Security, pages 210{218, 1994.</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Stream Cipher的第一部分：介绍了One Time Pad和Stream Cipher中的PRG。&lt;br&gt;其中OTP部分叙述了什么是Perfect Secrecy？为什么OTP很难在实践中应用？&lt;br&gt;Stream Cipher部分中，本文主要阐述了什么是PRG？Stream Cipher的另一种安全的定义（依靠PRG的unpredictable)。&lt;br&gt;本文后半部分，详细阐述了一种weak PRG——线性同余生成器，它是如何工作的？它为什么不安全？如何attack it elegantly?&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cryptography-Dan" scheme="https://f1ed.github.io/categories/Cryptography-Dan/"/>
    
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="Cryptography" scheme="https://f1ed.github.io/tags/Cryptography/"/>
    
      <category term="StreamCipher" scheme="https://f1ed.github.io/tags/StreamCipher/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Error</title>
    <link href="https://f1ed.github.io/2020/03/15/error/"/>
    <id>https://f1ed.github.io/2020/03/15/error/</id>
    <published>2020-03-14T16:00:00.000Z</published>
    <updated>2020-07-03T08:41:50.041Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章叙述了进行regression时，where dose the error come from?<br>这篇文章除了解释了error为什么来自bias和variance，还给出了当error产生时应该怎么办？如何让模型在实践应用中也能表现地和测试时几乎一样的好？</p><a id="more"></a><h1 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h1><p>在中的<a href="/2020/02/29/Regression/" title="2.4节">2.4节</a>，我们比较了不同的Model。下图为不同Model下，testing data error的变化。</p><p><a href="https://imgchr.com/i/88gavT"><img src="https://s1.ax1x.com/2020/03/15/88gavT.md.png" alt="88gavT.md.png"></a> </p><p>可以发现，随着模型越来越复杂，testing data的error变小一些后，爆炸增大。</p><p>越复杂的模型在testing data上不一定能得到好的performance。</p><p>所以，where dose the error come from?</p><p>：<strong>bias</strong> and <strong>variance</strong> </p><h2 id="Bias-and-Variance-of-Estimator"><a href="#Bias-and-Variance-of-Estimator" class="headerlink" title="Bias and Variance of Estimator"></a>Bias and Variance of Estimator</h2><p>用打靶作比，如果你的准心，没有对准靶心，那打出的很多发子弹的中心应该离靶心有一段距离，这就是bias。</p><p>但把准心对准靶心，你也不一定能打中靶心，可能会有风速等一系列原因，让子弹落在靶心周围，这就是variance。</p><p><a href="https://imgchr.com/i/88gN80"><img src="https://s1.ax1x.com/2020/03/15/88gN80.md.png" alt="88gN80.md.png"></a> </p><p>上图中，可以直观体现出bias 和 variance的影响。</p><p><strong>概率论中</strong> ：</p><blockquote><p>一个通过样本值得到了估计量，有三个评判准则：无偏性、有效性和相和性。</p></blockquote><p>这里的无偏性的偏也就是bias。</p><blockquote><p>概率论中定义：设 $\hat{\theta}(X_1,X_2,…,X_n)$ 是未知参数 $\theta$ 的估计量，若 $E(\hat{\theta})=\theta$ ，则称 $\hat{\theta}$ 是 $\theta$ 的无偏估计。</p></blockquote><p>变量 $x$  ，假设他的期望是 $\mu$ ，他的方差是 $\sigma^2$.</p><p> 对于样本： $x^1,x^2,…,x^N$ ，估计他的期望和方差。</p><p>概率论的知识：  $m=\frac{1}{N} \sum_{n} x^{n} \quad s^{2}=\frac{1}{N} \sum_{n}\left(x^{n}-m\right)^{2}$ </p><p>$E(m)=\mu$ ，所以用 $m$ 是 $\mu$ 的无偏估计。(unbiased)</p><p>但是  $E\left[s^{2}\right]=\frac{N-1}{N} \sigma^{2} \quad \neq \sigma^{2}$  ，所以这样的估计是有偏差的。(biased)</p><p>因此统计学中用样本估算总体方差都进行了修正。</p><hr><p>而在机器学习中，Bias和Variance通常与模型相关。</p><p><a href="https://imgchr.com/i/88gJ5n"><img src="https://s1.ax1x.com/2020/03/15/88gJ5n.md.png" alt="88gJ5n.md.png"></a> </p><p>上图中，假设黑色的线是 true function，红色的线是训练得到的函数，蓝色的线是，训练函数的平均函数。</p><p>可见，随着函数模型越来越复杂，bias在变小，但variance也在增大。</p><p>右下角图中，红色的线接近铺满了，variance已经很大了，模型过拟合了。</p><hr><p><strong>对机器学习中模型对bias影响的直观解释</strong></p><p><a href="https://imgchr.com/i/88gtCq"><img src="https://s1.ax1x.com/2020/03/15/88gtCq.md.png" alt="88gtCq.md.png"></a> </p><p>左图的model简单，右图的model复杂。</p><p>简单的model，包含的函数集较小，可能集合圈根本没有包括target（true function），因此在这个model下，无论怎么训练，得到的函数都有 large bias。</p><p>而右图中，因为函数非常复杂，所以大概率包含了target，因此训练出的函数可能variacne很大，但有 small bias。</p><h1 id="what-to-do-with-large-bias-variance"><a href="#what-to-do-with-large-bias-variance" class="headerlink" title="what to do with large bias/variance"></a>what to do with large bias/variance</h1><p><a href="https://imgchr.com/i/88gQKS"><img src="https://s1.ax1x.com/2020/03/15/88gQKS.md.png" alt="88gQKS.md.png"></a> </p><p>上图中，红色的线表示bias的误差，绿色的线表示variance的误差，蓝色的线表示观测的误差。</p><p>当模型过于简单时：来自bias的误差会较大，来自vaiance的误差较小，也就是 <strong>Large Bias Small Variance</strong> </p><p>当模型过雨复杂时：来自bias的误差会较小，来自variance的误差会很大，也就是 <strong>Small Bias Large Variance</strong></p><p><strong>2 case</strong> :</p><ul><li><strong>Underfitting</strong> ：If your model <strong>cannot even fit the training examples</strong>, then you have large bias.</li><li><strong>Overfitting</strong> : If you can fit the traning data, but <strong>large error on testing data</strong> , then you probably have large variance.</li></ul><h2 id="With-Large-Bias"><a href="#With-Large-Bias" class="headerlink" title="With Large Bias"></a>With Large Bias</h2><p>For bias, redesign your model.</p><ul><li><u>Add more features as input.</u></li><li><u>A more complex model.</u></li></ul><p>考虑更多的feature；使用稍微复杂些的模型。</p><h2 id="With-Large-Variance"><a href="#With-Large-Variance" class="headerlink" title="With Large Variance"></a>With Large Variance</h2><ul><li><u>More data</u></li><li><u>Regularization</u> (在<a href="/2020/02/29/Regression/" title="这篇2.5.2">这篇2.5.2</a>文章中有叙述什么是regularization)</li></ul><h2 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h2><ul><li>There is usually a trade-off beween bias and variance.</li><li>Select a model that balances two kinds of error to minimize total error.</li></ul><p>选择模型需要在bias和variance中平衡，尽量使得总error最小。</p><p><font color=#f00>What you should NOT do: </font></p><p><a href="https://imgchr.com/i/88gugf"><img src="https://s1.ax1x.com/2020/03/15/88gugf.md.png" alt="88gugf.md.png"></a> </p><p>以上，描述的是这样的一个情形：在traning data中，得到了三个自认不错的模型，kaggle的公开的testing data测试，分别得到三个模型的error，认为第三个模型最好！</p><p>但是，当把kaggle用private的testing data 进行测试时，error肯定是大于0.5的，最好的model也不一定是第三个。</p><p>同理，当把我们训练出的model拿来实际应用时，可能会发现情况很糟，并且，这个model可能选的是测试中最好的，但在应用中并不是最好的。</p><h3 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h3><p>什么是Cross Validation(交叉验证)？</p><p>在机器学习中，就是下图过程：</p><p><a href="https://imgchr.com/i/88g8Ej"><img src="https://s1.ax1x.com/2020/03/15/88g8Ej.md.png" alt="88g8Ej.md.png"></a> </p><ol><li><p>把Traning Set 分成两个部分：Training Set和Validation Set。</p></li><li><p>在Training Set部分选出模型。</p></li><li><p>用Validation Set来判断哪个模型好：计算模型在Validate Set的error。</p></li><li><p>再用模型预测Testing Set(public)，得到的error一定是比Validation Set中大的。</p></li><li><p><font color=#f00>Not recommend</font> : </p><p><strong>Not用public testing data的误差结果去调整你的模型。</strong></p><p>这样会让模型在public的performance比private的好。</p><p>但模型在private testing data的performance才是我们真正关注的。</p></li><li><p>那么当模型预测private testing set时（投入应用时），能尽最大可能的保证模型和在预测public testing data相近。</p></li></ol><h3 id="N-fold-Cross-Validation"><a href="#N-fold-Cross-Validation" class="headerlink" title="N-fold Cross Validation"></a>N-fold Cross Validation</h3><p>N-fold Cross Validation（N-折交叉验证）的过程如下：</p><p><a href="https://imgchr.com/i/88R9mD"><img src="https://s1.ax1x.com/2020/03/15/88R9mD.md.png" alt="88R9mD.md.png"></a> </p><ul><li>把Training Set 分为3（3-fold）份，每一次拿其中一份当Validation Set，另外两份当作Training Set。</li><li>每一次用Train Set来训练。得到了三个Model。</li><li>要判断哪一个Model好？<ul><li>每一个Model都计算出不同Validation Set的error。</li><li>得到一个Average Error。</li></ul></li><li>最后选这个average error最小的model。</li><li>最后应用在public traning set，来评估模型应用在private training set的performance。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章叙述了进行regression时，where dose the error come from?&lt;br&gt;这篇文章除了解释了error为什么来自bias和variance，还给出了当error产生时应该怎么办？如何让模型在实践应用中也能表现地和测试时几乎一样的好？&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="error" scheme="https://f1ed.github.io/tags/error/"/>
    
  </entry>
  
  <entry>
    <title>Adagrag-demo</title>
    <link href="https://f1ed.github.io/2020/03/09/Adagrad-demo/"/>
    <id>https://f1ed.github.io/2020/03/09/Adagrad-demo/</id>
    <published>2020-03-08T16:00:00.000Z</published>
    <updated>2020-07-03T08:38:48.748Z</updated>
    
    <content type="html"><![CDATA[<p>实现<a href="/2020/03/01/Gradient/" title="这篇文章">这篇文章</a>中前面两个tips。</p><a id="more"></a><p>实现了tip1 Adagrad + tip2 Stochastic Gradient Descent</p><h3 id="demo代码"><a href="#demo代码" class="headerlink" title="demo代码"></a>demo代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################</span></span><br><span class="line"><span class="comment"># 2020/03/06    #</span></span><br><span class="line"><span class="comment"># Adagrad demo  #</span></span><br><span class="line"><span class="comment">#################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># data</span></span><br><span class="line">x_data = [[<span class="number">338.</span>], [<span class="number">333.</span>], [<span class="number">328.</span>], [<span class="number">207.</span>], [<span class="number">226.</span>], [<span class="number">25.</span>], [<span class="number">179.</span>], [<span class="number">60.</span>], [<span class="number">208.</span>], [<span class="number">606.</span>]]</span><br><span class="line"></span><br><span class="line">y_data = [<span class="number">640.</span>, <span class="number">633.</span>, <span class="number">619.</span>, <span class="number">393.</span>, <span class="number">428.</span>, <span class="number">27.</span>, <span class="number">193.</span>, <span class="number">66.</span>, <span class="number">226.</span>, <span class="number">1591.</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># coordinate</span></span><br><span class="line">x = np.arange(<span class="number">-200</span>, <span class="number">-100</span>, <span class="number">1</span>)</span><br><span class="line">y = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">Z = np.zeros((len(y), len(x)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># cal the Loss of every point(function)</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(y)):</span><br><span class="line">        b = x[i]</span><br><span class="line">        w = y[j]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">            Z[j][i] += (y_data[k] - b - w * x_data[k][<span class="number">0</span>])**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># initial</span></span><br><span class="line">b = <span class="number">-120</span></span><br><span class="line">w = <span class="number">-4</span></span><br><span class="line">lr = <span class="number">1</span> <span class="comment"># learning rate</span></span><br><span class="line">iteration = <span class="number">100000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># record the iteration</span></span><br><span class="line">b_his = [b]</span><br><span class="line">w_his = [w]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adagrad</span></span><br><span class="line">b_grad_sum2 = <span class="number">0.0</span></span><br><span class="line">w_grad_sum2 = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(iteration):</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(len(x_data)):</span><br><span class="line">        b_grad = <span class="number">2</span> * (y_data[k] - b - w * x_data[k][<span class="number">0</span>]) * (<span class="number">-1</span>)</span><br><span class="line">        w_grad = <span class="number">2</span> * (y_data[k] - b - w * x_data[k][<span class="number">0</span>]) * (-x_data[k][<span class="number">0</span>])</span><br><span class="line">        b_grad_sum2 += b_grad**<span class="number">2</span></span><br><span class="line">        w_grad_sum2 += w_grad**<span class="number">2</span></span><br><span class="line">        b = b - lr / np.sqrt(b_grad_sum2) * b_grad</span><br><span class="line">        w = w - lr / np.sqrt(w_grad_sum2) * w_grad</span><br><span class="line">        b_his.append(b)</span><br><span class="line">        w_his.append(w)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn linear model</span></span><br><span class="line">reg = linear_model.LinearRegression()</span><br><span class="line">print(reg.fit(x_data, y_data))</span><br><span class="line">print(reg.coef_[<span class="number">0</span>])</span><br><span class="line">print(reg.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># display the figure</span></span><br><span class="line">plt.contourf(x, y, Z, <span class="number">50</span>, alpha=<span class="number">0.5</span>, cmap=plt.get_cmap(<span class="string">'jet'</span>))</span><br><span class="line">plt.plot(reg.intercept_, reg.coef_, <span class="string">'x'</span>, ms=<span class="number">13</span>, lw=<span class="number">1.5</span>, color=<span class="string">'orange'</span>)</span><br><span class="line">plt.plot(b_his, w_his, <span class="string">'o-'</span>, ms=<span class="number">3</span>, lw=<span class="number">1.5</span>, color=<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">-200</span>, <span class="number">-100</span>)</span><br><span class="line">plt.ylim(<span class="number">-5</span>, <span class="number">5</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$b$'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.ylabel(<span class="string">'$w$'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line">plt.savefig(<span class="string">"Loss.png"</span>)</span><br></pre></td></tr></table></figure><h3 id="Loss-迭代图"><a href="#Loss-迭代图" class="headerlink" title="Loss 迭代图"></a>Loss 迭代图</h3><p>画出的图片很直观</p><p><a href="https://imgchr.com/i/89aAf0"><img src="https://s2.ax1x.com/2020/03/09/89aAf0.png" alt="89aAf0.png"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;实现&lt;a href=&quot;/2020/03/01/Gradient/&quot; title=&quot;这篇文章&quot;&gt;这篇文章&lt;/a&gt;中前面两个tips。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://f1ed.github.io/categories/Python/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="Gradient-Descent" scheme="https://f1ed.github.io/tags/Gradient-Descent/"/>
    
  </entry>
  
  <entry>
    <title>「Python」：Module &amp; Method</title>
    <link href="https://f1ed.github.io/2020/03/07/python/"/>
    <id>https://f1ed.github.io/2020/03/07/python/</id>
    <published>2020-03-06T16:00:00.000Z</published>
    <updated>2020-07-03T08:42:56.875Z</updated>
    
    <content type="html"><![CDATA[<p>长期记录帖：关于遇到过的那些Python 的Packets &amp; Module &amp; Method &amp; Attribute。中英记录。</p><a id="more"></a><h1 id="Tricky"><a href="#Tricky" class="headerlink" title="Tricky"></a>Tricky</h1><h2 id="list-comprehension"><a href="#list-comprehension" class="headerlink" title="list comprehension"></a>list comprehension</h2><ul><li><p>List comprehension provides a concise way to create lists. </p><ul><li>e.g. : squares = [x**2 for x in range(10)]</li></ul></li><li><p>A list comprehension consists of brackets containing an expression followed by a for clause, then zero or more <em>for</em> or <em>if</em> clauses. The result will be a new list resulting from evaluating the expression in the context of the for and if clauses which follow it.</p><ul><li>e.g. : Double loop: [(x,y) for x in [1,2,3] for y in [3,1,4] if x != y]<ul><li>输出7个</li></ul></li><li>e.g. : (Using zip() to loop together): [(x, y) for x,y in zip([1,2,3], [3,1,4]) if x!=y]<ul><li>输出2个</li></ul></li></ul></li></ul><h1 id="Python-Build-function"><a href="#Python-Build-function" class="headerlink" title="Python-Build function"></a>Python-Build function</h1><h2 id="print"><a href="#print" class="headerlink" title="print"></a>print</h2><ul><li>print(*objects, sep=’ ‘, end=’\n’, file=sys.stdout)</li></ul><h2 id="len"><a href="#len" class="headerlink" title="len"></a>len</h2><ul><li>Return the length(the number of items) of an object.</li></ul><h2 id="str-format"><a href="#str-format" class="headerlink" title="str.format()"></a>str.format()</h2><ul><li>字符串格式化</li><li>eg:<ul><li>“{} {}”.format(“Hello”,”World)</li><li>‘Hello World’</li></ul></li></ul><h2 id="zip-iterables"><a href="#zip-iterables" class="headerlink" title="zip(*iterables)"></a>zip(*iterables)</h2><ul><li><p>Make an iterator that aggregates【聚集】 elements from each of the iterales.</p></li><li><p>Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. </p><p>【返回一个元组的迭代器】</p></li><li><p>使用zip可以同时对多个迭代器进行迭代</p></li></ul><h2 id="enumerate"><a href="#enumerate" class="headerlink" title="enumerate"></a>enumerate</h2><ul><li><p>Enumerate is a built-in funciton of Python. It allows us to loop over something and have an automatic counter.</p></li><li><p>e.g. </p><ul><li><p>for counter, value in enumerate(some_list):</p><p>print(counter, value)</p></li></ul></li><li><p>e.g. : an optional argument: tell enumerate from where to start the index.</p><ul><li><p>for c, value in enumerate(my_list, 1):</p><p>print(c, value)</p></li></ul></li></ul><h2 id="with-open-path-as-f"><a href="#with-open-path-as-f" class="headerlink" title="with open(path) as f:"></a>with open(path) as f:</h2><ul><li><p>由于读写文件都可能产生IOError，一旦出错，后面的f.close()就不会调用。</p></li><li><p>用try……finally来实现，比较麻烦。</p><p>try:</p><p>​    f = open(path, ‘r’)</p><p>​    print(f.read())</p><p>finally:</p><p>​    if f:</p><p>​        f.close()</p></li><li><p>用with as 简化</p><p>with open(path, ‘r’) as f:</p><p>​    print(f.read())</p></li></ul><h1 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h1><h2 id="numpy-argsort"><a href="#numpy-argsort" class="headerlink" title="numpy.argsort"></a>numpy.argsort</h2><ul><li><p>numpy.argsort(a, axis=-1, kind=None, order=None)</p></li><li><p>Returns the indices that would sort an array.</p><p>Perform an indirect sort along the given axis using the algorithm specified by the kind keyword. It returns an array of indices of the same shape as a that index data along the given axis in sorted order.</p></li><li><p>Parameters:</p><ul><li><p><strong>a</strong> :array_like.</p></li><li><p><strong>axis</strong> : int or None, optional</p><p>Axis along which to sort. The default is -1(the last axis). 【默认按照最后一个维度】</p><ul><li>2-D: axis = 0按列排序</li><li>2-D: axis = 1 按行排序</li></ul></li><li><p><strong>kind</strong> :{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, optional</p><p>The default is ‘quicksort’</p></li></ul></li><li><p>Return: index_array: ndarray, int.【返回的是降序排列的索引数组】</p><ul><li><p>e.g.:</p><p>x = np.array([5, 1, 2])</p><p>np.argsort(x)  # 降序</p><p>array([1,2,0])</p><p>np.argsort(-x) # 升序</p><p>array([0,2,1])</p></li></ul></li></ul><h2 id="Linear-algebra-numpy-linalg"><a href="#Linear-algebra-numpy-linalg" class="headerlink" title="Linear algebra(numpy.linalg)"></a>Linear algebra(numpy.linalg)</h2><h3 id="numpy-dot"><a href="#numpy-dot" class="headerlink" title="numpy.dot"></a>numpy.dot</h3><ul><li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html">numpy.dot(a,b)</a> </li><li>Dot product of two arrays.<ul><li>If both a and b are 1-D arrays, it is inner porduct of vectors.</li><li>If both a and b are 2-D arrays, it is matrix multiplication, but using matmul is preferred.</li><li>Id either a or b is 0-D(scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a*b is preferred.</li><li>……</li></ul></li></ul><h3 id="numpy-matmul"><a href="#numpy-matmul" class="headerlink" title="numpy.matmul"></a>numpy.matmul</h3><ul><li>Matrix product of two arrays.</li><li>numpy.matmul(x1, x2)</li></ul><h3 id="numpy-linalg-inv-a"><a href="#numpy-linalg-inv-a" class="headerlink" title="numpy.linalg.inv(a)"></a>numpy.linalg.inv(a)</h3><ul><li><p>Compute the inverse of a matrix.</p><p>Given a square matrix a, return the matrix ainv satisfying dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])/</p></li><li><p>numpy.linalg.inv(a)</p></li><li><p>Parameters:</p><ul><li><strong>a</strong> :(…, M, M) array_like. Matrix to e inverted.</li></ul></li><li><p>Return: ainv.</p></li></ul><h3 id="numpy-linalg-svd"><a href="#numpy-linalg-svd" class="headerlink" title="numpy.linalg.svd"></a>numpy.linalg.svd</h3><ul><li><p>numpy.linalg.svd(a, full_matrices=True,  compute_uv=True, hermitian=False)</p></li><li><p>Singular Value Decomposition</p></li><li><p>矩阵的奇异值分解</p><p>A = u @ s @ vh</p><p>u, vh是标准正交矩阵, inv(u) = uh</p><p>s是对角矩阵</p></li><li><p>Parameters:</p><ul><li><p><strong>a</strong> :array_like, a real or complex array with a.ndim &gt;=2</p></li><li><p><strong>full_matrices</strong> :bool, optional. True(default)</p><p>If True, u and vh have the shapes(…, M, M) and (…, N, N), respectively.</p><p>Otherwise, the shapes are(…, M, K) and (…, K, N), respectively, where K = min(M,N)</p></li><li><p><strong>compute_uv</strong> : bool, optional.True(default)</p><p>Whether or not to compute u and vh in addition to s.【注，vh就是v的转置】</p></li></ul></li><li><p>Return：</p><ul><li>u: array</li><li>s: array</li><li>vh:array</li></ul></li></ul><h2 id="numpy-zeros"><a href="#numpy-zeros" class="headerlink" title="numpy.zeros"></a>numpy.zeros</h2><ul><li><p>numpy.zeros(shape, dtype=float, order=’C’)</p></li><li><p>Return a new array of given shape and type, filled with zeros.</p></li><li><p>parameters:</p><ul><li><strong>shape</strong>:  int or truple of ints. e.g.,(2,3) or 2</li><li><strong>dtype</strong>: data-type, optional.(Defaults is numpy.float64)</li><li><strong>oder</strong>: optional</li></ul></li><li><p>Returns: out ndarray</p></li></ul><h2 id="numpy-full"><a href="#numpy-full" class="headerlink" title="numpy.full"></a>numpy.full</h2><ul><li>numpy.full(shape, fill_value, dtype=None)</li><li>Return a new array of given shape and type, filled with fill_value.</li><li>parameters:<ul><li><strong>shape</strong> :int or sequence of ints<ul><li>(2,3) or 2</li></ul></li><li><strong>fill_value</strong> :scalar</li><li><strong>dtype</strong> :data-type, optional</li></ul></li></ul><h2 id="numpy-arange"><a href="#numpy-arange" class="headerlink" title="numpy.arange"></a>numpy.arange</h2><ul><li><p>numpy.arange([start, ]stop, [step, ]dtype=None)</p></li><li><p>Return evenly spaced values within a given interval.</p></li><li><p>parameters:</p><ul><li><strong>start</strong>: number, optional. (Defaults is 0)</li><li><strong>stop</strong> :number. <strong>[start,stop)</strong></li><li><strong>step</strong> :number, optional(Defaults is 1)</li><li><strong>dtype</strong> :</li></ul></li><li><p>Returns :ndarray </p></li><li><p>differ with built-in range function</p><ul><li>numpy.arange returnan ndarray rathan than a list.</li><li>numpy.arange’s step can be float.</li></ul></li></ul><h2 id="numpy-meshgrid"><a href="#numpy-meshgrid" class="headerlink" title="numpy.meshgrid"></a>numpy.meshgrid</h2><ul><li>numpy.meshgrid(x, y)</li><li>生成用x向量为行，y向量为列的矩阵（坐标系）</li><li>返回 X矩阵和Y矩阵<ul><li>X矩阵：网格上所有点的x值</li><li>Y矩阵：网格上所有点的y值</li></ul></li><li>e.g., X, Y = np.meshgrid(x, y) 【X,Y 都是网格点坐标矩阵】</li></ul><h2 id="numpy-genfromtext"><a href="#numpy-genfromtext" class="headerlink" title="numpy.genfromtext"></a>numpy.genfromtext</h2><ul><li><p>numpy.genfromtxt (fname, delimiter=None)</p></li><li><p>Load data from a text file, with missing values handled as specified. </p><p>Each line past the first <em>skip_header</em> lines is split at the delimiter character, and characters following the <em>comments</em> characters are discarded.</p></li><li><p>Parameters:</p><ul><li><strong>fname</strong> :file, str, list of str, generator.</li><li><strong>dtype</strong> :dtype, optional. </li><li><strong>delimiter</strong> :str, int, or sequence, optional. <ul><li>(default = whitespace)</li><li>The strin used to separate values. </li></ul></li></ul></li><li><p>Python的列表读取处理数据很慢，numpy.genfromtext就很棒。</p></li></ul><h2 id="numpy-isnan"><a href="#numpy-isnan" class="headerlink" title="numpy.isnan"></a>numpy.isnan</h2><ul><li>numpy.isnan(x)</li><li>Test element-wise for NaN(Not a number) and return result as a boolean array.</li><li>Parameters:<ul><li><strong>x</strong> :array_like</li></ul></li><li>Returns:  y:ndarray or bool.<ul><li>True where x is NaN, false otherwise.</li></ul></li></ul><h2 id="numpy-empty"><a href="#numpy-empty" class="headerlink" title="numpy.empty"></a>numpy.empty</h2><ul><li>nmpy.empty(shape, dtype=float, order=’C’)</li><li>Return a new arry of given shape and type, without initializing entries.</li><li>Parameters:<ul><li><strong>shape</strong> :int or tuple of int</li><li><strong>dtype</strong> :data-type,optional<ul><li>Default is numpy.float64.</li></ul></li></ul></li><li>Returns: <ul><li>out: ndarray</li></ul></li></ul><h2 id="numpy-reshape"><a href="#numpy-reshape" class="headerlink" title="numpy.reshape"></a>numpy.reshape</h2><ul><li>numpy.reshape(a, newshape, order=’C’)</li><li>Gives a new shape to an array without changing its data.【改变张量的shape，不改变张量的数据】</li><li>Parameters:<ul><li><strong>a</strong> : array-like</li><li><strong>newshape</strong> : int or tuple of ints<ul><li>One shape dimension can be -1. The value is inferred from the length of the array and remaning dimensions.</li></ul></li></ul></li><li>Returns: reshaped_array:ndarray</li></ul><h2 id="numpy-mean"><a href="#numpy-mean" class="headerlink" title="numpy.mean"></a>numpy.mean</h2><ul><li>numpy.mean(a, axis=None)</li><li>Compute the arithmetic mean along the specifiied axis.(the average of the array elements)</li><li>Parameters:<ul><li><strong>a</strong> : array_like</li><li><strong>axis</strong> ：None or int or tuple of ints, optional<ul><li>Axis or axes along which the means are computed.</li><li>axis=0 ：沿行的垂直往下（列）</li><li>axis=1 ：沿列的方向水平向右（行）</li></ul></li></ul></li></ul><h2 id="numpy-std"><a href="#numpy-std" class="headerlink" title="numpy.std"></a>numpy.std</h2><ul><li>numpy.std(a, axis=None,)</li><li>Compute the standard deviation along the specified axis.【标准差】</li><li>Parameters:<ul><li><strong>a</strong> :array_like</li><li><strong>axis</strong> :Axis or axes along which the means are computed.</li></ul></li></ul><h2 id="numpy-shape"><a href="#numpy-shape" class="headerlink" title="numpy.shape"></a>numpy.shape</h2><ul><li>attribute</li><li>Tuple of array dimensions.</li></ul><h2 id="numpy-concatenate"><a href="#numpy-concatenate" class="headerlink" title="numpy.concatenate"></a>numpy.concatenate</h2><ul><li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html">numpy.concatenate((a1, a2, …), axis=0)</a> </li><li>Join a sequence of arrays along an existing axis.</li><li>Parameters: <ul><li><strong>a1, a2, …</strong> :sequence of array_like<ul><li>The arrays must have the same shape, excepting in the dimension corresponding to axis.【除了axia方向，其他维度的shape要相同】</li><li>If axis is None, arrays are flattened before use.【值为None，就先将向量变成一维的】</li><li>Default=0</li></ul></li></ul></li></ul><h2 id="numpy-ndarray-astype"><a href="#numpy-ndarray-astype" class="headerlink" title="numpy.ndarray.astype"></a>numpy.ndarray.astype</h2><ul><li>method</li><li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html">numpy.ndarray.astype(dtype)</a></li><li>Copy of the array cast to a specified type.【强制转换数据类型】</li><li>Parameters:<ul><li><strong>dtype</strong> : str or dtype</li></ul></li></ul><h2 id="numpy-ones"><a href="#numpy-ones" class="headerlink" title="numpy.ones"></a>numpy.ones</h2><ul><li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html">numpy.ones(shape, dtype=None)</a></li><li>Return a new array of given shape and type, filled with ones.</li><li>Parameters:<ul><li><strong>shape</strong> : int or sequence of ints.</li><li><strong>dtype</strong> : data-type, optional</li></ul></li></ul><h2 id="numpy-array"><a href="#numpy-array" class="headerlink" title="numpy.array"></a>numpy.array</h2><ul><li><p>numpy.array(object, dtype = none)</p></li><li><p>Create an array</p></li><li><p>Parameters:</p><ul><li><p><strong>object</strong> :array_like</p><p>An array, any object exposing the array interface, an object whose <em>array</em> method returns an array, or any(nested) sequence.</p></li></ul></li></ul><h2 id="numpy-ndarray-运算"><a href="#numpy-ndarray-运算" class="headerlink" title="numpy ndarray 运算"></a>numpy ndarray 运算</h2><ul><li>[[1]]*3 = [[1],[1],[1]]</li><li>A * B 元素相乘</li><li>numpy.dot(A, B) 矩阵相乘</li></ul><h2 id="numpy-power"><a href="#numpy-power" class="headerlink" title="numpy.power"></a>numpy.power</h2><ul><li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.power.html">numpy.power(x1, x2)</a> </li><li>First array elements raised to powers from second array.</li><li>Parameters:<ul><li><strong>x1</strong> :array_like .<ul><li>The bases.</li></ul></li><li><strong>x2</strong> :array_like<ul><li>The exponents. </li></ul></li></ul></li></ul><h2 id="numpy-sum"><a href="#numpy-sum" class="headerlink" title="numpy.sum"></a>numpy.sum</h2><ul><li>numpy.sum(a, axis=None, dtype=None)</li><li>Sum of arrays elements over a given axis.</li><li>Parameters:<ul><li><strong>a</strong> :array_like<ul><li>Elements to sum.</li></ul></li><li><strong>axis</strong> :None or int or tuple of ints, optional<ul><li>Axis or axes along which a sum is perfomed.</li><li>The default, None, will sum all of the elementsof the input array.</li></ul></li></ul></li></ul><h2 id="numpy-transpose"><a href="#numpy-transpose" class="headerlink" title="numpy.transpose"></a>numpy.transpose</h2><ul><li><p>numpy.transpose(a, axes=None)</p></li><li><p>Permute the dimensions of the array.【tensor的维度换位】</p></li><li><p>Parameters:</p><ul><li><strong>a</strong> : array_like</li><li><strong>axes</strong> : list of ints, optinal<ul><li>Default, reverse the dimensions.</li><li>Otherwise permute the axes according to the values given.</li></ul></li></ul></li><li><p>Returns : ndarray</p></li><li><p>张量a的shape是(10,2,15), numpy.transport(2,0,1)的shape就是(15,10,2)</p></li><li><p>对于一维：行向量变成列向量</p></li><li><p>对于二维：矩阵的转置</p></li></ul><h2 id="numpy-save"><a href="#numpy-save" class="headerlink" title="numpy.save"></a>numpy.save</h2><ul><li><p><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html">numpy.save(file, arr)</a> </p></li><li><p>Save an array to a binary file in Numpy .npy format.</p></li><li><p>Parameters:</p><ul><li><p><strong>file</strong> :file, str, or pathlib</p></li><li><p><strong>arr</strong> :array_like</p><p>Array data to be saved.</p></li></ul></li></ul><h2 id="numpy-clip"><a href="#numpy-clip" class="headerlink" title="numpy.clip"></a>numpy.clip</h2><ul><li>Clip(limit) the values in an array</li><li>numpy.clip(a, a_min, a_max)</li><li>Parameters:<ul><li><strong>a</strong> :array_like</li><li><strong>a_min</strong> : scalar or array_like</li><li><strong>a_max</strong> :scalar or array_like</li></ul></li></ul><h2 id="numpy-around"><a href="#numpy-around" class="headerlink" title="numpy.around"></a>numpy.around</h2><ul><li><p>Evenly round to the given number of decimals(十进制)</p></li><li><p>numpy.around(a)</p></li><li><p>Parameters：</p><ul><li><strong>a</strong> :array_like</li></ul></li><li><p>Notes: For values exactly halfway between rounded decimal values, Numpy rounds to the nearest even values.</p><p>【这什么意思呢？ 就是说对于0.5的这种，为了统计上平衡，不会全部向上取整或者向下取整，会向最近的偶数取整，around（2.5）=2】</p></li></ul><h2 id="numpy-log"><a href="#numpy-log" class="headerlink" title="numpy.log"></a>numpy.log</h2><ul><li>The natural logarithm log is the inverse of exponential functions, so that log(exp(x))=x.</li><li>numpy.log(x)</li><li>Parameters:<ul><li><strong>x</strong> : array_like</li></ul></li></ul><h2 id="numpy-ndarray-T"><a href="#numpy-ndarray-T" class="headerlink" title="numpy.ndarray.T"></a>numpy.ndarray.T</h2><ul><li>attribute, the transpose array.</li><li>ndarray.T</li></ul><h2 id="numpy-random-shuffle"><a href="#numpy-random-shuffle" class="headerlink" title="numpy.random.shuffle"></a>numpy.random.shuffle</h2><ul><li><p>Modify a sequence in-space by shufflng its contents.</p><p>This function only shuffles the array along the first axis of a multi-diensional array. The order of sub-arrays is changed but their contents remains the same.</p></li><li><p>numpy.random.shuffle(x)</p></li><li><p>Parameters:</p><ul><li><strong>x</strong> : array_like</li></ul></li><li><p><strong>e.g. shuffle two list, X and Y, together.</strong> <strong>【以相同的顺序打乱两个array】</strong></p><ul><li><p>np.random.seed(0)</p><p>randomize = np.arrange(len(X))</p><p>np.random.shuffle(randomize)</p><p>return X[randomize], Y[randomize]</p></li></ul></li></ul><h1 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h1><h2 id="skelearn-linear-model-LinearRegression"><a href="#skelearn-linear-model-LinearRegression" class="headerlink" title="skelearn.linear_model.LinearRegression"></a>skelearn.linear_model.LinearRegression</h2><ul><li>class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)</li><li>Ordinary least squares Linear Regress(最小二乘法回归运算)</li><li>Parameters:<ul><li><strong>fit_intercept</strong> :bool, optional, defalut True【True：需要bias的截距项】</li><li><strong>normalize</strong> :bool, optional, default False 【True：对样本做feature scaling】</li></ul></li><li>Attributes：<ul><li><strong>coef_</strong> :array of shape(n_features) 【权重】</li><li><strong>intercept_</strong> :bias </li></ul></li><li>Methods：<ul><li><strong>fit(self,X,y[,sample_weight])</strong> :Fit linear model<ul><li>e.g. , LinearRegression().fit(x_data, y_data)</li></ul></li></ul></li></ul><h1 id="matplotlib-pyplot"><a href="#matplotlib-pyplot" class="headerlink" title="matplotlib.pyplot"></a>matplotlib.pyplot</h1><h2 id="matplotlib-pyplot-contourf"><a href="#matplotlib-pyplot-contourf" class="headerlink" title="matplotlib.pyplot.contourf"></a>matplotlib.pyplot.contourf</h2><ul><li><p>contour and contourf draw contour lines and filled contours, respectively.【一个画等高线，一个填充等高线/轮廓】</p></li><li><p>contour([X, Y, ] Z, [levels], **kwargs)</p></li><li><p>Parameters</p><ul><li><p><strong>X, Y</strong>: The coordinates of the values in Z. A and Y must both be 2-D with the sanme shape as Z(e.g. created via numpy.meshgrid), or they must both be 1-D such that len(X) == M is the number of columns in Z and len(Y) = N is the number of rows in Z.</p><p>【X，Y要么是由像numpy.mershgrid(x, y) 生成的网格点坐标矩阵，要么X，Y是（基）向量，X向量是x轴的，对应到Z矩阵，是Z矩阵的列数，Y向量同理】</p></li><li><p><strong>Z</strong> ：array-like(N,M)</p></li><li><p><strong>levels</strong> : int or array-like, optional. Determines the number and positions of contour lines / religions.【划分多少块等高区域】</p></li><li><p><strong>alpha</strong> :float, optional. Between 0(transparent) and 1(opaque).【透明度】</p></li><li><p><strong>cmap</strong> :str or Colormap, optional.</p></li></ul></li><li><p>e.g., pyplot.contourf(x, y, Z, 50, alpha=0.5, cmap=pyplot.get_cmap(‘jet’))【‘jet’是常用的那种红橙黄绿青蓝紫】</p></li></ul><h2 id="matplotlib-pyplot-plot"><a href="#matplotlib-pyplot-plot" class="headerlink" title="matplotlib.pyplot.plot"></a>matplotlib.pyplot.plot</h2><ul><li>plot([x], y, [fmt], <em>, data=None, *</em>kwargs)</li><li>The coordinates of the points or line nodes are given by x, y.</li><li>Parameters:<ul><li><strong>x, y</strong> :array-like or scalar.</li><li><strong>fmt</strong> :str, optional. A format string.<ul><li>e.g., ‘.’, point marker. ‘-‘, solid line style. ‘–’,dashed line style. ‘b’, blue.</li></ul></li><li><strong>ms/markersize</strong> : float</li><li><strong>lw/linewidth</strong> :float</li><li><strong>color</strong> :</li></ul></li></ul><h2 id="matplotlib-pyplot-xlim"><a href="#matplotlib-pyplot-xlim" class="headerlink" title="matplotlib.pyplot.xlim"></a>matplotlib.pyplot.xlim</h2><ul><li>xlim(<em>args, *</em>kwargs)</li><li>Get or set the x limits of the current axes.</li><li>e.g. <ul><li>left, right = xlim() :get</li><li>xlim(left, right) :set</li></ul></li></ul><h2 id="matplotlib-pyplot-show"><a href="#matplotlib-pyplot-show" class="headerlink" title="matplotlib.pyplot.show"></a>matplotlib.pyplot.show</h2><ul><li>show(<em>args, *</em>kwargs)</li><li>display a figure.</li></ul><h2 id="matplotlib-pyplot-vlines"><a href="#matplotlib-pyplot-vlines" class="headerlink" title="matplotlib.pyplot.vlines"></a>matplotlib.pyplot.vlines</h2><ul><li>Plot vertical lines.</li><li>vlines(x, ymin, ymax, color=’k’, linestyles=’solid’)</li><li>Parameters:<ul><li><strong>x</strong> :scalar or 1D array_like</li><li><strong>ymin, ymax</strong> :scalar or 1D array_like</li></ul></li></ul><h2 id="matplotlib-pyplot-hlines"><a href="#matplotlib-pyplot-hlines" class="headerlink" title="matplotlib.pyplot.hlines"></a>matplotlib.pyplot.hlines</h2><ul><li>Plot horizontal lines.</li><li>vlines(y, xmin, xmax, color=’k’, linestyles=’solid’)</li><li>Parameters:<ul><li><strong>y</strong> :scalar or 1D array_like</li><li><strong>xmin, xmax</strong> :scalar or 1D array_like</li></ul></li></ul><h2 id="matplotlib-pyplot-savefig"><a href="#matplotlib-pyplot-savefig" class="headerlink" title="matplotlib.pyplot.savefig"></a>matplotlib.pyplot.savefig</h2><ul><li>Save the current figure.</li><li>savefig(fname)</li><li>Parameters:<ul><li><strong>fname</strong> :str ot Pathlike</li></ul></li></ul><h2 id="matplotlib-pyplot-legend"><a href="#matplotlib-pyplot-legend" class="headerlink" title="matplotlib.pyplot.legend"></a>matplotlib.pyplot.legend</h2><ul><li>Place a lengend on the axes.</li><li>e.g. : legend() Labeling exisiting plot elements<ul><li>plt.plot(train_loss)</li><li>plt.plot(dev_loss)</li><li>plt.legend([‘train’, ‘dev’])</li></ul></li><li>e.g. : le</li></ul><h1 id="sys"><a href="#sys" class="headerlink" title="sys"></a>sys</h1><h2 id="sys-argv"><a href="#sys-argv" class="headerlink" title="sys.argv[]"></a>sys.argv[]</h2><ul><li>python a.py data.csv<ul><li>sys.argv = [‘a.py’, ‘data.csv’]</li></ul></li></ul><h2 id="重定向到文件"><a href="#重定向到文件" class="headerlink" title="重定向到文件"></a>重定向到文件</h2><ul><li><p>f = open(‘out.csv’, ‘w’)</p><p>sys.stdout = f</p><p>print(‘此时print掉用的就是文件对象的write方法’)</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;长期记录帖：关于遇到过的那些Python 的Packets &amp;amp; Module &amp;amp; Method &amp;amp; Attribute。中英记录。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://f1ed.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://f1ed.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-Dan」：Introduction</title>
    <link href="https://f1ed.github.io/2020/03/04/Dan-Intro/"/>
    <id>https://f1ed.github.io/2020/03/04/Dan-Intro/</id>
    <published>2020-03-03T16:00:00.000Z</published>
    <updated>2020-07-03T08:40:56.053Z</updated>
    
    <content type="html"><![CDATA[<p>本系列是学习Dan Boneh教授的Online Cryptography Course。</p><p>这是Dan教授的第一讲：对密码学的一些Introduction。</p><a id="more"></a><h1 id="What-is-cryptography"><a href="#What-is-cryptography" class="headerlink" title="What is cryptography?"></a>What is cryptography?</h1><h2 id="Crypto-core：安全通信"><a href="#Crypto-core：安全通信" class="headerlink" title="Crypto core：安全通信"></a>Crypto core：安全通信</h2><ol><li><p>Secret key establishment (密钥的建立)：</p><p><a href="https://imgchr.com/i/3oweeJ"><img src="https://s2.ax1x.com/2020/03/04/3oweeJ.md.png" alt="3oweeJ.md.png"></a> </p><p>Alice 和 Bob 会得到一个shared secret key，而且Alice 知道她是在和Bob通信，Bob也知道他是在和Alice通信。而attacker不能从通信中获取key。</p></li><li><p>Secure communicati （安全通信）：</p><p><a href="https://imgchr.com/i/3owEyF"><img src="https://s2.ax1x.com/2020/03/04/3owEyF.md.png" alt="3owEyF.md.png"></a> </p><p>在通信中，Alice、Bob用k将信息加密，保证了通信的confidentiality（机密性）；同时attacker也无法篡改通信的信息，保证了通信的integrity（完整性）。</p></li></ol><h2 id="Crypto-can-do-much-more"><a href="#Crypto-can-do-much-more" class="headerlink" title="Crypto can do much more"></a>Crypto can do much more</h2><p>密码学除了能保证安全通信，密码学还能做很多其他的事。</p><h3 id="Digital-signature-amp-Anonymous"><a href="#Digital-signature-amp-Anonymous" class="headerlink" title="Digital signature &amp; Anonymous"></a>Digital signature &amp; Anonymous</h3><ol><li><p><strong>Digital signatures（数字签名）：</strong></p><p>现实中，人们对不同的文档进行签名，虽然是不同的文档，但是签名的字是相同的。</p><p>如果这应用在网络的文档签名中，这将是很危险的。攻击者只需要将签名进行复制、粘贴，就可以将你的签名签在你并不想签的文档中。</p><p>数字签名的主要思想：数字签名其实是代签内容的函数值，所以如果攻击者只是复制数字签名（原签名的函数值），那么攻击者得到的数字签名也是无效的（函数值不同）。</p><p>在后面的课程系列中会详细讲这部分的内容。[1]</p></li><li><p><strong>Anonymous communication（匿名通信）：</strong></p><p><a href="https://imgchr.com/i/3owiWV"><img src="https://s2.ax1x.com/2020/03/04/3owiWV.png" alt="3owiWV.png"></a> </p><p>匿名通信的实现，有<a href="https://en.wikipedia.org/wiki/Mix_network">Mix network</a> （wiki详细介绍）协议，这是一种路由协议，通过使用混合的代理服务器链来实现难以追踪的通信。</p><p>通过这些代理的不断加密解密可以实现：</p><ul><li><p>Bob不知道与之通信的是Alice。</p></li><li><p>代理也不知道是Alice和Bob在通信。</p></li><li><p>双向通信：虽然Bob不知与之通信的是Alice，但也能respond。</p></li></ul></li><li><p><strong>Anonymous digital cash（匿名数字现金）：</strong></p><p><a href="https://imgchr.com/i/3owSds"><img src="https://s2.ax1x.com/2020/03/04/3owSds.md.png" alt="3owSds.md.png"></a> </p><p>现实中，我们可以去超市花掉一元钱，而超市不知道我是谁。</p><p>在网络中，如果Alice想去网上商店花掉数字现金一元钱，网上商店可以不知道是谁花掉的这一元钱吗？</p><p>这就是匿名数字现金需要解决的问题：</p><ul><li>可以在匿名的情况下花掉数字现金吗？</li></ul><p>如果可以，当Alice将这一元钱复制多次（数字现金都是数据串），得到了三元钱，再去把它花掉，由于匿名的原因，没人知道是谁花掉的这三元钱，商店找不到责任人。</p><p>这是匿名数字现金需要解决的第二个问题：</p><p>如何防止 double spending情况的发生？</p><p>可以用这样的机制去实现匿名数字现金：当Alice花费这一块 <strong>once</strong>时，系统保证Alice的匿名性；但当Alice花费这一块 <strong>more than once</strong> ,系统立即揭露Alice的全部信息。</p></li></ol><h3 id="Protocols"><a href="#Protocols" class="headerlink" title="Protocols"></a>Protocols</h3><p>在介绍什么是Protocols之前，先介绍两种应用场景。</p><ol><li><p><strong>Elections</strong></p><p><a href="https://imgchr.com/i/3odvLQ"><img src="https://s2.ax1x.com/2020/03/04/3odvLQ.png" alt="3odvLQ.png"></a> </p><p>有5个人要进行投票选举0和1号候选人，但是需要保证：每个人除了知道自己的投票结果，互相不知道其他人的投票情况。在这种情况下怎么知道最后的winner是谁吗？</p><p>如上图，可以引入一个第三方——election center，第三方验证每一个人只能投一次，最后统计票数决策出最后的winner。</p></li><li><p><strong>Private auctions</strong></p><p>介绍一种拍卖机制，<a href="https://en.wikipedia.org/wiki/Vickrey_auction">Vickery auction</a>：对一个拍卖品，每个投标者在不知道其他人投标价格的情况下进行投标，最后的acution winner： highest bidder &amp; pays 2nd highers bid。即是标价最高者得标，但他只需要付第二高的标价。</p><p>所以public知道的信息只有：中标者和第二高投标者的标价。</p><p>需要实现这种机制，也可以引入一个第三方——auction center。</p></li></ol><hr><p>但是引入第三方真的安全吗？安全第三方也不安全。</p><p><a href="https://imgchr.com/i/3odqRf"><img src="https://s2.ax1x.com/2020/03/04/3odqRf.png" alt="3odqRf.png"></a> </p><p>再看上面那个Election的例子，如果把上面四个人的投票情况作为输入，第三方的任务其实是输出一个函数 $f(x_1,x_2,x_3,x_4)$ 而不公开其他信息。</p><p>因为安全第三方也许并不安全，所以如果去掉第三方，上面四个人遵从某种协议，相互通信，最后能否得出这个 $f(x_1,x_2,x_3,x_4)$ 这个结果函数，而不透露其投票信息？</p><p>答案是 “Yes”。</p><p>有一个惊人的定理：任何能通过第三方做到的事，也能不通过第三方做到。</p><blockquote><p><strong>Thm</strong>: anythong that can done with trusted auth. can also be done without.</p></blockquote><p>怎么做到？答案是 Secure multi-party computation（安全多方计算）。</p><p>挖坑博文：姚氏百万富翁问题[2]</p><h3 id="Crypto-magic"><a href="#Crypto-magic" class="headerlink" title="Crypto magic"></a>Crypto magic</h3><ol><li><p><strong>Privately outsourcing computation</strong> (安全外包计算)</p><p><a href="https://imgchr.com/i/3ododA"><img src="https://s2.ax1x.com/2020/03/04/3ododA.md.png" alt="3ododA.md.png"></a> </p><p>Alice想要在Google服务器查询信息，为了不让别人知道她查询的是什么，她把search query进行加密。</p><p>Google服务器接收到加密的查询请求，虽然Google不知道她实际想查询什么信息，但是服务器能根据E[query]返回E[results]。</p><p>最后Alice将收到的E[results]解密，得到真正的results。</p><p>这就是安全外包计算的简单过程：Encryption、Search、Decryption。</p></li><li><p><strong>Zero knowl（proof of knowledge)</strong> (零知识证明)：</p><p><a href="https://imgchr.com/i/3odRxO"><img src="https://s2.ax1x.com/2020/03/04/3odRxO.md.png" alt="3odRxO.md.png"></a> </p><p>Alice 知道p、q(两个1000位的质数)相乘等于N。</p><p>Bob只知道N的值，不知道具体的p、q值。</p><p>Alice 给 Bob说她能够分解数N，但她不用告诉Bob N的具体因子是什么，只需要证明我能分解N，证明这是我的知识。</p><p>最后Bob知道Alice能够分解N，但他不知道怎么分解（也就是不知道N的因子到底是什么）。</p></li></ol><h3 id="A-rigorous-science"><a href="#A-rigorous-science" class="headerlink" title="A rigorous science"></a>A rigorous science</h3><p>在密码学的研究中，通常是这样的步骤：</p><ol><li><p>Precisely specify threat model.</p><p>准确描述其威胁模型或为达到的目的。比如签名的目的：unforgeable（不可伪造）。</p></li><li><p>Propose a construction.</p></li><li><p>Prove that breaking construction under threat mode will solve an underlying hard problem.</p><p>证明攻击者攻击这个系统必须解决一个很难的问题（大整数分解问题之类的NP问题）。</p><p>这样也就证明了这个系统是安全的。</p></li></ol><h1 id="History"><a href="#History" class="headerlink" title="History"></a>History</h1><h2 id="Substitution-cipher（替换）"><a href="#Substitution-cipher（替换）" class="headerlink" title="Substitution cipher（替换）"></a>Substitution cipher（替换）</h2><h3 id="what-is-it"><a href="#what-is-it" class="headerlink" title="what is it"></a>what is it</h3><p><a href="https://imgchr.com/i/3odBqJ"><img src="https://s2.ax1x.com/2020/03/04/3odBqJ.png" alt="3odBqJ.png"></a> </p><p>替换密码很好理解，如上图的这种替换表（key）。</p><p>比较historic的替换密码——Caesar Cipher（凯撒密码），凯撒密码是一种替换规则：向后移三位，因此也可以说凯撒密码没有key。</p><h3 id="the-size-of-key-space"><a href="#the-size-of-key-space" class="headerlink" title="the size of key space"></a>the size of key space</h3><p>用$\mathcal{K}$ （花体的K）来表示密钥空间。</p><p>英语字母的替换密码，易得密钥空间的大小是 $|\mathcal{K}|=26!\approx2^{88}$ （即26个字母的全排列）。</p><p>这是一个就现在而言也就比较perfect的密钥空间。</p><p>但替换密码也很容易被破解。</p><h3 id="how-to-break-it"><a href="#how-to-break-it" class="headerlink" title="how  to break it"></a>how  to break it</h3><p>问：英语文本中最commom的字母是什么？</p><p>答：“E”</p><p>在英语文本（大量）中，每个字母出现的频率并不是均匀分布，我们可以利用一些最common的字母和字母组合来破解替换密码。</p><ol><li><p>Use frequency of English letters.</p><p>Dan教授统计了标准文献中字母频率： “e”: 12.7% , “t”: 9.1% , “a” : 8.1%.</p><p>统计密文中（大量）出现频率最高、次高、第三高的字母，他们的明文也就是e、t、a。</p></li><li><p>Use frequency of pairs of letters (diagrams).（二合字母）</p><p>频率出现较高的二合字母：”he”, “an”, “in” , “th”</p><p>也能将h, n,i等破解出。</p></li><li><p>trigrams（继续使用三合字母）</p></li><li><p>……直至全部破解</p></li></ol><p>因此substitution cipher是<strong>CT only attack！</strong>（惟密文攻击：仅凭密文就可以还原出原文）</p><h2 id="Vigener-cipher"><a href="#Vigener-cipher" class="headerlink" title="Vigener cipher"></a>Vigener cipher</h2><h3 id="Encryption"><a href="#Encryption" class="headerlink" title="Encryption"></a>Encryption</h3><p><a href="https://imgchr.com/i/3od0r4"><img src="https://s2.ax1x.com/2020/03/04/3od0r4.md.png" alt="3od0r4.md.png"></a> </p><p>加密过程如上图所示：</p><ol><li>密钥是 “CRYPTO”, 长度为6，将密钥重复书写直至覆盖整个明文长度。</li><li>将密钥的字母和对应的明文相加模26，得到密文。</li></ol><h3 id="Decryption"><a href="#Decryption" class="headerlink" title="Decryption"></a>Decryption</h3><p>解密只需要将密文减去密钥字母，再模26即可。</p><h3 id="How-to-break-it"><a href="#How-to-break-it" class="headerlink" title="How to break it"></a>How to break it</h3><p>破解方法和替换密码类似，思想也是使用字母频率来破解。</p><p>这里分两种情况讨论：</p><p><strong>第一种：已知密钥长度</strong></p><p>破解过程：</p><ol><li><p>将密文按照密钥长度分组，按照图中的话，6个一组。</p></li><li><p>统计每组的的第一个位置的字母出现频率。</p><ul><li>假设密文中第一个位置最common的是”H”</li><li>密钥的第一个字母是：”H”-“E”=”C”</li></ul></li><li><p>统计剩下位置的字母频率，直至完全破解密钥。</p></li></ol><p><strong>第二种：未知密钥长度</strong></p><p>未知密钥长度，只需要依次假设密钥长度是1、2、3…，再按照第一种情况破解，直至破解情况合理。</p><h2 id="Rotor-Machines"><a href="#Rotor-Machines" class="headerlink" title="Rotor Machines"></a>Rotor Machines</h2><p>Rotor: 轴轮。</p><p>所以这种密码的加密核心是：输入明文字母，轴轮旋转一定角度，映射为另一字母。</p><h3 id="single-rotor"><a href="#single-rotor" class="headerlink" title="single rotor"></a>single rotor</h3><p><a href="https://imgchr.com/i/3odwMF"><img src="https://s2.ax1x.com/2020/03/04/3odwMF.md.png" alt="3odwMF.md.png"></a> </p><p>早期的是单轴轮，rotor machine的密钥其实是图右中间那个圆圆的可以旋转的柱子。</p><p>图左是变动的密钥映射表。</p><p>变动过程：</p><ol><li>第一次输入A，密文是K。</li><li>轴轮旋转一个字母位：看图中E，从最下到最上（一个圈，只相隔一位）。</li><li>所以第二次再输入A，密文是E。</li><li>……</li></ol><h3 id="Most-famous-：the-Enigma"><a href="#Most-famous-：the-Enigma" class="headerlink" title="Most famous ：the Enigma"></a>Most famous ：the Enigma</h3><p><a href="https://imgchr.com/i/3odU2T"><img src="https://s2.ax1x.com/2020/03/04/3odU2T.md.png" alt="3odU2T.md.png"></a> </p><p><a href="https://en.wikipedia.org/wiki/Enigma_machine">Enigma machine</a>是二战时期纳粹德国使用的加密机器，因此完全破解了Enigma是盟军提前胜利的关键。</p><p>左图中可以看出Enigma机器中是有4个轴轮，每个轴轮都有自己的旋转字母位大小，因此密钥空间大小是 $|\mathcal{K}|=26^4\approx2^{18}$ (在plugboard中，实际是 $2^{36}$)。</p><p>密钥空间很小，放在现在很容易被暴力破解。</p><blockquote><p>plugboard 允许操作员重新配置可变接线，实现两个字母的交换。plugboard比额外的rotor提供了更多的加密强度。</p><p>对于Enigma machine的更多的具体介绍可以戳<a href="https://en.wikipedia.org/wiki/Enigma_machine#Plugboard">Enigma machine</a> 的wiki链接。</p></blockquote><h3 id="Data-Encryption-Standard"><a href="#Data-Encryption-Standard" class="headerlink" title="Data Encryption Standard"></a>Data Encryption Standard</h3><p>DES：#keys = $2^{56}$ ,block siez = 64bits，一次可以加密8个字母。</p><p><strong>Today</strong>：AES（2001）、Salsa20（2008）……</p><p>这里只是简单介绍。</p><h1 id="Discrete-Probability"><a href="#Discrete-Probability" class="headerlink" title="Discrete Probability"></a>Discrete Probability</h1><p>这个segment比较简单，概率论基本完全cover了，这里只讲一些重点。</p><h2 id="Randomized-algorithms"><a href="#Randomized-algorithms" class="headerlink" title="Randomized algorithms"></a>Randomized algorithms</h2><p>随机算法有两种，一种是Deterministic algorithm（也就是伪随机），另一种是Randomized algorithm。</p><h3 id="Deterministic-algorithm"><a href="#Deterministic-algorithm" class="headerlink" title="Deterministic algorithm"></a>Deterministic algorithm</h3><p><a href="https://imgchr.com/i/3odNGV"><img src="https://s2.ax1x.com/2020/03/04/3odNGV.png" alt="3odNGV.png"></a> </p><p>$ y\longleftarrow A(m)$ ，这是一个确定的函数，输入映射到唯一输出。</p><h3 id="Randomized-algorithm"><a href="#Randomized-algorithm" class="headerlink" title="Randomized algorithm"></a>Randomized algorithm</h3><p><a href="https://imgchr.com/i/3odaxU"><img src="https://s2.ax1x.com/2020/03/04/3odaxU.png" alt="3odaxU.png"></a> </p><p>$y\longleftarrow A(m ; r) \quad \text { where } r \stackrel{R}{\longleftarrow}{0,1}^{n}$ </p><p>output： $y \stackrel{R}{\longleftarrow} A(m)$ ，y is a random variable.</p><blockquote><p>$ r \stackrel{R}\longleftarrow { 0,1 }^n $ :意思是r是n位01序列中的任意一个取值。R，random。变量r服从在 ${0,1}^n$ 取值的均匀分布。</p></blockquote><p>由于随机变量r，对于给定m，$A(m;r)$ 是 ${0,1}^n$ 中的一个子集。</p><p>所以，对m的加密结果y，也是一个的随机变量，而且，y在 $A(m,r)$ 也是服从均匀分布。</p><p>因此，由于r的影响，对于给定m，加密结果不会映射到同一个值。（如上图所示）</p><h2 id="XOR"><a href="#XOR" class="headerlink" title="XOR"></a>XOR</h2><p>XOR有两种理解：（  $x \oplus y $  ）</p><ul><li>一种是：x,y的bit位相比较，相同则为0，相异为1.</li><li>另一种是：x,y的bit位相加 mod2.</li></ul><p>异或在密码学中被频繁使用，主要是因为异或有一个重要的性质。</p><p><strong>异或的重要性质：有两个在 ${0,1}^n$ （n位01串）取值的随机变量X、Y。X、Y相互独立，X服从任意某种分布，随机变量Y服从均匀分布。那么 $Z=Y\oplus X$ ，Z在 ${0,1}^n$ 取值，且Z服从均分分布。</strong></p><blockquote><p><strong>Thm</strong>: Y a rand. var. over ${0,1}^n$ , X an index. uniform var. on ${0,1}^n$ </p><p>​            Then Z := Y $\oplus$ X is uniform var. on ${0,1}^n$ .</p></blockquote><p><strong>Proof：</strong></p><ol><li><p>当n=1</p></li><li><p>画出联合分布</p><p><a href="https://imgchr.com/i/3odGan"><img src="https://s2.ax1x.com/2020/03/04/3odGan.png" alt="3odGan.png"></a> </p></li><li><p>Pr[ Z=0 ]=Pr[ (x,y)=(0,0)] + Pr[(x,y)=(1,1)]=1/2</p></li><li><p>每一bit位都服从均匀分布，可以容易得出 Z是服从难过均匀分布。</p></li></ol><h2 id="The-birthday-paradox（生日悖论）"><a href="#The-birthday-paradox（生日悖论）" class="headerlink" title="The birthday paradox（生日悖论）"></a>The birthday paradox（生日悖论）</h2><p>更具体的分析见 <a href="https://en.wikipedia.org/wiki/Birthday_problem">Birthday problem</a> 。</p><p>问题前提：一个人的生日在365天的任意一天是均匀分布的（实际当然不是，貌似更多集中在9月）。</p><p>根据信鸽理论（有N个鸽子，M个隔间，如果N&gt;M，那么一定有一个隔间有两只鸽子），所以367个人中，以100%的概率有两个人的生日相同。但是，当只有70个人时，就有99.9%的概率，其中两人生日相同；当只有23人，这个概率可以达到50%。</p><p>其实这并不是一个悖论，只是直觉误导，理性和感性认识的矛盾。当只有一个人，概率为0，当有367人时，为100%，所以我们直觉认为，这是线性增长的，其实不然。</p><p>概率论知识：</p><p>设事件A：23个人中，有两个人生日相等。</p> $P\left(A^{\prime}\right)=\frac{365}{365} \times \frac{364}{365} \times \frac{363}{365} \times \frac{362}{365} \times \cdots \times \frac{343}{365}$ $P\left(A^{\prime}\right)=\left(\frac{1}{365}\right)^{23} \times(365 \times 364 \times 363 \times \cdots \times 343)$ $P\left(A^{\prime}\right) \approx 0.492703$ $P(A) \approx 1-0.492703=0.507297 \quad(50.7297 \%)$ 推广到一般情况，n个人(n<=365)中，有两个人生日相等的概率 $P=\frac{365\times 364 \times (365-n+1)}{365^n}$<p><a href="https://imgchr.com/i/3odJ5q"><img src="https://s2.ax1x.com/2020/03/04/3odJ5q.png" alt="3odJ5q.png"></a> </p><p>通过泰勒级数的近似处理（wiki上还有很多种近似方法），可以画出函数图如上图，概率值爆炸增。</p><p><strong>从生日问题可以推导到一个定理：</strong></p><p><strong>n个独立同分布的随机变量 $r_1,r_2,…r_n$ ,且随机变量的取值 $\in \text{U}$ ,那么当 n = $1.2\times|\text{U}|^{1/2}$  时，有两个随机变量取值相等的概率大于等于1/2，即 Pr[ ∃i≠j: ri = rj ] ≥ 1/2。</strong></p><blockquote><p>Let $r_1$, …, $r_n$ ∈ U be indep. identically distributed random vars. </p><p><strong>Thm</strong>: when n = $1.2\times|\text{U}|^{1/2}$  then Pr[ ∃i≠j: ri = rj ] ≥ 1/2</p></blockquote><p>后面应该会讲到生日攻击[3]，挖坑QAQ。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] 数字签名</p><p>[2] 姚氏百万富翁问题</p><p>[3] 生日攻击</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本系列是学习Dan Boneh教授的Online Cryptography Course。&lt;/p&gt;
&lt;p&gt;这是Dan教授的第一讲：对密码学的一些Introduction。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cryptography-Dan" scheme="https://f1ed.github.io/categories/Cryptography-Dan/"/>
    
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="Cryptography" scheme="https://f1ed.github.io/tags/Cryptography/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Gradient</title>
    <link href="https://f1ed.github.io/2020/03/01/Gradient/"/>
    <id>https://f1ed.github.io/2020/03/01/Gradient/</id>
    <published>2020-02-29T16:00:00.000Z</published>
    <updated>2020-07-03T08:42:12.163Z</updated>
    
    <content type="html"><![CDATA[<p>总结「李宏毅老师-机器学习」的Gradient，主要从以下三个方面展开：调节learning rate；加快训练速度；对数据进行Feature Scaling。</p><a id="more"></a><p>阅读完三个tips，具体实现可<a href="/2020/03/09/Adagrad-demo/" title="demo">demo</a></p><h1 id="Tip-1-Tuning-your-learning-rates-carefully"><a href="#Tip-1-Tuning-your-learning-rates-carefully" class="headerlink" title="Tip 1: Tuning your learning rates carefully"></a>Tip 1: Tuning your learning rates carefully</h1><h2 id="Visualize-损失函数随着参数变化的函数图"><a href="#Visualize-损失函数随着参数变化的函数图" class="headerlink" title="Visualize 损失函数随着参数变化的函数图"></a>Visualize 损失函数随着参数变化的函数图</h2><p><a href="https://imgchr.com/i/3gbjfJ"><img src="https://s2.ax1x.com/2020/03/01/3gbjfJ.md.png" alt="3gbjfJ.md.png"></a></p><p>左图是Loss Function的函数图，红色是最好的Step，当Step过小（蓝色），会花费很多时间，当Step过大（绿色、黄色），会发现Loss越来越大，找不到最低点。</p><p>所以在Training中，尽可能的visualize loss值的变化。</p><p>但是当参数大于等于三个时， $loss function$的函数图就不能visualize了。</p><p>因此，在右图中，visualize Loss随着参数更新的变化，横轴即迭代次数，当图像呈现蓝色（small）时，就可以把learning rate 调大一些。</p><h2 id="Adaptive-Learning-Rates-Adagrad"><a href="#Adaptive-Learning-Rates-Adagrad" class="headerlink" title="Adaptive Learning Rates(Adagrad)"></a>Adaptive Learning Rates(Adagrad)</h2><p>但是手动调节 $\eta$是低效的，我们更希望能自动地调节。</p><p>直观上的原则是：</p><ul><li>$\eta$ 的大小应该随着迭代次数的增加而变小。<ul><li>最开始，初始点离minima很远，那step应该大一些，所以learning rate也应该大一些。</li><li>随着迭代次数的增加，离minima越来越近，就应该减小 learning rate。</li><li>E.g. 1/t decay： $\eta^t=\eta/ \sqrt{t+1}$</li></ul></li><li>不同参数的 $\eta$应该不同（cannot be one-size-fits-all)。</li></ul><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Adagrad 的主要思想是：Divide the learning rate of each parameter <strong>by the root mean squear of its previous derivatives.</strong>(通过除这个参数的 计算出的所有导数 的均方根)</p><blockquote><p>root mean squar :   $ \sqrt{\frac{1}{n}(x_1^2+x_2^2+...+x_n^2)} $ </p></blockquote><p><u><strong>Vanilla Gradient descent</strong></u></p><p>$w^{t+1}  \leftarrow w^{t}-\eta^{t} g^{t}$</p><p><u><strong>Adagrad</strong></u></p> $w^{t+1}  \leftarrow w^{t}-\frac{\eta^{t}}{\sigma^{t}} g^{t} $  <blockquote><p>$\eta^t$：第t次迭代的leaning rate</p> $ \eta^{t}=\frac{\eta}{\sqrt{t+1}}$   $g^{t}=\frac{\partial L\left(\theta^{t}\right)}{\partial w} $  <p>$\sigma^t$：root mean squar of previous derivatives of w</p> $\tau^{t}=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}\left(g^{i}\right)^{2}} $  </blockquote><p>对比上面两种Adaptive Gradient，Adagrade的优势是learning rate 是和parameter dependent（参数相关的）。</p><h3 id="Adagrad步骤简化"><a href="#Adagrad步骤简化" class="headerlink" title="Adagrad步骤简化"></a>Adagrad步骤简化</h3><p><strong>步骤：</strong></p><p><a href="https://imgchr.com/i/3gbXY4"><img src="https://s2.ax1x.com/2020/03/01/3gbXY4.md.png" alt="3gbXY4.md.png"></a></p><p><strong>简化公式：</strong></p>  $ w^{t+1}  \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}$  <p>(  $ \eta^{t}=\frac{\eta}{\sqrt{t+1}} $   ,   $ \sigma^{t}=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}\left(g^{i}\right)^{2}}$   ,约掉共同项即可)</p><h3 id="Adagrad-Contradiction-——Adagrad原理解释"><a href="#Adagrad-Contradiction-——Adagrad原理解释" class="headerlink" title="Adagrad Contradiction? ——Adagrad原理解释"></a>Adagrad Contradiction? ——Adagrad原理解释</h3><p><u><strong>Vanilla Gradient descent</strong></u></p><p>$w^{t+1}  \leftarrow w^{t}-\eta^{t} g^{t}$</p><p><u><strong>Adagrad</strong></u></p>  $ w^{t+1}  \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}$  <p>在Vanilla Gradient descent中， $g^t$越大，也就是当前梯度大，也就有更大的step。</p><p>而在Adagrad中，当 $g^t$越大，有更大的step,而当  $\sqrt{\sum_{i=0}^t (g^i)^2} $  越大，反而有更小step。</p><p>Contradiction？</p><h4 id="「Intuitive-Reason（直观上解释）」"><a href="#「Intuitive-Reason（直观上解释）」" class="headerlink" title="「Intuitive Reason（直观上解释）」"></a>「Intuitive Reason（直观上解释）」</h4>  $ w^{t+1}  \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}$    $\sqrt{\sum_{i=0}^t (g^i)^2} $  是为了造成反差的效果。<p><a href="https://imgchr.com/i/3gbqTU"><img src="https://s2.ax1x.com/2020/03/01/3gbqTU.md.png" alt="3gbqTU.md.png"></a></p><p>类比一下，如果一个一直很凶的人，突然温柔了一些，你会觉得他特别温柔。所以同样是 $0.1$,第一行中，你会觉得特别大，第二行中，你会觉得特别小。</p><p>因此  $\sqrt{\sum_{i=0}^t (g^i)^2} $  这一项的存在就能体现 $g^t$的变化有多surprise。</p><h4 id="「数学一些的解释」"><a href="#「数学一些的解释」" class="headerlink" title="「数学一些的解释」"></a>「数学一些的解释」</h4><h5 id="1-Larger-Gradient-larger-steps"><a href="#1-Larger-Gradient-larger-steps" class="headerlink" title="1. Larger Gradient,larger steps?"></a>1. <strong>Larger Gradient,larger steps?</strong></h5><p>在前面我们都深信不疑这一点，但这样的描述真的是正确的吗？</p><p><a href="https://imgchr.com/i/3gbbwT"><img src="https://s2.ax1x.com/2020/03/01/3gbbwT.md.png" alt="3gbbwT.md.png"></a></p><p>在这张图中，只有一个参数，认为当该点的导数越大，离minima越远，这样看来，Larger Gradient,larger steps是正确的。</p><p>在上图中的 $x_0$点，该点迭代更新的best step 应该正比于 $|x_0+\frac{b}{2a}|$ ，即 $\frac{|2,a, x_0+b|}{2a}$。</p><p>而 $\frac{|2,a, x_0+b|}{2a}$的分子也就是该点的一阶导数的绝对值。</p><p><a href="https://imgchr.com/i/3gbcef"><img src="https://s2.ax1x.com/2020/03/01/3gbcef.png" alt="3gbcef.png"></a></p><p>上图中，有 $w_1,w_2$两个参数。</p><p>横着用蓝色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较a、b两点，a点导数大，离minima远。</p><p>竖着用绿色的线切一刀，得到的是 $w_2$ 固定，loss随着 $w_1$变化的图像：比较c、d两点，c点导数大，离minima远。</p><p>但是，如果比较a、c两点呢？</p><p>a点对 $w_1$ 的偏导数和c点对 $w_2$的偏导数比较？</p><p>比较出来，c点点偏导数更大，离minima更远吗？</p><p>再看左图的图像，横着的弧度更平滑，竖着的弧度更尖一些，直观上看应该c点离minima更近一些。</p><p><strong>所以Larger Gradient,larger steps点比较方法不能（cross parameters)跨参数比较。</strong></p><p><strong>所以最好的step $\propto$ 一阶导数（Do not cross parameters)。</strong></p><h5 id="2-Second-Derivative"><a href="#2-Second-Derivative" class="headerlink" title="2.** Second Derivative**"></a>2.** Second Derivative**</h5><p><a href="https://imgchr.com/i/3gbHmV"><img src="https://s2.ax1x.com/2020/03/01/3gbHmV.md.png" alt="3gbHmV.md.png"></a></p><p>前面讨论best step $\frac{|2,a, x_0+b|}{2a}$的分子是该点一阶导数，那么其分母呢？</p><p>当对一阶导数再求导时，可以发现其二阶导数就是best step的分母。</p><p><strong>得出结论：the best step $\propto$  |First dertivative| /  Second derivative。</strong></p><p><a href="https://imgchr.com/i/3gbTO0"><img src="https://s2.ax1x.com/2020/03/01/3gbTO0.md.png" alt="3gbTO0.md.png"></a></p><p>因此，再来看两个参数的情况，比较a点和c点，a点的一阶导数更小，二阶导数也更小；c点点一阶导数更大，二阶导数也更大。</p><p>所以如果要比较a、c两点，谁离minima更远，应该比较其一阶导数的绝对值除以其二阶导数的大小。</p><h4 id="回到"><a href="#回到" class="headerlink" title="回到 "></a>回到  $ w^{t+1}  \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}$  </h4><p>上一部分得出的结论是：the best step $\propto$  |First dertivative| /  Second derivative。</p><p>所以我们的learning rate 也应该和 |First dertivative| /  Second derivative相关。</p><p>$g^t$也就是一阶导数，但为什么  $\sqrt{\sum_{i=0}^t (g^i)^2} $  能代表二阶导数呢？</p><p><a href="https://imgchr.com/i/3gbIln"><img src="https://s2.ax1x.com/2020/03/01/3gbIln.md.png" alt="3gbIln.md.png"></a></p><p>上图中，蓝色的函数图有更小的二阶导数，绿色的函数图有更大的二阶导数。</p><p>在复杂函数中，求二阶导数是一个很复杂的计算。</p><p>所以我们想<strong>用一阶导数来反映二阶导数的大小</strong>。</p><p>在一阶导数的函数图中，认为一阶导数值更小的，二阶导数也更小，但是取一个点显然是片面的，所以考虑取多个点。</p><p>也就是用  $ \sqrt{\text{(first derivative)}^2}$  来代表best step中的二阶导数。</p><h4 id="总结一下"><a href="#总结一下" class="headerlink" title="总结一下"></a>总结一下</h4><p>Adagrad的为了找寻最好的learning rate，从找寻best step下手，用简单的二次函数为例，得出 best step $\propto$  |First dertivative| /  Second derivative。</p><p>但是复杂函数的二阶导数是难计算的，因此考虑用多个点的一阶导数来反映其二阶导数。</p><p>得出 $ w^{t+1}  \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}$  。</p><p>直观来解释公式中的一阶导数的root mean square，即来为该次迭代的一阶导数造成反差效果。</p><p>其他文献中的Adaptive Gradient理应都是为了调节learning rate使之有best step。(待补充的其他Gradient)[1]</p><h1 id="Tip-2-Stochastic-Gradient-Descent"><a href="#Tip-2-Stochastic-Gradient-Descent" class="headerlink" title="Tip 2:Stochastic Gradient Descent"></a>Tip 2:Stochastic Gradient Descent</h1><h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>在linear model中，我们这样计算Loss function： $L=\sum_{n}\left(\hat{y}^{n}-\left(b+\sum w_{i} x_{i}^{n}\right)\right)^{2} $  </p><p>每求一次Loss function，L都对所有training examples的 $\text{error}^2$求和，因此每一次的loss function的计算，都是一重循环。</p><p>在Stochastic Gradient Descent中，每一次求loss function，只取一个example $x^n$，减少一重循环，无疑更快。</p><p><u> <strong>Stochastic Gradient Descent</strong></u></p><p>Pick an example $x^n$</p> $L=\left(\hat{y}^{n}-\left(b+\sum w_{i} x_{i}^{n}\right)\right)^{2} $  <p><a href="https://imgchr.com/i/3gbhWj"><img src="https://s2.ax1x.com/2020/03/01/3gbhWj.md.png" alt="3gbhWj.md.png"></a></p><p>上图中，传统的Gradient Descent看完一次所有的examples，离minima还很远；而Stochastic Gradient Descent ，看完一次，已经离minima较近了。</p><h1 id="Tip-3-Feature-Scaling"><a href="#Tip-3-Feature-Scaling" class="headerlink" title="Tip 3:Feature Scaling"></a>Tip 3:Feature Scaling</h1><h2 id="What-is-Feature-Scaling"><a href="#What-is-Feature-Scaling" class="headerlink" title="What is Feature Scaling"></a>What is Feature Scaling</h2><p><a href="https://imgchr.com/i/3gbfYQ"><img src="https://s2.ax1x.com/2020/03/01/3gbfYQ.md.png" alt="3gbfYQ.md.png"></a></p><p>如上图所示，希望能让不同的feature能有相同的scale（定义域/规模）</p><h2 id="Why-Feature-Scaling"><a href="#Why-Feature-Scaling" class="headerlink" title="Why Feature Scaling"></a>Why Feature Scaling</h2><p>假设model都是 $y = b+ w_1 x_1 +w_2 x_2$。</p><p><a href="https://imgchr.com/i/3gb2TS"><img src="https://s2.ax1x.com/2020/03/01/3gb2TS.md.png" alt="3gb2TS.md.png"></a></p><p>上图中，左边 $x_2$的规模更大，可以认为 $x_1$ 对loss 的影响更小， $ x_2$对loss的影响更大。</p><p>即当 $w_1,w_2$轻微扰动时，同时加上相同的 $\Delta w$时，$x_2$ 使 $y$的取值更大，那么对loss 的影响也更大。</p><p>如图中下方的函数图 $w_1$方向的L更平滑， $w_2$ 方向更陡峭些，Gradient descent的步骤如图所示。</p><p>但当对 $x_2$进行feature scaling后，图像会更像正圆，Gradient descent使，参数更新向着圆心走，更新会更有效率。</p><h2 id="How-Feature-Scaling"><a href="#How-Feature-Scaling" class="headerlink" title="How Feature Scaling"></a>How Feature Scaling</h2><p>概率论知识：标准化。</p><blockquote><p>概率论：</p></blockquote> 随机变量 $X$ 的期望和方差均存在，且 $ D(X)>0$,令 $X^*=\frac{X-E(X)}{\sqrt{D(X)}}$  那么 $E(X^*)=0,D(X)=1 $ , $ X^* $ 称为X的标准化随机变量。<p><a href="https://imgchr.com/i/3gbgw8"><img src="https://s2.ax1x.com/2020/03/01/3gbgw8.md.png" alt="3gbgw8.md.png"></a></p><p>对所有向量的每一维度，进行标准化处理： $x_{i}^{r} \leftarrow \frac{x_{i}^{r}-m_{i}}{\sigma_{i}} $ </p><p>（ $m_i$是该维度变量的均值， $\sigma_i$ 是该维度变量的方差）</p><p>标准化后，每一个feature的期望都是0，方差都是1。</p><h1 id="Gradient-Descent-Theory-公式推导"><a href="#Gradient-Descent-Theory-公式推导" class="headerlink" title="Gradient Descent Theory(公式推导)"></a>Gradient Descent Theory(公式推导)</h1><p>当用Gradient Descent解决 $\theta^*=\arg \min_\theta L(\theta)$时，我们希望每次更新 $\theta $ 都能得到 $L(\theta^0)&gt;L(\theta^1)&gt;L(\theta^2)&gt;…$ 这样的理论结果，但是不总能得到这样的结果。</p><p><a href="https://imgchr.com/i/3gb0Wd"><img src="https://s2.ax1x.com/2020/03/01/3gb0Wd.md.png" alt="3gb0Wd.md.png"></a></p><p>上图中，我们虽然不能一下知道minima的方向，但是我们希望：当给一个点 $\theta^0$ 时，我们能很容易的知道他附近（极小的附近）的最小的loss 是哪个方向。</p><p>所以怎么做呢？</p><h2 id="Tylor-Series"><a href="#Tylor-Series" class="headerlink" title="Tylor Series"></a>Tylor Series</h2><p>微积分知识：Taylor Series（泰勒公式）。</p><p>Tylor Series:<br>函数 $h(x)$ 在 $x_0$ 无限可导，那么 </p> $\begin{aligned}\mathrm{h}(\mathrm{x}) &=\sum_{k=0}^{\infty} \frac{\mathrm{h}^{(k)}\left(x_{0}\right)}{k !}\left(x-x_{0}\right)^{k} \\&=h\left(x_{0}\right)+h^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{h^{\prime \prime}\left(x_{0}\right)}{2 !}\left(x-x_{0}\right)^{2}+\ldots\end{aligned}$  <p>当 x 无限接近 $x_0$ 时，忽略后面无穷小的高次项，  $h(x) \approx h\left(x_{0}\right)+h^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)  $ </p><p><a href="https://imgchr.com/i/3gbrQI"><img src="https://s2.ax1x.com/2020/03/01/3gbrQI.md.png" alt="3gbrQI.md.png"></a></p><p>上图中，用 $\pi/4$ 处的一阶泰勒展示来表达 $\sin(x)$ ,图像是直线，和 $\sin(x)$ 图像相差很大，但当 x无限接近 $\pi/4$ 是，函数值估算很好。</p><p><strong>Multivariable Taylor Series</strong></p> $h(x, y)=h\left(x_{0}, y_{0}\right)+\frac{\partial h\left(x_{0}, y_{0}\right)}{\partial x}\left(x-x_{0}\right)+\frac{\partial h\left(x_{0}, y_{0}\right)}{\partial y}\left(y-y_{0}\right) +\text{something raleted to} (x-x_x^0)^2 \text{and} (y-y_0)^2+…$ <p>当  $(x,y)$ 接近 $(x_0,y_0)$ 时， $h(x,y)$ 用 $(x_0,y_0)$ 处的一阶泰勒展开式估计。</p> $ h(x, y) \approx h\left(x_{0}, y_{0}\right)+\frac{\partial h\left(x_{0}, y_{0}\right)}{\partial x}\left(x-x_{0}\right)+\frac{\partial h\left(x_{0}, y_{0}\right)}{\partial y}\left(y-y_{0}\right)$ <h2 id="Back-to-Formal-Derivation"><a href="#Back-to-Formal-Derivation" class="headerlink" title="Back to Formal Derivation"></a>Back to Formal Derivation</h2><p><a href="https://imgchr.com/i/3gLTaT"><img src="https://s2.ax1x.com/2020/03/01/3gLTaT.png" alt="3gLTaT.png"></a></p><p>当图中的红色圆圈足够小时，红色圆圈中的loss 值就可以用 $(a,b)$ 处的一阶泰勒展开式来表示。</p> $ \mathrm{L}(\theta) \approx \mathrm{L}(a, b)+\frac{\partial \mathrm{L}(a, b)}{\partial \theta_{1}}\left(\theta_{1}-a\right)+\frac{\partial \mathrm{L}(a, b)}{\partial \theta_{2}}\left(\theta_{2}-b\right) $ <p>$(\theta_1-a)^2+(\theta_2-b)^2 \leq d^2$ ,d 足够小。</p><p>用 $s=L(a,b)$ ,  $ u=\frac{\partial \mathrm{L}(a, b)}{\partial \theta_{1}}, v=\frac{\partial \mathrm{L}(a, b)}{\partial \theta_{2}} $ 表示。</p><p><strong>最后问题变成：</strong></p>  $L(\theta)\approx s+u(\theta_1-a)+v(\theta_2-b)$ <p><strong>找 $(\theta_1,\theta_2)$，且满足 $(\theta_1-a)^2+(\theta_2-b)^2 \leq d^2$，使 $L(\theta)$ 最小。</strong></p><p>变成了一个简单的最优化问题。</p><p>令 $\Delta \theta_1=\theta_1-a$ , $\Delta\theta_2=\theta_2-b$</p><p>问题简化为：</p><p> $\text{min}:u \Delta \theta_1+v\Delta\theta_2$</p><p>$\text{subject to}:{\Delta\theta_1}^2+{\Delta\theta_2}^2\leq d^2$</p><p><a href="https://imgchr.com/i/3gbwJH"><img src="https://s2.ax1x.com/2020/03/01/3gbwJH.png" alt="3gbwJH.png"></a></p><p>画出图，就是初中数学了。更新的方向应该是 $(u,v)$ 向量反向的方向。</p><p>所以：</p> $\left[\begin{array}{l}\Delta \theta_{1} \\\Delta \theta_{2}\end{array}\right]=-\eta\left[\begin{array}{l}u \\v\end{array}\right] $  <br> $\left[\begin{array}{l}\theta_{1} \\\theta_{2}\end{array}\right]=\left[\begin{array}{l}a \\b\end{array}\right]-\eta\left[\begin{array}{l}u \\v\end{array}\right] $ <br> $ \left[\begin{array}{l}\theta_{1} \\\theta_{2}\end{array}\right]=\left[\begin{array}{l}a \\b\end{array}\right]-\eta\left[\begin{array}{l}u \\v\end{array}\right]=\left[\begin{array}{l}a \\b\end{array}\right]-\eta\left[\begin{array}{l}\frac{\partial \mathrm{L}(a, b)}{\partial \theta_{1}} \\\frac{\partial \mathrm{L}(a, b)}{\partial \theta_{2}}\end{array}\right] $ <h1 id="Limitation-of-Gradient-Descent"><a href="#Limitation-of-Gradient-Descent" class="headerlink" title="Limitation of Gradient Descent"></a>Limitation of Gradient Descent</h1><p><a href="https://imgchr.com/i/3gbsyt"><img src="https://s2.ax1x.com/2020/03/01/3gbsyt.md.png" alt="3gbsyt.md.png"></a></p><ol><li>Gradient Descent 可能会卡在local minima或者saddle point（鞍点：一个方向是极大值，一个方向是极小值，导数为0）</li><li>实践中，我们往往会在导数无穷接近0的时候停下来（&lt; 1e-7)，Gradient Descent 可能会停在plateau(高原；增长后的稳定)</li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] 待补充的其他Gradient</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;总结「李宏毅老师-机器学习」的Gradient，主要从以下三个方面展开：调节learning rate；加快训练速度；对数据进行Feature Scaling。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="Gradient" scheme="https://f1ed.github.io/tags/Gradient/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Regression</title>
    <link href="https://f1ed.github.io/2020/02/29/Regression/"/>
    <id>https://f1ed.github.io/2020/02/29/Regression/</id>
    <published>2020-02-28T16:00:00.000Z</published>
    <updated>2020-07-03T08:43:13.142Z</updated>
    
    <content type="html"><![CDATA[<p> 在<a href="https://www.youtube.com/watch?v=fegAeph9UaA">YouTube上看台大李宏毅老师的课</a>，看完Regression讲座的感受就是： 好想去抓Pokemon！！！<br> 这篇文章将总结李宏毅老师Regression的讲座，并尝试实现其demo。 </p><a id="more"></a><!--toc--><h1 id="Regression（回归）"><a href="#Regression（回归）" class="headerlink" title="Regression（回归）"></a>Regression（回归）</h1><h2 id="Define"><a href="#Define" class="headerlink" title="Define"></a>Define</h2><p><strong>Regression</strong>：是找到一个$function$，进行预测。对输入的feature，输出一个$Scalar$(数值，标量)。</p><h2 id="Example-Application"><a href="#Example-Application" class="headerlink" title="Example Application"></a>Example Application</h2><p><a href="https://imgchr.com/i/3rgUIg"><img src="https://s2.ax1x.com/2020/02/28/3rgUIg.md.png" alt="3rgUIg.md.png"></a></p><p>Look for a $function$</p><ul><li><p>Stock Market Forecast（股票预测）</p><p>$input$：过去的股价信息</p><p>$output$：明天的股价平均值（$Scalar$)</p></li><li><p>Self-Driving Car(自动驾驶)</p><p>$input$：路况信息</p><p>$output$：方向盘角度（$Scalar$)</p></li><li><p>Recommendation（推荐系统）</p><p>$input$：使用者A、商品B</p><p>$output$：使用者A购买商品B的可能性</p></li></ul><p>可见，$input$都是一些特征信息，$output$都是一个标量数值，这就是Regression。</p><h1 id="Regression-Case-Pokenmon"><a href="#Regression-Case-Pokenmon" class="headerlink" title="Regression Case: Pokenmon"></a>Regression Case: Pokenmon</h1><blockquote><p>看完这节课，感想：好想去抓宝可梦QAQ</p></blockquote><p>预测一个pokemon进化后的CP（Combat Power，战斗力）值。</p><p>为什么要预测呐？</p><p>如果进化后的CP值高，就进化他，不然就把他当糖果，因为宝可梦很难抓的。（？没玩过，我也不懂o r z）</p><p><a href="https://imgchr.com/i/3rgNdS"><img src="https://s2.ax1x.com/2020/02/28/3rgNdS.md.png" alt="3rgNdS.md.png"></a></p><ul><li><p>上图妙蛙种子的信息(可能的$input$)：</p><p>$x_{cp}$：CP值</p><p>$x_s$:物种</p><p>$x_{hp}$:生命值</p><p>$x_w$:重量</p><p>$x_h$:高度</p></li><li><p>output：进化后的CP值。</p></li></ul><blockquote><p>$x_{cp}$：用下标表示一个object的component。</p><p>$x^1$：用上标表示一个完整的object。</p></blockquote><h2 id="Step-1-找一个Model（function-set）"><a href="#Step-1-找一个Model（function-set）" class="headerlink" title="Step 1: 找一个Model（function set）"></a>Step 1: 找一个Model（function set）</h2><p><strong>Model</strong> ：$y = b + w \cdot x_{cp}$</p><p>假设用上式作为我们的Model，那么这些函数： $\begin{aligned}&\mathrm{f}_{1}: \mathrm{y}=10.0+9.0 \cdot \mathrm{x}_{\mathrm{cp}}\\&f_{2}: y=9.8+9.2 \cdot x_{c p}\\&f_{3}: y=-0.8-1.2 \cdot x_{c p}\end{aligned}$ 等都属于这个集合，但是显然像$f_3$这种函数是bad，CP值不可能是负数。bad functions 很多，所以在下面的步骤，会说明如何判别一个函数的好坏，自动的选出最好的那个 $function$。</p><p>把Model 1一般化，得到线代中的 <strong>Linear Model</strong>：$y = b+\sum w_ix_i$ </p><p>$x_i$：x的feature</p><p>$b$：bias,偏置值</p><p>$w_i$：weight，权重</p><h2 id="Step-2-判别Goodness-of-Function-Training-Data"><a href="#Step-2-判别Goodness-of-Function-Training-Data" class="headerlink" title="Step 2: 判别Goodness of Function(Training Data)"></a>Step 2: 判别Goodness of Function(Training Data)</h2><h3 id="Training-Data"><a href="#Training-Data" class="headerlink" title="Training Data"></a>Training Data</h3><p>假定使用<strong>Model</strong> ：$y = b + w \cdot x_{cp}$</p><p><a href="https://imgchr.com/i/3rgtZ8"><img src="https://s2.ax1x.com/2020/02/28/3rgtZ8.md.png" alt="3rgtZ8.md.png"></a></p><p>Training Data：十只宝可梦，用向量的形式表示。</p><p>使用Training data来judge the goodness of function.。</p><h3 id="Loss-Function-损失函数"><a href="#Loss-Function-损失函数" class="headerlink" title="Loss Function(损失函数)"></a>Loss Function(损失函数)</h3><p>概率论：做线性回归，一般使用最小二乘法。一般回归，大多使用极大似然估计。</p><p><strong>Loss function $L$</strong> ：$L(f)=L(w,b)=\sum_{n=1}^{10}(\hat{y}^n-(b+w\cdot x_{cp}^n))^2$</p><p>其中的 $\hat{y}^n-(b+w\cdot x_{cp}^n)$是Estimation error(估测误差)</p><p><strong>Loss Function的意义</strong>：它的 $input$是一个 $function$，它的 $output$体现了how bad it is,这个函数有多糟/好。</p><h3 id="Figure-the-Result"><a href="#Figure-the-Result" class="headerlink" title="Figure the Result"></a>Figure the Result</h3><p><a href="https://imgchr.com/i/3rgJqf"><img src="https://s2.ax1x.com/2020/02/28/3rgJqf.md.png" alt="3rgJqf.md.png"></a></p><p>上图横纵坐标是函数 $L$的参数 $w 、b$，图中的每一个point都是一个 $function $。</p><p> color：体现函数的输出，越红越大，说明选择的函数越bad。</p><p>所以我们要选择紫色区域结果最小的函数。</p><p>而这个得到best function的过程是可以通过无数次迭代实现的。（重复的迭代当时是交给计算机做了）</p><h2 id="Step-3-迭代找出Best-Function"><a href="#Step-3-迭代找出Best-Function" class="headerlink" title="Step 3:迭代找出Best Function"></a>Step 3:迭代找出Best Function</h2>$L(w,b)=\sum_{n=1}^{10}(\hat{y}^n-(b+w\cdot x_{cp}^n))^2$ <p>找到Best Function: $f^{*}=\arg \min _{f} L(f)$</p><p>也就是找到参数 $w^{*},b^{*}=\arg \min_{w,b} L(w,b)=\arg \min_{w,b}\sum_{n=1}^{10}(\hat{y}^n-(b+w\cdot x_{cp}^n))^2$ </p><blockquote><p>arg ：argument,变元</p><p>arg min：使之最小的变元</p><p>arg max：使之最大的变元</p></blockquote><p>据悉，线性回归的参数可以用线性代数的知识，解出closed-form solution（解析解），我先挖个坑QAQ，以后来填这块知识。[1]</p><p>在机器学习中，只要<strong>$L$函数可微分</strong>， 即可用Gradient Descent（梯度下降）的方法来求解。</p><h3 id="Gradient-Decent（梯度下降）"><a href="#Gradient-Decent（梯度下降）" class="headerlink" title="Gradient Decent（梯度下降）"></a>Gradient Decent（梯度下降）</h3><p>和概率论中的梯度下降估计参数的原理相同，只是计算机不能直接解出方程的解，所以计算机的方法是迭代。</p><h3 id="考虑一个参数w"><a href="#考虑一个参数w" class="headerlink" title="考虑一个参数w*"></a>考虑一个参数w*</h3><p>$w^*=\arg \min_w L(w)$</p><p><strong>步骤：</strong></p><ol><li><p>随机选取一个初始值 $w^0$</p></li><li><p>计算 $ \frac{{\rm d}L}{{\rm d}w}|_{w=w^0}$   &nbsp; &nbsp; $\begin{equation}   w^{1} \leftarrow w^{0}-\left.\eta \frac{d L}{d w}\right|_{w=w^{0}}   \end{equation}$ </p></li><li><p>计算 $ \frac{{\rm d}L}{{\rm d}w}|_{w=w^0}$  &nbsp;&nbsp; $\begin{equation}   w^{2} \leftarrow w^{1}-\left.\eta \frac{d L}{d w}\right|_{w=w^{1}}   \end{equation}$</p></li><li><p>…until  $ \frac{{\rm d}L}{{\rm d}w}|_{w=w^n}=0$</p></li></ol><p><a href="https://imgchr.com/i/3rgGsP"><img src="https://s2.ax1x.com/2020/02/28/3rgGsP.md.png" alt="3rgGsP.md.png"></a></p><p><strong>上图迭代过程的几点说明</strong></p><ul><li><p>$\begin{equation}<br>\left.\frac{\mathrm{d} L}{\mathrm{d} w}\right|_{w=w^{i}}<br>\end{equation}$的正负</p><p>如果是negative，也就是该点切线斜率是负的，那应该Increse w，以找到最低点。</p><ul><li>Negative $\rightarrow$ Increase  w</li><li>Positive $\rightarrow$ Decrease w</li></ul></li><li><p>$-\left.\eta \frac{d L}{d w}\right|_{w=w^{i}}$：步长</p><ul><li>$\eta$：learning rate（学习速度），事先设好的值。</li><li>$-$(负号)：如果 $\begin{equation}<br>\left.\frac{\mathrm{d} L}{\mathrm{d} w}\right|_{w=w^{i}}<br>\end{equation}$是负的，应该增加w。</li></ul></li><li><p>Local optimal：局部最优和全局最优</p><ul><li>如果是以上图像，则得到的w不是全局最优。</li><li>但线性回归的损失函数是凸函数，存在一个全局最优，没有局部最优。</li></ul></li></ul><h3 id="考虑多个参数"><a href="#考虑多个参数" class="headerlink" title="考虑多个参数 "></a>考虑多个参数  $w^{*},b^{*}$ </h3><p>微积分知识：gradient（梯度，向量)： $\nabla L=\left[\begin{array}{l}<br>\frac{\partial L}{\partial w} \<br>\frac{\partial L}{\partial b}<br>\end{array}\right]$</p><p>考虑多个参数和考虑一个参数思路相同，每次迭代，迭代两个参数。</p> $w^{*}, b^{*}=\arg \min _{w, b} L(w, b)$ <p><strong>步骤</strong>：</p><ol><li><p>随机选取初值 $w^0,b^0$</p></li><li><p>计算 $\left.\left.\frac{\partial L}{\partial w}\right|_{w=w^{0}, b=b^{0},} \frac{\partial L}{\partial b}\right|_{w=w^{0}, b=b^{0}}$ &nbsp; &nbsp; $w^{1} \leftarrow w^{0}-\left.\eta \frac{\partial L}{\partial w}\right|_{w=w^{0}, b=b^{0}} \quad b^{1} \leftarrow b^{0}-\left.\eta \frac{\partial L}{\partial b}\right|_{w=w^{0}, b=b^{0}}$ </p></li><li><p>计算 $\left.\left.\frac{\partial L}{\partial w}\right|_{w=w^{1}, b=b^{1},} \frac{\partial L}{\partial b}\right|_{w=w^{1}, b=b^{1}}$ &nbsp; &nbsp; &nbsp; $w^{2} \leftarrow w^{1}-\left.\eta \frac{\partial L}{\partial w}\right|_{w=w^{1}, b=b^{1}} \quad b^{2} \leftarrow b^{1}-\left.\eta \frac{\partial L}{\partial b}\right|_{w=w^{1}, b=b^{1}}$ </p></li><li><p>…until $ \frac{{\rm d}L}{{\rm d}w}|_{w=w^n}=0$,    $ \frac{{\rm d}L}{{\rm d}b}|_{b=b^n}=0$</p></li></ol><p><a href="https://imgchr.com/i/3rg1xI"><img src="https://s2.ax1x.com/2020/02/28/3rg1xI.md.png" alt="3rg1xI.md.png"></a></p><p>上图，坐标为 $L(w,b)$函数的参数，Color代表 $L$的大小，越紫值越小。</p><p>每一个点都是一个 $function$，沿着梯度方向（图中法线方向）迭代，找到全局最优点。</p><p><strong>再次说明</strong>：线性回归中，损失函数是convex（凸函数），没有局部最优解。</p><h3 id="frac-partial-L-partial-w-和-frac-partial-L-partial-b-的公式推导"><a href="#frac-partial-L-partial-w-和-frac-partial-L-partial-b-的公式推导" class="headerlink" title="$\frac{\partial L}{\partial w}$和 $\frac{\partial L}{\partial b}$的公式推导"></a>$\frac{\partial L}{\partial w}$和 $\frac{\partial L}{\partial b}$的公式推导</h3><p>$L(w, b)=\sum_{n=1}^{10}\left(\hat{y}^{n}-\left(b+w \cdot x_{c p}^{n}\right)\right)^{2}$</p><p>微积分的知识，显然。</p><blockquote><p>数学真香。———我自己</p></blockquote><p>$\frac{\partial L}{\partial w}=\sum_{n=1}^{10}2\left(\hat{y}^{n}-\left(b+w \cdot x_{c p}^{n}\right)\right)（-x_{cp}^n)$</p><p>$\frac{\partial L}{\partial b}=\sum_{n=1}^{10}2\left(\hat{y}^{n}-\left(b+w \cdot x_{c p}^{n}\right)\right)(-1)$</p><h2 id="实际结果分析"><a href="#实际结果分析" class="headerlink" title="实际结果分析"></a>实际结果分析</h2><h3 id="Training-Data-1"><a href="#Training-Data-1" class="headerlink" title="Training Data"></a>Training Data</h3><p><a href="https://imgchr.com/i/3rglRA"><img src="https://s2.ax1x.com/2020/02/28/3rglRA.md.png" alt="3rglRA.md.png"></a></p><p>Training Data的Error=31.9，但我们真正关心的是Testing Data的error。</p><p>Testing Data 是new Data：另外的Pokemon！。</p><h3 id="Testing-Data"><a href="#Testing-Data" class="headerlink" title="Testing Data"></a>Testing Data</h3><p><strong>Model 1</strong>： $y = b+w\cdot x_{cp}$</p><p>error = 35,比Training Data error更大。</p><p><a href="https://imgchr.com/i/3rglRA"><img src="https://s2.ax1x.com/2020/02/28/3rglRA.md.png" alt="3rglRA.md.png"></a></p><p><strong>Model 2</strong>：$y = b+w_1\cdot x_{cp}+w_2\cdot (x_{cp})^2$</p><p>Testing error=18.4，比Model 1 好。</p><p><a href="https://imgchr.com/i/3rgMPH"><img src="https://s2.ax1x.com/2020/02/28/3rgMPH.md.png" alt="3rgMPH.md.png"></a></p><p><strong>Model 3</strong>：$y = b+w_1\cdot x_{cp}+w_2\cdot (x_{cp})^2+w_3\cdot(x_{cp})^3$</p><p>Testing error=18.1，比Model 2好。</p><p><a href="https://imgchr.com/i/3rgVr6"><img src="https://s2.ax1x.com/2020/02/28/3rgVr6.md.png" alt="3rgVr6.md.png"></a></p><p><strong>Model 4</strong>:$y = b+w_1\cdot x_{cp}+w_2\cdot (x_{cp})^2+w_3\cdot(x_{cp})^3+w_4 \cdot (x_{cp})^4$</p><p>Testing error =28.8,比Model3更差。</p><p><a href="https://imgchr.com/i/3rgZqK"><img src="https://s2.ax1x.com/2020/02/28/3rgZqK.md.png" alt="3rgZqK.md.png"></a></p><p><strong>Model 5</strong>：$y = b+w_1\cdot x_{cp}+w_2\cdot (x_{cp})^2+w_3\cdot(x_{cp})^3+w_4 \cdot (x_{cp})^4+w_5\cdot (x_{cp})^5$</p><p>Testing error = 232.1,爆炸了一样的差。</p><p><a href="https://imgchr.com/i/3rgmVO"><img src="https://s2.ax1x.com/2020/02/28/3rgmVO.md.png" alt="3rgmVO.md.png"></a></p><h3 id="Overfiting（过拟合了）"><a href="#Overfiting（过拟合了）" class="headerlink" title="Overfiting（过拟合了）"></a>Overfiting（过拟合了）</h3><p>从上面5个Model中可以得出，越复杂的函数模型，在Testing data上不一定能得到更好的结果。（过拟合使Training data 的误差越来越小）</p><p>所以在选择Model时，需要选择合适的Model。</p><h2 id="对模型进行改进"><a href="#对模型进行改进" class="headerlink" title="对模型进行改进"></a>对模型进行改进</h2><p>如果收集更多的Training Data，可以发现他好像不是一个Linear Model。</p><p><a href="https://imgchr.com/i/3rguIe"><img src="https://s2.ax1x.com/2020/02/28/3rguIe.md.png" alt="3rguIe.md.png"></a></p><h3 id="Back-to-step-1-Redesigh-the-Model"><a href="#Back-to-step-1-Redesigh-the-Model" class="headerlink" title="Back  to step 1:Redesigh the Model"></a>Back  to step 1:Redesigh the Model</h3><p>从上面那张图，感觉他不是一个Linear Model,而是需要if 是伊布，模型是…，if 是…,可见是和物种有关系。</p><p>（很抱歉，我只认识右上角时伊布，QAQ，我也说不出名字）</p><p>但用 $\delta$(微积分学的狄拉克函数)表示条件语句，可以发现，他仍然是一个线性模型。</p><blockquote> $\delta(x_s= \text{Pidgey)}\left\{\begin{array}{ll}=1 & \text { If } x_{s}=\text { Pidgey } \\ =0 & \text { otherwise }\end{array}\right.$ </blockquote><p>$y = b_1\cdot \delta_1+w_1\cdot \delta_1+b2\cdot \delta_2+w_2\cdot \delta_2+…$是一个linear model。</p><p>拟合出来，Training Data 和Testing Data的error都蛮小的。</p><p>如果想让拟合误差更小，还可以考虑其他的feature，重量、高度、HP等。</p><p>但同样的，如果函数过于复杂，也会出现Overfitting的情况。</p><h3 id="Back-to-Step-2-Regularization（正则化）"><a href="#Back-to-Step-2-Regularization（正则化）" class="headerlink" title="Back to Step 2:Regularization（正则化）"></a>Back to Step 2:Regularization（正则化）</h3><p>对于Linear Model :$y = b+\sum w_i x_i$</p><h4 id="为什么要正则化？"><a href="#为什么要正则化？" class="headerlink" title="为什么要正则化？"></a>为什么要正则化？</h4><p>我们希望得到的函数是较平滑的，这样测试时，函数的输出对输入的noise不sensitive，即输入x的细微扰动，并不太会影响输出的结果。</p><p>所以当参数越接近0，函数越平滑。因此在原本的loss function后加入 $\lambda \sum(w_i)^2$项（ $\lambda$需手调），可以保证函数较平滑。</p><p><strong>正则化</strong>： $L = \sum_n(\hat{y}^n-(b+\sum w_i x_i))^2 + \lambda\sum(w_i)^2$</p><h4 id="lambda-大小的选择"><a href="#lambda-大小的选择" class="headerlink" title="$\lambda $大小的选择"></a>$\lambda $大小的选择</h4><p><a href="https://imgchr.com/i/3rgnaD"><img src="https://s2.ax1x.com/2020/02/28/3rgnaD.md.png" alt="3rgnaD.md.png"></a></p><p>可以得出结论：</p><ul><li><p>$\lambda $越大，Training Error变大了。</p><p>当 $\lambda$更大，损失函数更考虑w参数的取值，更关心函数的平滑程度，而更少的关心拟合的error。</p></li><li><p>$\lambda $越大，Testing Error变小了，当 $\lambda$过大时，又变大。</p><p>$\lambda $较小时，$\lambda $增大，函数更平滑，能良好适应数据的扰动。</p><p>$\lambda $较大时，函数过于平滑，宛如直线，这显然不能准确预测。</p></li></ul><p>因此，在调节$\lambda $大小时，也要适当选择。</p><h4 id="正则化的一个注意点"><a href="#正则化的一个注意点" class="headerlink" title="正则化的一个注意点"></a>正则化的一个注意点</h4><p>在regularization中，我们只考虑了w参数，<strong>没有考虑bias偏置值参数</strong>。</p><p>因为正则化是寻找较平滑拟合，而偏置参数只是让函数平移，与平滑无关。</p><p><strong>Again：Regularization不考虑bias</strong></p><h1 id="Fllowing"><a href="#Fllowing" class="headerlink" title="Fllowing"></a>Fllowing</h1><ul><li>Gradient descent[2]</li><li>Overfitting and regularization[3]</li><li>Validation[4]</li></ul><p>由于博主也是在学习阶段，学习后，会po上下面内容的链接。</p><p>希望能在学习、写博客的过程中，锻炼自己的表达能力，尽量让文风言简意赅又科学严谨。</p><p>写博客是为了记录与分享，感谢指正。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] “周志华西瓜书p55,待补充”</p><p>[2]</p><p>[3]</p><p>[4] </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 在&lt;a href=&quot;https://www.youtube.com/watch?v=fegAeph9UaA&quot;&gt;YouTube上看台大李宏毅老师的课&lt;/a&gt;，看完Regression讲座的感受就是： 好想去抓Pokemon！！！&lt;br&gt; 这篇文章将总结李宏毅老师Regression的讲座，并尝试实现其demo。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f1ed.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f1ed.github.io/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f1ed.github.io/tags/open-classes/"/>
    
      <category term="Regression" scheme="https://f1ed.github.io/tags/Regression/"/>
    
  </entry>
  
</feed>
