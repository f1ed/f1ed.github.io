<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>fred&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://f7ed.com/"/>
  <updated>2020-10-25T01:19:40.815Z</updated>
  <id>https://f7ed.com/</id>
  
  <author>
    <name>f1ed</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>「PyTorch」：Tensors Explained And Operations</title>
    <link href="https://f7ed.com/2020/10/21/Tensors/"/>
    <id>https://f7ed.com/2020/10/21/Tensors/</id>
    <published>2020-10-20T16:00:00.000Z</published>
    <updated>2020-10-25T01:19:40.815Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch框架学习。</p><p>本篇文章主要介绍PyTorch中的Tensor及其基本操作，主要分为四个方面：Reshape, Element-wise, Reduction和Access。</p><p>Tensor的具体操作介绍，建议配合Colab笔记使用：</p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/PyTorch%20Tensors%20Explained.ipynb">PyTorch Tensors Explained</a> </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> </p><p> <a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operations: Element-wise</a>  </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a>  </p><p>英文的表达解释都是比较清晰且精确的，所以以英语的形式作为主要记录，文中会夹带一些中文总结语句，方便阅读。</p><a id="more"></a><h1 id="Introducing-Tensors"><a href="#Introducing-Tensors" class="headerlink" title="Introducing Tensors"></a>Introducing Tensors</h1><h2 id="Tensor-Explained-Data-Structures-of-Deep-Learning"><a href="#Tensor-Explained-Data-Structures-of-Deep-Learning" class="headerlink" title="Tensor Explained - Data Structures of Deep Learning"></a>Tensor Explained - Data Structures of Deep Learning</h2><h3 id="What-Is-A-Tensor"><a href="#What-Is-A-Tensor" class="headerlink" title="What Is A Tensor?"></a>What Is A Tensor?</h3><p>A tensor is the primary data structure used by neural networks.</p><p>【Tensor是NN中最主要的数据结构】</p><h4 id="Indexes-Required-To-Access-An-Element"><a href="#Indexes-Required-To-Access-An-Element" class="headerlink" title="Indexes Required To Access An Element"></a>Indexes Required To Access An Element</h4><p>The relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure.</p><p>【以下pairs都是需要同等数量的indexes才能确定特定的元素。】</p><p>【而tensor是generalizations，是一种统一而普遍的定义。】</p><table><thead><tr><th>Indexes required</th><th>Computer science</th><th>Mathematics</th></tr></thead><tbody><tr><td>0</td><td>number</td><td>scalar</td></tr><tr><td>1</td><td>array</td><td>vector</td></tr><tr><td>2</td><td>2d-array</td><td>matrix</td></tr></tbody></table><h3 id="Tensors-Are-Generalizations"><a href="#Tensors-Are-Generalizations" class="headerlink" title="Tensors Are Generalizations"></a>Tensors Are Generalizations</h3><p>When more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language.</p><h4 id="Mathematics"><a href="#Mathematics" class="headerlink" title="Mathematics"></a>Mathematics</h4><p>In mathematics, we stop using words like scalar, vector, and matrix, and we start using the word <em>tensor</em> or nd-tensor. The n tells us the number of indexes required to access a specific element within the structure.</p><p>【数学中，当我们需要用大于两个的indexes才能确定特点元素时，我们使用tensor或者nd-tensor来表示该数据结构，说明需要n个index才能确定该数据结构中的特定元素。】</p><h4 id="Computer-Science"><a href="#Computer-Science" class="headerlink" title="Computer Science"></a>Computer Science</h4><p>In computer science, we stop using words like, number, array, 2d-array, and start using the word <em>multidimensional array</em> or nd-array. The <code>n</code> tells us the number of indexes required to access a specific element within the structure.</p><p>【计算机科学中，我们使用nd-array来表示，因此，nd-array和tensor实则是一个东西。】</p><table><thead><tr><th>Indexes required</th><th>Computer science</th><th>Mathematics</th></tr></thead><tbody><tr><td>n</td><td>nd-array</td><td>nd-tensor</td></tr></tbody></table><p>Tensors and nd-arrays are the same thing!</p><p>One thing to note about the dimension of a tensor is that it differs from what we mean when we refer to the dimension of a vector in a vector space. The dimension of a tensor does not tell us how many components exist within the tensor.</p><p>【需要注意的地方是，tensor中的维度和vector向量空间中的维度不是同一个东西，vector向量空间中的维度表示该vector有多少个元素组成的，而tensor中的维度是下文中rank的含义。】</p><h2 id="Rank-Axes-And-Shape-Explained"><a href="#Rank-Axes-And-Shape-Explained" class="headerlink" title="Rank, Axes, And Shape Explained"></a>Rank, Axes, And Shape Explained</h2><p>【下文会详细解释深度学习tensor的几个重要性质：Rank, Axes, Shape.】</p><p>The concepts of rank, axes, and shape are the tensor attributes that will concern us most in deep learning.</p><ul><li>Rank</li><li>Axes</li><li>Shape</li></ul><h3 id="Rank-And-Indexes"><a href="#Rank-And-Indexes" class="headerlink" title="Rank And Indexes"></a>Rank And Indexes</h3><p>We are introducing the word <em>rank</em> here because it is commonly used in deep learning when referring to the number of dimensions present within a given tensor. </p><p>The rank of a tensor tells us how many indexes are required to access (refer to) a specific data element contained within the tensor data structure.</p><p>A tensor’s rank tells us how many indexes are needed to refer to a specific element within the tensor.</p><p>【这里的rank实则就是tensor的维度。】</p><p>【tensor的rank值告诉我们需要多少个indexes才能确定该tensor中的特定元素。】</p><h3 id="Axes-Of-A-Tensor"><a href="#Axes-Of-A-Tensor" class="headerlink" title="Axes Of A Tensor"></a>Axes Of A Tensor</h3><p>If we have a tensor, and we want to refer to a specific <em>dimension</em>, we use the word <em>axis</em> in deep learning.</p><p>An axis of a tensor is a specific dimension of a tensor.</p><p>Elements are said to exist or run along an axis. This <em>running</em> is constrained by the length of each axis. Let’s look at the length of an axis now.</p><h4 id="Length-Of-An-Axis"><a href="#Length-Of-An-Axis" class="headerlink" title="Length Of An Axis"></a>Length Of An Axis</h4><p>The length of each axis tells us how many indexes are available along each axis.</p><p>【当我们关注tensor的某一具体维度时，在深度学习中我们使用axis来表达。】</p><p>【元素被认为是在某一axie上存在或延伸的，元素延伸的长度取决于axis的长度。】</p><p>【Axis的长度表示在每一维度（axis）上有多少个索引】</p><h3 id="Shape-Of-A-Tensor"><a href="#Shape-Of-A-Tensor" class="headerlink" title="Shape Of A Tensor"></a>Shape Of A Tensor</h3><p>The <em>shape</em> of a tensor is determined by the length of each axis, so if we know the shape of a given tensor, then we know the length of each axis, and this tells us how many indexes are available along each axis.</p><p>The shape of a tensor gives us the length of each axis of the tensor.</p><p>【tensor的shape由每一axis的长度决定，即每一axis的索引数目】</p><p>Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called <em>reshaping</em>.</p><p>Reshaping changes the shape but not the underlying data elements.</p><p>【tensor的常见操作reshape只改变tensor的shape，而不改变底层的数据。】</p><h2 id="CNN-Tensors-Shape-Explained"><a href="#CNN-Tensors-Shape-Explained" class="headerlink" title="CNN Tensors Shape Explained"></a>CNN Tensors Shape Explained</h2><p>CNN的相关介绍，可见 <a href="/2020/04/25/CNN/" title="这篇文章">这篇文章</a></p><p>What I want to do now is put the concepts of rank, axes, and shape to use with a practical example. To do this, we’ll consider an image input as a tensor to a CNN.</p><p>Remember that the shape of a tensor encodes all the relevant information about a tensor’s axes, rank, and indexes, so we’ll consider the shape in our example, and this will enable us to work out the other values. </p><p>【tensor的shape能体现tensor的axes、rank、index所有信息】</p><p>【以CNN为例来说明rank, axes, shape.】</p><h3 id="Shape-Of-A-CNN-Input"><a href="#Shape-Of-A-CNN-Input" class="headerlink" title="Shape Of A CNN Input"></a>Shape Of A CNN Input</h3><p>The shape of a CNN input typically has a length of four. This means that we have a rank-4 tensor with four axes. Each index in the tensor’s shape represents a specific axis, and the value at each index gives us the length of the corresponding axis.</p><p>【CNN的input 是一个rank4-tensor.】</p><p>Each axis of a tensor usually represents some type of real world or logical feature of the input data. If we understand each of these features and their axis location within the tensor, then we can have a pretty good understanding of the tensor data structure overall.</p><p>【tensor的每个axis往往代表着某一个逻辑feature，所以理解features和tensor中axis的位置的关系能帮助我们更好的理解tensor。】</p><h4 id="Image-Height-And-Width"><a href="#Image-Height-And-Width" class="headerlink" title="Image Height And Width"></a>Image Height And Width</h4><p>To represent two dimensions, we need two axes.</p><p>The image height and width are represented on the last two axes.</p><p>【表示图像的height和width，需要2个axes，使用最后两个axes表示。】</p><h4 id="Image-Color-Channels"><a href="#Image-Color-Channels" class="headerlink" title="Image Color Channels"></a>Image Color Channels</h4><p>The next axis represents the color channels. Typical values here are <code>3</code> for RGB images or <code>1</code> if we are working with grayscale images. This color channel interpretation only applies to the input tensor.</p><p>【下一个axis(从右至左)表示图像的color channels（颜色通道，如灰度图像就有1个颜色通道，RGB图像有三个）。】</p><p>【注意：color channel的说法只适用于input tensor。】</p><h4 id="Image-Batches"><a href="#Image-Batches" class="headerlink" title="Image Batches"></a>Image Batches</h4><p>This brings us to the first axis of the four which represents the batch size. In neural networks, we usually work with batches of samples opposed to single samples, so the length of this axis tells us how many samples are in our batch.</p><p>Suppose we have the following shape <code>[3, 1, 28, 28]</code> for a given tensor. Using the shape, we can determine that we have a batch of three images.</p><p>【第一个axis表示batch属性，表明该batch的size。在深度学习中，我们通常使用一批样本，而不是一个单独的样本，所以这一维度表明了我们的batch中有多少样本。】</p><p>tensor：[Batch, Channels, Height, Width]</p><p>Each image has a single color channel, and the image height and width are <code>28 x 28</code> respectively.</p><ol><li>Batch size</li><li>Color channels</li><li>Height</li><li>Width</li></ol><h4 id="NCHW-vs-NHWC-vs-CHWN"><a href="#NCHW-vs-NHWC-vs-CHWN" class="headerlink" title="NCHW vs NHWC vs CHWN"></a>NCHW vs NHWC vs CHWN</h4><p>It’s common when reading API documentation and academic papers to see the <code>B</code> replaced by an <code>N</code>. The <code>N</code> standing for <em>number of samples</em> in a batch.</p><p>【在API文档或学术论文中，N经常会代替代替B，表示the number of samples in a batch。】</p><p>Furthermore, another difference we often encounter in the wild is a <em>reordering</em> of the dimensions. Common orderings are as follows:</p><ul><li><code>NCHW</code></li><li><code>NHWC</code></li><li><code>CHWN</code></li></ul><p>【除此之外，也会经常遇到这些axes的其他顺序。】</p><p>As we have seen, PyTorch uses <code>NCHW</code>, and it is the case that TensorFlow and Keras use <code>NHWC</code> by default (it can be configured). Ultimately, the choice of which one to use depends mainly on performance. Some libraries and algorithms are more suited to one or the other of these orderings.</p><p>【PyTorch 默认使用NCHW，而TensorFlow和Keras使用NHWC】</p><h3 id="Output-Channels-And-Feature-Maps"><a href="#Output-Channels-And-Feature-Maps" class="headerlink" title="Output Channels And Feature Maps"></a>Output Channels And Feature Maps</h3><p>Let’s look at how the interpretation of the color channel axis changes after the tensor is transformed by a convolutional layer.</p><p>Suppose we have three convolutional filters, and lets just see what happens to the channel axis.</p><p>Since we have three convolutional filters, we will have three channel outputs from the convolutional layer. These channels are outputs from the convolutional layer, hence the name output <em>channels opposed</em> to <em>color channels</em>.</p><p>【tensor送入convolutional layer（卷积层）后，color channel 这一axis的长度发生变化。</p><p>【在中解释到，有几个convolutional filters，卷积层输出的tensor就有几个channel（channel代替color channel的表达）。】</p><h4 id="Feature-Maps"><a href="#Feature-Maps" class="headerlink" title="Feature Maps"></a>Feature Maps</h4><p>With the output channels, we no longer have color channels, but modified channels that we call <em>feature maps</em>. These so-called feature maps are the outputs of the convolutions that take place using the input color channels and the convolutional filters.</p><p>Feature maps are the output channels created from the convolutions.</p><p>【卷积层输出tensor的channel维度代替color channels的叫法。】</p><p>【卷积层的输出也叫叫feature maps】</p><h1 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch Tensors"></a>PyTorch Tensors</h1><p>When programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form.</p><p>【数据预处理往往是编写NN的第一步，将原始数据转换为tensor form。】</p><p>Tensor的基本操作见Colab运行笔记链接：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/PyTorch%20Tensors%20Explained.ipynb">PyTorch Tensors Explained</a> </p><p>(不会用的也可以直接看<a href="https://github.com/f1ed/PyTorch-Notebook/blob/master/PyTorch%20Tensors%20Explained.ipynb">github</a> 上的)</p><h2 id="PyTorch-Tensors-Attributes"><a href="#PyTorch-Tensors-Attributes" class="headerlink" title="PyTorch Tensors Attributes"></a>PyTorch Tensors Attributes</h2><ul><li><p>torch.dtype：tensor包含数据类型。</p><p>常见数据类型：</p><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td>torch.float32</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64-bit floating point</td><td>torch.float64</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr><tr><td>16-bit floating point</td><td>torch.float16</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr><tr><td>8-bit integer (signed)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer (signed)</td><td>torch.int16</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32-bit integer (signed)</td><td>torch.int32</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64-bit integer (signed)</td><td>torch.int64</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr></tbody></table></li><li><p>torch.device: tensor数据所分配的设备，如CPU，cuda:0</p></li><li><p>torch.layout: tensor在内存中的存储方式。</p></li></ul><p>As neural network programmers, we need to be aware of the following:</p><ol><li>Tensors contain data of a uniform type (<code>dtype</code>).</li><li>Tensor computations between tensors depend on the <code>dtype</code> and the <code>device</code>.</li></ol><p>【Tensors包含相同类型的数据】</p><p>【Tensors之间的计算取决于他的类型和他所分配的设备】</p><h2 id="Creating-Tensors"><a href="#Creating-Tensors" class="headerlink" title="Creating Tensors"></a>Creating Tensors</h2><p>These are the primary ways of creating tensor objects (instances of the <code>torch.Tensor</code> class), with data (array-like) in PyTorch:</p><p>Creating Tensors with data.</p><p>【四种用数据创建tensor的方式】</p><ol><li><code>torch.Tensor(data)</code></li><li><code>torch.tensor(data)</code></li><li><code>torch.as_tensor(data)</code></li><li><code>torch.from_numpy(data)</code> </li></ol><h3 id="torch-Tensor-Vs-torch-tensor"><a href="#torch-Tensor-Vs-torch-tensor" class="headerlink" title="torch.Tensor() Vs torch.tensor()"></a><code>torch.Tensor()</code> Vs <code>torch.tensor()</code></h3><p>The first option with the uppercase <code>T</code> is the constructor of the <code>torch.Tensor</code> class, and the second option is what we call a <em>factory function</em> that constructs <code>torch.Tensor</code> objects and returns them to the caller.</p><p>However, the factory function <code>torch.tensor()</code> has better documentation and more configuration options, so it gets the winning spot at the moment.</p><p>【<code>torch.Tensor(data)</code> 是 <code>torch.Tensor</code> class的Constructor，而<code>torch.tensor(data)</code> 是生成/返回 torch.Tensor class的函数（factory functions)】</p><p>【因为<code>torch.tensor()</code> 有更多的选项设置，比如可以设置数据类型，所以一般用<code>torch.tensor()</code> 来生成。】</p><h3 id="Default-dtype-Vs-Inferred-dtype"><a href="#Default-dtype-Vs-Inferred-dtype" class="headerlink" title="Default dtype Vs Inferred dtype"></a>Default <code>dtype</code> Vs Inferred <code>dtype</code></h3><p>The difference here arises in the fact that the <code>torch.Tensor()</code> constructor uses the default <code>dtype</code> when building the tensor. The other calls choose a dtype based on the incoming data. This is called <em>type inference</em>. The <code>dtype</code> is inferred based on the incoming data.</p><p>【<code>torch.Tensor()</code> 在生成tensor时，使用的是默认<code>dtype=torch.float32</code> ，而其他三种是使用的引用<code>dtype</code> ，即生成tensor的数据类型和输入的数据类型一致。】</p><h3 id="Sharing-Memory-For-Performance-Copy-Vs-Share"><a href="#Sharing-Memory-For-Performance-Copy-Vs-Share" class="headerlink" title="Sharing Memory For Performance: Copy Vs Share"></a>Sharing Memory For Performance: Copy Vs Share</h3><p><code>torch.Tensor()</code> and <code>torch.tensor()</code> <em>copy</em> their input data while <code>torch.as_tensor()</code> and <code>torch.from_numpy()</code> <em>share</em> their input data in memory with the original input object.</p><p>This sharing just means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the <code>torch.Tensor</code> and the <code>numpy.ndarray</code>.</p><p>Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory.</p><p>【<code>torch.Tensor()</code> 和 <code>torch.tensor()</code> 在根据data创建tensor时，在内存中额外复制数据】</p><p>【<code>torch.as_tensor()</code> 和 <code>torch.from_numpy()</code> 在根据data创建tensor时，是和原输入数据共享的内存，即原numpy.ndarry的数据改变，相应的tensor也会改变。】</p><table><thead><tr><th>Share Data</th><th>Copy Data</th></tr></thead><tbody><tr><td>torch.as_tensor()</td><td>torch.tensor()</td></tr><tr><td>torch.from_numpy()</td><td>torch.Tensor()</td></tr></tbody></table><p>Some things to keep in mind about memory sharing (it works where it can):</p><ol><li><p>Since <code>numpy.ndarray</code> objects are allocated on the CPU, the <code>as_tensor()</code> function must copy the data from the CPU to the GPU when a GPU is being used.</p><p>【在使用GPU时， <code>as_tensor()</code> 也会将ndarray数据从CPU复制到GPU上。】</p></li><li><p>The memory sharing of <code>as_tensor()</code> doesn’t work with built-in Python data structures like lists.</p><p>【<code>as_tensor()</code> 在Python内置数据结构时不会共享内存】</p></li><li><p>The <code>as_tensor()</code> performance improvement will be greater if there are a lot of back and forth operations between <code>numpy.ndarray</code> objects and tensor objects. </p><p>【<code>as_tensor()</code> 在ndarry和tensor之间大量连续操作时能有效提高性能】</p></li></ol><h3 id="torch-as-tensor-Vs-torch-from-numpy"><a href="#torch-as-tensor-Vs-torch-from-numpy" class="headerlink" title="torch.as_tensor() Vs torch.from_numpy()"></a><code>torch.as_tensor()</code> Vs <code>torch.from_numpy()</code></h3><p>This establishes that <code>torch.as_tensor()</code> and <code>torch.from_numpy()</code> both share memory with their input data. However, which one should we use, and how are they different?</p><p>The <code>torch.from_numpy()</code> function only accepts <code>numpy.ndarray</code>s, while the <code>torch.as_tensor()</code> function accepts a wide variety of array-like objects, including other PyTorch tensors. </p><p>【这两个都是和输入数据共享内存，但 <code>torch.from_numpy()</code> 只能接受<code>numpy.ndarrays</code> 类型的数据，而<code>torch.as_tensor()</code> 能接受array-like(像list, tuple)等类型，所以一般<code>torch.as_tensor()</code> 更常用。】</p><p>If we have a <code>torch.Tensor</code> and we want to convert it to a <code>numpy.ndarray</code></p><p>【用<code>torch.numpy()</code> 把tensor转换为ndarray】</p><hr><p>Creating Tensors without data.</p><p>【还有几种创建常见tensor的方式】</p><ol><li><code>torch.eyes(n)</code> : 创建2-D tensor，即n*n的单位向量。</li><li><code>torch.zeros(shape)</code> : 创建shape=shape的全0tensor。</li><li><code>torch.ones(shape)</code> : 创建全1tensor。</li><li><code>torch.rand(shape)</code> : 创建随机值tensor。</li></ol><h1 id="Tensor-Operation"><a href="#Tensor-Operation" class="headerlink" title="Tensor Operation"></a>Tensor Operation</h1><p>关于Tensor 操作的Colab运行笔记。对照使用最佳。如果打不开也可以看<a href="https://github.com/f1ed/PyTorch-Notebook">github</a> </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operations: Element-wise</a> </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> </p><p>We have the following high-level categories of operations:</p><ol><li>Reshaping operations</li><li>Element-wise operations</li><li>Reduction operations</li><li>Access operations</li></ol><p>【对tensor的操作主要分为4种：reshape, element-wise, reduction, access】</p><h2 id="Reshape"><a href="#Reshape" class="headerlink" title="Reshape"></a>Reshape</h2><p>As neural network programmers, we have to do the same with our tensors, and usually shaping and reshaping our tensors is a frequent task.</p><p>【reshape在NN编程中是很常见的操作】</p><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">t = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line">t.reshape([<span class="number">2</span>,<span class="number">6</span>])</span><br><span class="line">t.reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>Reshaping changes the tensor’s shape but not the underlying data. Our tensor has <code>12</code> elements, so any reshaping must account for exactly <code>12</code> elements.</p><p>【reshape操作不改变底层的数据，只是改变tensor的shape】</p><p> In PyTorch, the <code>-1</code> tells the <code>reshape()</code> function to figure out what the value should be based on the number of elements contained within the tensor.</p><p>【reshape中传入的-1参数，PyTorch可以自动计算该值，因为PyTorch要保证tensor的元素个数不变】</p><h3 id="Squeezing-And-Unsqueezing"><a href="#Squeezing-And-Unsqueezing" class="headerlink" title="Squeezing And Unsqueezing"></a>Squeezing And Unsqueezing</h3><ul><li><p><em>Squeezing</em> a tensor removes the dimensions or axes that have a length of one.</p><p>【Squeezing操作：移除tensor中axis长度为1的维度】</p></li><li><p><em>Unsqueezing</em> a tensor adds a dimension with a length of one.</p><p>【Unsqueezing操作：增加一个axis长度为1的维度】</p></li></ul><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t.squeeze()</span><br><span class="line">t.squeeze().unsqueeze(dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="Concatenation-Tensors"><a href="#Concatenation-Tensors" class="headerlink" title="Concatenation Tensors"></a>Concatenation Tensors</h3><p>We combine tensors using the <code>cat()</code> function, and the resulting tensor will have a shape that depends on the shape of the two input tensors.</p><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.cat((t1,t2,t3), dim=<span class="number">0</span>)</span><br><span class="line">torch.cat((t1,t2,t3), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h3><p>这里从CNN的例子看Flatten，CNN的相关细节见：<a href="/2020/04/25/CNN/" title="这篇文章">这篇文章</a></p><p>A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input.</p><p>【flatten在卷积层网络很常见，因为输入必须flatten后才能连接到一个全连接网络层】</p><p>对于MNIST数据集中18*18的手写数字，在前文说到CNN的输入是<code>[Batch Size, Channels, Height, Width]</code> ，怎么才能flatten tensor的部分axis，而不是全部维度。</p><p>CNN的输入，需要flatten的axes：(C,H,W)</p><p>从dim1维度开始flatten（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.flatten(start_dim=<span class="number">1</span>, end_dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Broadcasting-and-Element-Wise"><a href="#Broadcasting-and-Element-Wise" class="headerlink" title="Broadcasting and Element-Wise"></a>Broadcasting and Element-Wise</h2><p>An <em>element-wise</em> operation operates on corresponding elements between tensors.</p><p>【element-wise操作两个tensor之间对应的元素。】</p><h3 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h3><p>Broadcasting describes how tensors with different shapes are treated during element-wise operations.</p><p>Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors.</p><p>【broadcast描述了不同shape之间的tensor如何进行element-wise操作】</p><p>【broadcast运行我们增加scalars到高维度】</p><p>Let’s think about the <code>t1 + 2</code> operation. Here, the scaler valued tensor is being broadcasted to the shape of <code>t1</code>, and then, the element-wise operation is carried out.</p><p>【在t1+2时，scalar 2实际是先被broadcast到和t1相同的shape, 再执行element-wise操作】</p><p>We have two tensors with different shapes. The goal of broadcasting is to make the tensors have the same shape so we can perform element-wise operations on them.</p><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operations: Element-wise</a> ）</p><h3 id="Broadcasting-Details"><a href="#Broadcasting-Details" class="headerlink" title="Broadcasting Details"></a>Broadcasting Details</h3><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operations: Element-wise</a> ）</p><ul><li><p>Same Shapes: 直接操作</p></li><li><p>Same Rank, Different Shape:</p><ol><li><p>Determine if tensors are compatible（兼容）.</p><p>【两个tensor兼容，才可以对tensor broadcast，再执行element-wise操作】</p><p>We compare the shapes of the two tensors, starting at their last dimensions and working backwards. Our goal is to determine whether each dimension between the two tensors’ shapes is compatible.</p><p>【从最后一个维度向前判断，每个维度是否兼容】</p><p>【判断该维度兼容的条件是满足下面两个条件其一：维度长度相同；或者其中一个为1】</p><p>The dimensions are compatible when either:</p><ul><li>They’re equal to each other.</li><li>One of them is 1.</li></ul></li><li><p>Determine the shape of the resulting tensor.</p><p>【操作的结果是一个新的tensor，结果tensor的每个维度长度是原tensors在该维度的最大值】</p></li></ol></li><li><p>Different Ranks:</p><ol><li><p>Determine if tensors are compatible.(同上)</p><p>When we’re in a situation where the ranks of the two tensors aren’t the same, like what we have here, then we simply substitute a one in for the missing dimensions of the lower-ranked tensor.</p><p>【对低维度的tensor的缺失维度，用1来代替，比如shape为(1,3) 和 ()，低维度的shape变为(1,1)】</p></li><li><p>Determine the shape of the resulting tensor.</p></li></ol></li></ul><h2 id="ArgMax-and-Reduction"><a href="#ArgMax-and-Reduction" class="headerlink" title="ArgMax and Reduction"></a>ArgMax and Reduction</h2><p>A <em>reduction operation</em> on a tensor is an operation that reduces the number of elements contained within the tensor.</p><p>【reduction 操作是能减少tensor元素数量的操作。】</p><p>Reshaping operations gave us the ability to position our elements along particular axes. Element-wise operations allow us to perform operations on elements between two tensors, and reduction operations allow us to perform operations on elements within a single tensor.</p><p>【Reshape操作让我们能沿着某一axis操纵tensor 中的元素位置；Element-wise操作让我们能对tensors之间对应元素进行操作；Reduction操作能让我们对单个tensor间的元素操作。】</p><p>(具体操作见colab笔记本：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t.sum()</span><br><span class="line">t.prod()</span><br><span class="line">t.mean()</span><br><span class="line">t.std()</span><br></pre></td></tr></table></figure><h3 id="Reducing-Tensors-By-Axes"><a href="#Reducing-Tensors-By-Axes" class="headerlink" title="Reducing Tensors By Axes"></a>Reducing Tensors By Axes</h3><p>只需要对这些方法传一个维度对参数。</p><p>(具体操作见colab笔记本：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t.sum(dim=<span class="number">0</span>)</span><br><span class="line">t.sum(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Argmax"><a href="#Argmax" class="headerlink" title="Argmax"></a>Argmax</h3><p><em>Argmax</em> returns the index location of the maximum value inside a tensor.</p><p>【Argmax返回最大value的index】</p><p>(具体操作见colab笔记本：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.argmax(dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="Aceessing-Elements-Inside-Tensors"><a href="#Aceessing-Elements-Inside-Tensors" class="headerlink" title="Aceessing Elements Inside Tensors"></a>Aceessing Elements Inside Tensors</h2><p>The last type of common operation that we need for tensors is the ability to access data from within the tensor.</p><p>【Access操作能获得tensor中的数据，即将tensor中的数据拿出来放在Python内置的数据结构中】</p><p>(具体操作见colab笔记本：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.mean().item()</span><br><span class="line">t.mean(dim=<span class="number">0</span>).tolist()</span><br><span class="line">t.mean(dim=<span class="number">0</span>).numpy()</span><br></pre></td></tr></table></figure><h3 id="Advanced-Indexing-And-Slicing"><a href="#Advanced-Indexing-And-Slicing" class="headerlink" title="Advanced Indexing And Slicing"></a>Advanced Indexing And Slicing</h3><p>PyTorch Tensor支持大多数NumPy的index和slicing操作。</p><p>坑：<a href="https://numpy.org/doc/stable/reference/arrays.indexing.html">https://numpy.org/doc/stable/reference/arrays.indexing.html</a></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>挖坑：advanced indexing and slicing: <a href="https://numpy.org/doc/stable/reference/arrays.indexing.html">https://numpy.org/doc/stable/reference/arrays.indexing.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch框架学习。&lt;/p&gt;
&lt;p&gt;本篇文章主要介绍PyTorch中的Tensor及其基本操作，主要分为四个方面：Reshape, Element-wise, Reduction和Access。&lt;/p&gt;
&lt;p&gt;Tensor的具体操作介绍，建议配合Colab笔记使用：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/PyTorch%20Tensors%20Explained.ipynb&quot;&gt;PyTorch Tensors Explained&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb&quot;&gt;Tensor Operations: Reshape&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;a href=&quot;https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb&quot;&gt;Tensor Operations: Element-wise&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb&quot;&gt;Tensor Operation: Reduction and Access&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;英文的表达解释都是比较清晰且精确的，所以以英语的形式作为主要记录，文中会夹带一些中文总结语句，方便阅读。&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyTorch" scheme="https://f7ed.com/categories/PyTorch/"/>
    
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="DEEPLIZARD" scheme="https://f7ed.com/tags/DEEPLIZARD/"/>
    
      <category term="PyTorch" scheme="https://f7ed.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:Unsupervised Learning：Word Embedding</title>
    <link href="https://f7ed.com/2020/10/11/unsupervised-learning-word-embedding/"/>
    <id>https://f7ed.com/2020/10/11/unsupervised-learning-word-embedding/</id>
    <published>2020-10-10T16:00:00.000Z</published>
    <updated>2020-10-11T15:33:36.817Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要是介绍一种无监督学习——Word Embedding（词嵌入）。</p><p>文章开篇介绍了word编码的1-of-N encoding方式和word class方式，但这两种方式得到的单词向量表示都不能很好表达单词的语义和单词之间的语义联系。</p><p>Word Embedding可以很好的解决这个问题。</p><p>Word Embedding有count based和prediction based两种方法。文章主要介绍了prediction based的方法，包括如何predict the word vector? 为什么这样的模型works？介绍了prediction based的变体；详细阐述了该模型中sharing parameters的做法和其必要性。</p><p>文章最后简单列举了word embedding的相关应用，包括multi-lingual embedding, multi-domain embedding, document embedding 等。</p><a id="more"></a><h1 id="Word-to-Vector"><a href="#Word-to-Vector" class="headerlink" title="Word to Vector"></a>Word to Vector</h1><p>如何把word转换为vector?</p><h2 id="1-of-N-Encoding"><a href="#1-of-N-Encoding" class="headerlink" title="1-of-N Encoding"></a>1-of-N Encoding</h2><p>第一种方法是1-of-N Encoding：</p><p>Vector的维度是单词总数，每一维度都代表一个单词。</p><img src="https://s1.ax1x.com/2020/10/11/0gLLGT.png" alt="0gLLGT.png" style="zoom:25%;" /> <p>1-of-N Encoding的方法简单，但这种向量的表示方式not imformative，即向量表示不能体现单词之间的语义关系。</p><h2 id="Word-Class"><a href="#Word-Class" class="headerlink" title="Word Class"></a>Word Class</h2><p>对1-of-N Encoding方式改进，Word Class采用聚类cluster的方式，根据类别训练一个分类器。</p><img src="https://s1.ax1x.com/2020/10/11/0gLqiV.png" alt="0gLdqiV.png" style="zoom:33%;" /> <p>但这种人为分类的方式，信息是会部分丢失的，即光做clustering是不够的，会丢失单词的部分信息。</p><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>第三种方式是Word Embedding。（词嵌入）</p><p>Word Embedding: Machine learns the meaning of words from reading a lot of documents without supervision.</p><p>Word Embedding，机器通过阅读大量文章学习单词的含义，用vector的形式表示单词的语义。训练时只需要给机器大量文章，不需要label，因此是无监督学习。</p><h1 id="Word-Embedding-1"><a href="#Word-Embedding-1" class="headerlink" title="Word Embedding"></a>Word Embedding</h1><p>如何做Word Embedding呢？</p><h2 id="auto-encoder？"><a href="#auto-encoder？" class="headerlink" title="auto-encoder？"></a>auto-encoder？</h2><p>能否用auto-encoder的方式来做词嵌入呢？</p><p>即用1-of-N encoding的方式对单词编码，作为训练的输入和输出。</p><p>word2vec时，把model中的某一hidden layer的输出作为该单词的向量表示。</p><p>这种方式是不可以的，不可以用auto-encoder。因为auto-encoder不能学到informative的信息，即用auto-encoder表示的向量不能表达word的语义。</p><h2 id="Exploit-the-Context"><a href="#Exploit-the-Context" class="headerlink" title="Exploit the Context"></a>Exploit the Context</h2><p>A word can be understood by its context.</p><p>所以Word Embedding可以利用上下文来学习word的语义。</p><p>如何利用单词的上下文来学习呢？</p><ul><li><p>Count based</p><p>如果两个单词 $w_i$ 和 $w_j$ 在文章中经常同时出现，那么 $V(w_i)$ ( $w_i$ 的向量表示)和 $V(w_j)$ 的向量表示会很close.</p><p>E.g. Glove Vector: <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></p><p>GloVe的表示法有两个亮点：</p><ol><li><p>Nearest neighbors：vectors之间的欧几里得距离（或者余弦相似度）能较好表示words之间的语义相似度。</p></li><li><p>Linear substructures：用GloVe方法表示的vectors有有趣的线性子结构。</p><img src="https://s1.ax1x.com/2020/10/11/0gLfxg.png" alt="0gLfxg.png" style="zoom:%;" />  </li></ol></li><li><p>Prediction based</p><p>使用预测的方式来表示。</p></li></ul><h2 id="Prediction-based"><a href="#Prediction-based" class="headerlink" title="Prediction based"></a>Prediction based</h2><h3 id="How-to-predict？"><a href="#How-to-predict？" class="headerlink" title="How to predict？"></a>How to predict？</h3><p>prediction based的方法是用前一个单词来预测当前单词。</p><img src="https://s1.ax1x.com/2020/10/11/0gLHI0.png" alt="0gLHI0.png" style="zoom:25%;" /> <img src="https://s1.ax1x.com/2020/10/11/0gLORU.png" alt="0gLORU.png" style="zoom:33%;" /> <p>训练时： $w_{i-1}$ 的1-of-N encoding编码作为输入，$w_i$ 的1-of-N encoding的编码作为输出。</p><p>NN如上图，$w_{i-1}$ 的1-of-N encoding编码作为输入，输出的vector表示下一个单词是 $w_i$ 的概率。</p><p>word2vec : $w_{i-1}$ 的1-of-N encoding编码作为NN的输入，$w_i$ 的向量表示为第一个hidden layer的neurons的输入 $z$ 。</p><h3 id="Why-it-works"><a href="#Why-it-works" class="headerlink" title="Why it works?"></a>Why it works?</h3><p>直觉的解释他为什么能work。</p><img src="https://s1.ax1x.com/2020/10/11/0gL7aq.png" alt="0gdL7aq.png" style="zoom:33%;" /> <p>如上图，蔡英文 宣誓就职 和 马英九 宣誓就职，虽然 $w_{i-1}$ 不同，但NN的输出中，“宣誓就职”的概率应该最大。</p><p>即hidden layers必须把不同的 $w_{i-1}$ project到相同的space，要求hidden layer的input是相近的，NN的输出才是相近的。</p><h3 id="Prediction-based-：Various-Architecture"><a href="#Prediction-based-：Various-Architecture" class="headerlink" title="Prediction-based ：Various Architecture"></a>Prediction-based ：Various Architecture</h3><p>因为一个单词的下一个单词范围非常大，所以使用前一个单词预测当前单词的方法，performance是较差的。</p><p>因此常常会使用多个单词来预测下一个单词，NN的输入是多个单词连接在一起组成的向量，一般NN的输入至少为10个单词，word embedding的performance较好。</p><p>除了使用多个单词的方法，prediction-based的方法还用两种变体结构。</p><ul><li><p>Continuous bag of word (CBOW) model: predicting the word given its context.</p><p>使用单词的前后文（前一个单词和后一个单词）来预测当前单词。</p><img src="https://s1.ax1x.com/2020/10/11/0gL5rj.png" alt="0gL5drj.png" style="zoom:25%;" /> </li><li><p>Skip-gram: predicting the context given a word.</p><p>使用中间单词来预测单词的前一个单词和后一个单词。</p><img src="https://s1.ax1x.com/2020/10/11/0gLIqs.png" alt="0gLdIqs.png" style="zoom:25%;" />  </li></ul><h3 id="Sharing-Parameters"><a href="#Sharing-Parameters" class="headerlink" title="Sharing Parameters"></a>Sharing Parameters</h3><p>使用多个单词作为NN的输入，提高了word embedding的performance，但也大幅增加了模型训练的参数数量。</p><p>使用sharing parameters（共享参数）能大量减少模型的参数数量。</p><img src="https://s1.ax1x.com/2020/10/11/0gOfFx.png" alt="0gOfFix.png" style="zoom:33%;" /> <p>如上图，输入单词连接到neurons的权重应该是相同的。</p><p>除了能减少参数，sharing parameters也是必要的。否则，如果NN的输入的单词顺序交换，那么得到的单词向量是不同的。</p><p><strong>How to train sharing parameters?</strong> </p><p>假设两个单词相同维度连接到neuron的weight是 $w_i,w_j$ ，在训练中，如何让 $w_i=w_j$ ?</p><ol><li><p>Given the same initialization.(相同的初始化)</p></li><li><p>原来的参数更新：<br>$$<br>w_i \longleftarrow w_i - \frac{\partial C}{\partial w_i} \<br>w_j \longleftarrow w_j - \frac{\partial C}{\partial w_j}<br>$$<br>虽然有相同的初始化，但在Backpropagation求偏微分时，$\frac{\partial C}{\partial w_i}$ 和 $\frac{\partial C}{\partial w_j}$ 不一样，那么参数 $w_i$ 和 $w_j$ 更新一次后就不同了。</p><p>在训练sharing parameters的参数更新：<br>$$<br>w_i \longleftarrow w_i - \frac{\partial C}{\partial w_i} -\frac{\partial C}{\partial w_j}\<br>w_j \longleftarrow w_j - \frac{\partial C}{\partial w_j}-\frac{\partial C}{\partial w_i}<br>$$<br>这样更新后，$w_i$ 和 $w_j$ 仍保持一致。如果有多个单词，亦然。</p></li></ol><hr><p><strong>Word2Vec</strong> </p><p>在word2vec时，根据sharing parameters的性质，计算单词的向量表示时，可以简化运算。</p><img src="https://s1.ax1x.com/2020/10/11/0gLWRS.png" alt="0gLWRS.png" style="zoom:33%;" /> <p>如上图，用前文单词 $x_{i-1},x_{i-2}$  表示单词 $x_i$ 的向量表示 $z=W_1x_{i-2}+W_2x_{i-1}=W(x_{i-2}+x_{i-1})$ .</p><p>其中  $x_{i-1},x_{i-2}$ 的维度是|V|，$x_i$ 的向量表示 $z$ 的维度是 |Z|，$W_1=W_2=W$ 的维度为|Z|*|V|。</p><h1 id="Advantages-of-Word-Embedding"><a href="#Advantages-of-Word-Embedding" class="headerlink" title="Advantages of Word Embedding"></a>Advantages of Word Embedding</h1><p>Word Embedding能得到一些有趣的特性。</p><ul><li><p>向量之间有趣的线性子结构</p><img src="https://s1.ax1x.com/2020/10/11/0gL2Pf.png" alt="0gL2Pf.png" style="zoom:40%;" /> </li><li><p>相近的向量有相近的语义</p><img src="https://s1.ax1x.com/2020/10/11/0gLTZn.png" alt="0gLTZn.png" style="zoom:40%;" /> </li><li><p>向量之间表示的语义特性</p><img src="https://s1.ax1x.com/2020/10/11/0gLRG8.png" alt="0gLRG8.png" style="zoom:35%;" />  </li></ul><h2 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h2><h3 id="Multi-lingual-Embedding：实现翻译"><a href="#Multi-lingual-Embedding：实现翻译" class="headerlink" title="Multi-lingual Embedding：实现翻译"></a>Multi-lingual Embedding：实现翻译</h3><img src="https://s1.ax1x.com/2020/10/11/0gORT1.png" alt="0gORdT1.png" style="zoom:40%;" />  <p>不同语言之间分开训练，训练出的不同语言所对应词汇的向量表示肯定不同，再将对应词汇的向量project到同一点，即实现了翻译。</p><h3 id="Multi-domain-Embedding"><a href="#Multi-domain-Embedding" class="headerlink" title="Multi-domain Embedding"></a>Multi-domain Embedding</h3><p>还可以做影像嵌入。</p><img src="https://s1.ax1x.com/2020/10/11/0gLcIP.png" alt="0gLcIP.png" style="zoom:25%;" /> <h3 id="Document-Embedding：将文件表示为一个向量"><a href="#Document-Embedding：将文件表示为一个向量" class="headerlink" title="Document Embedding：将文件表示为一个向量"></a>Document Embedding：将文件表示为一个向量</h3><ul><li><p>Bag of Word:</p><p>用Bag-of-word的方式编码文件，再实现semantic embedding。得到的文件表示向量可以表示文件的语义主题。</p><img src="https://s1.ax1x.com/2020/10/11/0gLyVI.png" alt="0gLyVI.png" style="zoom:33%;" /> </li><li><p>Beyond Bag of Word:</p><p>句子中单词的顺序也很大程度影响句子的语义。</p><p>因此，下图的两句话有相同的bag-of-word，但表达的含义完全相反。</p><img src="https://s1.ax1x.com/2020/10/11/0gLrqA.png" alt="0gLrqA.png" style="zoom:25%;" /> <p>关于beyond bag of word的相关工作参考reference 2.</p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a> </p></li><li><p>beyond bag of word:</p><img src="https://s1.ax1x.com/2020/10/11/0gL4MQ.png" alt="0gL4MQ.png" style="zoom:33%;"  />    </li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章主要是介绍一种无监督学习——Word Embedding（词嵌入）。&lt;/p&gt;
&lt;p&gt;文章开篇介绍了word编码的1-of-N encoding方式和word class方式，但这两种方式得到的单词向量表示都不能很好表达单词的语义和单词之间的语义联系。&lt;/p&gt;
&lt;p&gt;Word Embedding可以很好的解决这个问题。&lt;/p&gt;
&lt;p&gt;Word Embedding有count based和prediction based两种方法。文章主要介绍了prediction based的方法，包括如何predict the word vector? 为什么这样的模型works？介绍了prediction based的变体；详细阐述了该模型中sharing parameters的做法和其必要性。&lt;/p&gt;
&lt;p&gt;文章最后简单列举了word embedding的相关应用，包括multi-lingual embedding, multi-domain embedding, document embedding 等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="Unsupervised-learning" scheme="https://f7ed.com/tags/Unsupervised-learning/"/>
    
      <category term="Word Embedding" scheme="https://f7ed.com/tags/Word-Embedding/"/>
    
  </entry>
  
  <entry>
    <title>「LeetCode」：Math</title>
    <link href="https://f7ed.com/2020/10/10/Leetcode-math/"/>
    <id>https://f7ed.com/2020/10/10/Leetcode-math/</id>
    <published>2020-10-09T16:00:00.000Z</published>
    <updated>2020-10-10T10:12:05.425Z</updated>
    
    <content type="html"><![CDATA[<p>LeetCode Math 专题记录。</p><p>10月初。</p><p>Albert Einstein:</p><p> “I believe that not everything that can be counted counts, and not everything that counts can be counted”</p><p>「并非所有重要的东西都是可以被计算的，也并不是所有能被计算的东西都那么重要。」</p><a id="more"></a><h2 id="7-Reverse-Integer-E"><a href="#7-Reverse-Integer-E" class="headerlink" title="7. Reverse Integer[E]"></a>7. Reverse Integer[E]</h2><p><a href="https://leetcode.com/problems/reverse-integer/">7. Reverse Integer[E]</a> </p><p>Problem:</p><p>反转32bits的有符号数字，如果反转后会溢出则返回0.</p><p>Solution：</p><p>直观的解决它，先算出反转后的数字，用比较大小来看是否溢出。（最开始还想着转换为bit串来看，就复杂了）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverse</span><span class="params">(self, x: int)</span> -&gt; int:</span></span><br><span class="line">        n_min = -(<span class="number">2</span> ** <span class="number">31</span>)</span><br><span class="line">        n_max = <span class="number">2</span> ** <span class="number">31</span> - <span class="number">1</span></span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        flag = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">            x = -x</span><br><span class="line">        <span class="keyword">while</span> x != <span class="number">0</span>:</span><br><span class="line">            r = x % <span class="number">10</span></span><br><span class="line">            s = s * <span class="number">10</span> + r</span><br><span class="line">            x //=<span class="number">10</span></span><br><span class="line">        <span class="keyword">if</span> flag <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">            s = -s</span><br><span class="line">        <span class="keyword">if</span> s &lt; n_min <span class="keyword">or</span> s &gt; n_max:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><hr><ul><li><p>十进制转换为二进制、八进制、十六进制：</p><ul><li>二进制：<code>bin(a)</code>  ,也可以直接赋二进制的值<code>0b10101</code> </li><li>八进制：<code>oct(a)</code> ，赋值八进制的值<code>0o263361</code> </li><li>十六进制：<code>hex(a)</code> ,赋值十六进制<code>0x1839ac29</code> </li></ul></li></ul><h2 id="165-Compare-Version-Numbers-M"><a href="#165-Compare-Version-Numbers-M" class="headerlink" title="165. Compare Version Numbers[M]"></a>165. Compare Version Numbers[M]</h2><p><a href="https://leetcode.com/problems/compare-version-numbers/">165. Compare Version Numbers[M]</a> </p><p>Problem：</p><p>比较版本号。</p><p>Solution：</p><p>直观～Easy～</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compareVersion</span><span class="params">(self, version1: str, version2: str)</span> -&gt; int:</span></span><br><span class="line">        li1 = version1.split(<span class="string">'.'</span>)</span><br><span class="line">        li2 = version2.split(<span class="string">'.'</span>)</span><br><span class="line">        n_1 = len(li1)</span><br><span class="line">        n_2 = len(li2)</span><br><span class="line">        n = max(n_1, n_2)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            a = int(li1[i]) <span class="keyword">if</span> i &lt; n_1 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            b = int(li2[i]) <span class="keyword">if</span> i &lt; n_2 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> a &gt; b:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> a &lt; b:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><hr><ul><li>Python中的强制转换：<ol><li>字符串转换为int : <code>int_value = int(str_value)</code> </li><li>int转换为字符串：<code>str.value = str(int_value)</code> </li><li>int转换为unicode： <code>unicode(int_value)</code> </li><li>unicode转换为int：<code>int(unicode_value)</code> </li><li>字符串转换为unicode：<code>unicode(str_value)</code> </li><li>unicode转换为字符串：<code>str(unicode_value)</code> </li></ol></li><li>Java中的强制转换：<ol><li>字符串String转化为int：<code>int_value = String.parseInt(string_value)</code> 或者 <code>(int)string_value)</code> </li><li>int转化为字符串String：<code>string_value = (String)int_value</code> </li></ol></li></ul><h2 id="66-Plus-One-E"><a href="#66-Plus-One-E" class="headerlink" title="66. Plus One[E]"></a>66. Plus One[E]</h2><p><a href="https://leetcode.com/problems/plus-one/">66. Plus One[E]</a> </p><p>Problem:</p><p>用列表表示一个正数，返回正数+1的列表结果。</p><p>Solution：</p><p>记录一个最高位的进位情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plusOne</span><span class="params">(self, digits: List[int])</span> -&gt; List[int]:</span></span><br><span class="line">        c_bit = <span class="number">0</span></span><br><span class="line">        n = len(digits)</span><br><span class="line">        i = n - <span class="number">1</span></span><br><span class="line">        digits[i] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> i &gt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> digits[i] &lt; <span class="number">10</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            digits[i] %= <span class="number">10</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                c_bit = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                digits[i<span class="number">-1</span>] += <span class="number">1</span></span><br><span class="line">            i -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> c_bit == <span class="number">1</span>:</span><br><span class="line">            digits.insert(<span class="number">0</span>, c_bit)</span><br><span class="line">        <span class="keyword">return</span> digits</span><br></pre></td></tr></table></figure><hr><ul><li><p>Python中list添加元素的集中方法：（append(); extend(); insert(); +加号）</p><ol><li><p>append() ：在List尾部追加单个元素，只接受一个参数，参数可以是任意数据类型。</p></li><li><p>extend() ：在list尾部追加一个列表，将该参数列表中的每个元素连接到原列表。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; a = [1,2,3]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; b = [3,4,5]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; a.extend(b)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; a</span></span><br><span class="line">[1, 2, 3, 3, 4, 5]</span><br></pre></td></tr></table></figure></li><li><p>insert(index, object)：将一个元素插入到列表中，第一个参数是插入的索引点，第二个是插入的元素。</p></li><li><p>+加号：将两个list相加，返回一个新的list对象。</p></li></ol><p>区别：前三种方法(append, extend, insert)可以对列表增加元素，没有返回值，是直接修改原数据对象，而+加号是需要创建新的list对象，需要消耗额外的内存。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LeetCode Math 专题记录。&lt;/p&gt;
&lt;p&gt;10月初。&lt;/p&gt;
&lt;p&gt;Albert Einstein:&lt;/p&gt;
&lt;p&gt; “I believe that not everything that can be counted counts, and not everything that counts can be counted”&lt;/p&gt;
&lt;p&gt;「并非所有重要的东西都是可以被计算的，也并不是所有能被计算的东西都那么重要。」&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/tags/LeetCode/"/>
    
      <category term="Math" scheme="https://f7ed.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>「LeetCode」：String</title>
    <link href="https://f7ed.com/2020/09/29/LeetCode_String/"/>
    <id>https://f7ed.com/2020/09/29/LeetCode_String/</id>
    <published>2020-09-28T16:00:00.000Z</published>
    <updated>2020-09-29T06:56:50.890Z</updated>
    
    <content type="html"><![CDATA[<p>LeetCode String 专题记录。</p><p>9月毕。</p><p>「我祝福你有时有坏运气，你会意识到概率和运气在人生中扮演的角色，并理解你的成功并不完全是你应得的，其他人的失败也并不完全是他们应得的。」</p><p>「不想要刚好错过的悔恨，那就要有完全碾压的实力。」</p><a id="more"></a><h1 id="String"><a href="#String" class="headerlink" title="String"></a>String</h1><h2 id="28-Implement-strStr"><a href="#28-Implement-strStr" class="headerlink" title="28-Implement strStr()"></a>28-Implement strStr()</h2><p><a href="https://leetcode.com/problems/implement-strstr/solution/">28-Implement strStr()</a> </p><p>Problem:</p><p>返回第一个字串出现的下标</p><p>Solution：</p><p>Python就暴力匹配。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">strStr</span><span class="params">(self, haystack: str, needle: str)</span> -&gt; int:</span></span><br><span class="line">        n1 = len(haystack)</span><br><span class="line">        n2 = len(needle)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n1-n2+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> haystack[i:i+n2] == needle:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h2 id="14-Longest-Common-Prefix"><a href="#14-Longest-Common-Prefix" class="headerlink" title="14-Longest Common Prefix"></a>14-Longest Common Prefix</h2><p><a href="https://leetcode-cn.com/problems/longest-common-prefix/">14-Longest Common Prefix</a></p><p>Problem:</p><p>返回串的公共最长前缀。</p><p>Solution：</p><p>暴力匹配长度就好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        LCP = <span class="number">0</span></span><br><span class="line">        n = len(strs)</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n):</span><br><span class="line">                <span class="keyword">if</span> LCP &lt; len(strs[i]) <span class="keyword">and</span> strs[i][LCP] == strs[<span class="number">0</span>][LCP]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">return</span> strs[<span class="number">0</span>][<span class="number">0</span>: LCP]</span><br><span class="line">            LCP += <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="58-Length-of-Last-Word"><a href="#58-Length-of-Last-Word" class="headerlink" title="58-Length of Last Word"></a>58-Length of Last Word</h2><p><a href="https://leetcode.com/problems/length-of-last-word/">58-Length of Last Word</a> </p><p>Problem:</p><p>单词串由字母和空格组成，返回最后一个单词的长度。</p><p>Solution：</p><p>注意串最后的空格。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLastWord</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        end = n<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> end<span class="number">-1</span> &gt;= <span class="number">0</span> <span class="keyword">and</span> s[end] == <span class="string">' '</span>:</span><br><span class="line">            end -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(end, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> s[i] == <span class="string">' '</span>:</span><br><span class="line">                <span class="keyword">return</span> length</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                length += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> length</span><br></pre></td></tr></table></figure><h2 id="58-First-Unique-Character-in-a-String"><a href="#58-First-Unique-Character-in-a-String" class="headerlink" title="58-First Unique Character in a String"></a>58-First Unique Character in a String</h2><p>Problem:</p><p>找第一个没有重复出现的字符下标。</p><p>Solution：</p><p>暴力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">firstUniqChar</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        a_ascii = ord(<span class="string">'a'</span>)</span><br><span class="line">        cnt = [<span class="number">0</span>]*(<span class="number">26</span>+<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">            cnt[ord(i)-a_ascii] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> cnt[ord(i)-a_ascii] == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> s.index(i)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><hr><p>寻找子串开始索引：</p><ol><li><strong>str.find(substr, beg=0, end=len(string))</strong> <ul><li>substr: 字串</li><li>beg: 开始索引</li><li>end: 结束索引，默认字符串长度。</li><li>如果字符串不包含子串，则返回<code>-1</code> </li></ul></li><li><strong>str.index(str, beg=0, end=len(string))</strong> <ul><li>和find差不多，如果不包含子串会抛出异常。</li></ul></li></ol><h2 id="383-Ransom-Note"><a href="#383-Ransom-Note" class="headerlink" title="383-Ransom Note"></a>383-Ransom Note</h2><p><a href="https://leetcode.com/problems/ransom-note/">383-Ransom Note</a> </p><p>Problem:</p><p>给两个字符串，判断串1的字符能否由串2的字符组成。</p><p>Solution：</p><p>字典计数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canConstruct</span><span class="params">(self, ransomNote: str, magazine: str)</span> -&gt; bool:</span></span><br><span class="line">        ransomDir = &#123;&#125;</span><br><span class="line">        magazineDir = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> ransomNote:</span><br><span class="line">            ransomDir.setdefault(ch, <span class="number">0</span>)</span><br><span class="line">            ransomDir[ch] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> magazine:</span><br><span class="line">            magazineDir.setdefault(ch, <span class="number">0</span>)</span><br><span class="line">            magazineDir[ch] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> (k, v) <span class="keyword">in</span> ransomDir.items():</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> magazineDir:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> ransomDir[k] &gt; magazineDir[k]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><hr><ol><li>初始化字典的值：<code>dic.setdefault(ch, 0)</code> </li></ol><h2 id="344-Reverse-String"><a href="#344-Reverse-String" class="headerlink" title="344-Reverse String"></a>344-Reverse String</h2><p><a href="https://leetcode.com/problems/reverse-string/">344-Reverse String</a> </p><p>Problem: </p><p>in-place 反转字符串 with O(1) 的额外空间。</p><p>Solution：</p><p>前后两个指针交换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseString</span><span class="params">(self, s: List[str])</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Do not return anything, modify s in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        l = <span class="number">0</span></span><br><span class="line">        r = n<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            s[l], s[r] = s[r], s[l]</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">            r -= <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="151-Reverse-Words-in-a-String"><a href="#151-Reverse-Words-in-a-String" class="headerlink" title="151-Reverse Words in a String"></a>151-Reverse Words in a String</h2><p><a href="https://leetcode.com/problems/reverse-words-in-a-string/">151-Reverse Words in a String</a> </p><p>Problem:</p><p>反转字符串word by word.(结果中单词间只能有一个空格)</p><p>Solution：</p><p>把单词存入列表，再输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseWords</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        li = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; n:</span><br><span class="line">            <span class="keyword">while</span> i &lt; n <span class="keyword">and</span> s[i] == <span class="string">' '</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            l = i</span><br><span class="line">            <span class="keyword">while</span> i &lt; n <span class="keyword">and</span> s[i] != <span class="string">' '</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            r = i</span><br><span class="line">            <span class="keyword">if</span> r != l:</span><br><span class="line">                li.append(s[l:r])</span><br><span class="line"></span><br><span class="line">        ans = <span class="string">' '</span>.join(li[<span class="number">-1</span>::<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><p><a href="https://segmentfault.com/a/1190000015475309">Python连接字符串总结</a> </p><ol><li>加号连接：<code>&#39;a&#39; + &#39;b&#39;</code> </li><li>逗号连接，只能用于print打印: <code>print(a, b)</code> </li><li>直接连接: <code>print(&#39;a&#39; &#39;b&#39;)</code> </li><li>使用 <code>%</code> 格式化字符串：<code>&#39;%s %s&#39; % (&#39;hello&#39;, &#39;world&#39;)</code> </li><li><code>format</code> 格式化字符串：<code>&#39;{}{}&#39;.format(&#39;hello&#39;, &#39;world&#39;)</code> </li><li><code>join</code> 内置方法：用字符来连接一个序列，数组或列表等：<code>&#39;-&#39;.join([&#39;aa&#39;, &#39;bb&#39;, &#39;cc&#39;])</code> </li><li><code>f-string</code> 方法：<code>aa, bb = &#39;hello&#39;, &#39;world&#39;</code> , <code>f&#39;{aa} {bb}&#39;</code> </li><li><code>*</code> 操作符：字符串乘法。</li></ol><p>反转列表：<code>[-1: : -1]</code> </p><h2 id="186-Reverse-Words-in-a-String-II"><a href="#186-Reverse-Words-in-a-String-II" class="headerlink" title="186-Reverse Words in a String II"></a>186-Reverse Words in a String II</h2><p><a href="https://leetcode-cn.com/problems/reverse-words-in-a-string-ii/">186-Reverse Words in a String II</a> </p><p>Problem:</p><p>反转单词in-places.</p><p>Solution:</p><p>两次反转，第一次整体反转，第二次再单词反转。</p><p>（不额外开个数组来逐个赋值AC不了，不知道为啥q w q)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseWords</span><span class="params">(self, s: List[str])</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Do not return anything, modify s in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        temp = s[<span class="number">-1</span>::<span class="number">-1</span>]</span><br><span class="line">        n = len(temp)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; n:</span><br><span class="line">            l = i</span><br><span class="line">            <span class="keyword">while</span> i &lt; n <span class="keyword">and</span> temp[i] != <span class="string">' '</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            r = i</span><br><span class="line">            temp[l:r] = list(reversed(temp[l:r]))</span><br><span class="line"></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(n):</span><br><span class="line">            s[index] = temp[index]</span><br></pre></td></tr></table></figure><hr><ul><li><p>Python 反转列表的方法：</p><ol><li><code>list(reversed(a))</code> , reversed(a)返回的是迭代器，转换成list。</li><li><code>a[::-1]</code> </li></ol></li><li><p>Python 字符串(str)和列表(list)互相转换：</p><ol><li><p>str 转换为 list</p><ul><li><code>list()</code> 转换为单个字符列表</li><li><code>str.split()</code> 或者<code>str.split(&#39; &#39;)</code> 空格分割转换</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">str1 = <span class="string">"123"</span></span><br><span class="line">list1 = list(str1)</span><br><span class="line"><span class="keyword">print</span> list1</span><br><span class="line"><span class="comment"># ['1', '2', '3']</span></span><br><span class="line"></span><br><span class="line">str2 = <span class="string">"123 sjhid dhi"</span></span><br><span class="line">list2 = str2.split() <span class="comment">#or list2 = str2.split(" ")</span></span><br><span class="line"><span class="keyword">print</span> list2</span><br><span class="line"><span class="comment"># ['123', 'sjhid', 'dhi']</span></span><br><span class="line"></span><br><span class="line">str3 = <span class="string">"www.google.com"</span></span><br><span class="line">list3 = str3.split(<span class="string">"."</span>)</span><br><span class="line"><span class="keyword">print</span> list3</span><br><span class="line"><span class="comment"># ['www', 'google', 'com']</span></span><br></pre></td></tr></table></figure></li><li><p>list转换为str:</p><ul><li><code>&quot;&quot;.join(list)</code> 无空格连接</li><li><code>&quot;.&quot;.join(list)</code>  </li></ul></li></ol></li></ul><h2 id="345-Reverse-Vowels-of-a-String"><a href="#345-Reverse-Vowels-of-a-String" class="headerlink" title="345-Reverse Vowels of a String"></a>345-Reverse Vowels of a String</h2><p><a href="https://leetcode.com/problems/reverse-vowels-of-a-string/">345-Reverse Vowels of a String</a> </p><p>Problem:</p><p>反转字符串中的元音字母。</p><p>Solution：</p><p>元音字母，包括大写元音字母和小写元音字母。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseVowels</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        s = list(s)</span><br><span class="line">        dic = &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'e'</span>: <span class="number">1</span>, <span class="string">'i'</span>: <span class="number">1</span>, <span class="string">'o'</span>: <span class="number">1</span>, <span class="string">'u'</span>: <span class="number">1</span>&#125;</span><br><span class="line">        rev = [<span class="number">0</span>] * n</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> s[index] <span class="keyword">in</span> dic <span class="keyword">or</span> s[index].lower() <span class="keyword">in</span> dic:</span><br><span class="line">                rev[index] = <span class="number">1</span></span><br><span class="line">        l = <span class="number">0</span></span><br><span class="line">        r = n - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> rev[l] == <span class="number">0</span>:</span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> rev[r] == <span class="number">0</span>:</span><br><span class="line">                r -= <span class="number">1</span></span><br><span class="line">            s[l], s[r] = s[r], s[l]</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">            r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(s)</span><br></pre></td></tr></table></figure><hr><ul><li><p>Python大小写转换：</p><ul><li>所有字符转换为大写：<code>str.upper()</code> </li><li>所有字符转换为小写：<code>str.lower()</code> </li><li>第一个字母转换为大写字母，其余小写：<code>str.capitalize()</code> </li><li>把每个单词的第一个字母转换为大写，其余小写。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">"www.runoob.com"</span></span><br><span class="line">print(str.upper())          <span class="comment"># 把所有字符中的小写字母转换成大写字母</span></span><br><span class="line">print(str.lower())          <span class="comment"># 把所有字符中的大写字母转换成小写字母</span></span><br><span class="line">print(str.capitalize())     <span class="comment"># 把第一个字母转化为大写字母，其余小写</span></span><br><span class="line">print(str.title())          <span class="comment"># 把每个单词的第一个字母转化为大写，其余小写</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># WWW.RUNOOB.COM</span></span><br><span class="line"><span class="comment"># www.runoob.com</span></span><br><span class="line"><span class="comment"># Www.runoob.com</span></span><br><span class="line"><span class="comment"># Www.Runoob.Com</span></span><br></pre></td></tr></table></figure></li><li><p>Python中string是不可变对象，不能通过下标的方式（如<code>str[0]=&#39;a&#39;</code> )改变字符串。</p></li></ul><h2 id="205-Isomorphic-Strings"><a href="#205-Isomorphic-Strings" class="headerlink" title="205-Isomorphic Strings"></a>205-Isomorphic Strings</h2><p><a href="https://leetcode.com/problems/isomorphic-strings/">205-Isomorphic Strings</a> </p><p>Problem:</p><p>判断是否同构字符串。</p><p>Solution：</p><p>字符到字符的映射，必须是单射。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isIsomorphic</span><span class="params">(self, s: str, t: str)</span> -&gt; bool:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        dic = &#123;&#125;</span><br><span class="line">        vSet = set()  <span class="comment"># satisfy single map</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">            ch = s[idx]</span><br><span class="line">            <span class="comment"># single map</span></span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> dic <span class="keyword">and</span> t[idx] <span class="keyword">not</span> <span class="keyword">in</span> vSet:</span><br><span class="line">                dic[ch] = t[idx]</span><br><span class="line">                vSet.add(t[idx])</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">in</span> dic <span class="keyword">and</span> dic[ch] == t[idx]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><hr><ul><li><p>Python 集合的操作：</p><ol><li><p>创建空集合：<code>set()</code> </p></li><li><p>创建有初值的集合：<code>SET = {v0, v1, v2}</code> 或者<code>SET = set(v0)</code> </p></li><li><p>判断元素是否在集合中：<code>x in SET</code> </p></li><li><p>集合运算：</p><p><code>a-b</code> :属于a集合不属于b集合</p><p><code>a|b</code> :属于a集合或属于b集合</p><p><code>a&amp;b</code> :集合a和集合b都包含的元素</p><p><code>a^b</code> : 不同时包含于集合a和集合b的元素</p></li><li><p>集合中添加元素：<code>s.add(x)</code> </p></li><li><p>集合中添加元素，且参数可以是列表、元组、字典(是每个元素都添加进去）等：<code>s.update(x)</code> </p></li><li><p>移除元素：<code>s.remove(x)</code> ，如果元素不存在，则会发生错误。</p></li><li><p>移除元素：<code>s.discard(x)</code> ，如果元素不存在，不会发生错误。</p></li><li><p>随机删除集合中的一个元素：<code>s.pop()</code> （原理：对集合无序排序，然后删除无序排列集合的第一个）</p></li><li><p>计算集合元素的个数：<code>len(s)</code> </p></li><li><p>清空集合<code>s.clear()</code> </p></li></ol></li><li><p>List Comprehension &amp;&amp; Set Comprehension &amp;&amp; Dictionary Comprehension</p><p>这个相当于数学中的 $S={2\cdot x\mid x\in \left[0,9\right)}$ 的表达。</p><ol><li><p>List Comprehension</p><p>如果用数学中的这个表达来看下面的式子，就很显而易见了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arr = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure><p>再看看加了其他限制的例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># filter the elements</span></span><br><span class="line">arr1 = [x <span class="keyword">for</span> x <span class="keyword">in</span> arr <span class="keyword">if</span> x % <span class="number">2</span>==<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># add more conditions</span></span><br><span class="line">arr2 = [x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> arr <span class="keyword">if</span> x &gt;= <span class="number">3</span> <span class="keyword">and</span> x % <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># use nested for loops</span></span><br><span class="line">arr3 = [(x, y) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>) <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br></pre></td></tr></table></figure><p>使用List Comprehension不仅优美，而且效率也会很高。</p></li><li><p>Set Comprehension</p><p>同样的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s = &#123;x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">100</span>) <span class="keyword">if</span> x%<span class="number">2</span> != <span class="number">0</span> <span class="keyword">and</span> x%<span class="number">3</span> != <span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></li><li><p>Dictionary Comprehension</p><p>Syntax：<code>{expression(variable): expression(variable) for variable, variable in input_set [predicate][, …]}</code>  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [(set_k), (set_v)]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>)]&#125;</span><br><span class="line">&#123;<span class="number">1</span>: <span class="number">2</span>, <span class="number">3</span>: <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;n: n <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>)&#125;</span><br><span class="line">&#123;<span class="number">0</span>: <span class="number">0</span>, <span class="number">1</span>: <span class="number">1</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;chr(n): n <span class="keyword">for</span> n <span class="keyword">in</span> (<span class="number">65</span>, <span class="number">66</span>, <span class="number">66</span>)&#125;</span><br><span class="line">&#123;<span class="string">'A'</span>: <span class="number">65</span>, <span class="string">'B'</span>: <span class="number">66</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ((k1, v1), (k2, v2))</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> ((<span class="string">'I'</span>, <span class="number">1</span>), (<span class="string">'II'</span>, <span class="number">2</span>))&#125;</span><br><span class="line">&#123;<span class="string">'I'</span>: <span class="number">1</span>, <span class="string">'II'</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> ((<span class="string">'a'</span>, <span class="number">0</span>), (<span class="string">'b'</span>, <span class="number">1</span>)) <span class="keyword">if</span> v == <span class="number">1</span>&#125;</span><br><span class="line">&#123;<span class="string">'b'</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h2 id="68-Text-Justification"><a href="#68-Text-Justification" class="headerlink" title="68-Text Justification"></a>68-Text Justification</h2><p><a href="https://leetcode.com/problems/text-justification/">68-Text Justification</a> </p><p>Problem:</p><p>文本对齐，总结下来有以下几点要求。</p><ul><li>如果不是最后一行，且该行不止一个单词，则要求左右对齐。<ul><li>左右对齐：尽可能让单词间的空格均匀分布，如果不能均匀分布，则单词左边的空格应该比右边的空格多。</li><li>贪心的思想：应该尽可能的多放单词。</li></ul></li><li>如果是最后一行，或者该行只有一个单词，则要求左对齐。</li></ul><p>Solution：</p><p>分两种情况处理，判断是左对齐，还是右对齐。</p><ol><li><p>左对齐：该行有x个单词</p><p>前x-1个单词的后面都应该只有一个空格。</p><p>最后一个单词后面就应该补齐所有空格。</p></li><li><p>左右对齐：该行有x个单词，有x-1个空格间隙。</p><p>计算得到该行的空格数w，则如果能均匀分配，则每个间隙应该有aver = w // (x-1) 个空格。</p><p>但也许不会均匀分配，因此，可能会多出m个空格（m &lt; x-1 )</p><p>即前m个单词，单词的后面应该有（aver+1)个空格，后面的(x-1) - m个单词应该有aver个空格。</p><p>最后一个单词的后面没有空格。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.words = <span class="literal">None</span></span><br><span class="line">        self.maxWidth = <span class="literal">None</span></span><br><span class="line">        self.sum = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">leftJustify</span><span class="params">(self, l: int, r: int)</span> -&gt; str:</span></span><br><span class="line">        wordsNum = r - l + <span class="number">1</span></span><br><span class="line">        lengthNum = self.sum[r] <span class="keyword">if</span> l == <span class="number">0</span> <span class="keyword">else</span> self.sum[r] - self.sum[l - <span class="number">1</span>]</span><br><span class="line">        spaceNum = self.maxWidth - lengthNum</span><br><span class="line">        temp = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l, r):</span><br><span class="line">            temp += self.words[i] + <span class="string">" "</span></span><br><span class="line">        temp += self.words[r] + <span class="string">" "</span>*(spaceNum - (wordsNum - <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">leftRightJustify</span><span class="params">(self, l: int, r: int)</span> -&gt; str:</span></span><br><span class="line">        wordsNum = r - l + <span class="number">1</span></span><br><span class="line">        lengthNum = self.sum[r] <span class="keyword">if</span> l == <span class="number">0</span> <span class="keyword">else</span> self.sum[r] - self.sum[l - <span class="number">1</span>]</span><br><span class="line">        spaceNum = self.maxWidth - lengthNum</span><br><span class="line">        temp = <span class="string">""</span></span><br><span class="line">        averSpace = spaceNum // (wordsNum - <span class="number">1</span>)</span><br><span class="line">        moreSpace = spaceNum - averSpace*(wordsNum - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(moreSpace):</span><br><span class="line">            temp += self.words[l+i] + <span class="string">" "</span> * (averSpace + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l+moreSpace, r):</span><br><span class="line">            temp += self.words[i] + <span class="string">" "</span> * averSpace</span><br><span class="line">        temp += self.words[r]</span><br><span class="line">        <span class="keyword">return</span> temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fullJustify</span><span class="params">(self, words: List[str], maxWidth: int)</span> -&gt; List[str]:</span></span><br><span class="line">        self.words = words</span><br><span class="line">        self.maxWidth = maxWidth</span><br><span class="line">        n = len(words)</span><br><span class="line">        sum = [<span class="number">0</span>]*n</span><br><span class="line">        sum[<span class="number">0</span>] = len(words[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># sum prefix length of words</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            sum[i] = sum[i - <span class="number">1</span>] + len(words[i])</span><br><span class="line">        self.sum = sum</span><br><span class="line">        l = <span class="number">0</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">while</span> l &lt; n:</span><br><span class="line">            r = l</span><br><span class="line">            lengthNum = len(self.words[l])</span><br><span class="line">            <span class="keyword">while</span> r+<span class="number">1</span> &lt; n <span class="keyword">and</span> lengthNum + len(self.words[r+<span class="number">1</span>]) + <span class="number">1</span> &lt;= maxWidth:</span><br><span class="line">                lengthNum += len(self.words[r+<span class="number">1</span>]) + <span class="number">1</span>  <span class="comment"># space</span></span><br><span class="line">                r += <span class="number">1</span></span><br><span class="line">            <span class="comment"># only one word or the last line</span></span><br><span class="line">            <span class="keyword">if</span> r - l + <span class="number">1</span> == <span class="number">1</span> <span class="keyword">or</span> r == n - <span class="number">1</span>:</span><br><span class="line">                ans.append(self.leftJustify(l, r))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ans.append(self.leftRightJustify(l, r))</span><br><span class="line">            l = r + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><ul><li><p>Python的三元运算符：</p><p>#如果条件为真，返回真 否则返回假<br>condition_is_true if condition else condition_is_false</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">is_fat = <span class="literal">True</span></span><br><span class="line">state = <span class="string">"fat"</span> <span class="keyword">if</span> is_fat <span class="keyword">else</span> <span class="string">"not fat"</span></span><br></pre></td></tr></table></figure></li><li><p>Python的整除是：<code>\\</code> ，实数除是：<code>\</code> </p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Python中字符串的连接方法总结： <a href="https://segmentfault.com/a/1190000015475309">https://segmentfault.com/a/1190000015475309</a></li><li></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LeetCode String 专题记录。&lt;/p&gt;
&lt;p&gt;9月毕。&lt;/p&gt;
&lt;p&gt;「我祝福你有时有坏运气，你会意识到概率和运气在人生中扮演的角色，并理解你的成功并不完全是你应得的，其他人的失败也并不完全是他们应得的。」&lt;/p&gt;
&lt;p&gt;「不想要刚好错过的悔恨，那就要有完全碾压的实力。」&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/categories/LeetCode/"/>
    
    
      <category term="Algorithms" scheme="https://f7ed.com/tags/Algorithms/"/>
    
      <category term="LeetCode" scheme="https://f7ed.com/tags/LeetCode/"/>
    
      <category term="Data-Structure" scheme="https://f7ed.com/tags/Data-Structure/"/>
    
      <category term="String" scheme="https://f7ed.com/tags/String/"/>
    
  </entry>
  
  <entry>
    <title>「LeetCode」：Array</title>
    <link href="https://f7ed.com/2020/09/15/LeetCode_array/"/>
    <id>https://f7ed.com/2020/09/15/LeetCode_array/</id>
    <published>2020-09-14T16:00:00.000Z</published>
    <updated>2020-10-09T13:24:32.989Z</updated>
    
    <content type="html"><![CDATA[<p>8月某司实训+准备开学期末考，我可太咕了q w q…dbq，（希望）高产博主我.我..又回来了。</p><p>LeetCode Array专题，持久更新。（<a href="https://github.com/f1ed/LeetCode">GitHub</a>)</p><a id="more"></a><h1 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h1><h2 id="27-Remove-Elements"><a href="#27-Remove-Elements" class="headerlink" title="27-Remove Elements"></a>27-Remove Elements</h2><p><a href="https://leetcode.com/problems/remove-element/">27-Remove Elements</a> </p><p>Solution</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeElement</span><span class="params">(self, nums: List[int], val: int)</span> -&gt; int:</span></span><br><span class="line"></span><br><span class="line">        tot = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> element <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> element == val:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                nums[tot] = element</span><br><span class="line">                tot = tot + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> tot</span><br></pre></td></tr></table></figure><ol><li><p>Python的参数传递和函数返回值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeElement</span><span class="params">(self, nums:List[int], val:int)</span> -&gt; int:</span></span><br></pre></td></tr></table></figure></li><li><p>题目要求：</p><p>“remove all instances of that value <a href="https://en.wikipedia.org/wiki/In-place_algorithm"><strong>in-place</strong></a> and return the new length.”</p><p>“Do not allocate extra space for another array, you must do this by <strong>modifying the input array <a href="https://en.wikipedia.org/wiki/In-place_algorithm">in-place</a></strong> with O(1) extra memory.”</p><p>“Confused why the returned value is an integer but your answer is an array?</p><p>Note that the input array is passed in by <strong>reference</strong>, which means modification to the input array will be known to the caller as well.”</p><p>题目要求是在原数组上删除数值，不能额外开新的空间存储数组。</p><p>意思就是说，虽然函数返回的是一个数值，但实际返回答案是一个数组。</p><p>因为数组的传递是指针传递，返回的是数组长度，则相当于返回了这个in-place的新数组。</p></li></ol><h2 id="26-Remove-Duplicates-from-Sorted-Array"><a href="#26-Remove-Duplicates-from-Sorted-Array" class="headerlink" title="26-Remove Duplicates from Sorted Array"></a>26-Remove Duplicates from Sorted Array</h2><p><a href="https://leetcode.com/problems/remove-duplicates-from-sorted-array/">26-Remove Duplicates from Sorted Array</a> </p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeDuplicates</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line"></span><br><span class="line">        tot = <span class="number">0</span></span><br><span class="line">        before = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[index] != before:</span><br><span class="line">                nums[tot] = nums[index]</span><br><span class="line">                before = nums[index]</span><br><span class="line">                tot = tot + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tot</span><br></pre></td></tr></table></figure><hr><ol><li><p>第一次提交的时候<code>before = nums[0] - 1</code> 报错了，原因是传入数组长度为0，下标越界。</p><p>注意空数组的下标越界问题。</p></li></ol><h2 id="80-Remove-Duplicates-from-Sorted-Array-II"><a href="#80-Remove-Duplicates-from-Sorted-Array-II" class="headerlink" title="80-Remove Duplicates from Sorted Array II"></a>80-Remove Duplicates from Sorted Array II</h2><p><a href="https://leetcode.com/problems/remove-duplicates-from-sorted-array-ii/">80-Remove Duplicates from Sorted Array II</a> </p><p>Solution:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeDuplicates</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        before = <span class="literal">None</span></span><br><span class="line">        before_cnt = <span class="number">0</span></span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[index] != before:</span><br><span class="line">                nums[length] = nums[index]</span><br><span class="line">                before = nums[index]</span><br><span class="line">                length += <span class="number">1</span></span><br><span class="line">                before_cnt = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                before_cnt += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> before_cnt &lt;= <span class="number">2</span>:</span><br><span class="line">                    nums[length] = nums[index]</span><br><span class="line">                    length += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> length</span><br></pre></td></tr></table></figure><h2 id="189-Rotate-Array（S123）"><a href="#189-Rotate-Array（S123）" class="headerlink" title="189-Rotate Array（S123）"></a>189-Rotate Array（S123）</h2><p><a href="https://leetcode.com/problems/rotate-array/">189-Rotate Array</a> </p><p>Problem:</p><p>简述题目大意，给一个列表nums，一个 $k$ 值，要求原址让列表循环右移 $k$ 位。</p><p>Solution:</p><p>其实以下三种做法时间空间复杂度差别不大，主要看个思路吧。</p><table><thead><tr><th>S</th><th>Runtime</th><th>Memory</th><th align="center">Language</th></tr></thead><tbody><tr><td>S1</td><td>64ms</td><td>15.2MB</td><td align="center">pyhon3</td></tr><tr><td>S2</td><td>64ms</td><td>15.1MB</td><td align="center">python3</td></tr><tr><td>S3</td><td>116ms</td><td>15.1MB</td><td align="center">python3</td></tr></tbody></table><h3 id="S1-简单做法：空间换时间"><a href="#S1-简单做法：空间换时间" class="headerlink" title="S1-简单做法：空间换时间"></a>S1-简单做法：空间换时间</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ ，循环右移时多开了一个数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Do not return anything, modify nums in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        length = len(nums)</span><br><span class="line">        a = [<span class="number">0</span>] * length</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(length):</span><br><span class="line">            a[(index + k)%length] = nums[index]</span><br><span class="line">        nums[:] = a</span><br></pre></td></tr></table></figure><h3 id="S2-利用数学同余关系"><a href="#S2-利用数学同余关系" class="headerlink" title="S2-利用数学同余关系"></a>S2-利用数学同余关系</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ ，循环右移时只多开了一个变量。</p><p>原理：</p><blockquote><p>定理[1]：</p><p>设 $m$ 是正整数，整数 $a$ 满足 $(a,m)=1$ ，$b$ 是任意整数。若 $x$ 遍历模 $m$ 的一个完全剩余系，则 $ax+b$ 也遍历模 $m$ 的一个完全剩余系。</p></blockquote><p>由以上定理可以得知，设 $n$ 为列表长度， $x$ 是列表的下标，遍历 $n$ 的一个完全剩余系。</p><ul><li><p>如果 $(k,n)=1$ ， $kx$ 也遍历 $n$ 的一个完全剩余系。这种情况，列表下标通过 $k$ 的倍数的顺序连成一个环。</p><p>：只需要额外一个变量 $temp$ 存储移动占用的值。</p></li><li><p>如果 $(k,n)\neq 1$ ，那么 $kx$ 不会遍历一个 $n$ 的完全剩余系，会出现下图的情况（如绿色的线的元素的 $idx = kx+0$ ，红色线的元素都是 $idx=kx+1$ ），会在 $k$ 的某个剩余类一直循环。</p><p>：遍历每个 $k$ 的剩余类。 在每次循环移位时，需要记录该次循环的起始位，防止重复。</p><img src="https://s1.ax1x.com/2020/09/14/wrcSVs.png" alt="wrcSVs.png" style="zoom:50%;" /></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Do not return anything, modify nums in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        length = len(nums)</span><br><span class="line">        k %= length</span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> cnt &lt; length:</span><br><span class="line">            current, prev = start, nums[start]</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                current = (current + k) % length</span><br><span class="line">                prev, nums[current] = nums[current], prev</span><br><span class="line">                cnt += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> current == start:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            start += <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="S3-利用反转列表的思路"><a href="#S3-利用反转列表的思路" class="headerlink" title="S3-利用反转列表的思路"></a>S3-利用反转列表的思路</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>原理：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Original List                   : 1 2 3 4 5 6 7</span><br><span class="line">After reversing all numbers     : 7 6 5 4 3 2 1</span><br><span class="line">After reversing first k numbers : 5 6 7 4 3 2 1</span><br><span class="line">After revering last n-k numbers : 5 6 7 1 2 3 4 --&gt; Result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Solution 3: Reverse</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverse</span><span class="params">(self, nums: list, begin: int, end: int)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">while</span> begin &lt; end:</span><br><span class="line">            nums[begin], nums[end] = nums[end], nums[begin]</span><br><span class="line">            begin += <span class="number">1</span></span><br><span class="line">            end -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        k %= n</span><br><span class="line">        self.reverse(nums, <span class="number">0</span>, n<span class="number">-1</span>)</span><br><span class="line">        self.reverse(nums, <span class="number">0</span>, k<span class="number">-1</span>)</span><br><span class="line">        self.reverse(nums, k, n<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><hr><ol><li><p>Python用引用管理对象。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a1 = <span class="number">1</span>, *p = &amp;a1;</span><br><span class="line"><span class="keyword">int</span> a2 = <span class="number">2</span>, &amp;b = a2;</span><br></pre></td></tr></table></figure><ul><li>指针：指针变量是一个新变量，这个变量存储的是（变量a1的）地址，该地址指向一个存储单元。（该存储单元存放的是a1的值）。</li><li>引用：引用的实质是变量的别名，所以a2和b实际是一个东西，在内存中占有同一个存储单元。</li></ul><p>所以python中交换对象可以直接<code>a,b = b,a</code> </p></li><li><p>Python 列表的操作：切片。</p></li></ol><h2 id="41-First-Missing-Positive"><a href="#41-First-Missing-Positive" class="headerlink" title="41-First Missing Positive"></a>41-First Missing Positive</h2><p><a href="https://leetcode.com/problems/first-missing-positive/">41-First Missing Positive</a>  </p><p>Solution:</p><p>排一下序，维护一个expect变量就行了。</p><p>时间复杂度：$\mathcal{O}(n\log{n})$ ，题目没有卡常。</p><p>Runtime: 36 ms, faster than 70.96% of Python3 online submissions for First Missing Positive.</p><p>空间复杂度：$\mathcal{O}(n)$</p><p>Memory Usage: 13.8 MB, less than 69.19% of Python3 online submissions for First Missing Positive. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">firstMissingPositive</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        expect = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> element <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> element == expect:</span><br><span class="line">                expect += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> element &gt; expect:</span><br><span class="line">                <span class="keyword">return</span> expect</span><br><span class="line">        <span class="keyword">return</span> expect</span><br></pre></td></tr></table></figure><h2 id="299-Bulls-and-Cows"><a href="#299-Bulls-and-Cows" class="headerlink" title="299-Bulls and Cows"></a>299-Bulls and Cows</h2><p><a href="https://leetcode.com/problems/bulls-and-cows/">299-Bulls and Cows</a> </p><p>Problem:</p><p>题目大意是：给定两个相同长度的字符串，计算这两个字符串有多少个对应位数字相同，和多少个位置不对应但数字相同的个数。</p><p>Solution:</p><p>应用字符0-9本身数字的性质。</p><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getHint</span><span class="params">(self, secret: str, guess: str)</span> -&gt; str:</span></span><br><span class="line">        <span class="comment"># 0-9 cnt for guess (expect the same digit)</span></span><br><span class="line">        cnt = [<span class="number">0</span>] * <span class="number">10</span></span><br><span class="line">        bulls = cows = <span class="number">0</span></span><br><span class="line">        g = <span class="keyword">lambda</span> a: ord(a) - ord(<span class="string">'0'</span>)</span><br><span class="line">        <span class="keyword">for</span> si, gi <span class="keyword">in</span> zip(secret, guess):</span><br><span class="line">            <span class="keyword">if</span> si == gi:</span><br><span class="line">                bulls += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cnt[g(gi)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> si, gi <span class="keyword">in</span> zip(secret, guess):</span><br><span class="line">            <span class="keyword">if</span> si == gi:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">elif</span> cnt[g(si)] &gt; <span class="number">0</span>:</span><br><span class="line">                cows += <span class="number">1</span></span><br><span class="line">                cnt[g(si)] -= <span class="number">1</span></span><br><span class="line">        output = <span class="string">"&#123;&#125;A&#123;&#125;B"</span>.format(bulls, cows)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><hr><ol><li><p>ord()函数和chr()函数</p><p>ord()返回字符的ASCII码，chr函数返回ASCII码对应的字符。</p></li><li><p>浅析lambda表达式，匿名函数，类似于C语言的宏。</p><p>格式：<code>lambda [arg1[, arg2,...]] : expression</code> </p></li><li><p>双变量同时遍历使用zip()函数</p></li></ol><h2 id="134-Gas-Station（S12）"><a href="#134-Gas-Station（S12）" class="headerlink" title="134-Gas Station（S12）"></a>134-Gas Station（S12）</h2><p><a href="https://leetcode.com/problems/gas-station/">134-Gas Station</a> </p><p>Problem：</p><p>题目大意是：有N个环形加油站，每个加油站能加油gas[i]，一个汽车起始油量为0，且从i个站开到第i+1个站需要花费cost[i]的油量。找出这个车能顺时针跑完一圈的起始点（如果有，则唯一），如果不能返回-1。</p><p>Solution：</p><table><thead><tr><th>Solution</th><th>Runtime</th><th>Memory</th><th>Language</th></tr></thead><tbody><tr><td>S1-简单做法</td><td>3244ms</td><td>14.9MB</td><td>python3</td></tr><tr><td>S2-原理优化</td><td>104ms</td><td>14.8MB</td><td>python3</td></tr></tbody></table><h3 id="S1-简单解法"><a href="#S1-简单解法" class="headerlink" title="S1-简单解法"></a>S1-简单解法</h3><p>时间复杂度：$\mathcal{O}(n^2)$ ，实际远达不到 $n^2$ ，算有一点贪心叭。</p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>该汽车从起点i能跑完的必要条件：</p><ul><li><p>起始点 <code>gas[i] - cost[i] &gt;= 0</code> 。并且维护一个数组存放 <code>gas[i] - cost[i]</code> ，即这个站自给自足的油量余量。</p></li><li><p>如果从满足条件1的起点开始跑一圈， 要求路程中的油量必须大于等于0。维护一个汽车当前总油量 $S$ （用前缀和维护），每跑过一段路程，都要求 $S&gt;=0$ 。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canCompleteCircuit</span><span class="params">(self, gas: List[int], cost: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(gas)</span><br><span class="line">        remain = []</span><br><span class="line">        <span class="keyword">for</span> g, c <span class="keyword">in</span> zip(gas, cost):</span><br><span class="line">            remain.append(g - c)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> remain[i] &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                S = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                    S += remain[(i + j) % n]</span><br><span class="line">                    <span class="keyword">if</span> S &lt; <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> S &gt;= <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h3 id="S2-对问题分析进行再简化"><a href="#S2-对问题分析进行再简化" class="headerlink" title="S2-对问题分析进行再简化"></a>S2-对问题分析进行再简化</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  i: <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line">g[i]: <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span></span><br><span class="line">c[i]: <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">1</span> <span class="number">2</span></span><br><span class="line">g-c :<span class="number">-2</span><span class="number">-2</span><span class="number">-2</span> <span class="number">3</span> <span class="number">3</span></span><br><span class="line">sum :<span class="number">-2</span><span class="number">-4</span><span class="number">-6</span><span class="number">-3</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><ul><li><p>如果 $S=\sum_{i=0}^{n-1} g[i]-c[i],S&lt;0$  那么一定无解， $S$ 称为总积累油量。</p></li><li><p>如果 $S&gt;=0$，如果有找出最优解的方法，则一定有解。</p><ol><li><p>起点满足 $g[i]-c[i]&gt;=0$ ，把这些点称为正余量点。</p></li><li><p>用 $\mathcal{O}(1)$ 算出从第 $i$ 个点出发到第 $n$ （n就是第0个点） 个点所积累的油量： $res[i] =S-sum[i-1]$ .即用总积累油量减去前 $i-1$ 段路程能积累的油量（一般积累为负）。(sum数组就是 g-c的前缀和)</p></li><li><p>对于满足起点要求 $g[i]-c[i]&gt;=0$ 的所有点，计算从第 $i$ 个点出发到第 $n$ 个点到油量积累。那么有最大油量积累的点即为最优起始点。（题目规定如果存在，则点唯一）</p></li><li><p>因此，因为 $S$ 固定，只需要找到 $sum$ 数组中的最小值的下标，下标+1即是结果。</p></li></ol><ul><li>证明其正确性：<ol><li>如果满足 $g[i]-c[i]&gt;=0$ 的上述点 $i,j（i&lt;j)$  ，如果 $res[i]&gt;=res[j]$ ，说明从 $i$ 到 $j$ 是正油量积累，贪心的思想，那肯定积累的油量越多越好， $i$ 比 $j$ 优。</li><li>如果 $res[i]&lt;=res[j]$ ，说明从 $i$ 到 $j$ 是负油量积累，如果从 $i$ 点出发，到 $j$ 点就负油量了；如果从 $j$ 点出发，该车最后再跑 $i$ 到$j$  段，因为保证了总积累油量是正，所以最后一定有足够的油量能跑完 $i$ 到 $j$ 段。</li><li>再证只要 $S&gt;=0$ 则一定有解。是动态尝试起始点，( $i,j$ 都满足 $g[i]-c[i]&gt;=0$ ）从点 $i$ 开始，跑到点 $j$ 时，如果该途中途有出现油量不够，那就把 $i$ 到 $j$ 的这段路程放到路途的后面来跑，等油量积累够了再跑这段。</li></ol></li></ul></li></ul><p>Code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canCompleteCircuit</span><span class="params">(self, gas: List[int], cost: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(gas)</span><br><span class="line">        sum = <span class="number">0</span></span><br><span class="line">        remain = []</span><br><span class="line">        <span class="keyword">for</span> g, c <span class="keyword">in</span> zip(gas, cost):</span><br><span class="line">            remain.append(g - c)</span><br><span class="line">            sum += g - c</span><br><span class="line">        <span class="keyword">if</span> sum &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            remain[i] += remain[i - <span class="number">1</span>]</span><br><span class="line">        min_idx = remain.index(min(remain))</span><br><span class="line">        <span class="keyword">return</span> (min_idx + <span class="number">1</span>) % n</span><br></pre></td></tr></table></figure><hr><ol><li>前缀和</li><li><code>list.index(value)</code> 找出list中值为<code>value</code> 的第一个下标。</li><li><code>min(list)</code> 返回list中的最小值。</li></ol><h2 id="118-Pascal’s-Triangle"><a href="#118-Pascal’s-Triangle" class="headerlink" title="118-Pascal’s Triangle"></a>118-Pascal’s Triangle</h2><p><a href="https://leetcode.com/problems/pascals-triangle/submissions/">118-Pascal’s Triangle</a> </p><p>Problem:</p><p>给定一个数字，输出如下规则的值。</p><img src="https://s1.ax1x.com/2020/09/15/wyWI9s.gif" alt="wyWI9s.gif" style="zoom:50%;" />   <p>Solution：</p><p>注意边界吧。（不太喜欢这种题qwq</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(self, numRows: int)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, numRows):</span><br><span class="line">            temp = [<span class="number">1</span>] * (i+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, i):</span><br><span class="line">                temp[j] = ans[i<span class="number">-1</span>][j<span class="number">-1</span>] + ans[i<span class="number">-1</span>][j]</span><br><span class="line">            ans.append(temp)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><ol><li><p>Python 中的append会出现值被覆盖的情况：变量在循环外定义，但在循环中对该变量做出一定改变，然后append到列表，最后发现列表中的值都是一样的。</p><p>因为Python中很多时候都是以对象的形式管理对象，因此append给列表的是一个地址。</p></li></ol><h2 id="119-Pascal’s-Triangle-II"><a href="#119-Pascal’s-Triangle-II" class="headerlink" title="119-Pascal’s Triangle II"></a>119-Pascal’s Triangle II</h2><p><a href="https://leetcode.com/problems/pascals-triangle-ii/">119-Pascal’s Triangle II</a> </p><p>Problem：</p><p>给定一个数字，输出某一行。</p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getRow</span><span class="params">(self, rowIndex: int)</span> -&gt; List[int]:</span></span><br><span class="line">        temp = [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, rowIndex):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">                temp[j] += temp[j<span class="number">-1</span>]</span><br><span class="line">            temp.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> temp</span><br></pre></td></tr></table></figure><h2 id="169-Majority-Element"><a href="#169-Majority-Element" class="headerlink" title="169-Majority Element"></a>169-Majority Element</h2><p><a href="https://leetcode.com/problems/majority-element/">169-Majority Element</a> </p><p>Problem:</p><p>给一串数字，找到出现次数大于 <code>n/2</code> 的数字。</p><p>Solution：</p><p>用字典计数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">majorityElement</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        cnt = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> ele <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> ele <span class="keyword">in</span> cnt:</span><br><span class="line">                cnt[ele] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cnt[ele] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> max(cnt, key=cnt.get)</span><br></pre></td></tr></table></figure><hr><ol><li>返回值最大/最小的键/索引。<ul><li>列表：<ul><li>最大值的索引：list.index(max(list))</li><li>最小值的索引：list.index(min(list))</li></ul></li><li>字典：<ul><li>最大值的键：max(dict, key=dict.get)</li><li>最小值的键：min(dict, key=dict.get)</li></ul></li></ul></li></ol><h2 id="229-Majority-Element-II"><a href="#229-Majority-Element-II" class="headerlink" title="229-Majority Element II"></a>229-Majority Element II</h2><p><a href="https://leetcode.com/problems/majority-element-ii/">229-Majority Element II</a> </p><p>Problem: </p><p>给一串数字，返回出现次数大于 <code>n/3</code> 的数字。</p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">majorityElement</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        cnt = &#123;&#125;</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> ele <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> ele <span class="keyword">in</span> cnt:</span><br><span class="line">                cnt[ele] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cnt[ele] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> (k, v) <span class="keyword">in</span> cnt.items():</span><br><span class="line">            <span class="keyword">if</span> v &gt; n/<span class="number">3</span>:</span><br><span class="line">                ans.append(k)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><ol><li><p>字典的实用方法：</p><table><thead><tr><th>操作</th><th>实现方法</th></tr></thead><tbody><tr><td>删除字典元素</td><td><code>del dict[&#39;Name&#39;]</code></td></tr><tr><td>清空字典所有条目</td><td><code>dict.clear()</code></td></tr><tr><td>删除字典</td><td><code>del dict</code></td></tr><tr><td>返回指定键的值，如果值不存在返回default的值</td><td><code>dict.get(key, default)</code></td></tr><tr><td>如果键不存在字典中，添加键并将值设为default,于get类似</td><td><code>dict.setdefault(key, default=None)</code></td></tr><tr><td>判读键是否存在</td><td>1. <code>if k in dict</code> 2. <code>dict.has_key(key)</code> 存在返回true</td></tr><tr><td>以列表返回可遍历的（键，值）元祖数组</td><td><code>dict.items()</code></td></tr><tr><td>以列表返回一个字典的所有键</td><td><code>dict.keys()</code></td></tr><tr><td>以列表返回字典中的所有值</td><td><code>dict.values()</code></td></tr><tr><td>返回最大值的键值</td><td><code>max(dict, key=dict.get)</code></td></tr><tr><td>返回最小值的键值</td><td><code>min(dict, key=dict.get)</code></td></tr></tbody></table></li><li><p>遍历字典的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">dict=&#123;<span class="string">"a"</span>:<span class="string">"Alice"</span>,<span class="string">"b"</span>:<span class="string">"Bruce"</span>,<span class="string">"J"</span>:<span class="string">"Jack"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例一：键循环</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dict:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"dict[%s]="</span> % i,dict[i]</span><br><span class="line"><span class="comment"># 结果:</span></span><br><span class="line"><span class="comment"># dict[a]= Alice</span></span><br><span class="line"><span class="comment"># dict[J]= Jack</span></span><br><span class="line"><span class="comment"># dict[b]= Bruce</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例二：键值元组循环</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span>  dict.items():</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="comment"># 结果:</span></span><br><span class="line"><span class="comment"># ('a', 'Alice')</span></span><br><span class="line"><span class="comment"># ('J', 'Jack')</span></span><br><span class="line"><span class="comment"># ('b', 'Bruce')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例三：键值元组循环</span></span><br><span class="line"><span class="keyword">for</span> (k,v) <span class="keyword">in</span>  dict.items():</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"dict[%s]="</span> % k,v</span><br><span class="line"><span class="comment"># 结果:</span></span><br><span class="line"><span class="comment"># dict[a]= Alice</span></span><br><span class="line"><span class="comment"># dict[J]= Jack</span></span><br><span class="line"><span class="comment"># dict[b]= Bruce</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="274-H-Index"><a href="#274-H-Index" class="headerlink" title="274-H-Index"></a>274-H-Index</h2><p><a href="https://leetcode.com/problems/h-index/">274-H-Index</a> </p><p>Problem:</p><p>给出研究人员论文的论文引用次数，计算它的H指数（有h篇论文的引用次数至少为h，剩下N-h篇论文的引用次数不超过h）。</p><p>Solution：</p><p>时间复杂度：$\mathcal{O}(n\log{n})$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>排序后，再二分。（感觉自己的二分写的有点丑qwq</p><p>还有一种思路是，排序完，从最大的h开始递减遍历，满足条件就返回。反正排序也要$\mathcal{O}(n\log{n})$ 的复杂度…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hIndex</span><span class="params">(self, citations: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(citations)</span><br><span class="line">        citations.sort()</span><br><span class="line">        begin = <span class="number">0</span></span><br><span class="line">        end = n - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> begin &lt;= end:</span><br><span class="line">            mid = (begin + end) &gt;&gt; <span class="number">1</span></span><br><span class="line">            h = n - mid</span><br><span class="line">            <span class="keyword">if</span> citations[mid] &gt;= h:</span><br><span class="line">                end = mid</span><br><span class="line">                <span class="keyword">if</span> begin == end:</span><br><span class="line">                    <span class="keyword">return</span> h</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                begin += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> n-begin</span><br></pre></td></tr></table></figure><hr><ol><li>非递归写二分：<code>while begin &lt;= end</code> </li></ol><h2 id="275-H-Index-II"><a href="#275-H-Index-II" class="headerlink" title="275-H-Index II"></a>275-H-Index II</h2><p><a href="https://leetcode.com/problems/h-index-ii/">275-H-Index II</a> </p><p>Problem:</p><p>和274一样，给了递增的论文引用数，希望能用指数时间返回H指数。</p><p>Solution：</p><p>啊，就二分鸭。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hIndex</span><span class="params">(self, citations: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(citations)</span><br><span class="line">        begin = <span class="number">0</span></span><br><span class="line">        end = n - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> begin &lt;= end:</span><br><span class="line">            mid = (begin + end) &gt;&gt; <span class="number">1</span></span><br><span class="line">            h = n - mid</span><br><span class="line">            <span class="keyword">if</span> citations[mid] &gt;= h:</span><br><span class="line">                end = mid</span><br><span class="line">                <span class="keyword">if</span> begin == end:</span><br><span class="line">                    <span class="keyword">return</span> h</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                begin += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> n - begin</span><br></pre></td></tr></table></figure><h2 id="243-Shortest-Word-Distance"><a href="#243-Shortest-Word-Distance" class="headerlink" title="243-Shortest Word Distance"></a>243-Shortest Word Distance</h2><p>qwq 这道题还收费来着，于是于是就开了个中国区的会员（中国区的会员便宜好多啊！！）</p><p><a href="https://leetcode-cn.com/problems/shortest-word-distance/">243-Shortest Word Distance</a> </p><p>Problem：</p><p>给定一串单词，单词1和单词2，计算单词1单词2在单词列表中的距离。</p><p>Solution：</p><table><thead><tr><th>Solution</th><th>Runtime</th><th>Memory</th><th>Language</th></tr></thead><tbody><tr><td>S1-二分查找</td><td>44ms</td><td>15.7MB</td><td>python3</td></tr><tr><td>S2-线性维护</td><td>40ms</td><td>15.6MB</td><td>python3</td></tr></tbody></table><h3 id="S1-二分查找"><a href="#S1-二分查找" class="headerlink" title="S1-二分查找"></a>S1-二分查找</h3><p>时间复杂度：$\mathcal{O}(n\log{n})$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>（最开始还很疑惑啥是单词距离…单词1和单词2可能在单词列表中重复出现）</p><p>计算出单词1和单词2在单词列表中出现的索引值列表，是递增有序的。</p><p>对于单词1索引列表中的每个值，在单词2索引列表中查找该值的lower_bound，计算距离。</p><p>同理，对于单词2索引列表中的每个值，也同样计算距离。</p><p>找出最小距离。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span><span class="params">(a: list, x: int)</span> -&gt; int:</span></span><br><span class="line">    n = len(a)</span><br><span class="line">    begin = <span class="number">0</span></span><br><span class="line">    end = n - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> begin &lt;= end:</span><br><span class="line">        mid = (begin + end) &gt;&gt; <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> a[mid] &gt;= x:</span><br><span class="line">            end = mid</span><br><span class="line">            <span class="keyword">if</span> begin == end:</span><br><span class="line">                <span class="keyword">return</span> begin</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            begin += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    ans = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findShortest</span><span class="params">(self, li1: list, li2: list)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> li1:</span><br><span class="line">            min_dis_idx = lower_bound(li2, idx)</span><br><span class="line">            <span class="keyword">if</span> min_dis_idx == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.ans = min(self.ans, li2[min_dis_idx] - idx)</span><br><span class="line">                <span class="keyword">if</span> self.ans == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortestDistance</span><span class="params">(self, words: List[str], word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        n = len(words)</span><br><span class="line">        li1 = []</span><br><span class="line">        li2 = []</span><br><span class="line">        self.ans = n</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># li1 and li2 are ordered</span></span><br><span class="line">            <span class="keyword">if</span> words[idx] == word1:</span><br><span class="line">                li1.append(idx)</span><br><span class="line">            <span class="keyword">if</span> words[idx] == word2:</span><br><span class="line">                li2.append(idx)</span><br><span class="line">        self.findShortest(li1, li2)</span><br><span class="line">        <span class="keyword">if</span> self.ans &gt; <span class="number">1</span>:</span><br><span class="line">            self.findShortest(li2, li1)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.ans</span><br></pre></td></tr></table></figure><h3 id="S2-线性维护："><a href="#S2-线性维护：" class="headerlink" title="S2-线性维护："></a>S2-线性维护：</h3><p>时间复杂度：$\mathcal{O}(n)$  </p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>问题还能再简化，线性扫描单词列表，维护两个变量，单词1出现的最近索引，单词2出现的最近索引。扫描时计算距离，每当单词1或单词2出现时，就用另一个单词的最近索引计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortestDistance</span><span class="params">(self, words: List[str], word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        n = len(words)</span><br><span class="line">        lst1 = <span class="literal">None</span></span><br><span class="line">        lst2 = <span class="literal">None</span></span><br><span class="line">        ans = n</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> words[idx] == word1:</span><br><span class="line">                lst1 = idx</span><br><span class="line">                <span class="keyword">if</span> lst2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    ans = min(ans, lst1-lst2)</span><br><span class="line">            <span class="keyword">if</span> words[idx] == word2:</span><br><span class="line">                lst2 = idx</span><br><span class="line">                <span class="keyword">if</span> lst1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    ans = min(ans, lst2-lst1)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><ol><li><p>C++中：</p><p>lower_bound(begin, end, num)：返回num的下界，即大于等于num的第一个索引位置。</p><p>upper_bound(begin, end, num)：返回num的上界，即大于num的第一个索引位置。</p><p>Python中用二分实现这两个函数。 </p></li></ol><h2 id="244-Shortest-Word-Distance-II"><a href="#244-Shortest-Word-Distance-II" class="headerlink" title="244-Shortest Word Distance II"></a>244-Shortest Word Distance II</h2><p><a href="https://leetcode-cn.com/problems/shortest-word-distance-ii/">244-Shortest Word Distance II</a></p><p>Problem:</p><p>和上题题干类似，计算单词距离，但是此问是每一个单词列表，可能有多个询问。</p><p>Solution：</p><p>对每一个单词列表，都可能有多个询问。</p><p>因此，之前243的解法每次询问都会遍历一遍单词列表。如果对每个单词列表询问数为 $M$ ，那么时间复杂度为 $\mathcal{O}(NM)$ ，会超时，所以希望能将单词列表的有关信息存下来，再用常数时间处理每一个询问。</p><p>这里的解法是用一个字典把每个单词出现的index列表存下来，键是单词，值是index列表。这个列表相对于单词列表的数目应该是远远小于的，因此用二重循环应该也能过吧（没有尝试二重循环解法）</p><p>这里有两种思路，一种是自己想的归并思路，还有一种是官方解答的思路，官方思路比归并的思路更优雅一些，问题抽象的更好。（代码差距不大，时间差距也不太大）</p><table><thead><tr><th>Solution</th><th>Runtime</th><th>Memory</th><th>Language</th></tr></thead><tbody><tr><td>S1-归并思路查询</td><td>96ms</td><td>20.8MB</td><td>python3</td></tr><tr><td>S2-交叉跳跃查询</td><td>80ms</td><td>20.4MB</td><td>python3</td></tr></tbody></table><p>双指针：$i$ 指向列表1，$j$指向列表2.</p><h3 id="S1-归并思路"><a href="#S1-归并思路" class="headerlink" title="S1-归并思路"></a>S1-归并思路</h3><p>归并思路：列表的值都是有序的，再把两个列表的值按归并的思想再排序，可以想成把点一个一个有序放在数轴上。</p><p>$i$指针前进的情况：（排序时，取列表1的下一个数字）</p><ol><li>$i+1&lt;len1$  and $li1[i+1] &lt; li[j]$</li><li>$i+1&lt;len1$  and $li1[i+1] &lt; li[j+1]$ </li><li>$i+1 &lt; len1$ and $j+1 == len2$  ($j$ 已无法移动)</li></ol><p>其余情况：$j$ 移动。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordDistance</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, words: List[str])</span>:</span></span><br><span class="line">        self.words = words</span><br><span class="line">        self.len = len(words)</span><br><span class="line">        self.dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(self.len):</span><br><span class="line">            word = words[index]</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> self.dict:</span><br><span class="line">                temp = self.dict[word]</span><br><span class="line">                temp.append(index)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                temp = [index]</span><br><span class="line">                self.dict[word] = temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortest</span><span class="params">(self, word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        ans = self.len</span><br><span class="line">        li1 = self.dict[word1]</span><br><span class="line">        li2 = self.dict[word2]</span><br><span class="line">        len1 = len(li1)</span><br><span class="line">        len2 = len(li2)</span><br><span class="line">        i = j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len1 <span class="keyword">and</span> j &lt; len2:</span><br><span class="line">            ans = min(ans, abs(li1[i] - li2[j]))</span><br><span class="line">            <span class="keyword">if</span> ans == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> ans</span><br><span class="line">            <span class="comment"># i goes ahead</span></span><br><span class="line">            <span class="keyword">if</span> i + <span class="number">1</span> &lt; len1 <span class="keyword">and</span> ((li1[i + <span class="number">1</span>] &lt; li2[j]) <span class="keyword">or</span> (j+<span class="number">1</span> == len2) <span class="keyword">or</span> (j + <span class="number">1</span> &lt; len2 <span class="keyword">and</span> li1[i + <span class="number">1</span>] &lt; li2[j + <span class="number">1</span>])):</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h3 id="S2-交叉比较"><a href="#S2-交叉比较" class="headerlink" title="S2-交叉比较"></a>S2-交叉比较</h3><p>对于当前指向列表1和列表2的两个元素 $li1[i]$ 和 $li2[j]$ ，对 $li1[i]$来说，只需要和旁边的属于列表2的元素比较，对 $li2[j]$ 同理。</p><p>因此，当  $li1[i]&gt;li2[j]$ 时，下一次的比较应该让 $j$ 指针前移一位，继续计算指针 $i$ 所指元素和其旁边的列表2的元素。同理，当 $li1[i]&lt;li2[j]$ 时，下一次的比较应该让 $i$ 指针前移一位，继续计算指针 $j$ 和其旁边的列表1的元素。</p><p>具体移动如下图。</p><img src="https://s1.ax1x.com/2020/09/19/wIuW4K.jpg" alt="wIuW4K.jpg" style="zoom:50%;" /><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordDistance</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, words: List[str])</span>:</span></span><br><span class="line">        self.words = words</span><br><span class="line">        self.len = len(words)</span><br><span class="line">        self.dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(self.len):</span><br><span class="line">            word = words[index]</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> self.dict:</span><br><span class="line">                temp = self.dict[word]</span><br><span class="line">                temp.append(index)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                temp = [index]</span><br><span class="line">                self.dict[word] = temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortest</span><span class="params">(self, word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        ans = self.len</span><br><span class="line">        li1 = self.dict[word1]</span><br><span class="line">        li2 = self.dict[word2]</span><br><span class="line">        len1 = len(li1)</span><br><span class="line">        len2 = len(li2)</span><br><span class="line">        i = j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len1 <span class="keyword">and</span> j &lt; len2:</span><br><span class="line">            ans = min(ans, abs(li1[i] - li2[j]))</span><br><span class="line">            <span class="keyword">if</span> ans == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> ans</span><br><span class="line">            <span class="comment"># i goes ahead</span></span><br><span class="line">            <span class="keyword">if</span> li1[i] &lt; li2[j]:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="277-Find-the-Celebrity"><a href="#277-Find-the-Celebrity" class="headerlink" title="277-Find the Celebrity"></a>277-Find the Celebrity</h2><p><a href="https://leetcode-cn.com/problems/find-the-celebrity/">277-Find the Celebrity</a> </p><p>Problem: </p><p>已有know(i, j) API，判断i是否知道j，i是名人的充要条件是其他所有人知道i，而i不知道其他所有人。</p><p>Solution：</p><p>两种思路，第一种较为直观，使用二重循环，但剪枝多，远达不到 $\mathcal{O}(n^2)$ ，第二种稍做优化。因此两种解法差距不太大。</p><table><thead><tr><th align="left">提交时间</th><th align="left">运行时间</th><th align="left">内存消耗</th><th>语言</th></tr></thead><tbody><tr><td align="left">S几秒前</td><td align="left">1896ms</td><td align="left">13.6MB</td><td>python3</td></tr><tr><td align="left">5 分钟前</td><td align="left">1772ms</td><td align="left">13.6MB</td><td>python3</td></tr></tbody></table><h3 id="S1-直观思路"><a href="#S1-直观思路" class="headerlink" title="S1-直观思路"></a>S1-直观思路</h3><p>时间复杂度：远不到 $\mathcal{O}(n^2)$ </p><p>用了二重循环，但剪枝很多，所以远达不到 $\mathcal{O}(n^2)$ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findCelebrity</span><span class="params">(self, n: int)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            fg = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># i knows j ?</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> i == j:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> knows(i, j):</span><br><span class="line">                    fg = <span class="literal">False</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> fg:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># j knows i ?</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> i == j:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> knows(j, i):</span><br><span class="line">                    fg = <span class="literal">False</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> fg:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h3 id="S2-排除法"><a href="#S2-排除法" class="headerlink" title="S2-排除法"></a>S2-排除法</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>排除i：根据<code>know(i, j)=True</code> 可以认为i不是名人，j可能是名人。</p><p>对于n-1个关系，最后从n个人中选出一个可能的人，再根据名人的充要条件去判断他是否是名人。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findCelebrity</span><span class="params">(self, n: int)</span> -&gt; int:</span></span><br><span class="line">        celebrity = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> knows(celebrity, i):</span><br><span class="line">                celebrity = i</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> celebrity == i:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">not</span> knows(celebrity, i)) <span class="keyword">and</span> knows(i, celebrity):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">return</span> celebrity</span><br></pre></td></tr></table></figure><h2 id="245-Shortest-Word-Distance-III"><a href="#245-Shortest-Word-Distance-III" class="headerlink" title="245-Shortest Word Distance III"></a>245-Shortest Word Distance III</h2><p><a href="https://leetcode-cn.com/problems/shortest-word-distance-iii/">245-Shortest Word Distance III</a> </p><p>Problem:</p><p>题意增加了两个单词可能相同，分两种情况就好了。</p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortestWordDistance</span><span class="params">(self, words: List[str], word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        n = len(words)</span><br><span class="line">        ptr1 = <span class="literal">None</span></span><br><span class="line">        ptr2 = <span class="literal">None</span></span><br><span class="line">        ans = n</span><br><span class="line">        <span class="keyword">if</span> word1 == word2:</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> words[idx] == word1:</span><br><span class="line">                    ptr1, ptr2 = idx, ptr1</span><br><span class="line">                <span class="keyword">if</span> (ptr1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (ptr2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">                    ans = min(ans, ptr1-ptr2)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> words[idx] == word1:</span><br><span class="line">                    ptr1 = idx</span><br><span class="line">                <span class="keyword">if</span> words[idx] == word2:</span><br><span class="line">                    ptr2 = idx</span><br><span class="line">                <span class="keyword">if</span> (ptr1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (ptr2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">                    ans = min(ans, abs(ptr1-ptr2))</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="217-Contains-Duplicate-E"><a href="#217-Contains-Duplicate-E" class="headerlink" title="217-Contains Duplicate[E]"></a>217-Contains Duplicate[E]</h2><p><a href="https://leetcode.com/problems/contains-duplicate/">217-Contains Duplicate</a> </p><p>Problem:</p><p>判断数组中有无重复元素出现。</p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsDuplicate</span><span class="params">(self, nums: List[int])</span> -&gt; bool:</span></span><br><span class="line">        S = set()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> S:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            S.add(i)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h2 id="219-Contains-Duplicate-II-E"><a href="#219-Contains-Duplicate-II-E" class="headerlink" title="219-Contains Duplicate II[E]"></a>219-Contains Duplicate II[E]</h2><p><a href="https://leetcode.com/problems/contains-duplicate-ii/">219-Contains Duplicate II</a> </p><p>Problem:</p><p>判断数组中是否有两个相同的值，他们的索引之差小于等于k。</p><p>Solution：</p><p>存放出现该值的最近的索引，扫一遍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyDuplicate</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; bool:</span></span><br><span class="line">        dict = &#123;&#125;</span><br><span class="line">        n = len(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> nums[i] <span class="keyword">not</span> <span class="keyword">in</span> dict:</span><br><span class="line">                dict.setdefault(nums[i], i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> i - dict[nums[i]] &lt;= k:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                dict[nums[i]] = i</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h2 id="4-Median-of-Two-Sorted-Arrays-H-（2S）"><a href="#4-Median-of-Two-Sorted-Arrays-H-（2S）" class="headerlink" title="4-Median of Two Sorted Arrays[H] （2S）"></a>4-Median of Two Sorted Arrays[H] （2S）</h2><p><a href="https://leetcode.com/problems/median-of-two-sorted-arrays/">4-Median of Two Sorted Arrays[H]</a> </p><p>Problem:</p><p>给两个排好序的数组，返回一个中位数</p><p>Solution：</p><table><thead><tr><th>S</th><th>运行时间</th><th>内存消耗</th></tr></thead><tbody><tr><td>S1</td><td>$\mathcal{O}(m+n)$ ：92ms</td><td>14.3MB</td></tr><tr><td>S2</td><td>$\mathcal{O}(\log(m+n))$ ：52ms</td><td>13.3MB</td></tr></tbody></table><h3 id="S1"><a href="#S1" class="headerlink" title="S1:"></a>S1:</h3><p>归并排序的做法。</p><p>时间复杂度：$\mathcal{O}(m+n)$ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMedianSortedArrays</span><span class="params">(self, nums1: List[int], nums2: List[int])</span> -&gt; float:</span></span><br><span class="line">        n1 = len(nums1)</span><br><span class="line">        n2 = len(nums2)</span><br><span class="line">        median1 = median2 = <span class="literal">None</span></span><br><span class="line">        i = j = <span class="number">0</span></span><br><span class="line">        tot = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; n1 <span class="keyword">or</span> j &lt; n2:</span><br><span class="line">            <span class="keyword">if</span> (i &lt; n1 <span class="keyword">and</span> j &lt; n2 <span class="keyword">and</span> nums1[i] &lt;= nums2[j]) <span class="keyword">or</span> (i &lt; n1 <span class="keyword">and</span> j &gt;= n2):</span><br><span class="line">                tot += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> tot == (n1 + n2)//<span class="number">2</span>:</span><br><span class="line">                    median1 = nums1[i]</span><br><span class="line">                <span class="keyword">if</span> tot == (n1 + n2)//<span class="number">2</span> + <span class="number">1</span>:</span><br><span class="line">                    median2 = nums1[i]</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tot += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> tot == (n1 + n2) // <span class="number">2</span>:</span><br><span class="line">                    median1 = nums2[j]</span><br><span class="line">                <span class="keyword">if</span> tot == (n1 + n2) // <span class="number">2</span> + <span class="number">1</span>:</span><br><span class="line">                    median2 = nums2[j]</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (n1 + n2) % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> median2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (median1 + median2)/<span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="S2"><a href="#S2" class="headerlink" title="S2:"></a>S2:</h3><p>二分的思路</p><p>时间复杂度：$\mathcal{O}(\log(m+n))$ </p><p><strong>一、</strong>首先讨论一个数组的中位数，数组有n个元素，如果n为奇数，则第(n+1)/2个是中位数；如果n为偶数，则第(n+1)/2和第(n+2)/2的平均值为中位数。</p><p>回到本题，因为是两个数组，如果根据奇偶性分类讨论就过于麻烦了，所以将两种情况统一以简化解题思路。</p><p>即无论n是奇数还是偶数，数组的中位数都是第(n+1)/2和第(n+2)/2的平均数。</p><p>回到本题，设数组1有n个元素，数组2有m个元素，则中位数为两个数组的有序序列的第(n+m+1)/2个和第(n+m+2)/2个的平均数。</p><p><strong>二、</strong>因此，题目需要求解的问题改为求这两个有序数组的有序序列的第k个数。</p><p>二分思想：两个数组分别找第k/2个数，（假设都存在），比较，<strong>如果第一个数组的这个数小于第二个数组，说明第k个数肯定不在第一个数组的前k/2个数中，因此就可以直接去掉数组1的前k/2个元素</strong>，查找有序序列的第k-k/2个数；同理，如果大于，则说明第k个数肯定不在第二个数组的前k/2个数中，去掉数组2的前k/2个元素。</p><p>使用一个数组起始指针l1和l2来实现数组的“去掉”前k/2个元素。</p><p>设数组1的元素个数为n，数组2的元素个数为m。</p><p><strong>递归函数Find(l1, l2, k)：查找起始指针为l1, l2的两个有序数组的第k个数。</strong></p><ul><li><p>讨论边界情况，有数组为空的情况。即 <code>l1 == n</code>  或者 <code>l2 == m</code> .</p><p>如果第一个数组已为空，则直接返回第二个数组的第k个数；</p><p>同理，如果第二个数组为空，则直接返回第一个数组的第k个数。</p></li><li><p>两个数组都不为空的情况。即 <code>l1 &lt; n</code>  或者 <code>l2 &lt; m</code> .</p><ul><li><p>递归边界： <code>k == 1</code>  ,即返回 <code>nums1[l1]</code> 和 <code>nums2[l2]</code> 中较小的那一个。</p></li><li><p>数组长度边界：即有数组的剩余元素个数小于 <code>k/2</code> ，那么拿出来比较的就应该是数组的最后一个元素。</p><p>维护两个值 <code>k1</code> 和 <code>k2</code> 来分别表示用两个数组的第 k1和k2个来比较。</p><p>(k1 k2都小于等于k/2)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># to avoid the rest length of nums1/nums2 is shorter than k//2</span></span><br><span class="line">k1 = k//<span class="number">2</span> <span class="keyword">if</span> l1+k//<span class="number">2</span> &lt;= n <span class="keyword">else</span> n-l1</span><br><span class="line">k2 = k//<span class="number">2</span> <span class="keyword">if</span> l2+k//<span class="number">2</span> &lt;= m <span class="keyword">else</span> m-l2</span><br></pre></td></tr></table></figure></li><li><p>比较<code>nums1[l1+k1-1]</code> 和 <code>nums2[l2+k2-1]</code> 的大小，递归：</p><ol><li><p>相等：</p><p>如果 <code>k-k1-k2 == 0</code> 说明nums1的前k1个和nums2的前k2个就是有序序列的前k个，返回 <code>nums1[l1+k1-1]</code> 。</p><p>否则，（即某一个数组的剩余长度小于k/2），分别去掉两个数组的前k1和k2个数，递归调用 <strong>Find(l1+k1, l2+k2, k-k1-k2)</strong> 。</p></li><li><p><code>nums1[l1+k1-1] &gt; nums2[l2+k2-1]</code>  </p><p>说明可以去掉数组2的前k2个数，递归调用 <strong>Find(l1, l2+k2, k-k2)</strong> </p></li><li><p><code>nums1[l1+k1-1] &gt; nums2[l2+k2-1]</code>  </p><p>说明可以去掉数组1的前k1个数，递归调用 <strong>Find(l1+k1, l2, k-k1)</strong> </p></li></ol></li></ul></li></ul><p>Code：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.nums1 = <span class="literal">None</span></span><br><span class="line">        self.nums2 = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findKthOfTwo</span><span class="params">(self, l1: int, l2: int, k: int)</span> -&gt; int:</span></span><br><span class="line">        nums1 = self.nums1</span><br><span class="line">        nums2 = self.nums2</span><br><span class="line">        n = len(nums1)</span><br><span class="line">        m = len(nums2)</span><br><span class="line">        <span class="comment"># nums1 is empty</span></span><br><span class="line">        <span class="keyword">if</span> l1 == n:</span><br><span class="line">            <span class="keyword">return</span> nums2[l2+k<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># nums2 is empty</span></span><br><span class="line">        <span class="keyword">if</span> l2 == m:</span><br><span class="line">            <span class="keyword">return</span> nums1[l1+k<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># both not empty</span></span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> nums1[l1] <span class="keyword">if</span> nums1[l1] &lt;= nums2[l2] <span class="keyword">else</span> nums2[l2]</span><br><span class="line">        <span class="comment"># to avoid the rest length of nums1/nums2 is shorter than k//2</span></span><br><span class="line">        k1 = k//<span class="number">2</span> <span class="keyword">if</span> l1+k//<span class="number">2</span> &lt;= n <span class="keyword">else</span> n-l1</span><br><span class="line">        k2 = k//<span class="number">2</span> <span class="keyword">if</span> l2+k//<span class="number">2</span> &lt;= m <span class="keyword">else</span> m-l2</span><br><span class="line">        <span class="keyword">if</span> nums1[l1+k1<span class="number">-1</span>] == nums2[l2+k2<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">return</span> nums1[l1+k1<span class="number">-1</span>] <span class="keyword">if</span> k-k1-k2 == <span class="number">0</span> <span class="keyword">else</span> self.findKthOfTwo(l1+k1, l2+k2, k-k1-k2)</span><br><span class="line">        <span class="keyword">elif</span> nums1[l1+k1<span class="number">-1</span>] &gt; nums2[l2+k2<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">return</span> self.findKthOfTwo(l1, l2+k2, k-k2)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.findKthOfTwo(l1+k1, l2, k-k1)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMedianSortedArrays</span><span class="params">(self, nums1: List[int], nums2: List[int])</span> -&gt; float:</span></span><br><span class="line">        n = len(nums1)</span><br><span class="line">        m = len(nums2)</span><br><span class="line">        self.nums1 = nums1</span><br><span class="line">        self.nums2 = nums2</span><br><span class="line">        <span class="comment"># median: the average of (n+m+1)//2 th  and  (n+m+2)//2 th</span></span><br><span class="line">        <span class="keyword">return</span> (self.findKthOfTwo(<span class="number">0</span>, <span class="number">0</span>, (n+m+<span class="number">1</span>)//<span class="number">2</span>) + self.findKthOfTwo(<span class="number">0</span>, <span class="number">0</span>, (n+m+<span class="number">2</span>)//<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ol><li>《信息安全数学基础》 2.2同余类和剩余系。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;8月某司实训+准备开学期末考，我可太咕了q w q…dbq，（希望）高产博主我.我..又回来了。&lt;/p&gt;
&lt;p&gt;LeetCode Array专题，持久更新。（&lt;a href=&quot;https://github.com/f1ed/LeetCode&quot;&gt;GitHub&lt;/a&gt;)&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/categories/LeetCode/"/>
    
    
      <category term="Algorithms" scheme="https://f7ed.com/tags/Algorithms/"/>
    
      <category term="LeetCode" scheme="https://f7ed.com/tags/LeetCode/"/>
    
      <category term="Array" scheme="https://f7ed.com/tags/Array/"/>
    
      <category term="Data-Structure" scheme="https://f7ed.com/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>「Web」:HTML and CSS</title>
    <link href="https://f7ed.com/2020/07/22/html-css/"/>
    <id>https://f7ed.com/2020/07/22/html-css/</id>
    <published>2020-07-21T16:00:00.000Z</published>
    <updated>2020-07-25T03:44:57.007Z</updated>
    
    <content type="html"><![CDATA[<p>温故知新：对Web基础知识——HTML和CSS的持续更新。</p><a id="more"></a><h1 id="说在前面"><a href="#说在前面" class="headerlink" title="说在前面"></a>说在前面</h1><h3 id="B-S-软件结构"><a href="#B-S-软件结构" class="headerlink" title="B/S 软件结构"></a>B/S 软件结构</h3><p>C/S： Client Server（JavaSE）</p><p>B/S：Browser Server（JavaEE）</p><h3 id="前端开发流程"><a href="#前端开发流程" class="headerlink" title="前端开发流程"></a>前端开发流程</h3><ol><li>美术实现：网页设计</li><li>前端工程师：设计为静态网页</li><li>Java程序员：后端工程师修改为动态页面</li></ol><h3 id="网页端组成部分"><a href="#网页端组成部分" class="headerlink" title="网页端组成部分"></a>网页端组成部分</h3><p>内容：页面中可以看到的数据。一般使用html技术。</p><p>表现：内容在页面上的展示形式。一般使用CSS。</p><p>行为：页面中的元素与输入设备交互。一般使用javascript技术。</p><h1 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h1><h2 id="创建HTML文件"><a href="#创建HTML文件" class="headerlink" title="创建HTML文件"></a>创建HTML文件</h2><ol><li>创建一个Web静态工程</li><li>在工程下创建html页面</li></ol><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span><span class="comment">&lt;!--声明--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"zh_CN"</span>&gt;</span><span class="comment">&lt;!--html中包含两部分：head和body--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="comment">&lt;!--head中包含：title标签、CSS样式、js代码--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">Hello World!</span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="HTML标签"><a href="#HTML标签" class="headerlink" title="HTML标签"></a>HTML标签</h2><ul><li>标签名大小写不敏感</li><li>标签有自己的属性<ul><li>基本属性：修改简单样式</li><li>事件属性：设置事件响应后的代码</li></ul></li><li>标签分为单标签&lt;标签/&gt;和双标签&lt;标签&gt;&lt;/标签&gt;</li><li>标签的属性必须要有值，属性值加双引号。</li><li>显示特殊标签：&lt; &gt; 空格等等，建议查阅文档。</li></ul><h3 id="字体标签"><a href="#字体标签" class="headerlink" title="字体标签"></a>字体标签</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">"red"</span> <span class="attr">size</span>=<span class="string">"7"</span>&gt;</span></span><br><span class="line">        哒哒哒。</span><br><span class="line">    <span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="标题标签：h1-到-h6"><a href="#标题标签：h1-到-h6" class="headerlink" title="标题标签：h1 到 h6"></a>标题标签：h1 到 h6</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">h1</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span>标题1<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h2</span> <span class="attr">align</span>=<span class="string">"left"</span>&gt;</span>标题2<span class="tag">&lt;/<span class="name">h2</span>&gt;</span><span class="comment">&lt;!--align：显示位置,默认左--&gt;</span></span><br></pre></td></tr></table></figure><h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://baidu.com"</span> <span class="attr">target</span>=<span class="string">"_self"</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="comment">&lt;!--_self属性：当前窗口跳转--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://baidu.com"</span> <span class="attr">target</span>=<span class="string">"_blank"</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="comment">&lt;!--_blank属性：打开新窗口跳转--&gt;</span></span><br></pre></td></tr></table></figure><h3 id="列表标签"><a href="#列表标签" class="headerlink" title="列表标签"></a>列表标签</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">ul</span> <span class="attr">type</span>=<span class="string">"none"</span>&gt;</span><span class="comment">&lt;!--无序列表--&gt;</span><span class="comment">&lt;!--type属性可以更改列表前的符号--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ol</span>&gt;</span><span class="comment">&lt;!--有序表格--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>1<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>2<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>3<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>4<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ol</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="img标签"><a href="#img标签" class="headerlink" title="img标签"></a>img标签</h3><ul><li><p>属性src:图片等路径位置</p></li><li><p>JavaSE中路径</p><ul><li>相对路径：从工程名字开始算</li><li>绝对路径：硬盘中的路径</li></ul></li><li><p>Web中的路径</p><ul><li><p>相对路径：</p><p><code>.</code>    ：表示当前文件所在的目录</p><p><code>..</code>   ：表示当前文件所在的上级目录</p><p><code>文件名</code>：表示当前文件所在目录的文件，相当于<code>./文件名</code></p></li><li><p>绝对路径：<code>http://ip:port/工程名/资源路径</code></p></li></ul></li><li><p>属性：weight; height; </p><ul><li>border：设置图片边框大小。</li><li>alt：当指定路径找不到图片时，用来代替显示的文本内容。</li></ul></li></ul><h3 id="表格标签：实现跨行跨列"><a href="#表格标签：实现跨行跨列" class="headerlink" title="表格标签：实现跨行跨列"></a>表格标签：实现跨行跨列</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span> <span class="attr">border</span>=<span class="string">"1"</span> <span class="attr">width</span>=<span class="string">"300"</span>&gt;</span><span class="comment">&lt;!--表格标签--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--border：设置边框、width：设置宽度、height：设置高度--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--align：设置表格对齐方式--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--cellspacing:单元格间距--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span><span class="comment">&lt;!--行标签--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">th</span>&gt;</span>h1<span class="tag">&lt;/<span class="name">th</span>&gt;</span><span class="comment">&lt;!--表头标签--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">th</span>&gt;</span>h2<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">th</span>&gt;</span>h3<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>1.1<span class="tag">&lt;/<span class="name">td</span>&gt;</span><span class="comment">&lt;!--单元格标签--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span>1.2<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--align：设置单元格文本对齐方式--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>1.3<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span> <span class="attr">colspan</span>=<span class="string">"2"</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">td</span>&gt;</span><span class="comment">&lt;!--colspan:列的宽度,实现单元格跨列--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span> <span class="attr">rowspan</span>=<span class="string">"2"</span>&gt;</span>2.2<span class="tag">&lt;/<span class="name">td</span>&gt;</span><span class="comment">&lt;!--rowspan:行的宽度，实现单元格跨行--&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span> <span class="attr">colspan</span>=<span class="string">"2"</span>&gt;</span>3.1<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="iframe框架标签"><a href="#iframe框架标签" class="headerlink" title="iframe框架标签"></a>iframe框架标签</h3><p>可以在html页面上开辟一个小区域加载单独的页面，实现内嵌窗口。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">iframe</span> <span class="attr">src</span>=<span class="string">"hello.html"</span> <span class="attr">width</span>=<span class="string">"400"</span> <span class="attr">height</span>=<span class="string">"600"</span> <span class="attr">name</span>=<span class="string">"abc"</span>&gt;</span><span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--name：表示该区域的名字--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"welcome.html"</span> <span class="attr">target</span>=<span class="string">"abc"</span>&gt;</span>欢迎<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--target：打开窗口显示的位置--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--a标签的target属性设置为iframe的name属性，就在开辟的区域打开链接窗口--&gt;</span></span><br></pre></td></tr></table></figure><h3 id="表单标签"><a href="#表单标签" class="headerlink" title="表单标签"></a>表单标签</h3><p>表单：html中用来收集用户信息的元素集合，将这些信息发送给服务器处理。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span>&gt;</span><span class="comment">&lt;!--表单标签--&gt;</span></span><br><span class="line">    用户名称：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">value</span>=<span class="string">"User"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span><span class="comment">&lt;!--input输入框标签--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--type：输入类型 value：默认值--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--text：文本类型--&gt;</span></span><br><span class="line">    用户密码：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> /&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--password：密码类型--&gt;</span></span><br><span class="line">    确认密码：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    性别：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>男</span><br><span class="line">     <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span>/&gt;</span>女<span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--radio：单选框; name属性：可对其分组; checked：默认选项--&gt;</span></span><br><span class="line">    兴趣爱好：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>Java</span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span>/&gt;</span>JavaScript<span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--checkbox：复选框; checked:默认选项--&gt;</span></span><br><span class="line">    国籍：</span><br><span class="line">        <span class="tag">&lt;<span class="name">select</span>&gt;</span><span class="comment">&lt;!--下拉列表框标签--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">option</span>&gt;</span>--请选择国籍--<span class="tag">&lt;/<span class="name">option</span>&gt;</span><span class="comment">&lt;!--选项标签--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">option</span> <span class="attr">selected</span>=<span class="string">"selected"</span>&gt;</span>中国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--selected：默认选择--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">option</span>&gt;</span>美国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">option</span>&gt;</span>日本<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">select</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    自我评价：<span class="tag">&lt;<span class="name">textarea</span> <span class="attr">rows</span>=<span class="string">"10"</span> <span class="attr">cols</span>=<span class="string">"30"</span>&gt;</span>默认值<span class="tag">&lt;/<span class="name">textarea</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--textarea标签：多行文本输入框；属性 rows:行数; 属性 cols：列数--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--textarea起始标签和结束标签中的内容是默认值--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"reset"</span> <span class="attr">value</span>=<span class="string">"重新输入"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--reset：重置按钮; value属性：更改按钮文本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"submit"</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--submit：提交按钮--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">value</span>=<span class="string">"按钮"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--button:按钮--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"file"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--file:文件上传--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"hidden"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--hidden:隐藏域，需要发送一些不需要用户参与的信息至服务器，可使用隐藏域--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="表单格式化"><a href="#表单格式化" class="headerlink" title="表单格式化"></a>表单格式化</h3><p>把表单放入表格，使表单排列整齐。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">table</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户名称：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">value</span>=<span class="string">"User"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> /&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>确认密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>性别：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>男</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span>/&gt;</span>女<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="表单提交的细节"><a href="#表单提交的细节" class="headerlink" title="表单提交的细节"></a>表单提交的细节</h3><p>以下格式化的表单：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"https://localhost:8080"</span> <span class="attr">method</span>=<span class="string">"get"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--form标签属性--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--action：设置提交的服务器地址--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--method：设置提交的方式，默认GET（也可以是POST）--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"hidden"</span> <span class="attr">name</span>=<span class="string">"action"</span> <span class="attr">value</span>=<span class="string">"login"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">table</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户名称：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">value</span>=<span class="string">"User"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> /&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>确认密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>性别：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>男</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span>/&gt;</span>女<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>兴趣爱好：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>Java</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span>/&gt;</span>JavaScript</span><br><span class="line">            <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>国籍：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">select</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span>&gt;</span>--请选择国籍--<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">selected</span>=<span class="string">"selected"</span>&gt;</span>中国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span>&gt;</span>美国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span>&gt;</span>日本<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>自我评价:<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">textarea</span> <span class="attr">rows</span>=<span class="string">"10"</span> <span class="attr">cols</span>=<span class="string">"30"</span>&gt;</span>默认值<span class="tag">&lt;/<span class="name">textarea</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"reset"</span> <span class="attr">value</span>=<span class="string">"重新输入"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"submit"</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><p>格式化后的表单显示为：</p><img src="https://s1.ax1x.com/2020/07/25/UznJiT.png" alt="UznJiT.pndg" style="zoom:50%;" /> <p>表单提交后，url显示为：<code>https://localhost:8080/?action=login&amp;sex=on</code> </p><p>该url体现了三部分</p><ul><li>提交表单的服务器地址/action属性的值：localhost:8080/</li><li>分隔符：<code>?</code></li><li>请求参数/表单信息：action=login; sex=on</li></ul><hr><p><strong>表单提交的时候，数据没有发送给服务器的三种情况：</strong></p><ol><li>表单项input标签没有name属性值。</li><li>单选、复选输入标签以及下拉列表的option标签，还需要加value属性值，以便发送给服务器具体值，而不是on。</li><li>表单项不在提交的form标签中。</li></ol><p>修改后的表单代码如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"https://localhost:8080"</span> <span class="attr">method</span>=<span class="string">"get"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--action：设置提交的服务器地址--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--method：设置提交的方式，默认GET--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"hidden"</span> <span class="attr">name</span>=<span class="string">"action"</span> <span class="attr">value</span>=<span class="string">"login"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">table</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户名称：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"user"</span> <span class="attr">value</span>=<span class="string">"User"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> <span class="attr">name</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>确认密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> <span class="attr">name</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>性别：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">checked</span>=<span class="string">"checked"</span> <span class="attr">value</span>=<span class="string">"boy"</span>/&gt;</span>男</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">value</span>=<span class="string">"girl"</span>/&gt;</span>女<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>兴趣爱好：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">checked</span>=<span class="string">"checked"</span> <span class="attr">name</span>=<span class="string">"hobby"</span> <span class="attr">value</span>=<span class="string">"Java"</span>/&gt;</span>Java</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">name</span>=<span class="string">"hobby"</span> <span class="attr">value</span>=<span class="string">"js"</span>/&gt;</span>JavaScript</span><br><span class="line">            <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>国籍：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">select</span> <span class="attr">name</span>=<span class="string">"country"</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">"none"</span>&gt;</span>--请选择国籍--<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">selected</span>=<span class="string">"selected"</span> <span class="attr">value</span>=<span class="string">"中国"</span>&gt;</span>中国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">"美国"</span>&gt;</span>美国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">"日本"</span>&gt;</span>日本<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>自我评价:<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">textarea</span> <span class="attr">rows</span>=<span class="string">"10"</span> <span class="attr">cols</span>=<span class="string">"30"</span>&gt;</span>默认值<span class="tag">&lt;/<span class="name">textarea</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"reset"</span> <span class="attr">value</span>=<span class="string">"重新输入"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"submit"</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><p>表单提交后的url: <code>https://localhost:8080/?action=login&amp;user=fred&amp;password=123&amp;password=123&amp;sex=girl&amp;hobby=Java&amp;hobby=js&amp;country=中国</code> </p><hr><p><strong>表单标签method属性参数的区别</strong></p><ul><li><p>GET：</p><ol><li><p>浏览器的地址栏为：action属性值 + ? + 请求参数</p><p>请求参数格式为：<code>name=value&amp;name=value</code></p></li><li><p>不安全</p></li><li><p>有数据长度限制</p></li></ol></li><li><p>POST请求的特点：</p><ol><li>浏览器上的地址栏为：action属性值（没有请求参数）</li><li>相当于GET请求更安全</li><li>理论上没有数据长度限制</li></ol></li></ul><h3 id="div和span"><a href="#div和span" class="headerlink" title="div和span"></a>div和span</h3><ul><li>div 标签：默认独占一行</li><li>span 标签：长度是封装数据长度</li><li>p 标签：默认在段落的上方或下方各空出一行（如果已有空行则不空）</li></ul><h3 id="label标签"><a href="#label标签" class="headerlink" title="label标签"></a>label标签</h3><p>label标签为input元素定义标注。</p><p>该标签不会为用户呈现特殊的效果，但为鼠标用户改进了可用性，即在label元素内点击文本，就会触发该控件。即当用户选择该标签时，浏览器会自动将焦点转到和label标签绑定的表单项上。</p><p>常见的应用情况是：单选框/复选框，点击文本即可勾选，而不需要去点那个框。</p><ul><li>for : 表示该label是为表单中哪个控件服务，for属性点值设置为该元素的id属性值</li></ul><h1 id="CSS"><a href="#CSS" class="headerlink" title="CSS"></a>CSS</h1><h2 id="CSS简介"><a href="#CSS简介" class="headerlink" title="CSS简介"></a>CSS简介</h2><p>CSS：层叠样式表单，用于增强/控制网页样式，且允许将样式信息和网页内容分离的一种标记性语言。</p><p>语法规则：</p><img src="https://s1.ax1x.com/2020/07/25/UznYJU.png" alt="UznYJU.png" style="zoom:50%;" /> <ul><li><p>选择器：浏览器根据选择器决定受CSS样式影响到HTML元素/标签。</p></li><li><p>属性：<code>属性:值;</code> 形成一个完成的declaration。</p></li></ul><p>CSS中的注释：/**/</p><h2 id="CSS与HTML结合方式"><a href="#CSS与HTML结合方式" class="headerlink" title="CSS与HTML结合方式"></a>CSS与HTML结合方式</h2><h3 id="标签中的style"><a href="#标签中的style" class="headerlink" title="标签中的style"></a>标签中的style</h3><p>在标签的style属性设置<code>style=&quot;key: value1 value2;&quot;</code> </p><p>这种方式可读性差，且没有复用性。</p><h2 id="head标签中使用style标签"><a href="#head标签中使用style标签" class="headerlink" title="head标签中使用style标签"></a>head标签中使用style标签</h2><p>在head标签中，用style标签定义需要的css样式。</p><p>style标签中的语句是CSS语法。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span>&gt;</span></span><br><span class="line">        div&#123;</span><br><span class="line">            border: 1px solid red;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br></pre></td></tr></table></figure><p>可以在同一页面复用代码，不能在多个页面复用CSS代码，且维护不方便，需要修改每个页面。</p><h3 id="CSS文件"><a href="#CSS文件" class="headerlink" title="CSS文件"></a>CSS文件</h3><p>把CSS样式写成CSS文件，在html文件的head标签中通过link标签引用。</p><p>style.css</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">div</span>&#123;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">1px</span> red solid;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-tag">span</span>&#123;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">1px</span> red solid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>div.html</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--link标签专门在head中用来引入CSS样式代码--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"style.css"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--rel:文档间的关系--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--type:目标URL的类型--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--href:URL--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br></pre></td></tr></table></figure><p>可以在多个页面中复用CSS样式，且维护方便。</p><h2 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h2><h3 id="标签名选择器"><a href="#标签名选择器" class="headerlink" title="标签名选择器"></a>标签名选择器</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">标签名&#123;</span><br><span class="line">    属性:值;</span><br><span class="line">    属性:值;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>标签名选择器决定哪些标签被动的使用这个样式。</p><h3 id="id选择器"><a href="#id选择器" class="headerlink" title="id选择器"></a>id选择器</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#id</span>选择器&#123;</span><br><span class="line">    属性:值;</span><br><span class="line">    属性:值;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>id选择器通过id属性选择性的使用这个样式。</p><p>html文件</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"id001"</span>&gt;</span>div1<span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="comment">&lt;!--标签的id属性--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"id002"</span>&gt;</span>div2<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>CSS文件：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="selector-tag">style</span>&gt;</span><br><span class="line">        <span class="selector-id">#id001</span>&#123;</span><br><span class="line">            <span class="attribute">border</span>: yellow <span class="number">1px</span> solid;</span><br><span class="line">            <span class="attribute">font-size</span>: <span class="number">30px</span>;</span><br><span class="line">            <span class="attribute">color</span>: blue;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="selector-id">#id001</span>&#123;</span><br><span class="line">            <span class="attribute">border</span>: <span class="number">5px</span> blue dotted;</span><br><span class="line">            <span class="attribute">font-size</span>: <span class="number">20px</span>;</span><br><span class="line">            <span class="attribute">color</span>: red;</span><br><span class="line">        &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure><h3 id="class-选择器"><a href="#class-选择器" class="headerlink" title="class 选择器"></a>class 选择器</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.class</span>属性值&#123;</span><br><span class="line">    属性:值;</span><br><span class="line">    属性:值;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>class属性多用来分组定义CSS样式。</p><p>class选择器通过class属性值选择性使用这个样式。</p><p>html文件</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"class0"</span>&gt;</span>div1<span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="comment">&lt;!--标签的class属性--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"class0"</span>&gt;</span>div2<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>CSS文件：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="selector-tag">style</span>&gt;</span><br><span class="line">    <span class="selector-class">.div</span>&#123;</span><br><span class="line">        <span class="attribute">color</span>: blue;</span><br><span class="line">        <span class="attribute">font-size</span>: <span class="number">30px</span>;</span><br><span class="line">        <span class="attribute">border</span>: <span class="number">1px</span> yellow solid;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure><h3 id="组合选择器"><a href="#组合选择器" class="headerlink" title="组合选择器"></a>组合选择器</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.class0</span>, <span class="selector-id">#id001</span>&#123;</span><br><span class="line">    <span class="attribute">color</span>: blue;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">30px</span>;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">1px</span> yellow solid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>组合选择器可以让多个选择器共用同样的CSS样式。</p><h2 id="常用样式"><a href="#常用样式" class="headerlink" title="常用样式"></a>常用样式</h2><p>具体可查阅</p><ul><li><p>字体颜色</p><p><code>color : red;</code> </p><p><code>color : rgb(33,33,13);</code>  </p><p> <code>color : #00F666;</code>  </p></li><li><p>宽度</p><p><code>width : 19px;</code>  </p><p><code>width : 20%;</code> </p></li><li><p>高度</p><p><code>height : 19px;</code>  </p><p><code>height : 20%;</code> </p></li><li><p>背景颜色</p><p><code>background-color : #0F2222;</code> </p></li><li><p>字体大小</p><p><code>font-size : 20px;</code> </p></li><li><p>边框</p><p><code>border : 1px solid red;</code> </p></li><li><p>DIV居中（相当于页面的居中）</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">margin-left</span> : <span class="selector-tag">auto</span>;</span><br><span class="line"><span class="selector-tag">margin</span> <span class="selector-tag">-right</span> : <span class="selector-tag">auto</span>;</span><br></pre></td></tr></table></figure></li><li><p>文本居中</p><p><code>text-align : center;</code> </p></li><li><p>超链接去下划线</p><p><code>text-decoration : none;</code></p></li><li><p>表格细线</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">table</span>&#123;</span><br><span class="line">    <span class="attribute">border </span>: <span class="number">1px</span> solid black;</span><br><span class="line">    <span class="attribute">border-collapse </span>: collapse;<span class="comment">/*合并表格边框*/</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-tag">td</span>,<span class="selector-tag">th</span>&#123;</span><br><span class="line">    <span class="attribute">border </span>: <span class="number">1px</span>, solid black;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>列表去修饰符</p><p><code>list-style : none</code> </p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;温故知新：对Web基础知识——HTML和CSS的持续更新。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tools" scheme="https://f7ed.com/categories/Tools/"/>
    
    
      <category term="Web" scheme="https://f7ed.com/tags/Web/"/>
    
      <category term="HTML" scheme="https://f7ed.com/tags/HTML/"/>
    
      <category term="CSS" scheme="https://f7ed.com/tags/CSS/"/>
    
  </entry>
  
  <entry>
    <title>「Tools」：Docker</title>
    <link href="https://f7ed.com/2020/07/21/Docker/"/>
    <id>https://f7ed.com/2020/07/21/Docker/</id>
    <published>2020-07-20T16:00:00.000Z</published>
    <updated>2020-07-25T03:34:59.360Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章主要分四个部分，首先介绍了Docker是什么：为什么会有Docker技术的出现；虚拟化技术和容器虚拟化技术的区别；Docker的基本组成；Docker的运行为什么会比虚拟机快。</p><p>第二个部分主要介绍了Docker的常用命令，包括镜像命令和容器命令，文中还从底层的角度分析Docker镜像。</p><p>第三个部分介绍了Docker中的容器数据卷，和如何挂载数据卷。</p><p>最后一个部分，简单介绍了Dockerfile文件。</p><a id="more"></a><h1 id="Docker简介"><a href="#Docker简介" class="headerlink" title="Docker简介"></a>Docker简介</h1><h2 id="Docker-是什么"><a href="#Docker-是什么" class="headerlink" title="Docker 是什么"></a>Docker 是什么</h2><p>开发和运维之间的环境和配置问题：在我的机器上可以正常工作。</p><p>把代码/配置/系统/数据等全部打包成镜像，运维工程师带环境安装软件。</p><p>Docker基于Go语言实现的云开源项目，Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，做到一次封装，处处运行。</p><p>Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用运行在 Docker 容器上面，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作。</p><p>Docker解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体分布的容器虚拟化技术。</p><h2 id="能干嘛？"><a href="#能干嘛？" class="headerlink" title="能干嘛？"></a>能干嘛？</h2><h3 id="之前的虚拟化技术"><a href="#之前的虚拟化技术" class="headerlink" title="之前的虚拟化技术"></a>之前的虚拟化技术</h3><p>虚拟机是带环境安装的解决方案，可以在一种操作系统中运行另一种操作系统。</p><p>虚拟机用软件实现了硬件、内核、操作系统及应用程序，对底层来说，虚拟机就是一个普通文件。</p><p>虚拟机的缺点缺点：</p><ol><li>资源占用多</li><li>冗余步骤</li><li>启动慢</li></ol><h3 id="容器虚拟化技术"><a href="#容器虚拟化技术" class="headerlink" title="容器虚拟化技术"></a>容器虚拟化技术</h3><p>Linux容器（Linux Containers,LXC)，对进程隔离，将软件运行所需的资源打包到一个隔离的痛其中。</p><img src="https://s1.ax1x.com/2020/07/25/UzVUbV.png" alt="UzVUbV.png" style="zoom:50%;" /><p>Linux容器不是模拟一个完整的操作系统，而是将软件工作所需的库资源和设置等资源打包到一个隔离的容器中，因此Linux容器变得高效且轻量，并且能保证部署在任何环境中的软件都能始终如一地运行。在</p><p>宿主机上，Linux容器就是一个运行的进程，所以Linux容器是对进程进行隔离。</p><p>再看Docker的图标，上面的集装箱就是一个一个容器，鲸鱼就是宿主机的硬件、内核。</p><p>比较：</p><ol><li>传统虚拟机技术虚拟一套硬件，在其上运行一个完整的操作系统，再运行所需的应用进程。</li><li>容器内的应用直接运行于宿主的内核，容器内没有硬件虚拟，容器更轻便。</li><li>容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响。</li></ol><p>所以，可以认为容器是一个轻量的Linux。</p><h3 id="开发-运维（DevOps"><a href="#开发-运维（DevOps" class="headerlink" title="开发/运维（DevOps)"></a>开发/运维（DevOps)</h3><p>DevOps, Develop and Operations, 可以利用Docker实现开发自运维。</p><ol><li>更快速的应用交付和部署。</li><li>更便捷的升级和扩缩容器。</li><li>更简单的系统运维。</li><li>更高效的计算资源利用。</li></ol><h2 id="Docker的基本组成"><a href="#Docker的基本组成" class="headerlink" title="Docker的基本组成"></a>Docker的基本组成</h2><p><strong>Docker的三要素：</strong></p><ol><li>镜像(image)：只读的模版，类比Java中的类。镜像可以用来创造Docker容器。</li><li>容器(container)：镜像的实例，独立运行的一个或一组实例。可以把容器看作一个简易版的Linux环境。</li><li>仓库(repository)：保存镜像的场所。</li></ol><p>Docker本身是一个容器运行载体或管理引擎。</p><p>把应用程序和配置打包成为一个可交付的运行环境，打包好的运行环境就是一个image镜像文件，只有通过这个镜像文件才能生成Docker容器。image文件可以看作是容器的模版。Docker根据image文件生成容器的实例。</p><h2 id="Docker运行原理"><a href="#Docker运行原理" class="headerlink" title="Docker运行原理"></a>Docker运行原理</h2><p>Docker是一个C/S结构的系统。</p><p>Docker守护进程运行在宿主机上，客户通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。</p><h3 id="为什么比虚拟机快"><a href="#为什么比虚拟机快" class="headerlink" title="为什么比虚拟机快"></a>为什么比虚拟机快</h3><ol><li>Docker有比虚拟机更少的抽象层，不需要实现硬件资源虚拟化，运行在docker容器中的程序直接使用的都是实际物理机的硬件资源。</li><li>Docker使用宿主机上的内核，新建容器时，不需要和虚拟机一样重新加载一个操作系统内核。因此新建一个dock er容器只需要几秒钟。</li></ol><h2 id="Docker镜像加速"><a href="#Docker镜像加速" class="headerlink" title="Docker镜像加速"></a>Docker镜像加速</h2><p>可以登陆阿里云获得专属镜像加速器链接，配置本机Docker拉取镜像仓库的链接，将拉取镜像的链接从DockerHub换成阿里云的仓库，下载更快捷。</p><p>具体按照系统自行Google。</p><h1 id="Docker常用命令"><a href="#Docker常用命令" class="headerlink" title="Docker常用命令"></a>Docker常用命令</h1><p>docker version</p><p>docker info</p><p>docker –help 帮助命令</p><h2 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h2><ul><li><p>列出本地images</p><p><code>docker images</code></p><p>repo</p><ul><li>参数<ul><li>-a :包括中间映像层</li><li>-q : 只显示镜像id</li><li>–digests :显示摘要信息</li><li>–no-trunc :显示完整信息</li></ul></li></ul></li><li><p>从Docker Hub查询镜像名</p><p><code>docker search [OPTIONS] image_name</code> </p><ul><li>–no-trunc </li><li>-s n：收藏数不小于n的镜像</li><li>–automated</li></ul></li><li><p>下载/拉取镜像</p><p><code>docker pull 镜像名[:TAG]</code> </p><p>默认:latest</p></li><li><p>删除镜像</p><p><code>docker rmi 镜像唯一名字/镜像ID</code></p><p>-f :强制删除运行中的镜像文件</p><ul><li><p>删除单个：</p><p> <code>docker rmi -f 镜像ID</code></p></li><li><p>删除多个</p><p><code>docker rmi -f 镜像名1:TAG 镜像名2:TAG</code></p></li><li><p>删除全部：</p><p><code>docker rmi -f $(docker images -qa)</code> </p></li></ul></li></ul><h2 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h2><p>容器是一个建议的Linux。</p><ul><li><p>启动容器：</p><p><code>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</code></p><ul><li><code>--name 容器名</code> :为容器指定一个名字</li><li><code>-d</code> ：后台运行容器，返回</li><li><code>-i</code> : 以<strong>交互模式</strong>运行容器，通常与<code>-t</code> 一同使用</li><li><code>-t</code> :为容器重新分配一个伪输入终端，通常与<code>-i</code> 一同使用。</li><li><code>-p</code>  :主机端口和容器端口<ul><li><code>-p ip:hostPort:containerPort</code> </li><li><code>-p ip::containerPort</code> </li><li><code>-p hostPort:containerPort</code> </li><li><code>-p containerPort</code> </li></ul></li><li><code>-P</code> :随机分配端口</li></ul></li><li><p>列出当前运行所有容器：</p><p><code>docker ps</code> </p><ul><li><code>-a</code> : 列出当前所有正在运行的容器和历史上运行过的容器</li><li><code>-l</code> :显示最近创建的容器</li><li><code>-n</code> :显示最近创建的num个容器<ul><li><code>docker ps -n 3</code> </li></ul></li><li><code>-q</code> :静默模式，只显示容器编号</li><li><code>--no-trunc</code> : 不截断输出</li></ul></li><li><p>退出/停止容器</p><ul><li><p>容器停止退出</p><p><code>exit</code> </p></li><li><p>容器不停止退出</p><p>Ctrl + P + Q</p></li></ul></li><li><p>启动容器</p><p><code>docker start 容器名/容器ID</code> </p></li><li><p>重启容器</p><p><code>docker restart 容器名/容器ID</code> </p><p>重启成功后返回容器名/容器ID</p></li><li><p>停止容器</p><p><code>docker stop 容器名/容器ID</code></p></li><li><p>强制停止容器</p><p><code>docker kill 容器名/容器ID</code> </p></li><li><p>删除已停止的容器</p><p><code>docker rm 镜像ID</code> </p><ul><li><p>一次删除多个容器</p><p><code>docker rm -f $(docker ps -a -q)</code> </p><p><code>docker ps -a -q | xargs docker rm</code>  （管道传递参数）</p></li></ul></li></ul><hr><ul><li><p>启动守护式容器</p><p><code>docker run -d 镜像名/镜像ID</code> </p><p><code>docker run -d -p 主机端口:容器内端口 容器ID</code> </p><ul><li><p>如果使用 <code>docker ps -a</code> 查看，会发现容器已经退出</p></li><li><p><strong>Docker容器后台运行，就必须要有一个前台进程与之交互</strong> </p><p>如果容器后台运行，如果不是一直挂起的命令，他就会自动退出。</p></li><li><p>所以最佳的解决方式是将运行的进程以前台进程运行。</p></li></ul></li><li><p>查看容器日志</p><p><code>docker logs -f -t --tail 容器ID</code> </p><ul><li><code>-t</code>：显示加入时间戳</li><li><code>-f</code> ：持续显示最新的日志</li><li><code>--tail</code> ：显示最后多少条</li></ul></li><li><p>显示容器内运行的进程</p><p><code>docker top 容器ID</code> </p></li><li><p>查看容器内部的细节</p><p><code>docker inspect 容器ID</code> </p></li><li><p>进入正在运行的容器并以命令行与之交互</p><ul><li><p>直接进入容器启动命令的终端</p><p><code>docker attach 容器ID</code> </p></li><li><p>在容器中打开新的终端，并且可以启动新的进程。</p><p><code>docker exec -it 容器ID bashShell</code></p><p><code>docker exec -it 容器ID /bin/bash</code>  和<code>docker attach 容器ID</code> 相同。</p></li></ul></li><li><p>把容器内文件拷贝文件到主机上</p><p><code>docker cp 容器ID:容器内的路径 目录主机路径</code> </p><p><code>docker cp 130b1f6708dd:/x.txt /Users</code> </p></li></ul><h1 id="Docker镜像"><a href="#Docker镜像" class="headerlink" title="Docker镜像"></a>Docker镜像</h1><p>image：</p><p>镜像是轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，包含运行某个软件所需的所有内容，包括代码、库、环境变量、配置文件等。</p><h2 id="UnionFS"><a href="#UnionFS" class="headerlink" title="UnionFS"></a>UnionFS</h2><p>UnionFS（联合文件系统）是一种分层、轻量高性能的文件系统，支持对文件系统的修改作为一次提交来一层层的叠加，同时将不同目录挂载到同一个虚拟文件系统下。</p><p>Union文件系统时Docker镜像的基础。</p><p>镜像通过分层来进行继承，基于基础镜像可以制作各种具体的应用镜像。</p><p>特点：一次加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，最终的文件系统包含所有底层的文件和目录。</p><h2 id="Docker镜像的加载"><a href="#Docker镜像的加载" class="headerlink" title="Docker镜像的加载"></a>Docker镜像的加载</h2><p>Docker镜像实际是由一层一层的文件系统组成。</p><p>bootfs(boot file system)包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。</p><p>Docker镜像的最底层就是bootfs，这一层和典型的Linux/Unix系统是一样的，包含bootloader和kernel。</p><p>当boot加载完成后，整个kernel就在内存中了，此时内存的使用权已由bootfs转交给kernel，此时系统也会卸载bootfs。</p><p>rootfs（root file system)，在bootfs之上，包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Linux，Centos等。</p><p>平常安装等虚拟机的CentOS都是几个G，为什么docker版的centos只有几百兆？</p><p>对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库，因为底层直接使用宿主机的kernel，自己只需要提供rootfs就行了。</p><p>因此，对于不同的Linux发行版，bootfs基本一致，rootfs会有差别，因此不同的发行版可以共用bootfs。</p><h2 id="分层的镜像"><a href="#分层的镜像" class="headerlink" title="分层的镜像"></a>分层的镜像</h2><p>在docker image下载、删除时，可以发现是一层一层的。</p><p>分层的镜像的一个最大的好处是共享资源。</p><p>如果有多个镜像都是从相同的base镜像build而来，那宿主机中只需在磁盘上保存一份base镜像，同时内存中也只需要加载一份base镜像，就可以为所有的容器服务了。</p><h2 id="镜像commit操作"><a href="#镜像commit操作" class="headerlink" title="镜像commit操作"></a>镜像commit操作</h2><p>Docker镜像都是只读的，但当镜像实例化，启动容器时，一个新的可写层被加载到镜像的顶部，这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。</p><p>docker commit提交容器层副本使之成为一个新的镜像。</p><p><code>docker commit -m &quot;message&quot;  -a &quot;author&quot; 容器ID 命名空间/新建镜像名[:TAGS]</code> </p><h1 id="容器数据卷"><a href="#容器数据卷" class="headerlink" title="容器数据卷"></a>容器数据卷</h1><p>Docker理念：</p><p>将代码和运行的环境打包形成容器，运行伴随着容器，但希望运行中的数据是持久化的，希望容器之间是共享数据的。</p><p>如果不通过docker commit生成新的镜像，使得数据作为镜像的一部分保存下来，那么容器删除后，数据也没有了，为了保存数据，使用容器数据卷。</p><p>如果不使用commit 生成新的镜像，Docker容器产生的数据将随着容器的删除而一起删除，为了保存数据，我们使用卷。</p><h2 id="卷"><a href="#卷" class="headerlink" title="卷"></a>卷</h2><p>卷就是目录或者文件，存在于一个或多个容器中，由docker挂载到容器，但不属于UnionFS（联合文件系统），因此能绕过UnionFS，提供一些用于持续存储或共享数据的特性。</p><p>卷的设计目的就是为了数据持久化，完全独立于容器的生存周期，因此Docker不会在容器删除的时候删除其挂载的数据卷。</p><p>数据卷的特点：</p><ol><li>数据卷可以在容器之间共享或重用数据。</li><li>卷中的更改直接在所有共享该卷容器中生效。</li><li>数据卷中的更改不会包含在镜像的更新中。</li><li>数据卷的生命周期一直持续到没有容器使用它为止。</li></ol><h2 id="数据卷挂载"><a href="#数据卷挂载" class="headerlink" title="数据卷挂载"></a>数据卷挂载</h2><h3 id="直接命令添加"><a href="#直接命令添加" class="headerlink" title="直接命令添加"></a>直接命令添加</h3><ol><li><p>数据挂载(<code>-v</code> value)</p><p><code>docker run -it -v /宿主机目录:/容器内目录 镜像名</code> </p></li><li><p>查看挂载是否成功</p><p><code>docker inspect 镜像名</code> </p></li><li><p>宿主机和容器之间实现数据共享，在容器停止退出后，修改宿主机数据，数据完全同步。</p></li></ol><ul><li><p>带权限的数据挂载，加<code>:ro</code> (readonly) </p><p><code>docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名</code></p><p>此时容器中对数据卷只读。</p></li></ul><hr><p>当挂载主机目录事，Docker访问出现<code>cannot open directory .: Permission denied</code> </p><p>解决办法：在挂砸目录后加参数 <code>--privileged=true</code> </p><h3 id="DockerFile添加"><a href="#DockerFile添加" class="headerlink" title="DockerFile添加"></a>DockerFile添加</h3><p>在DockerFile中可以使用<code>VOLUME</code> 指令给镜像添加一个或多个数据卷。</p><p>注意：</p><p>Docker出于可移植性和分享的考虑，指令中只有容器内的地址，因为宿主主机目录依赖于特定的主机。</p><ol><li><p>Dockerfile文件构建</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> [<span class="string">"/dataVolumeContainer1"</span>, <span class="string">"/dataVolumeContainer2"</span>, <span class="string">"/dataVolumeContainer3"</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"finished,-----success"</span></span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> /bin/bash</span></span><br></pre></td></tr></table></figure><p>以上docker文件类似于一下命令挂载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -v /host1:/dataVolumeContainer1 -v/host1:/dataVolumeContainer2 -v /host3:/dataVolumeContainer3 centos /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>build构建镜像（<code>-f</code> file)</p><p><code>docker build -f DockerFile文件路径 -t 命名空间/镜像名 镜像生成路径</code></p><p><code>docker build -f ./Dockerfile  -t fred/centos .</code></p></li></ol><h2 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h2><p>数据容器卷：</p><p>命名的容器挂载数据卷，其他容器通过挂载这个父容器实现数据共享，挂载数据卷的容器称为数据卷容器。</p><p><strong>容器之间可以传递配置信息，数据卷的生命周期一直持续到没有容器使用它为止。</strong></p><ol><li><p>挂载数据卷到父容器（命名为<code>dc01</code> ）上：命令添加/Dockerfile添加</p></li><li><p>容器继承父容器的数据卷(<code>--volumes-from</code> )</p><p><code>docker run -it --name 子容器名 --volumes-from 父容器名 生成子容器的镜像名</code> </p><p>e.g: <code>docker run -it --name dc02 --volumes-from dc01 fred/centos</code> </p></li></ol><p>dc01已经挂载数据卷，此时dc02继承它，那么dc01挂载的数据卷，dc02也实现了共享。</p><h1 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h1><p>Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。</p><p>构建容器卷的步骤：</p><ol><li>编写Dockerfile文件</li><li>docker build构建</li><li>docker run启动容器</li></ol><p>Centos的Dockerfile文件</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> scratch</span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> centos-7.8.2003-x86_64-docker.tar.xz /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> \</span></span><br><span class="line"><span class="bash">    org.label-schema.schema-version=<span class="string">"1.0"</span> \</span></span><br><span class="line"><span class="bash">    org.label-schema.name=<span class="string">"CentOS Base Image"</span> \</span></span><br><span class="line"><span class="bash">    org.label-schema.vendor=<span class="string">"CentOS"</span> \</span></span><br><span class="line"><span class="bash">    org.label-schema.license=<span class="string">"GPLv2"</span> \</span></span><br><span class="line"><span class="bash">    org.label-schema.build-date=<span class="string">"20200504"</span> \</span></span><br><span class="line"><span class="bash">    org.opencontainers.image.title=<span class="string">"CentOS Base Image"</span> \</span></span><br><span class="line"><span class="bash">    org.opencontainers.image.vendor=<span class="string">"CentOS"</span> \</span></span><br><span class="line"><span class="bash">    org.opencontainers.image.licenses=<span class="string">"GPL-2.0-only"</span> \</span></span><br><span class="line"><span class="bash">    org.opencontainers.image.created=<span class="string">"2020-05-04 00:00:00+01:00"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure><h2 id="Dockerfile构建过程"><a href="#Dockerfile构建过程" class="headerlink" title="Dockerfile构建过程"></a>Dockerfile构建过程</h2><p><strong>基础规则：</strong></p><ol><li>保留字指令必须大写，且后面必须至少一个参数。</li><li>指令顺序执行。</li><li>注释符号：<code>#</code> </li><li>每条指令都会创建一个新的镜像层，并对该镜像进行提交。</li></ol><p><strong>执行流程：</strong></p><ol><li>从基础镜像运行一个容器</li><li>执行一条指令后并对容器进行修改</li><li>执行类似docker commit操作提交一个新的镜像层</li><li>docker再基于刚提交的镜像运行一个容器</li><li>直到文件所有指令执行完成</li></ol><hr><p>辨析Dockerfile，Docker镜像，Docker容器：</p><p>Dockerfile、Docker镜像与Docker容器从软件应用的角度分别代表软件的三个不同阶段：</p><ol><li><p>Dockerfile是软件的原材料，是面向开发的。</p><p>Dockerfile定义了进程需要的一切东西。Dockerfile设计的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程等等。</p></li><li><p>Docker镜像是软件的交付品，是交付标准。</p><p>在用Dockerfile定义一个文件之后，docker build会产生一个Docker镜像，运行 Docker镜像时，才真正开始提供服务。</p></li><li><p>Docker容器则可以认为是软件的运行态，涉及部署和运维。</p><p>Docker容器是直接提供服务的。</p></li></ol><h2 id="Dockfile体系结构"><a href="#Dockfile体系结构" class="headerlink" title="Dockfile体系结构"></a>Dockfile体系结构</h2><img src="https://s1.ax1x.com/2020/07/25/UzVdET.png" alt="UzVdET.png" style="zoom:50%;" /> <ul><li><p>FROM</p><p>基础镜像</p></li><li><p>MAINTAINER</p><p>镜像维护者的姓名和邮箱地址</p></li><li><p>RUN</p><p>容器构建时需要运行的命令</p></li><li><p>EXPOSE</p><p>当前容器对外暴露的端口号</p></li><li><p>WORKDIR</p><p>指定在创建容器后，终端默认登陆进来的工作目录</p></li><li><p>ENV</p><p>构建容器中的设置环境变量</p></li><li><p>ADD</p><p>将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包</p></li><li><p>COPY</p><p>拷贝文件和目录到镜像中</p><p><code>COPY src dest</code></p><p><code>COPY [&quot;src&quot;, &quot;dest&quot;]</code> </p></li><li><p>VOLUME</p><p>容器数据卷 用于数据保存和持久化工作</p></li><li><p>CMD</p><p>指定一个容器启动时运行的命令</p><ul><li><p>shell 格式：CMD &lt;命令&gt;</p></li><li><p>exec格式：CMD[“可执行文件”, “arg1”, “arg2”,…]</p></li><li><p>参数列表格式：CMD [“arg1”, “arg2”,…] 在指定来ENTRYPOINT指令后，用⌘指定具体的参数。</p></li></ul><p>只有最后一个CMD生效，CMD会被docker run之后的参数替换</p></li><li><p>ENTRYPOINT</p><p>指定一个容器启动时运行的命令</p><p>会在docker run后面追加参数</p></li><li><p>ONBUILD</p><p>当构建一个被继承的Dockerfile时，父镜像在被子镜像继承后父镜像的ONBUILD被触发</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章主要分四个部分，首先介绍了Docker是什么：为什么会有Docker技术的出现；虚拟化技术和容器虚拟化技术的区别；Docker的基本组成；Docker的运行为什么会比虚拟机快。&lt;/p&gt;
&lt;p&gt;第二个部分主要介绍了Docker的常用命令，包括镜像命令和容器命令，文中还从底层的角度分析Docker镜像。&lt;/p&gt;
&lt;p&gt;第三个部分介绍了Docker中的容器数据卷，和如何挂载数据卷。&lt;/p&gt;
&lt;p&gt;最后一个部分，简单介绍了Dockerfile文件。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tools" scheme="https://f7ed.com/categories/Tools/"/>
    
    
      <category term="Tools" scheme="https://f7ed.com/tags/Tools/"/>
    
      <category term="Docker" scheme="https://f7ed.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>「Tools」:Git and GitHub</title>
    <link href="https://f7ed.com/2020/07/18/Git-and-GitHub/"/>
    <id>https://f7ed.com/2020/07/18/Git-and-GitHub/</id>
    <published>2020-07-17T16:00:00.000Z</published>
    <updated>2020-07-25T03:40:33.971Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章具体讲述了Git工具的基本本地库操作和与远程库交互的基本操作，包括使用GitHub进行团队外的协作开发。</p><a id="more"></a><h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><h2 id="Git简介"><a href="#Git简介" class="headerlink" title="Git简介"></a>Git简介</h2><h3 id="Git历史："><a href="#Git历史：" class="headerlink" title="Git历史："></a>Git历史：</h3><p>1991 Linus本人手动合并代码</p><p>2002 商业软件，授予Linux社区免费使用版本控制</p><p>2005 Linus自己用C语言开发了一个分布式版本控制系统：Git</p><p>Talk is cheap, show me the code.</p><p>2008 Github上线</p><hr><p>Git的优势：</p><ul><li><p>大部分操作在本地完成，不需要联网</p></li><li><p>完整性保证：每次提交进行哈希</p></li><li><p>尽可能添加数据而不是删除/修改数据，版本都在</p></li><li><p>分支操作快捷流畅，以快照的形式</p></li><li><p>与Linux命令全面兼容</p></li></ul><h3 id="Git的结构"><a href="#Git的结构" class="headerlink" title="Git的结构"></a>Git的结构</h3><img src="https://s1.ax1x.com/2020/07/18/UcvvkQ.png" alt="Git的结构" style="zoom:50%;" /><h3 id="Git和代码托管中心"><a href="#Git和代码托管中心" class="headerlink" title="Git和代码托管中心"></a>Git和代码托管中心</h3><p>代码托管中心的任务：维护远程库</p><p>局域网环境：搭建GitLab作为代码托管中心</p><p>外网环境：可以用GitHub和码云作为代码托管中心</p><h3 id="本地库和远程库的交互"><a href="#本地库和远程库的交互" class="headerlink" title="本地库和远程库的交互"></a>本地库和远程库的交互</h3><p>团队内：</p><img src="https://s1.ax1x.com/2020/07/18/UcvxYj.png" alt="团队内交互" style="zoom:50%;" /><p>团队外：</p><img src="https://s1.ax1x.com/2020/07/18/Ucvzfs.png" alt="团队外交互" style="zoom:50%;" /><p>fork：复制一份属于自己的远程库</p><p>开发新的内容后向库的拥有者 pull request拉取请求，原拥有者可以审核，审核通过后执行merge操作合并到自己的远程库的分支上。</p><h2 id="Git命令行基本操作"><a href="#Git命令行基本操作" class="headerlink" title="Git命令行基本操作"></a>Git命令行基本操作</h2><h3 id="本地库初始化"><a href="#本地库初始化" class="headerlink" title="本地库初始化"></a>本地库初始化</h3><ul><li><p>初始化本地库</p><p><code>git init</code> </p></li></ul><p>.git文件存放的是本地库相关的子目录和文件，不要删除和随意修改。</p><h3 id="本地库设置签名"><a href="#本地库设置签名" class="headerlink" title="本地库设置签名"></a>本地库设置签名</h3><ul><li><p>形式：</p><p>用户名：</p><p>Email：</p></li><li><p>作用：区分不同开发人员的身份</p></li></ul><p>注：这里设置的签名与远程代码托管中心没有关系。</p><ul><li><p>命令：</p><ul><li><p>项目级别：设置签名仅在本地库起效（如果既有项目级别和用户级别的签名，按照项目级别为准）</p><p>设置用户名命令：<code>git config user.name ***</code></p><p>设置用户邮箱： <code>git config user.email *****@outlook.com</code></p><p>该信息保存在.git/config文件中。</p></li><li><p>用户级别：设置签名在当前操作系统的用户范围</p><p>设置用户名命令：<code>git config --global user.name ***</code></p><p>设置用户邮箱命令：<code>git config --global user.email ****</code></p><p>该消息保存在系统文件~/.gitconfig文件</p></li></ul></li></ul><h3 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h3><ul><li><p>查看工作区、暂存区状态。</p><p> <code>git status</code> </p></li></ul><h3 id="暂存区操作：添加-修改-提交-删除"><a href="#暂存区操作：添加-修改-提交-删除" class="headerlink" title="暂存区操作：添加/修改/提交/删除"></a>暂存区操作：添加/修改/提交/删除</h3><ul><li><p>添加/修改：将工作区的文件添加到暂存区（/或更新暂存区的文件）。</p><p> <code>git add [filename]</code> </p></li><li><p>删除：将文件从暂存区删除</p><p><code>git rm --cached [filename]</code>  </p></li><li><p>提交：将暂存区的文件提交到本地库。（输入提交信息）</p><p><code>git commit -m &quot;commit message&quot; [filename]</code></p></li><li><p>修改后的提交：提交修改后的文件至本地库（已在暂存区有旧版本），同时更新暂存区和本地库。</p><p><code>git commit -m &quot;message&quot; [filename]</code> </p></li></ul><h3 id="本地库版本信息查看"><a href="#本地库版本信息查看" class="headerlink" title="本地库版本信息查看"></a>本地库版本信息查看</h3><p>HEAD: 指针，表示当前版本的位置。</p><p>显示版本：</p><ul><li><p>完整的版本信息记录（包括完整版本哈希值、作者、提交时间）</p><p><code>git log</code> （空格向下翻页；b 向上翻页； q退出显示）</p></li><li><p>一行只显示一个版本，简洁版。</p><p><code>git log --pretty=oneline</code> </p></li><li><p>一行也只显示一个版本，终极简洁版，哈希值也只显示前面的一部分（当作该版本的局部索引）。</p><p><code>git log --oneline</code> </p></li><li><p>HEAD@{i}：i表示HEAD指针移动到该版本需要后退的步数。</p><p><code>git reflog</code> </p></li></ul><h3 id="版本前进-后退"><a href="#版本前进-后退" class="headerlink" title="版本前进/后退"></a>版本前进/后退</h3><p>本质是HEAD指针的移动。</p><ul><li><p>基于索引操作：版本可以后退和前进。(索引就是reflog形式下的局部哈希值)</p><p><code>git reset --hard [局部索引值]</code>  </p></li><li><p>使用<code>^</code> ： 版本只能往后退 。（基于reflog形式下的步数）</p><p><code>git reset --hard HEAD^^</code> (后退两步)</p></li><li><p>使用<code>~n</code> ：版本往后退n步。</p><p><code>git reset --hard HEAD~3</code> </p></li></ul><p><strong>版本前进/后退reset命令的参数对比：</strong></p><ul><li><p><code>--soft</code></p><ul><li><p>仅仅在本地库移动HEAD指针。</p><p>如下图，显得暂存区和工作区版本比本地库前进了一步。</p><img src="https://s1.ax1x.com/2020/07/18/UcvXTg.png" alt="--soft命令" style="zoom:50%;" /></li></ul></li><li><p><code>--mixed</code></p><ul><li><p>在本地库移动HEAD指针</p></li><li><p>并重置暂存区，暂存区和本地库一致。</p><p>如下图，显得工作区版本比本地库和暂存区版本前进了一步。</p><img src="https://s1.ax1x.com/2020/07/18/UcvO0S.png" alt="--mixed命令" style="zoom:50%;" /></li></ul></li><li><p><code>--hard</code></p><ul><li>在本地库移动HEAD指针</li><li>重置暂存区</li><li>重置工作区</li></ul></li></ul><h3 id="删除文件后找回"><a href="#删除文件后找回" class="headerlink" title="删除文件后找回"></a>删除文件后找回</h3><p>前提：删除前，文件存在的状态提交到了本地库。</p><p>操作：</p><ul><li><p>删除的操作已经提交到本地库</p><ul><li><p>删除操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rm a.txt</span><br><span class="line">git add a.txt</span><br><span class="line">git commit -m "delete a.txt" a.txt</span><br></pre></td></tr></table></figure></li><li><p><code>git reset --hard [历史版本指针位置]</code> </p></li></ul></li><li><p>删除操作未提交到本地库</p><ul><li><p>删除操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//工作区删除</span><br><span class="line">rm a.txt</span><br><span class="line">//缓存区也删除</span><br><span class="line">rm a.txt</span><br><span class="line">git add a.txt</span><br></pre></td></tr></table></figure></li><li><p><code>git reset --hard HEAD</code> </p></li></ul></li></ul><h3 id="比较文件差异"><a href="#比较文件差异" class="headerlink" title="比较文件差异"></a>比较文件差异</h3><ul><li><p>工作区文件和暂存区文件比较</p><p><code>git diff [filename]</code> </p></li><li><p>工作区文件和本地库文件比较，指针可以使用<code>HEAD^</code> </p><p><code>git diff [指针] [filename]</code> </p></li><li><p>可以不加文件名，即比较全部文件。</p></li></ul><h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><h3 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h3><p>分支：版本控制过程中，使用多条线同时推进多个任务。 </p><img src="https://s1.ax1x.com/2020/07/18/Ucz94H.png" alt="Ucz94H.png" style="zoom:50%;" /><p>master: 主版本分支/部署到服务器运行的分支。</p><p>feature_ ：开发其他功能的分支。</p><p>hot_fix: 热修复，bug修复分支。</p><p>分支的好处：</p><ul><li>并行：同时并行推进多个功能开发。</li><li>独立：各个分支在开发过程中，如果有个分支开发失败，不会影响其他分支。</li></ul><h3 id="分支操作：创建-查看-切换-合并"><a href="#分支操作：创建-查看-切换-合并" class="headerlink" title="分支操作：创建/查看/切换/合并"></a>分支操作：创建/查看/切换/合并</h3><ul><li><p>创建分支</p><p><code>git branch [branch name]</code> </p></li><li><p>查看分支</p><p><code>git branch -v</code> </p></li><li><p>切换分支</p><p><code>git checkout [branch name]</code> </p></li><li><p>合并分支</p><ol><li><p>切换到接受修改的分支（如master）</p><p><code>git checkout [合并到的主分支]</code> </p></li><li><p>执行merge合并操作</p><p><code>git merge [有修改的分支]</code></p></li></ol></li></ul><h3 id="解决合并分支后产生的冲突"><a href="#解决合并分支后产生的冲突" class="headerlink" title="解决合并分支后产生的冲突"></a>解决合并分支后产生的冲突</h3><p>冲突的表现，显示到有冲突的文件：</p><img src="https://s1.ax1x.com/2020/07/18/UczPCd.png" alt="冲突文件内容" style="zoom:50%;" /><p>冲突解决：</p><ol><li><p>删除文件中的特殊符号</p></li><li><p>协商再编辑文件</p></li><li><p>添加新文件</p><p><code>git add [filename]</code></p></li><li><p>提交（注意：此时的提交不能带文件名）</p><p><code>git commit -m &quot;message&quot;</code> </p></li></ol><h2 id="Git-基本原理"><a href="#Git-基本原理" class="headerlink" title="Git 基本原理"></a>Git 基本原理</h2><h3 id="哈希算法"><a href="#哈希算法" class="headerlink" title="哈希算法"></a>哈希算法</h3><p>特点：</p><ul><li>得到的加密密文长度相同。</li><li>算法确定，输入确定后，输出一定确定。</li><li>输入数据发生一点点变化，输出的变化会很大。</li><li>Git底层采用SHA-1算法。</li></ul><p>哈希算法保证了Git的数据完整性。</p><h3 id="Git保存版本的机制"><a href="#Git保存版本的机制" class="headerlink" title="Git保存版本的机制"></a>Git保存版本的机制</h3><p>集中式版本控制工具（如SVN）：保存的信息是每个基本文件和每个文件随时间逐步累积的差异。</p><p>Git是分布式的版本控制工具。</p><p>Git把数据看作是文件系统的快照（可以理解为当前内存版本的文件的索引），每次提交更新时Git对当前内存的全部文件制作一个快照并保存这个快照的索引。如果文件没有修改，Git不会重新存储该文件，只是保留一个连接指向之前存储的文件。</p><p>Git的提交对象：</p><img src="https://s1.ax1x.com/2020/07/18/UcvLm8.png" alt="UcvLm8.png" style="zoom:50%;" /><p>上图中，每个文件都有一个哈希值/索引，提交时新建一个树结点，其中包含指向每个文件的指针/索引，提交的对象包括该树结点的指针/哈希值。</p><p>Git版本对象链条：</p><img src="https://s1.ax1x.com/2020/07/18/UcvbOf.png" alt="Git版本对象链条" style="zoom:50%;" /><p>所以：</p><p>Git 分支的创建：等于新建一个指向版本的指针。</p><img src="https://s1.ax1x.com/2020/07/18/UcvH6P.png" alt="分支创建" style="zoom:50%;" /><p>Git分支的切换：改变HEAD指针所指的指针。</p><img src="https://s1.ax1x.com/2020/07/18/Ucv7lt.png" alt="分支切换" style="zoom:50%;" /><p>Git分支版本的移动：分支指针的移动。</p><img src="https://s1.ax1x.com/2020/07/18/UcvTSI.png" alt="分支版本移动" style="zoom:50%;" /><h1 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h1><h2 id="基本交互"><a href="#基本交互" class="headerlink" title="基本交互"></a>基本交互</h2><h3 id="创建-查看远程库地址别名"><a href="#创建-查看远程库地址别名" class="headerlink" title="创建/查看远程库地址别名"></a>创建/查看远程库地址别名</h3><p>在GitHub创建远程库后</p><ul><li><p>在本地添加远程库地址别名</p><p><code>git remote add [别名] [https/ssh 地址]</code></p><p><code>git remote add orgin https://...</code>  </p></li><li><p>查看当前所有远程库地址别名</p><p><code>git remote -v</code> </p></li></ul><h3 id="本地库内容推送到远程库"><a href="#本地库内容推送到远程库" class="headerlink" title="本地库内容推送到远程库"></a>本地库内容推送到远程库</h3><p>前提：本地库已添加远程库地址别名。</p><ul><li><p>在本地将本地库推送到远程分支</p><p><code>git push [别名] [分支名]</code> </p><p><code>git push origin master</code> </p></li></ul><h3 id="将远程库克隆到本地库"><a href="#将远程库克隆到本地库" class="headerlink" title="将远程库克隆到本地库"></a>将远程库克隆到本地库</h3><ul><li><code>git clone https/ssh_address</code> </li><li>效果：完整把远程库下载到本地；添加origin作为远程库地址别名；初始化本地库（含有.git文件）</li></ul><h2 id="团队内协作"><a href="#团队内协作" class="headerlink" title="团队内协作"></a>团队内协作</h2><h3 id="团队成员邀请"><a href="#团队成员邀请" class="headerlink" title="团队成员邀请"></a>团队成员邀请</h3><p>项目创建者在项目”Setting”-“Callaborators”里邀请成员。</p><h3 id="拉取：同步本地库"><a href="#拉取：同步本地库" class="headerlink" title="拉取：同步本地库"></a>拉取：同步本地库</h3><ul><li><p>在本地pull操作同步本地库与远程库相同。</p></li><li><p>fetch：查看远程库分支，可以切换至远程库分支，查看远程库分支的文件具体内容，决定是否合并。</p><p><code>git fetch [远程库地址别名] [远程分支名]</code> </p><ul><li><p>切换至远程库分支</p><p><code>git checkout orgin/master</code></p></li></ul></li><li><p>merge：（切换至本地库master分支），合并远程库分支。</p><p><code>git merge [远程库地址别名]/[远程分支名]</code> </p></li><li><p>pull = fetch + merge</p><p><code>git pull [远程库地址别名] [远程分支名]</code> </p></li></ul><p>注：如果是简单的修改，可以直接pull拉取，如果不确定远程库修改内容，可以先fetch后再合并分支。</p><h3 id="本地拉取与远程库冲突"><a href="#本地拉取与远程库冲突" class="headerlink" title="本地拉取与远程库冲突"></a>本地拉取与远程库冲突</h3><ul><li>冲突发生原因：不是基于GitHub远程库的最新版进行修改，就不能push，在修改之前必须pull。</li><li>pull拉取下来后如果进入冲突状态，就按照“分支冲突解决办法”</li></ul><h2 id="跨团队协作"><a href="#跨团队协作" class="headerlink" title="跨团队协作"></a>跨团队协作</h2><ol><li><p>fork操作：复制一份远程库。</p><p>团队外的人，在项目节目点fork，即可fork一份远程库，该远程库的来源是创建该库的开发者，而fork出的远程库的所有者是执行fork操作的人。</p></li><li><p>clone操作：下载到本地库。</p></li><li><p>push操作：本地修改，推送至远程库。</p></li><li><p>pull request 请求：在远程库（代码托管中心GitHub）执行pull request请求，请求合并该修改到原远程库。</p></li><li><p>（原远程库所有者）审核操作：确认是否合并。</p></li></ol><h2 id="SSH登陆"><a href="#SSH登陆" class="headerlink" title="SSH登陆"></a>SSH登陆</h2><ol><li><p>在当前用户的根目录，生产.ssh密钥目录</p><p><code>ssh-keygen -t rsa -C email@address</code> </p></li><li><p>将<code>.ssh/id_rsa.pub</code> 文件的内容复制到GitHub新建ssh密钥的窗口下。</p></li><li><p>创建ssh远程地址别名</p><p><code>git remote add origin ssh_address</code> </p></li></ol><ul><li><p>Git仓库和SSH-key关联</p><p><code>ssh-add &quot;id_rsa address</code> </p></li></ul><h1 id="Git工作流"><a href="#Git工作流" class="headerlink" title="Git工作流"></a>Git工作流</h1><p>待补充[1]</p><h1 id="Gitlab服务器搭建"><a href="#Gitlab服务器搭建" class="headerlink" title="Gitlab服务器搭建"></a>Gitlab服务器搭建</h1><p>待补充[2]</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Git工作流待补充</li><li>Gitlab服务器搭建</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章具体讲述了Git工具的基本本地库操作和与远程库交互的基本操作，包括使用GitHub进行团队外的协作开发。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tools" scheme="https://f7ed.com/categories/Tools/"/>
    
    
      <category term="Tools" scheme="https://f7ed.com/tags/Tools/"/>
    
      <category term="Git" scheme="https://f7ed.com/tags/Git/"/>
    
      <category term="GitHub" scheme="https://f7ed.com/tags/GitHub/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:Semi-supervised Learning</title>
    <link href="https://f7ed.com/2020/07/03/semi-supervised/"/>
    <id>https://f7ed.com/2020/07/03/semi-supervised/</id>
    <published>2020-07-02T16:00:00.000Z</published>
    <updated>2020-07-03T11:32:00.705Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？</p><p>再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。</p><p>对于Generative Model，文章重点讲述了如何用EM算法来训练模型。</p><p>对于Low-density，文章重点讲述了如何让模型进行Self-training，并且在训练中引入Entropy-based Regularization term来尽可能low-density的假设。</p><p>对于Smoothness Assumption，文章重点讲述了Graph-based Approach（基于图的方法），并且在训练中引入Smoothness Regularization term来尽可能满足Smoothness Assumption的假设。</p><p>对于Better Representation，本篇文章只是简单阐述了其思想，具体介绍见这篇博客。</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>什么是Semi-supervised learning(半监督学习)？和Supervised learning（监督式学习）的区别在哪？</p><p><strong>Supervised learning（监督式学习）</strong>：</p><p>用来训练的数据集 $R$  中的数据labeled data，即 ${(x^r,\hat{y}^r)}_{r=1}^R$ .</p><p>比如在图像分类数据集中： $x^r$ 是image，对应的target output $y^r$ 是分类的label。</p><p>而<strong>Semi-supervised learning（半监督式学习）</strong>：</p><p>用来的训练的数据集由两部分组成 $\{(x^r,\hat{y}^r)\}_{r=1}^R$   ,    $\{x^u\}_{u=R}^{R+U}$   ，即labeled data和unlabeled data，而且通常情况下，unlabeled data的数量远远高于labeled data是数量，即 $U&gt;&gt;R$ .</p><p>Semi-supervised learning 又分为两种，Transductive learning （转导/推论推导）和 Inductive learning（归纳推理）</p><ul><li>Transductive learing: unlabeled data is the testing data. 即测试数据在训练中用过。</li><li>Inductive learning: unlabeled data is not the testing data.测试数据是训练中没有用过的数据。</li><li>这里的使用testing data是指用testing data的feature，而不是使用testing data的label。</li></ul><hr><p>为什么会有semi-supervised learning？</p><ul><li><p>Collecting data is easy, but collecting “labelled” data is expensive.</p><p>【收集数据很简单，但收集有label的数据很难】</p></li><li><p>We do semi-supervised learning in our lives</p><p>【在生活中，更多的也是半监督式学习，我们能明白少量看到的事物，但看到了更多我们不懂的，即unlabeled data】</p></li></ul><h2 id="Why-Semi-supervised-learning-helps"><a href="#Why-Semi-supervised-learning-helps" class="headerlink" title="Why Semi-supervised learning helps"></a>Why Semi-supervised learning helps</h2><p>为什么半监督学习能帮助解决一些问题？</p><p>如上图所示，如果只有labeled data，分类所画的boundary可能是一条竖线。</p><img src="https://s1.ax1x.com/2020/07/03/NXRNz6.md.png" alt="NXRNz6.md.png" style="zoom:75%;" /><p>但如果有一些unlabeled data（如灰色的点），分类所画的boundary可能是一条斜线。</p><p>The distribution of the unlabeled data tell us something.</p><p>半监督式学习之所以有用，是因为这些unlabeled data的分布能告诉我们一些东西。</p><p>通常这也伴随着一些假设，所以半监督式学习是否有用往往取决于这些假设是否合理。</p><h1 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h1><h2 id="Supervised-Generative-Model"><a href="#Supervised-Generative-Model" class="headerlink" title="Supervised Generative Model"></a>Supervised Generative Model</h2><p>在<a href="/2020/03/21/Classification1/" title="这篇">这篇</a>文章中，有详细讲述分类问题中的generative model。</p><p>给定一个labelled training data $x^r\in C_1,C_2$ 训练集。</p><p>prior probability（先验概率）有 $P(C_i)$ 和 $P(x|C_i)$ ，假设是Gaussian模型，则 $P(x|C_i)$ 由Gaussian模型中的 $\mu^i,\Sigma$ 参数决定。</p><p>根据已有的labeled data，计算出假设的Gaussian模型的参数（如下图），从而得出prior probability。</p><img src="https://s1.ax1x.com/2020/07/03/NXonAS.md.png" alt="NXonAS.md.png" style="zoom:75%;" /><p>即可算出posterior probability  $P\left(C_{1} \mid x\right)=\frac{P\left(x \mid C_{1}\right) P\left(C_{1}\right)}{P\left(x \mid C_{1}\right) P\left(C_{1}\right)+P\left(x \mid C_{2}\right) P\left(C_{2}\right)}$ </p><h2 id="Semi-supervised-Generative-Model"><a href="#Semi-supervised-Generative-Model" class="headerlink" title="Semi-supervised Generative Model"></a>Semi-supervised Generative Model</h2><p>在只有labeled data的图中，算出来的 $\mu,\Sigma$ 参数如下图所示：</p><img src="https://s1.ax1x.com/2020/07/03/NXonAS.md.png" alt="NXonAS.md.png" style="zoom:75%;" /><p>但如果有unlabeled data（绿色点），会发现分布的模型参数更可能是是下图：</p><img src="https://s1.ax1x.com/2020/07/03/NXRtRx.md.png" alt="NXRtRx.md.png" style="zoom:75%;" /><p>The unlabeled data $x^u$ help re-estimate $P(C_1),P(C_2),\mu^1,\mu^2,\Sigma$ .</p><p>因此，unlabeled data会影响分布，从而影响prior probability，posterior probability，最终影响 boundary。</p><h3 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h3><p>所以有unlabeled data, 这个Semi-supervised 的算法怎么做呢？</p><p>其实就是<strong>EM</strong>（Expected-maximization algorithm，期望最大化算法。）</p><ol><li><p>Initialization : $\theta={P(C_1),P(C_2),\mu^1,\mu^2,\Sigma}$ .</p><p>初始化Gaussian模型参数，可以随机初始，也可以通过labeled data得出。</p><p>虽然这个算法最终会收敛，但是初始化的参数影响收敛结果，就像gradient descent一样。</p></li><li><p>E：Step 1: compute the posterior probability of unlabeled data $P_\theta(C_1|x^u)$ (depending on model $\theta$ )</p><p>根据当前model的参数，计算出unlabeled data的posterior probability $P(C_1|x^u)$ .(以$P(C_1|x^u)$ 为例) </p></li><li><p>M：Step 2: update model. Back to step1 until the algorithm converges enventually.</p><p>用E步得到unlabeled data的posterior probability来最大化极大似然函数，更新得到新的模型参数，公式很直觉。(以 $C_1$ 为例)</p><p>（$N$ ：data 的总数，包括unlabeled data; $N_1$ :label= $C_1$ 的data数）</p><ul><li><p>$P(C_1)=\frac{N_1+\Sigma_{x^u}P(C_1|x^u)}{N}$  </p><p>对比没有unlabeled data之前的式子， $P(C_1)=\frac{N_1}{N}$ ，除了已有label= $C_1$ ，还多了一部分，即unlabeled data中属于 $C_1$ 的概率和。</p></li><li>$\mu^{1}=\frac{1}{N_{1}} \sum_{x^{r} \in C_{1}} x^{r}+\frac{1}{\sum_{x^{u}} P\left(C_{1} \mid x^{u}\right)} \sum_{x^{u}} P\left(C_{1} \mid x^{u}\right) x^{u}$  <p>对比没有unlabeled data的式子 ，$\mu^{1}=\frac{1}{N_{1}} \sum_{x^{r} \in C_{1}} x^{r}$ ，除了已有的label= $C_1$ ，还多了一部分，即unlabeled data的 $x^u$ 的加权平均（权重为 $P(C_1\mid x^u)$ ，即属于 $C_1$ 的概率）。</p></li><li><p>$\Sigma$ 公式也包括了unlabeled data.</p></li></ul></li></ol><p>所以这个算法的Step 1就是EM算法的Expected期望部分，根据已有的labeled data得出极大似然函数的估计值；</p><p>Step 2就是EM算法的Maximum部分，利用unlabeled data（通过已有模型的参数）最大化E步的极大似然函数，更新模型参数。</p><p>最后反复迭代Step 1和Step 2，直至收敛。</p><h3 id="Why-EM"><a href="#Why-EM" class="headerlink" title="Why EM"></a>Why EM</h3><p>[1]挖坑EM详解。</p><p>为什么可以用EM算法来解决Semi-supervised?</p><ul><li><p>只有labeled data</p><p>极大似然函数 $\log{L(\theta)}=\sum_{x^r}\log{P_\theta(x^r,\hat{y}^r)}$ , 其中 $P_\theta(x^r,\hat{y}^r)=P_\theta(x^r\mid \hat{y}^r)P(\hat{y}^r)$ .</p><p>对上式子求导是有closed-form solution的。</p></li><li><p>有labeled data和unlabeled data</p><p>极大似然函数增加了一部分  $\log L(\theta)=\sum_{x^{r}} \log P_{\theta}\left(x^{r}, \hat{y}^{r}\right)+\sum_{x^{u}} \log P_{\theta}\left(x^{u}\right)$ .</p><p>将后部分用全概率展开， $P_{\theta}\left(x^{u}\right)=P_{\theta}\left(x^{u} \mid C_{1}\right) P\left(C_{1}\right)+P_{\theta}\left(x^{u} \mid C_{2}\right) P\left(C_{2}\right)$  .</p><p>如果要求后部分，因为是unlabeled data, 所以模型 $\theta$ 需要得知unlabeled data的label，即 $P(C_1\mid x^u)$ ,而求这个式子，也需要得到 prior probability $P(x^u\mid C_1)$ ,但这个式子需要事先得知模型 $\theta$ ，因此陷入了死循环。</p><p>因此这个极大似然函数不是convex（凸），不能直接求解，因此用迭代的EM算法逐步maximum极大似然函数。</p></li></ul><h1 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h1><p>另一种假设是Low-density Separation的假设，即这个世界是非黑即白的”Black-or-white”。</p><p>两种类别之间是low-density，交界处有明显的鸿沟，因此要么是类别1，要么是类别2，没有第三种情况。</p><h2 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h2><p>对于Low-density Separation Assumption的假设，使用Self-training的方法。</p><p>Given：labeled data set $={(x^r,\hat{y}^r}<em>{r=1}^R$ ,unlabeled data set $={x^u}</em>{u=l}^R+U$ .</p><p><strong>Repeat：</strong> </p><ol><li><p>Train model $f^<em>$ from labeled data set. ($f^</em>$ is independent to the model)</p><p>从labeled data set中训练出一个模型</p></li><li><p>Apply $f^*$ to the unlabeled data set. Obtain pseudo-label ${(x^u,y^u}_{u=l}^{R+U}$ .</p><p>用这个模型 $f^*$ 来预测unlabeled data set， 获得伪label</p></li><li><p>Remove a set of data from unlabeled data set, and add them into the labeled data set.</p><p>拿出一些unlabeled data(pseudo-label)，放到labeled data set中，回到步骤1，再训练。</p><ul><li><p>how to choose the data set remains open</p><p>如何选择unlabeled data 是自设计的</p></li><li><p>you can also provide a weight to each data.</p><p>训练中可以对unlabeled data(pseudo-label)和labeled data 赋予不同的权重.</p></li></ul></li></ol><p><strong>注意：</strong> Regression模型是不能self-training的，因为unlabeled data和其pseudo-label放在模型中的loss为0，无法再minimize。</p><h2 id="Hard-Label"><a href="#Hard-Label" class="headerlink" title="Hard Label"></a>Hard Label</h2><p><strong>V.S.  semi-supervised learning for generative model</strong> </p><p>Semi-supervised learning for generative model和Low-density Separation的区别其实是soft label 和soft label的区别。</p><p>generative model是利用来unlabeled data的 $P(C_1|x^u)$ posterior probability来计算新的prior probability，迭代更新模型。</p><p>而low-density是计算出unlabeled data的pseudo-label，选择性扩大labeled data set(即加入部分由pseudo-label的unlabeled data)来迭代训练模型。</p><p>因此，如果考虑Neural Network：</p><p>($\theta^*$ 是labeled data计算所得的network parameters)</p><p>如下图，unlabeled data $x^u$ 放入模型中预测，得到 $\begin{bmatrix} 0.7 \ 0.3\end{bmatrix}$ .</p><img src="https://s1.ax1x.com/2020/07/03/NXRYJ1.md.png" alt="NXRYJ1.md.png" style="zoom:75%;" /><p>如果是使用hard label，则 $x^u$ 的target是 $\begin{bmatrix} 1 \ 0\end{bmatrix}$ .</p><p>如果是使用soft label，则 $x^u$ 的target是 $\begin{bmatrix} 0.7 \ 0.3\end{bmatrix}$ .</p><p>如果是使用soft label，则self-training不会有效，因为新的data加进去，不会增大模型的loss，也就无法再minimize.</p><p><strong>所以基于Low-density Separation的假设，是非黑即白的，需要使用hard label来self-training。</strong> </p><h2 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h2><p>在训练模型中，我们需要尽量保证unlabeled data在模型中的分布是low-density separation。</p><p>即下图中，unlabeled data得到的pseudo-label的分布应该尽量集中，而不应该太分散。</p><img src="https://s1.ax1x.com/2020/07/03/NXRJiR.md.png" alt="NXRJiR.md.png" style="zoom:50%;" /> <p>所以，在训练中，<strong>如何评估 $y^u$ 的分布的集中度？</strong></p><p>根据信息学，使用 $y^u$ 的entropy，即  $E\left(y^{u}\right)=-\sum_{m=1}^{5} y_{m}^{u} \ln \left(y_{m}^{u}\right)$ </p><p>(注：这里的 $y^u_m$ 应该是  $y^u=m$ 的概率)</p><p>当 $E(y^u)$ 越小，说明 $y^u$ 分布越集中，如下图。</p><img src="https://s1.ax1x.com/2020/07/03/NXR3dJ.md.png" alt="NXR3dJ.md.png" style="zoom:50%;" /><hr><p>因此，在self-training中：</p><p>$L=\sum_{y^r} C(x^r,\hat{y}^r)+\lambda\sum_{x^u}E(y^u)$ </p><p>Loss function的前一项（cross entropy）minimize保证分类的正确性，后一项（entropy of  $y^u$ ) minimize保证 unlabeled data分布尽量集中，最大可能满足low-density separation的假设。</p><p>training：gradient decent.</p><p>因为这样的形式很像之前提到过的regularization(具体见<a href="/2020/04/21/tips-for-DL/" title="这篇文章的3.2">这篇文章的3.2</a>)，所以又叫entropy-based regularization.</p><h2 id="Outlook-Semi-supervised-SVM"><a href="#Outlook-Semi-supervised-SVM" class="headerlink" title="Outlook: Semi-supervised SVM"></a>Outlook: Semi-supervised SVM</h2><p>SVM也是解决semi-supervised learning的方法.</p><img src="https://s1.ax1x.com/2020/07/03/NXRaQK.md.png" alt="NXRaQK.md.png" style="zoom:50%;" /><p>上图中，在有unlabeled data的情况下，希望boundary 分的越开越好（largest margin）和有更小的error.</p><p>因此枚举unlabeled data所有可能的情况，但枚举在计算量上是巨大的，因此SVM（Support Vector Machines）可以实现枚举的目标，但不需要这么大的枚举量。</p><h1 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h1><p>Smoothness Assumption的思想可以用以下话归纳：</p><p>“You are known by the company you keep”</p><p>近朱者赤，近墨者黑。</p><p>蓬生麻中，不扶而直。白沙在涅，与之俱黑。</p><p>Assumption：“similar” $x$ has the same $\hat{y}$ .</p><p>【意思就是说：相近的 $x$ 有相同的label $\hat{y}$ .】</p><p><strong>More precise assumption：</strong></p><ul><li>x is not uniform</li><li>if $x^1$ and $x^2$ are close in a hign density region, $\hat{y}^1$ and $\hat{y}^2$ are the same.</li></ul><p>Smoothness Assumption假设更准确的表述是：</p><p> x不是均匀分布，如果 $x^1$ 和 $x^2$ 通过一个high density region的区域连在一起，且离得很近，则 $\hat{y}^1$ 和 $\hat{y}^2$ 相同。</p><p>如下图， $x^1$ 和 $x^2$ 通过high density region连接在一起，有相同的label，而 $x^2$ 和 $x^3$ 有不同的label.</p><img src="https://s1.ax1x.com/2020/07/03/NXR1Z4.md.png" alt="NXR1Z4.md.png" style="zoom:50%;" /><hr><p>Smoothness Assumption通过观察大量unlabeled data，可以得到一些信息。</p><p>比如下图中的两张人的左脸和右脸图片，都是unlabeled，但如果给大量的过渡形态（左脸转向右脸）unlabeled data，可以得出这两张图片是相似的结论.</p><p><a href="https://imgchr.com/i/NXoQpj"><img src="https://s1.ax1x.com/2020/07/03/NXoQpj.md.png" alt="NXoQpj.md.png"></a> </p><p>Smoothness Assumption还可以用在文章分类中，比如分类天文学和旅游学的文章。</p><p>如下图， 文章 d1和d3有overlap word（重叠单词），所以d1和d3是同一类，同理 d4和d2是一类。</p><img src="https://s1.ax1x.com/2020/07/03/NXoutg.md.png" alt="NXoutg.md.png" style="zoom:50%;" /><p>如果，下图中，d1和d3没有overlap word，就无法说明d1和d3是同一类。</p><img src="https://s1.ax1x.com/2020/07/03/NXoKhQ.md.png" alt="NXoKhQ.md.png" style="zoom:50%;" /><p>但是，如果我们收集到足够多但unlabeled data，如下图，通过high density region的连接和传递，也可以得出d1和d3一类，d2和d4一类。</p><img src="https://s1.ax1x.com/2020/07/03/NXol1s.md.png" alt="NXol1s.md.png" style="zoom:80%;" /><h2 id="Cluster-and-then-Label"><a href="#Cluster-and-then-Label" class="headerlink" title="Cluster and then Label"></a>Cluster and then Label</h2><p>在Smoothness Assumption假设下，直观的可以用cluster and then label，先用所有的data训练一个classifier。</p><p>直接聚类标记(比较难训练）。</p><h2 id="Graph-based-Approach"><a href="#Graph-based-Approach" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h2><p>另一种方法是利用图的结构（Graph structure）来得知 $x^1$ and $x^2$ are close in a high density region (connected by a high density path).</p><p>Represent the data points as a graph.</p><p>【把这些数据点看作一个图】</p><img src="https://s1.ax1x.com/2020/07/03/NXRKMT.md.png" alt="NXRKMT.md.png" style="zoom:50%;" /><p>建图有些时候是很直观的，比如网页中的超链接，论文中的引用。</p><p>但有的时候也需要自己建图。</p><p>注意：</p><p>如果是影像类，base on pixel，performance就不太好，一般会base on autoencoder，将feature抽象出来，效果更好。</p><h3 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h3><p>建图过程如下：</p><ol><li><p>Define the similarity $s(x^i, x^j)$ between $x^i$ and $x^j$ .</p><p>【定义data $x^i$ 和 $x^j$ 的相似度】</p></li><li><p>Add edge【定义数据点中加边（连通）的条件】</p><ul><li><p>K Nearest Neighbor【和该点最近的k个点相连接】</p><img src="https://s1.ax1x.com/2020/07/03/NXReGq.png" alt="NXReGq.png" style="zoom:50%;" /></li><li><p>e-Neighborhood【与离该点距离小于等于e的点相连接】</p><img src="https://s1.ax1x.com/2020/07/03/NXRZin.png" alt="NXRZin.png" style="zoom:50%;" /></li></ul></li><li><p>Edge weight is proportional to $s(x^i, x^j)$ 【边点权重就是步骤1定义的连接两点的相似度】</p><p>Gaussian Radial Basis Function： $s\left(x^{i}, x^{j}\right)=\exp \left(-\gamma\left\|x^{i}-x^{j}\right\|^{2}\right)$ </p><p>一般采用如上公式（经验上取得较好的performance）。</p><p>因为利用指数化后（指数内是两点的Euclidean distance），函数下降的很快，只有当两点离的很近时，该相似度 $s(x^i,x^j)$  才大，其他时候都趋近于0.</p></li></ol><h3 id="Graph-based-Approach-1"><a href="#Graph-based-Approach-1" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h3><p>图建好后：</p><p>The labeled data influence their neighbors.</p><p>Propagate through the graph.</p><p>【label data 不仅会影响他们的邻居，还会一直传播下去】</p><img src="https://s1.ax1x.com/2020/07/03/NXRmR0.md.png" alt="NXRmR0.md.png" style="zoom:50%;" /><p>如果data points够多，图建的好，就会像下图这样：</p><img src="https://s1.ax1x.com/2020/07/03/NXREIs.png" alt="NXREIs.png" style="zoom:50%;" /><p>但是，如果data较少，就可能出现下图这种label传不到unlabeled data的情况：</p><img src="https://s1.ax1x.com/2020/07/03/NXRPsS.png" alt="NXRPsS.png" style="zoom:50%;" /><h3 id="Smoothness-Definition"><a href="#Smoothness-Definition" class="headerlink" title="Smoothness Definition"></a>Smoothness Definition</h3><p>因为是基于Smoothness Assumption，所以最后训练出的模型应让得到的图尽可能满足smoothness的假设。</p><p><strong>注意：</strong> 这里的因果关系是，unlabeled data作为NN的输入，得到label $y$ ，该label $y$ 和labeled data的 label $\hat{y}$  一起得到的图是尽最大可能满足Smoothness Assumption的。</p><p>（<strong>而不是</strong>建好图，然后unlabeled data的label $y$ 是labeled data原有的 $\hat{y}$ 直接传播过来的，不然训练NN干嘛）</p><p>把unlabeled data作为NN的输入，得到label ，对labeled data和”unlabeled data” 建图。</p><p>为了在训练中使得最后的图尽可能满足假设，定义<strong>smoothness of the labels on the graph</strong>.</p>$S=\frac{1}{2} \sum_{i,j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}$  <p>（对于所有的labeled data 和 “unlabeled data”（作为NN输入后，有label））</p><p>按照上式计算，得到的Smoothness如下图所示：</p><p><a href="https://imgchr.com/i/NXRiqg"><img src="https://s1.ax1x.com/2020/07/03/NXRiqg.md.png" alt="NXRiqg.md.png"></a> </p><p><strong>Smaller means smoother.</strong> </p><p>【Smoothness $S$ 越小，表示图越满足这个假设】</p><hr><p>计算smoothness $S$ 有一种简便的方法：</p>$S=\frac{1}{2} \sum_{i, j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}=y^{T} L y$  (这里的1/2只是为了计算方便)<ul><li><p>$y$ : (R+U)-dim vector，是所有label data和”unlabeled data” 的label，所以是R+U维。</p><p>$y=\begin{bmatrix}…y^i…y^j…\end{bmatrix}^T$ </p></li><li><p>$L$ :(R+U) $\times$ (R+U) matrix，也叫Graph Laplacian（调和矩阵，拉普拉斯矩阵）</p><p>$L$ 的计算方法：$L=D-W$ </p><p>其中 $W$ 矩阵算是图的邻接矩阵（区别是无直接可达边的值是0）</p><p>$D$ 矩阵是一个对角矩阵，对角元素的值等于 $W$ 矩阵对应行的元素和</p><p>矩阵表示如下图所示：</p><img src="https://s1.ax1x.com/2020/07/03/NXRkZQ.md.png" alt="NXRkZQ.md.png" style="zoom:50%;" /></li></ul><p>（证明据说很枯燥，暂时略[2])</p><h3 id="Smoothness-Regularization"><a href="#Smoothness-Regularization" class="headerlink" title="Smoothness Regularization"></a>Smoothness Regularization</h3>$S=\frac{1}{2} \sum_{i, j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}=y^{T} L y$  <p> $S$ 中的 $y$ 其实是和network parameters有关的（unlabeled data的label），所以把 $S$ 也放进损失函数中minimize，以求尽可能满足smoothness assumption.</p><p>以满足smoothness assumption的损失函数： $L=\sum_{x^r} C\left(y^{r}, \hat{y}^{r}\right)+\lambda S$ </p><p>损失函数的前部分使labeled data的输出更贴近其label，后部分 $\lambda S$ 作为regularization term，使得labeled data和unlabeled data尽可能满足smoothness assumption.</p><p>除了让NN的output满足smoothness的假设，还可以让NN的任何一层的输出满足smoothness assumption，或者让某层外接一层embedding layer，使其满足smoothness assumption，如下图：</p><img src="https://s1.ax1x.com/2020/07/03/NXRAaj.png" alt="NXRAaj.png" style="zoom:50%;" /><h1 id="Better-Representation"><a href="#Better-Representation" class="headerlink" title="Better Representation"></a>Better Representation</h1><p>Better Presentation的思想就是：去芜存菁，化繁为简。</p><ul><li>Find the latent(潜在的) factors behind the observation.</li><li>The latent factors (usually simpler) are better representation.</li></ul><p>【找到所观察事物的潜在特征，即该事物的better representation】</p><p>该部分后续见这篇博客。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>挖坑：EM算法详解</p></li><li><p>挖坑：Graph Laplacian in smoothness.</p></li><li><p>Olivier Chapelle：Semi-Supervised Learning </p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？&lt;/p&gt;
&lt;p&gt;再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。&lt;/p&gt;
&lt;p&gt;对于Generative Model，文章重点讲述了如何用EM算法来训练模型。&lt;/p&gt;
&lt;p&gt;对于Low-density，文章重点讲述了如何让模型进行Self-training，并且在训练中引入Entropy-based Regularization term来尽可能low-density的假设。&lt;/p&gt;
&lt;p&gt;对于Smoothness Assumption，文章重点讲述了Graph-based Approach（基于图的方法），并且在训练中引入Smoothness Regularization term来尽可能满足Smoothness Assumption的假设。&lt;/p&gt;
&lt;p&gt;对于Better Representation，本篇文章只是简单阐述了其思想，具体介绍见这篇博客。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="Semi-supervised" scheme="https://f7ed.com/tags/Semi-supervised/"/>
    
  </entry>
  
  <entry>
    <title>「算法导论」:排序-总结</title>
    <link href="https://f7ed.com/2020/06/29/sort-preview/"/>
    <id>https://f7ed.com/2020/06/29/sort-preview/</id>
    <published>2020-06-28T16:00:00.000Z</published>
    <updated>2020-07-03T08:50:09.474Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章review了算法中的排序算法，包括冒泡排序、插入排序、归并排序、堆排序（以及用堆实现优先队列）、快速排序和计数排序。</p><p>分别从算法思路、算法伪代码实现、算法流程、算法时间复杂度四个方面阐述每个算法。</p><a id="more"></a><h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><p><strong>排序问题：</strong></p><p>输入：一个 $n$个数的序列 $&lt;a_1,a_1,…,a_n&gt;$ </p><p>输出：输入序列的一个重拍 $&lt;a_1’,a_2’,…,a_n’&gt;$ ，使得 $a_1’\leq a_2’ \leq…\leq a_n’$ .</p><p>在实际中，待排序的数很少是单独的数值，它们通常是一组数据，称为记录(record)。每个记录中包含一个关键字(key)，这就是需要排序的值。记录剩下的数据部分称为卫星数据(satellite data)，通常和关键字一同存取。</p><p><strong>原址排序：</strong>输入数组中仅有常数个元素需要在排序过程中存储在数组之外。</p><p>典型的原址排序有：插入排序、堆排序、快速排序。</p><hr><p><strong>符号说明：</strong> </p><ul><li><p>$\Theta$ 记号：</p><p>$\Theta$ 记号渐进给出一个函数的上界和下界。</p>  $\Theta(g(n))=\left\{f(n): \text { there exist positive constants } c_{1}, c_{2}, \text { and } n_{0}\text { such that } \right.  \left.0 \leq c_{1} g(n) \leq f(n) \leq c_{2} g(n) \text { for all } n \geq n_{0}\right\}$<p>$g(n)$ 称为 $f(n)$ 的一个渐进紧确界(asymptotically tight bound)</p></li><li><p>$O$ 记号</p><p>$O$ 记号只给出了函数的渐进上界。</p>  $O(g(n))=\left\{f(n): \text { there exist positive constants } c \text { and } n_{0}\text { such that } \right.  \left.0 \leq f(n) \leq c g(n) \text { for all } n \geq n_{0}\right\}$ </li><li><p>$\Omega$ 记号</p><p>$\Omega$ 记号给出了函数的渐进下界。</p>  $\Omega(g(n))=\left\{f(n): \text { there exist positive constants } c \text { and } n_{0}\text { such that } \right. \left.0 \leq c g(n) \leq f(n) \text { for all } n \geq n_{0}\right\}$ <p>符号比较如下图：</p></li></ul><p><a href="https://imgchr.com/i/Nh2if1"><img src="https://s1.ax1x.com/2020/06/29/Nh2if1.md.png" alt="Nh2if1.md.png"></a> </p><p><strong>排序算法运行时间一览</strong> ：</p><table><thead><tr><th>算法</th><th>最坏情况运行时间</th><th>平均情况/期望运行时间</th></tr></thead><tbody><tr><td>插入排序</td><td>$\Theta (n^2)$</td><td>$\Theta(n^2)$</td></tr><tr><td>归并排序</td><td>$\Theta(n\lg{n})$</td><td>$\Theta(n\lg{n})$</td></tr><tr><td>堆排序</td><td>$O(n\lg{n})$</td><td>-</td></tr><tr><td>快速排序</td><td>$\Theta(n^2)$</td><td>$\Theta(n\lg{n})$ (expected)</td></tr><tr><td>计数排序</td><td>$\Theta(k+n)$</td><td>$\Theta(k+n)$</td></tr><tr><td>基数排序</td><td>$\Theta(d(n+k))$</td><td>$\Theta(d(n+k))$</td></tr><tr><td>桶排序</td><td>$\Theta(n^2)$</td><td>$\Theta(n)$ (average-case)</td></tr></tbody></table><h1 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h1><p>反复交互相邻未按次序排列的元素。</p><p><strong>BUBBLESORT(A)</strong></p><ul><li>参数：A待排序数组</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to A.lengh<span class="literal">-1</span></span><br><span class="line"><span class="keyword">for</span> j = A.length downto i+<span class="number">1</span> //每次迭代找出A[<span class="type">i..j</span>]中最小的元素放在A[<span class="type">i</span>]位置</span><br><span class="line"><span class="keyword">if</span> A[<span class="type">j</span>] &lt; A[<span class="type">j</span>-<span class="number">1</span>]</span><br><span class="line">exchange A[<span class="type">j</span>] with A[<span class="type">j</span>-<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>冒泡排序是原址排序，流行但低效。</p><h1 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h1><p>如下图所示，插入排序就像打牌时排序一手扑克牌。</p><p><a href="https://imgchr.com/i/Nh2m0e"><img src="https://s1.ax1x.com/2020/06/29/Nh2m0e.png" alt="Nh2m0e.png"></a> </p><ol><li>开始时，我们的左手为空，桌子上的牌面向下。</li><li>然后，我们每次从桌子上拿走一张牌，想把它放在左手中的正确位置。</li><li>为了找到这张牌的正确位置，我们从右到左将这张牌和左手里的牌依次比较，放入正确的位置。</li><li>左手都是已排序好的牌。</li></ol><p><strong>INSERTION-SORT(A)</strong> </p><ul><li>A：待排序数组</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j = <span class="number">2</span> to A.length</span><br><span class="line">key = A[<span class="type">j</span>]</span><br><span class="line">//将key插入到已排序好的A[<span class="number">1</span><span class="type">..j</span>-<span class="number">1</span>]</span><br><span class="line">i = j - <span class="number">1</span> //pointer <span class="keyword">for</span> sorted sequence A[<span class="number">1</span><span class="type">..j</span>-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">while</span> i &gt; <span class="number">0</span> and A[<span class="type">i</span>] &gt; key</span><br><span class="line">A[<span class="type">i</span>+<span class="number">1</span>] = A[<span class="type">i</span>]</span><br><span class="line">i--</span><br><span class="line">A[<span class="type">i</span>+<span class="number">1</span>] = key</span><br></pre></td></tr></table></figure><p>插入排序是原址排序，对于少量元素是一个有效的算法。</p><p>最坏情况的运行时间： $\Theta(n^2)$ .</p><h1 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h1><h2 id="分治"><a href="#分治" class="headerlink" title="分治"></a>分治</h2><p><strong>分治</strong>： 将原问题分解为几个规模较小但类似于原问题的子问题，递归地求解这些子问题，然后再合并这些子问题的解来建立原问题的解。</p><p><strong>分治的步骤</strong>：</p><ol><li>分解：分解原问题为若干规模较小的子问题。</li><li>解决：递归地求解各子问题，规模较小，可直接求解。</li><li>合并：合并这些子问题的解成原问题的解。</li></ol><h2 id="算法核心"><a href="#算法核心" class="headerlink" title="算法核心"></a>算法核心</h2><p><strong>归并排序中的分治</strong> ：</p><ol><li>分解：分解待排序的n个元素序列成各n/2个元素序列的两个子序列。</li><li>解决：使用归并排序递归地排序两个子序列，当序列长度为1时，递归到达尽头。</li><li>合并：合并两个已经排序好的子序列以产生排序好的原序列。</li></ol><p><strong>核心</strong> ：合并两个已经排序好的子序列——MERGE(A, p, q, r)</p><ul><li>A: 待排序原数组。</li><li>p, q, r: 数组下标，满足 $p\leq q&lt;r$ 。</li><li>假设子数组 A[p..q] 和A[q+1..r]都已经排好序，合并这两个数组代替原来的A[p..r]子数组。</li></ul><hr><p><strong>MERGE算法理解：</strong> </p><ol><li>牌桌上有两堆牌面朝上，每堆都已排好序，最小的牌在顶上。希望将两堆牌合并成排序好的输出牌堆，且牌面朝下。</li><li>比较两堆牌顶顶牌，选取较小的那张，牌面朝下的放在输出牌堆。</li><li>重复步骤2直至某一牌堆为空。</li><li>将剩下的另一堆牌面朝下放在输出堆。</li></ol><p>MERGE合并的过程如下图所示：</p><p><a href="https://imgchr.com/i/Nh2E6K"><img src="https://s1.ax1x.com/2020/06/29/Nh2E6K.md.png" alt="Nh2E6K.md.png"></a> </p><p><strong>MERGE算法分析：</strong> </p><p>在上述过程中，n个元素，我们最多执行n个步骤，所以MERGE合并需要 $\Theta(n)$ 的时间。</p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><strong>MERGE(A, p, q, r)</strong> </p><ul><li>功能：合并已排序好的子数组A[p..q]和A[q+1..r]</li><li>参数：A为待排序数组，p, q, r为数组下标，且满足 $p\leq q&lt;r$ </li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Let S[<span class="type">p..r</span>] be new arrays</span><br><span class="line">k = p //pointer <span class="keyword">for</span> S[]</span><br><span class="line">i = p, j = q+<span class="number">1</span> //pointer <span class="keyword">for</span> subarray</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> k &lt;= r </span><br><span class="line"><span class="keyword">while</span> ( i &lt;= q and j &lt;= r and A[<span class="type">i</span>] &lt;= A[<span class="type">j</span>] ) or j &gt; r  // 取A[<span class="type">p..q</span>]牌堆</span><br><span class="line">S[<span class="type">k</span>++] = A[<span class="type">i</span>++]</span><br><span class="line"><span class="keyword">while</span> ( i &lt;= q and j &lt;= r and A[<span class="type">i</span>] &gt;= A[<span class="type">j</span>] ) or i &gt; q //取A[<span class="type">q</span>+<span class="number">1</span><span class="type">..r</span>]牌堆</span><br><span class="line"></span><br><span class="line">A[<span class="type">p..r</span>] = S[<span class="type">p..r</span>]</span><br></pre></td></tr></table></figure><hr><p><strong>MERGE-SORT(A, p, r)</strong> </p><ul><li>功能：排序子数组A[p..r]</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> p &lt; r</span><br><span class="line">q = (p+r)/<span class="number">2</span></span><br><span class="line"><span class="built_in">MERGE-SORT</span>(A, p, q)</span><br><span class="line"><span class="built_in">MERGE-SORT</span>(A, q+<span class="number">1</span>, r)</span><br><span class="line">MERGE(A, p, q, r)</span><br></pre></td></tr></table></figure><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><h3 id="分治算法运行时间分析"><a href="#分治算法运行时间分析" class="headerlink" title="分治算法运行时间分析"></a>分治算法运行时间分析</h3><p>分治算法运行时间递归式来自三个部分。</p><p>假设 $T(n)$ 是规模为 $n$ 的一个问题的运行时间。若规模问题足够小，则直接求解需要常量时间，将其写作 $\Theta(1)$ 。</p><p>假设把原问题分解成 $a$ 个子问题，每个子问题的规模是原问题的 $1/b$ (在归并排序中， $a$ 和 $b$ 都为2，但很多分治算法中 $a\neq b$ )。为了求解一个规模为 $n/b$ 规模的子问题，需要 $T(n/b)$ 的时间，所以需要 $aT(n/b)$ 的时间求解 $a$ 个子问题。</p><p>如果分解子问题需要 $D(n)$ 时间，合并子问题需要 $C(n)$ 时间。</p><p>递归式：</p>$$T(n)=\left\{\begin{array}{ll}\Theta(1) & \text { if } n \leq c \\ a T(n / b)+D(n)+C(n) & \text { otherwise }\end{array}\right.$$<h3 id="归并排序分析"><a href="#归并排序分析" class="headerlink" title="归并排序分析"></a>归并排序分析</h3><p>前文分析了MERGE(A, p, q, r) 合并两个子数组的时间复杂度是 $\Theta(n)$ ，即 $C(n)=\Theta(n)$ ，且 $D(n)=\Theta(n)$ .</p><p>归并排序的最坏情况运行时间 $T(n)$ :</p>$$T(n)=\left\{\begin{array}{ll}\Theta(1) & \text { if } n=1 \\ 2 T(n / 2)+\Theta(n) & \text { if } n>1\end{array}\right.$$<p>用递归树的思想求解递归式：</p><p><a href="https://imgchr.com/i/Nh2Al6"><img src="https://s1.ax1x.com/2020/06/29/Nh2Al6.md.png" alt="Nh2Al6.md.png"></a> </p><p>即递归树每层的代价为 $\Theta(n)=cn$ ，共有 $\lg{n}+1$ 层，所以归并排序的运行时间结果是 $\Theta(n\lg{n})$  .</p><h1 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h1><h2 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h2><p><strong>自由树</strong> ：连通的、无环的无向图。</p><p><strong>有根数</strong> ：是一棵自由树，其顶点存在一个与其他顶点不同的顶点，称为树的根。</p><p><strong>度</strong> ：有根树中一个结点 $x$ 孩子的数目称为 $x$ 的度。</p><p><strong>深度</strong> :从根 $r$ 到结点 $x$ 的简单路径。</p><p><strong>二叉树</strong> ：不包括任何结点，或者包括三个不相交的结点集合：一个根结点，一棵称为左子树的二叉树和一棵称为右子树的二叉树。</p><p><strong>完全k叉树</strong> ：所有叶结点深度相同，且所有内部结点度为k的k叉树。</p><hr><p><strong>（二叉）堆</strong> ：是一个数组，它可以被看成一个近似的完全二叉树，树上的每个结点对应数组中的一个元素。除了最底层外，该树被完全填满，并且是从左到右填充。如下图所示。</p><p><a href="https://imgchr.com/i/Nh2Ck9"><img src="https://s1.ax1x.com/2020/06/29/Nh2Ck9.md.png" alt="Nh2Ck9.md.png"></a> </p><p>堆的数组$A$ 有两个属性：</p><ul><li><p>$A.length$ ：数组元素的个数，A[1..A.length]中都存有值。</p></li><li><p>$A.heap-size$ ：有多少个堆元素在数组，A[1..heap-size]中存放的是堆的有效元素。</p><p>（$0\leq A.heap-size\leq A.lengh$ )</p></li></ul><p><strong>堆的性质：</strong> </p><ul><li><p>$A[1]$ :存放的是树的根结点。</p></li><li><p>对于给定的一个结点 $i$ ，很容易计算他的父结点、左孩子和右孩子的下标。</p><ul><li><p><strong>PARENT(i)</strong> </p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> i/<span class="number">2</span> //i&gt;&gt;&gt;<span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p><strong>LEFT(i)</strong> </p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> <span class="number">2</span>*i //i&lt;&lt;&lt;<span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p><strong>RIGHT(i)</strong> </p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> <span class="number">2</span>*i+<span class="number">1</span> //i&lt;&lt;&lt;<span class="number">1</span> | <span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>包含$n$ 个元素的堆的高度为 $\Theta(\lg{n})$ </p></li><li><p>堆结构上的基本操作的运行时间至多和堆的高度成正比，即时间复杂度为 $O(\lg{n})$ .</p></li><li><p>叶子结点：n/2+1 , n/2+2 , … , n</p></li></ul><p><strong>堆的分类：</strong> </p><ul><li><p>最大堆：</p><p>除了根以外的结点 $i$ 都满足 $A[\text{PARENT}(i)]\geq A[i]$ .</p><p>某个结点最多和其父结点一样大。</p><p>堆的最大元素存放在根结点中。</p></li><li><p>最小堆：</p><p>除了根以外的结点 $i$ 都满足 $A[\text{PARENT}(i)]\leq A[i]$ .</p><p>堆的最小元素存放在根结点中。</p></li></ul><p><strong>堆的基本过程</strong> :</p><ul><li>MAX-HEAPIFY：维护最大堆的过程，时间复杂度为 $O(\lg{n})$ </li><li>BUILD-MAX-HEAP：将无序的输入数据数组构造一个最大堆，具有线性时间复杂度 $O(n\lg{n})$ 。</li><li>HEAPSORT：对一个数组进行原址排序，时间复杂度为 $O(n\lg{n})$ </li><li>MAX-HEAP-INSERT、HEAP-EXTRACT-MAX、HEAP-INCREASE-KEY和HEAP-MAXIMUM：利用堆实现一个优先队列，时间复杂度为 $O(\lg{n})$ .</li></ul><h2 id="维护：MAX-HEAPIFY"><a href="#维护：MAX-HEAPIFY" class="headerlink" title="维护：MAX-HEAPIFY"></a>维护：MAX-HEAPIFY</h2><p>调用MAX-HEAPIFY的时候，假定根结点LEFT(i)和RIGHT(i)的二叉树都是最大堆，但A[i]可能小于其左右孩子，因此违背了堆的性质。</p><p>MAX-HEAPIFY通过让 A[i]“逐级下降”，从而使下标为i的根结点的子树满足最大堆的性质。</p><p><strong>MAX-HEAPIFY(A, i)</strong> </p><ul><li>功能：维护下标为i的根结点的子树，使其满足最大堆的性质。</li><li>参数：i 是该子树的根结点，其左子树右子树均满足最大堆的性质。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">l = LEFT(i)</span><br><span class="line">r = RIGHT(i)</span><br><span class="line"><span class="keyword">if</span> l &lt;= A.heap<span class="literal">-size</span> and A[<span class="type">l</span>] &gt; A[<span class="type">i</span>]</span><br><span class="line">largest = l</span><br><span class="line"><span class="keyword">else</span> largest = i</span><br><span class="line"><span class="keyword">if</span> r &lt;= A.heap<span class="literal">-size</span> and A[<span class="type">r</span>] &gt; A[<span class="type">i</span>]</span><br><span class="line">largest = r</span><br><span class="line"><span class="keyword">if</span> largest != i</span><br><span class="line">exchange A[<span class="type">i</span>] with A[<span class="type">largest</span>]</span><br><span class="line">MAX<span class="literal">-HEAPIFY</span>(A, largest)</span><br></pre></td></tr></table></figure><p>下图是执行 MAX-HEAPIFY(A, 2)的执行过程。A.heap-size=10, 图(a)(b)(c)依次体现了值为4的结点依次下降的过程。</p><p><a href="https://imgchr.com/i/Nh2PYR"><img src="https://s1.ax1x.com/2020/06/29/Nh2PYR.md.png" alt="Nh2PYR.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：</p><p>MAX-HEAPIFY的时间复杂度为 $O(lg{n})$.</p><h2 id="建堆：BUILD-MAX-HEAP"><a href="#建堆：BUILD-MAX-HEAP" class="headerlink" title="建堆：BUILD-MAX-HEAP"></a>建堆：BUILD-MAX-HEAP</h2><p>堆的性质：</p><p>子数组A[n/2+1..n]中的元素都是树的叶子结点。因为下标最大的父结点是n/2，所以n/2以后的结点都没有孩子。</p><p><strong>建堆</strong> ：每个叶结点都可以看成只包含一个元素的堆，利用自底向上的方法，对树中其他结点都调用一次MAX-HEAPIFY，把一个大小为n = A.length的数组A[1..n]转换为最大堆。</p><p><strong>BUILD-MAX-HEAP(A)</strong> </p><ul><li>功能：把A[1..n]数组转换为最大堆</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A.heap<span class="literal">-size</span> = A.length</span><br><span class="line"><span class="keyword">for</span> i = A.length/<span class="number">2</span> downto <span class="number">1</span></span><br><span class="line">MAX<span class="literal">-HEAPIFY</span>(A, i)</span><br></pre></td></tr></table></figure><p>下图是把A数组构造成最大堆的过程：</p><p><a href="https://imgchr.com/i/Nh2kSx"><img src="https://s1.ax1x.com/2020/06/29/Nh2kSx.md.png" alt="Nh2kSx.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：</p><p>BUILD-MAX-HEAP需要 $O(n)$ 次调用MAX-HEAPIFY，因此构造最大堆的时间复杂度是 $O(n\lg{n})$ .</p><h2 id="排序：HEAPSORT"><a href="#排序：HEAPSORT" class="headerlink" title="排序：HEAPSORT"></a>排序：HEAPSORT</h2><p><strong>算法思路</strong>： </p><ol><li>初始化时，调用BUILD-MAX-HEAP将输入数组A[1..n]建成最大堆，其中 n = A.length。</li><li>调用后，最大的元素在A[1]，将A[1]和A[n]互换，可以把元素放在正确的位置。</li><li>将n结点从堆中去掉(通过减少A.heap-size实现)，剩余结点中，原来根的孩子仍是最大堆，但根结点可能会违背堆的性质，调用MAX-HEAPIFY(A, 1)，从而构造一个新的最大堆。</li><li>重复步骤3，直到堆的大小从n-1降为2.</li></ol><p><strong>HEAPSORT(A)</strong> </p><ul><li>功能：利用堆对数组排序</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BUILD<span class="literal">-MAX</span><span class="literal">-HEAP</span>(A)</span><br><span class="line"><span class="keyword">for</span> i = A.length downto <span class="number">2</span></span><br><span class="line">exchange A[<span class="number">1</span>] with A[<span class="type">i</span>]</span><br><span class="line">A.heap<span class="literal">-size</span> = A.heap<span class="literal">-size</span> - <span class="number">1</span></span><br><span class="line">MAX<span class="literal">-HEAPIFY</span>(A, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下图为调用HEAPSORT的过程图：</p><p><a href="https://imgchr.com/i/NhgjyT"><img src="https://s1.ax1x.com/2020/06/29/NhgjyT.md.png" alt="NhgjyT.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：</p><p>建堆BUILD-MAX-HEAP的时间复杂度为 $O(n\lg{n})$ ，n-1次调用MAX-HEAPIFY的时间复杂度为 $O(n\lg{n})$ ，所以堆排序的时间复杂度为 $O(n\lg{n})$ .</p><h2 id="堆的应用：优先队列"><a href="#堆的应用：优先队列" class="headerlink" title="堆的应用：优先队列"></a>堆的应用：优先队列</h2><p>这里关注如何用最大堆实现最大优先队列。</p><p><strong>优先队列(priority queue)：</strong> </p><p>一种用来维护由一组元素构成的集合S的数据结构，其中每一个元素都有一个相关的值，称为关键字(key)。</p><p><strong>（最大）优先队列支持的操作</strong> ：</p><ul><li>INSERT(S, x)：把元素 $x$ 插入集合S中，时间复杂度为 $O(\lg{n})$ 。</li><li>MAXIMUM(S)：返回S中具有最大关键字的元素，时间复杂度为 $O(1)$ 。</li><li>EXTRACT-MAX(S)：去掉并返回S中的具有最大关键字的元素，时间复杂度为 $O(\lg{n})$ 。</li><li>INCREASE-KEY(S, x, k)：将元素 $x$ 的关键字值增加到k，这里假设k的大小不小于元素 $x$ 的原关键字值，时间复杂度为 $O(\lg{n})$ 。</li></ul><h3 id="MAXIMUM"><a href="#MAXIMUM" class="headerlink" title="MAXIMUM"></a>MAXIMUM</h3><p>将集合S已建立最大堆的前提下，调用HEAP-MAXIMUM在 $\Theta(1)$ 实现MAXIMUM的操作。</p><p><strong>HEAP-MAXIMUM(A)</strong> </p><ul><li>功能：实现最大优先队列MAXIMUM的操作，即返回集合中最大关键字的元素。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> A[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><strong>时间复杂度分析</strong> ：$\Theta(1)$ </p><h3 id="EXTRACT-MAX"><a href="#EXTRACT-MAX" class="headerlink" title="EXTRACT-MAX"></a>EXTRACT-MAX</h3><p>类似于HEAPSORT的过程。</p><ul><li><p>A[1]为最大的元素，A[1]的孩子都是最大堆。</p></li><li><p>将A[1]和A[heap-size]交换，减少堆的大小(heap-size)。</p></li><li><p>此时根结点的孩子满足最大堆，而根不一定满足最大堆性质，维护一下当前堆。</p></li></ul><p><strong>HEAP-EXTRACT-MAX(A)</strong> </p><ul><li>功能：实现最大优先队列EXTRACT-MAX的操作，即去掉并返回集合中最大关键字的元素。</li></ul><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> A.heap-<span class="keyword">size</span> &lt; <span class="number">1</span></span><br><span class="line"><span class="keyword">error</span> <span class="string">"heap underflow"</span></span><br><span class="line"><span class="keyword">max</span> = A[<span class="number">1</span>]</span><br><span class="line">A[<span class="number">1</span>] = A[A.heap-<span class="keyword">size</span>]</span><br><span class="line">A.heap-<span class="keyword">size</span> = A.heap-<span class="keyword">size</span> - <span class="number">1</span></span><br><span class="line">MAX-HEAPIFY(A, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">max</span></span><br></pre></td></tr></table></figure><p><strong>时间复杂度分析</strong> ：$O(\lg{n})$ .</p><h3 id="INCREASE-KEY"><a href="#INCREASE-KEY" class="headerlink" title="INCREASE-KEY"></a>INCREASE-KEY</h3><p>如果增加A[i]的关键词，可能会违反最大堆的性质，所以实现HEAP-INCREASE-KEY的过程类似插入排序：从当前i结点到根结点的路径上为新增的关键词寻找恰当的插入位置。</p><ol><li><p>当前元素不断和其父结点比较，如果当前元素的关键字更大，则和父结点进行交换。</p></li><li><p>步骤1不断重复，直至当前元素的关键字比父结点小。</p></li></ol><p><strong>HEAP-INCREASE-KEY(A, i, key)</strong> </p><ul><li>功能：实现最大优先队列INCREASE-KEY的功能，即将A[i]的关键字值增加为key.</li><li>参数：i为待增加元素的下标，key为新关键字值。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> key &lt; A[<span class="type">i</span>]</span><br><span class="line">error <span class="string">"new key is smaller than current key"</span></span><br><span class="line">A[<span class="type">i</span>] = key</span><br><span class="line"><span class="keyword">while</span> i &gt; <span class="number">1</span> and A[<span class="type">PARENT</span>(<span class="type">i</span>)] &lt; A[<span class="type">i</span>]</span><br><span class="line">exchange A[<span class="type">i</span>] with A[<span class="type">PARENT</span>(<span class="type">i</span>)]</span><br><span class="line">i = PARENT(i)</span><br></pre></td></tr></table></figure><p>下图展示了HEAP-INCREASE-KEY的过程：</p><p><a href="https://imgchr.com/i/Nh2Sw4"><img src="https://s1.ax1x.com/2020/06/29/Nh2Sw4.md.png" alt="Nh2Sw4.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：$O(\lg{n})$ </p><h3 id="INSERT"><a href="#INSERT" class="headerlink" title="INSERT"></a>INSERT</h3><p>如何插入一个元素扩展最大堆？</p><ol><li>先通过增加一个关键字值为 $-\infin$ 的叶子结点扩展最大堆。</li><li>再调用HEAP-INCREASE-KEY过程为新的结点设置对应的关键字值。</li></ol><p><strong>MAX-HEAP-INSERT(A, key)</strong></p><ul><li>功能：实现最大优先队列的INSERT功能，即将关键字值为key的新元素插入到最大堆中。</li><li>参数：key是待插入元素的关键字值。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A.heap<span class="literal">-size</span> = A.heap<span class="literal">-size</span> + <span class="number">1</span></span><br><span class="line">A[<span class="type">A.heap</span>-<span class="type">size</span>] = -∞</span><br><span class="line">HEAP<span class="literal">-INCREASE</span><span class="literal">-KEY</span>(A, A.heap<span class="literal">-size</span>, key)</span><br></pre></td></tr></table></figure><p><strong>时间复杂度分析</strong> ：$O(\lg{n})$ .</p><h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><p>对于包含 $n$个数的输入数组来说，快速排序是一个最坏情况时间复杂度为 $\Theta(n^2)$ 的排序算法。</p><p>虽然最坏情况时间复杂度很差，但是快速排序通常是实际排序应用中最好的选择，因为他的平均性能非常好：他的期望时间复杂度为 $\Theta(n\lg{n})$ ，而且 $\Theta(n\lg{n})$ 中隐含的常数因子非常小。</p><p>另外，它还能进行原址排序。</p><h3 id="分治-1"><a href="#分治-1" class="headerlink" title="分治"></a>分治</h3><p>对A[p..r]子数组进行快速排序的分治过程：</p><ul><li><p>分解：</p><p>数组A[p..r]被划分为两个（可能为空）的子数组A[p..q-1]和A[q+1..r]。</p><p>使得A[p..q-1]中的每个元素都小于等于A[q]，A[q+1..r]中的每个元素都大于等于A[q]。</p><p>其中计算下标q也是分解过程的一部分。</p></li><li><p>解决：通过递归调用快速排序，对子数组A[p..q-1]和A[q+1..r]进行排序。</p></li><li><p>合并：因为子数组都是原址排序的，所以不需要合并操作，A[p..r]已经排好序。</p></li></ul><h3 id="快速排序：QUICKSORT"><a href="#快速排序：QUICKSORT" class="headerlink" title="快速排序：QUICKSORT"></a>快速排序：QUICKSORT</h3><p>按照分治的过程。</p><p><strong>QUICKSORT(A, p, r)</strong> </p><ul><li>功能：快速排序子数组A[p..r]</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> p &lt; r</span><br><span class="line">q = PARTITION(A, p, r)</span><br><span class="line">QUICKSORT(A, p, q<span class="literal">-1</span>)</span><br><span class="line">QUICKSORT(A, q+<span class="number">1</span>, r)</span><br></pre></td></tr></table></figure><h3 id="数组的划分：PARTITION"><a href="#数组的划分：PARTITION" class="headerlink" title="数组的划分：PARTITION"></a>数组的划分：PARTITION</h3><p>快速排序的关键部分就在于如何对数组A[p..r]进行划分，即找到位置q。</p><p><strong>PARTITION(A, p, r)</strong> </p><ul><li>功能：对子数组A[p..r] 划分为两个子数组A[p..q-1]和子数组A[q+1..r]，其中A[p..q-1] 小于等于A[q]小于等于A[q+1..r]</li><li>返回：数组的划分下标q</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = A[<span class="type">r</span>]</span><br><span class="line">i = p - <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> j = p to r - <span class="number">1</span> // j is pointer <span class="keyword">for</span> comparation</span><br><span class="line"><span class="keyword">if</span> A[<span class="type">j</span>] &lt;= x</span><br><span class="line">i = i+<span class="number">1</span></span><br><span class="line">exchange A[<span class="type">i</span>] with A[<span class="type">j</span>]</span><br><span class="line">exchange A[<span class="type">i</span>+<span class="number">1</span>] with A[<span class="type">r</span>]</span><br><span class="line"><span class="keyword">return</span> i+<span class="number">1</span></span><br></pre></td></tr></table></figure><p>PARTITION总是选择一个 $x=A[r]$ 作为主元(pivot element)，并围绕它来划分子数组A[p..r]。</p><p>在循环中，数组被划分为下图四个（可能为空的）区域：</p><p><a href="https://imgchr.com/i/NhgvOU"><img src="https://s1.ax1x.com/2020/06/29/NhgvOU.md.png" alt="NhgvOU.md.png"></a> </p><ol><li>$p\leq k\leq i$ ，则 $A[k]\leq x$ .</li><li>$i+1\leq k \leq j-1$ ，则 $A[k]&gt;x$.</li><li>$k = r$ ，则 $A[k]=x$ .</li><li>$j\leq k\leq r-1$ ，则 $A[k]$ 与 $x$ 无关。</li></ol><p>下图是将样例数组PARTITION的过程：</p><p><a href="https://imgchr.com/i/Nh2pTJ"><img src="https://s1.ax1x.com/2020/06/29/Nh2pTJ.md.png" alt="Nh2pTJ.md.png"></a> </p><h3 id="快速排序的性能"><a href="#快速排序的性能" class="headerlink" title="快速排序的性能"></a>快速排序的性能</h3><p>[*]待补充</p><h3 id="快速排序的随机化版本"><a href="#快速排序的随机化版本" class="headerlink" title="快速排序的随机化版本"></a>快速排序的随机化版本</h3><p>与始终采用 $A[r]$ 作为主元的方法不同，随机抽样是从子数组A[p..r]随机选择一个元素作为主元。</p><p>加入随机抽样，在平均情况下，对子数组A[p..r]的划分是比较均匀的。</p><p><strong>RANDOMIZED-PEARTITION(A, p, r)</strong> </p><ul><li>功能：数组划分PARTITION的随机化主元版本</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i = RANDOM(p, r)</span><br><span class="line">exchange A[<span class="type">r</span>] with A[<span class="type">i</span>]</span><br><span class="line"><span class="keyword">return</span> PARTITION(A, p, r)</span><br></pre></td></tr></table></figure><p><strong>RANDOMIZED-QUICKSORT(A, p, r)</strong> </p><ul><li>功能：使用随机化主元的快速排序</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> p &lt; r</span><br><span class="line">q = RANDOMIZED<span class="literal">-PARTITION</span>(A, p, r)</span><br><span class="line">RANDOMIZED<span class="literal">-QUICKSORT</span>(A, p, q<span class="literal">-1</span>)</span><br><span class="line">RANDOMIZED<span class="literal">-QUICKSORT</span>(A, q+<span class="number">1</span>, r)</span><br></pre></td></tr></table></figure><h1 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h1><p><strong>计数排序</strong> ：</p><p>假设 $n$ 个输入元素中的每一个都是在 0到 $k$ 区间内到一个整数，其中 $k$ 为某个整数。当 $k = O(n)$ 时，排序的运行时间为 $\Theta(n)$ .</p><p><strong>计数排序的思想</strong> ：</p><p>对每一个输入元素 $x$，确定小于 $x$ 的元素个数。利用这个信息，就可以直接把 $x$ 放在输出数组正确的位置了。</p><p><strong>COUNTING-SORT(A, B, k)</strong> </p><ul><li>功能：计数排序</li><li>参数：<ul><li>A[1..n]输入的待排序数组，A.length = n</li><li>B[1..n] 存放排序后的输出数组；</li><li>临时存储空间 C[0..k] ，A[1..n]中的元素大小不大于k.</li></ul></li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">let C[<span class="number">0</span><span class="type">..k</span>] be a new array</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">0</span> to k</span><br><span class="line">C[<span class="type">i</span>] = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j = <span class="number">1</span> to A.length</span><br><span class="line">C[<span class="type">A</span>[<span class="type">j</span>]] = C[<span class="type">A</span>[<span class="type">j</span>]] + <span class="number">1</span></span><br><span class="line">//C[<span class="type">i</span>] now contains the number of elements equal to i.</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to k</span><br><span class="line">C[<span class="type">i</span>] = C[<span class="type">i</span>] + C[<span class="type">i</span>-<span class="number">1</span>]</span><br><span class="line">//C[<span class="type">i</span>] now contains the number of elements less than or equal to i.</span><br><span class="line"><span class="keyword">for</span> j = A.length downto <span class="number">1</span></span><br><span class="line">B[<span class="type">C</span>[<span class="type">A</span>[<span class="type">j</span>]]] = A[<span class="type">j</span>]</span><br><span class="line">C[<span class="type">A</span>[<span class="type">j</span>]] = C[<span class="type">A</span>[<span class="type">j</span>]] - <span class="number">1</span></span><br></pre></td></tr></table></figure><p>下图是计数排序的过程：</p><p><a href="https://imgchr.com/i/NhgzmF"><img src="https://s1.ax1x.com/2020/06/29/NhgzmF.md.png" alt="NhgzmF.md.png"></a> </p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Introduction to Algorithms.</li><li>算法导论</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章review了算法中的排序算法，包括冒泡排序、插入排序、归并排序、堆排序（以及用堆实现优先队列）、快速排序和计数排序。&lt;/p&gt;
&lt;p&gt;分别从算法思路、算法伪代码实现、算法流程、算法时间复杂度四个方面阐述每个算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法导论" scheme="https://f7ed.com/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/"/>
    
    
      <category term="Intro-to-Algorithms" scheme="https://f7ed.com/tags/Intro-to-Algorithms/"/>
    
      <category term="Sort" scheme="https://f7ed.com/tags/Sort/"/>
    
      <category term="Algorithms" scheme="https://f7ed.com/tags/Algorithms/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-Dan」:Stream Cipher 3</title>
    <link href="https://f7ed.com/2020/06/26/StreamCipher3/"/>
    <id>https://f7ed.com/2020/06/26/StreamCipher3/</id>
    <published>2020-06-25T16:00:00.000Z</published>
    <updated>2020-07-03T08:45:16.978Z</updated>
    
    <content type="html"><![CDATA[<p>Stream Cipher的第三篇文章。</p><p>文章主要分为两部分，前部分逐步定义Secure PRG的定义，通过引入statistical test（统计测试）和Advantage（优势）得出当且仅当PRG is unpredictable,PRG is secure的结论。</p><p>后部分介绍了密码学中的一个重要概念Semantic Security的定义，通过引入 computationally indistinguishable(计算上不可区分)的概念给出定义，并证明了OTP的语意安全和在安全PRG条件下的流密码的语意安全，得出如果流密码中使用的PRG is secure,那么流密码就具备semantic security。</p><p>文章开头，也简单介绍了密码学中negligible和non-negligible的含义。</p><a id="more"></a><h1 id="Negligible-and-non-negligible"><a href="#Negligible-and-non-negligible" class="headerlink" title="Negligible and non-negligible"></a>Negligible and non-negligible</h1><p><strong>In practice:</strong> $\epsilon$ is a scalar and </p><ul><li>$\epsilon$ non-neg: $\epsilon \geq 1/2^{30}$ (likely to happen over 1GB of data)</li><li>$\epsilon$ negligible: $\epsilon \leq 1/2^{80}$ (won’t happen over life of key)</li></ul><p>在实践中，$\epsilon$ 是一个数值，如果是non-neg不可忽略的话，大约在1GB的数据下就会发生，如果是可忽略的值，在密钥的生存周期内基本不会发生。</p><p><strong>In theory:</strong> $\epsilon$ is a function  $\varepsilon: Z^{\geq 0} \rightarrow R^{\geq 0}$  and</p><ul><li>$\epsilon$ non-neg:  $\exists d: \epsilon(\lambda)\geq 1/\lambda^d$ ($\epsilon \geq 1/\text{poly} $, for many $\lambda$ )</li><li>$\epsilon$ negligible:  $\forall d, \lambda \geq \lambda_{d}:  \varepsilon(\lambda) \leq 1 / \lambda^{d}$  ( $\epsilon \leq 1/\text{poly}$, for large $\lambda$ )</li></ul><p>[0]（理论中，这个还不太理解，待补充。）</p><h1 id="PRG-Security-Defs"><a href="#PRG-Security-Defs" class="headerlink" title="PRG Security Defs"></a>PRG Security Defs</h1><p>Let  $G:K\longrightarrow \{0,1\}^n$  be a PRG.</p><p><u>Goal:</u> define what it means that </p><p>[  $k\stackrel{R}{\longleftarrow} \mathcal{K}$   , output G(k) ] is “indistinguishable” from [  $r\stackrel{R}{\longleftarrow} \{0,1\}^n$   , output r] .</p><p>【使得PGR的输出和真随机是不可区分的】（ $\stackrel{R}{\longleftarrow}$ 的意思是在均匀分布中随机取）</p><p>下图中，红色的圈是全部的 ${0,1}^n$ 串，按照定义是均匀分布。而粉色G()是PRG的输出，由于seed很小，相对于全部的 ${0,1}^n$ ，所以G()的输出范围也很小。</p><p><a href="https://imgchr.com/i/NsuU4s"><img src="https://s1.ax1x.com/2020/06/26/NsuU4s.png" alt="NsuU4s.png"></a> </p><p>因此，attacker观测G(k)的输出，是不能和uniform distribution（均匀分布）的输出区分开。</p><p>这也就是我们所想构造的安全PGR的目标。</p><h2 id="Statistical-Tests"><a href="#Statistical-Tests" class="headerlink" title="Statistical Tests"></a>Statistical Tests</h2><p>Statistical test on ${0,1}^n$ ：有一个算法A，$x\in{0,1}^n$ ,A(x) 根据算法定义输出”0”或”1”.</p><p>统计测试是自定义的。</p><p>Example：</p><ol><li><p>A(x)=1 if  $| \#0(x)-\#1(x)|\leq 10\cdot\sqrt{n}$   </p><p>【期望串中0、1的数目差不多，这样look random】</p></li><li><p>A(x)=1 if  $|\#00(x)-\frac{n}{4}\leq10\cdot\sqrt{n}$   </p><p>【期望Pr(00)=1/4 ,串中00出现的概率为1/4，认为是look random】</p></li><li><p>A(x)=1if max-run-of-0(x) &lt; 10·log(n)</p><p>【期望串中0的最大游程不要超过规定值】</p></li></ol><p>上面的第三个例子，如果输出为全1，满足上述的统计条件输出1，但全1串看起来并不random。</p><p>统计测试也由于是自定义的，所以通过统计测试的也不一定是random，其PRG也不一定是安全的。</p><p>所以，如何评估一个统计测试的好坏？</p><p>下面引入一个重要的定义advantage，优势。</p><h2 id="Advantage"><a href="#Advantage" class="headerlink" title="Advantage"></a>Advantage</h2><p>引入Advantage（优势）来评估一个统计测试的好坏。</p><p>Let G: $k \rightarrow \{0,1\}^n$  be a PRG and A a stat. test on ${0,1}^n$ </p><p>【G是一个PRG，A是一个对01串的统计测试】</p><p>Define: the advantage of statisticaltest A relative to PRG G Adv$_\text{PRG}[A,G]$ </p>  $\text{Adv}_\text{PRG}[A,G]=|Pr[A(G(k))=1]-Pr[A(r)=1]|\in[0,1]$   ,  $k\stackrel{R}{\longleftarrow} \mathcal{K}, r\stackrel{R}{\longleftarrow}\{0,1\}^n$  <p>【定义：Adv$_\text{PRG}[A,G]$ 为统计测试A对于PRG G的优势为统计测试以PRG作为输入输出为1的概率 减去 统计测试以truly random string 作为输入输出为1的概率】</p><ul><li>Adv close to 1 : 统计测试能区分PRG的输出和truly random string，即adversary可以利用区分PRG的输出和random的这一点从而破解系统。</li><li>Adv close to 0 : 统计测试不能区分PRG的输出和truly random string, 即adversary认为PRG的输出和random别无二致。</li></ul><blockquote><p><a href="https://en.wikipedia.org/wiki/Advantage_(cryptography)">Advantage</a> 优势[1]</p><p>In cryptography, an adversary’s advantage is a measure of how successfully it can attack a cryptographic algorithm, by distinguishing it from an idealized version of that type of algorithm.Note that in this context, the “adversary” is itself an algorithm and not a person. A cryptographic algorithm is considered secure if no adversary has a non-negligible advantage, subject to specified bounds on the adversary’s computational resources (see concrete security). </p><p>在密码学中，adversary的优势是评估它通过某种理想算法破解一个加密算法的成功尺度。</p><p>这里的adversary是一种破解算法而不是指攻击者这个人。</p><p>当没有 adversary对该加密算法有不可忽略的优势时，该加密算法被认为是安全的，因为adversary只有有限的计算资源。</p></blockquote><p>e.g.1 : A(x) = 0，统计测试A对PRG的任何输出都输出0，则Adv[A,G] = 0.</p><p>e.g.2 : </p><p>G: $k \rightarrow {0,1}^n$ satisfies msb(G(k))=1 for 2/3 of keys in K.</p><p>Define statistical test A(x) as : if[ msb(x)=1 ] output “1” else output “0”</p><p>【PRG G(k)的2/3的输出的最高有效位是1，定义统计测试A(x),输入的最高有效位为1输出1，否则输出0】</p><blockquote><p>msb: most significant bit,最高有效位。</p></blockquote><p>则 Adv[A,G] = | Pr[A(G(k))] - Pr[A(r)] | = 2/3 - 1/2 = 1/6</p><p>即 A can break the generator G with advantage 1/6, PRG G is not good. </p><h2 id="Secure-PRGs-crypto-definition"><a href="#Secure-PRGs-crypto-definition" class="headerlink" title="Secure PRGs: crypto definition"></a>Secure PRGs: crypto definition</h2><p><u>Def:</u> We say that G: $k \rightarrow {0,1}^n$ is <strong>secure PRG</strong></p><p> if $\forall$ “eff” stat. tests A : Adv$_\text{PRG}$ [A,G]  is “negligible”.</p><p>这里的”eff”,指多项式时间内。</p><p>这个定义，“所有的统计测试”，这一点难以达到，因此没有provably secure PRGs。</p><p>但我们有heuristic candidates.（有限的stat. test 能满足）</p><h3 id="Easy-fact-a-secure-PRG-is-unpredictable"><a href="#Easy-fact-a-secure-PRG-is-unpredictable" class="headerlink" title="Easy fact: a secure PRG is unpredictable"></a>Easy fact: a secure PRG is unpredictable</h3><p>证明命题： a secure PRG is unpredictable.</p><p>即证明其逆否命题： PRG is predictable is insecure。</p><p>suppose A is an eddicient algorithm s.t. $\text{Pr}_{k\stackrel{R}{\longleftarrow}\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\frac{1}{2} + \epsilon$    for non-negligible $\epsilon$ .</p><p>【 算法A是一个有效的预测算法， $\text{Pr}_{k\stackrel{R}{\longleftarrow}\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\frac{1}{2} + \epsilon$   , $\epsilon$ 是不可忽略的值，即A能以大于1/2的概率推测下一位。】</p><p>Adversary能利用算法A来区分这个PRG和random依次来破解系统。</p><p>Define statistical test B as: B(x)=1 if $A(x|<em>{1,…,i})=x</em>{i+1}$ , else B(x)=0.</p><p>【定义一个统计测试B，如果预测算法A预测下一位正确输出1，否则输出0】</p><hr><p>计算Adv$_\text{PRG}$ :</p><ul><li> $r\stackrel{R}{\longleftarrow}\{0,1\}^n$ : Pr[B(r) = 1] = 1/2</li><li> $r\stackrel{R}{\longleftarrow}\{0,1\}^n$  : Pr[B(G(k)) = 1] = 1/2+ $\epsilon$ </li><li>Adv$_\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | &gt; $\epsilon$ </li></ul><p>Adversary 能以 $\epsilon$ 的优势区分PRG和random，因此PRG 不安全。</p><p>所以，如果A是一个好的预测算法，那么B就是一个好的统计算法，Adversary就能通过B以 $\epsilon$ 的优势破解系统。</p><p>在此，证明了 if A can predict PRG, PRG is insecure $\Rightarrow$ <strong>A secure PRG is unpredictable.</strong> </p><h3 id="Thm-Yao’82-an-unpredictable-PRG-is-secure"><a href="#Thm-Yao’82-an-unpredictable-PRG-is-secure" class="headerlink" title="Thm(Yao’82): an unpredictable PRG is secure"></a>Thm(Yao’82): an unpredictable PRG is secure</h3><p>上节证明了A secure PRG is unpredictable.</p><p>在1982 Yao 的论文[2]中证明了其逆命题： an unpredictable PRG is secure.</p><p>G: $k \rightarrow {0,1}^n$ be PRG</p><p><strong>“Thm</strong>“ : <strong>if $\forall i \in$ {0,…, n-1} PRG G is unpredictable at pos. i then G is a secure PRG.</strong></p><p>【定理：如果在任何位置PRG都是不可预测的，那么PRG就是安全的】</p><p><strong>Proof：</strong> </p><ul><li>A: 预测算法： $\forall i \quad\text{Pr}_{k\stackrel{R}{\longleftarrow}\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]=\frac{1}{2} $  </li><li>B:统计测试： B(x)=1 if $A(x|<em>{1,…,i})=x</em>{i+1}$ , else B(x)=0.</li><li>Adv$_\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | = 0 </li></ul><p>If next-bit predictors cannot distinguish G from random then no statistial test can!</p><p>【next-bit predictor指用预测算法的统计测试】</p><p>e.g.</p><p> Let G: $k \rightarrow {0,1}^n$ be PRG such that from the last n/2 bits of G(k) it is easy to compute the first n/2 bits.</p><p>Is G predictable for som i $\in$ {0, …, n-1} ?</p><p>: Yes.当n=2时，可以预测出第一位。 </p><p><strong>在此，可以得出结论：</strong></p><p><strong>Adversary不可区分PRG的输出和truly random string时被认为是安全的。</strong></p><p><strong>当且仅当PRG在任意位置不可预测时，PRG是安全的。</strong> </p><h2 id="More-generally-computationally-indistinguishable"><a href="#More-generally-computationally-indistinguishable" class="headerlink" title="More generally: computationally indistinguishable"></a>More generally: computationally indistinguishable</h2><p>Let P1 and P2 be two distributions over ${0,1}^n$ </p><p><u>Def</u> : We say that P1 and P2 are <strong>computationally indistinguishable</strong> (denoted $P_1\approx_p P_2$) </p><p>If $\forall$ “eff” stat. tests A  $\left|\text{Pr}_{x\leftarrow_{P_1}}[A(x)=1]-\text{Pr}_{x\leftarrow_{P_2}}[A(x)=1]\right|<\text{negligible}$   .</p><p>【P1,P2都是01串上的两个分布，且对任意的有效统计测试满足上式，则认为P1和P2两个分布在计算上不可区分，记做$P_1\approx_p P_2$】</p><p>所以，安全的PRG等价于G(k)的分布和均匀分布计算上不可区分，即 $\{\mathrm{k} \stackrel{\mathrm{R}}{\longleftarrow}{\mathrm{K}}: \mathrm{G}(\mathrm{k})\} \approx_{\mathrm{p}} \text { uniform }\left(\{0,1\}^{\mathrm{n}}\right)$  </p><h1 id="Semantic-Security"><a href="#Semantic-Security" class="headerlink" title="Semantic Security"></a>Semantic Security</h1><p>上文详细了定义secure PRGs 的过程，本节的目标是定义”secure” stream cipher,安全流密码。</p><p><strong>What is a secure cipher?</strong></p><p>attacker abilities: obtains one ciphertext(唯密文攻击)</p><p>下面这些可能的安全需求是否能定义是安全的加密解密算法？</p><p>Possible security requirements：</p><ol><li><p>attacker cannot recover secret key.</p><p>attacker不能恢复密钥。</p><p>E(k,m)=m直接输出明文，这个算法满足条件，但显然不安全。</p></li><li><p>attacker cannot recover all of plaintext.</p><p>attacker不能恢复所有的明文。</p><p>E(k, m0||m1) = m0||k $\oplus$ m1,该算法泄漏了一半的明文，不安全。</p></li><li><p>Shannon’s idea: CT should reveal no “info” about PT.</p></li></ol><h2 id="Recall-Shannon‘s-perfect-secrecy"><a href="#Recall-Shannon‘s-perfect-secrecy" class="headerlink" title="Recall Shannon‘s perfect secrecy"></a>Recall Shannon‘s perfect secrecy</h2><p>在<a href="/2020/03/15/StreamCipher1/" title="这篇文章1.2.2">这篇文章1.2.2</a> 定义了perfect secrecy。</p><p>A cipher $(E,D)$ over  $\mathcal{(K,M,C)}$ has <strong>perfect security</strong> if  $\forall m_0,m_1 \in \mathcal{M}\ (|m_0|=|m_1|) \quad \text{and} \quad \forall c\in \mathcal{C} $ </p>$$Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \qquad \text{where} \ k\overset{R}{\longleftarrow}\mathcal{K}$$<p>下面根据perfect security的定义逐步放宽条件定义：</p><p>Let (E,D) be a cipher over (K,M,C)</p><ul><li><p><a href="https://imgchr.com/i/NsutEQ"><img src="https://s1.ax1x.com/2020/06/26/NsutEQ.md.png" alt="NsutEQ.md.png"></a> </p><p>上图中的定义就是perfect security的定义，密钥加密m0和m1的分布完全相同。</p><p>引入上文中的computationally indistinguishable（计算不可区分）的概率放宽条件，得到下面的定义。</p></li><li><p><a href="https://imgchr.com/i/NsuGDS"><img src="https://s1.ax1x.com/2020/06/26/NsuGDS.md.png" alt="NsuGDS.md.png"></a></p><p>密钥加密m0和m1的分布计算上不可区分。</p></li><li><p>但枚举所有的m0和m1的定义也太强了，因此希望能让adversary列举计算上可列举的m0和m1.</p></li></ul><h2 id="Semantic-Security-for-OTP"><a href="#Semantic-Security-for-OTP" class="headerlink" title="Semantic Security for OTP"></a>Semantic Security for OTP</h2><h3 id="attack-game"><a href="#attack-game" class="headerlink" title="attack game"></a>attack game</h3><p>在定义OTP的semantic security （语意安全）之前，定义一个attack game。</p><p><strong>Attack Game</strong>[3]: For a given cipher $\mathcal{E}$ = (E, D), deﬁned over (K, M , C), and for a given adversary $\mathcal{A}$ , we deﬁne two experiments, Experiment 0 and Experiment 1. For b = 0, 1, we deﬁne</p><p>【定义一个adversary $\mathcal{A}$，两个实验也对应两个challenger，实验定义如下】</p><p><strong>Experiment b:</strong></p><ul><li><p>The adversary computes m0 , m1 ∈ M , of the same length, and sends them to the challenger.</p><p>【adversary向challenger发m0和m1，他们长度相同】</p></li><li><p>The challenger computes $k\stackrel{R}{\leftarrow}\mathcal{K}$, $c\stackrel{R}{\leftarrow}E(k,m_b)$, and sends c to the adversary.</p><p>【实验b中challenger随机选取一个密钥k,用密钥加密mb,再将结果c返回给adversary】</p></li><li><p>The adversary outputs a bit $\hat{b}\in$  { 0, 1 } .</p><p>【adversary收到c，猜测c是加密m0还是m1的，输出 $\hat{b}$ 】</p></li></ul><p>过程如下图所示，对于adversary $\mathcal{A}$ 来说，就像只有一个challenger，没有EXP(0)和EXP(1)的区别，因为它本来就是猜测challenger发送的c是加密m0所的还是m1所的。而对于challenger来说，才有EXP(0)和EXP(1)，在EXP(0)中，加密算法随机选择密钥加密m0返回，在EXP(1)中，加密算法随机选择密钥加密m1返回。</p><p><a href="https://imgchr.com/i/Nsuu4A"><img src="https://s1.ax1x.com/2020/06/26/Nsuu4A.md.png" alt="Nsuu4A.md.png"></a> </p><p>for b=0,1 $W_b$ = [event that $\mathcal{A}$  output 1 in experiment b].</p><p>【定义事件 $W_b$ 为在实验EXP(b)中$\mathcal{A}$ 输出1】</p><p><u>Def</u>： $\mathcal{A}$‘s <strong>semantic security advantage</strong> with respect to $\mathcal{E}$ as </p>$$\operatorname{Adv_{SS}}[\mathcal{A}, \mathcal{E}]:=\left|\operatorname{Pr}\left[W_{0}\right]-\operatorname{Pr}\left[W_{1}\right]\right|$$<p>【定义adversary $\mathcal{A}$ 对于cipher $\mathcal{E}$ 的语意安全优势为上式】</p><p>Note that in the above game, the events W0 and W1 are deﬁned with respect to the probability space determined by the random choice of k, the random choices made (if any) by the encryption algorithm, and the random choices made (if any) by the adversary. The value Advss[ A , E] is a number between 0 and 1.</p><p>【注意：上面的游戏中，W0和W1的概率空间分布是与密钥的随机选择，加密算法的随机选择（<strong>该加密算法选择的是challenger0加密的m0还是challenger1加密的m1，adversary并不知道</strong>）和adversary的”随机”输出（按照adversary的判断方式的)有关的】</p><p>当challenger用随机密钥加密m0/m1时，观测adversary的行为是否相同，如果   $\left|\operatorname{Pr}\left[W_{0}\right]-\operatorname{Pr}\left[W_{1}\right]\right|$   等于0，说明adversary无法区分这两个实验，如果  $\left|\operatorname{Pr}\left[W_{0}\right]-\operatorname{Pr}\left[W_{1}\right]\right|$   等于1，则adversary可以区分这两个实验。</p><h3 id="semantic-security"><a href="#semantic-security" class="headerlink" title="semantic security"></a>semantic security</h3><p><u>Def</u>：<strong>semantic security</strong>：A cipher $\mathcal{E}$ is semantially secure if for all efficient $\mathcal{A}$ , the value  $\operatorname{Adv_{SS}}[\mathcal{A}, \mathcal{E}]$  is negligible.</p><p>当所有的有效的攻击算法 $\mathcal{A}$ 对于加密算法 $\mathcal{E}$ 的语意安全优势是可忽略的，则认为加密算法 $\mathcal{E}$ 是语意安全的。</p><p>即没有有效的攻击算法能区分对m0和m1对加密结果。</p><p>即任意密钥加密m0 m1的分布是计算上不可区分的，for all explicit m0, m1 $\in M:$   $\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{0}\right)\right\} \approx_{\mathrm{p}}\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{1}\right)\right\}$ .</p><h3 id="example-1"><a href="#example-1" class="headerlink" title="example 1"></a>example 1</h3><p>Suppose efficient A can always deduce LSB of PT from CT.</p><p>【假设算法A总能从CT中推断出PT的最低有效位】</p><p>在下图cipher系统中，Adversary B算法向challenger发送的m0和m1，其发送的m0 m1特点是LSB(m0)=0,LSB(m1)=1。</p><p><a href="https://imgchr.com/i/NsuJHg"><img src="https://s1.ax1x.com/2020/06/26/NsuJHg.md.png" alt="NsuJHg.md.png"></a> </p><p>当adversary B收到密文c时，利用有效的算法A推断出对应明文的最低有效位LSB，如果是0，则输出0，即认为是实验0中challenger加密的m0，反之输出1。</p><p>adversary B对于cipher $\mathcal{E}$ 的优势是：  $\operatorname{Adv_{SS}}[\mathcal{B}, \mathcal{E}]:=\left|\operatorname{Pr}\left[\text{EXP}(0)=1\right]-\operatorname{Pr}\left[\text{EXP}(1)=1\right]\right|=|0-1|=1$ </p><p> 所以该cipher $\mathcal{E}$ 不具有语意安全。</p><p>不仅是LSB，<strong>如果adversary能从密文中学到明文的任何一位，则该cipher都不具有semantic security。</strong></p><h3 id="example-2-OTP"><a href="#example-2-OTP" class="headerlink" title="example 2: OTP"></a>example 2: OTP</h3><p>OTP虽然不实用，但是具备perfect security，下面来证明OTP也具有semantic security。</p><p><a href="https://imgchr.com/i/NsunNd"><img src="https://s1.ax1x.com/2020/06/26/NsunNd.md.png" alt="NsunNd.md.png"></a> </p><p>OTP的perfect security的性质是  $Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \qquad \text{where} \ k\overset{R}{\longleftarrow}\mathcal{K}$  ，k和任意m异或都是均匀分布，因此上图中的  $\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{0}\right)\right\} =\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{1}\right)\right\}$  是同分布。</p><p>For all A :  $\operatorname{Adv}_{\operatorname{ss}}[A, \text { OTP }]=\mid \operatorname{Pr}\left[A\left(\mathbf{k} \oplus m_{0}\right)=1\right]-\operatorname{Pr}\left[A\left(\mathbf{k} \oplus m_{1}\right)=1\right]|=0$   </p><h2 id="Stream-Ciphers-are-semantically-secure"><a href="#Stream-Ciphers-are-semantically-secure" class="headerlink" title="Stream Ciphers are semantically  secure"></a>Stream Ciphers are semantically  secure</h2><p>前面介绍了什么是安全的PRG，什么是语意安全，而流密码是否具有语意安全呢？</p><p><strong>Thm:  Let G: $k \rightarrow {0,1}^n$ is a secure PRG $\Rightarrow$ stream cipher E derived from G is semantic secure.</strong></p><p> 要证明以上定理：即要证明 $\forall$ sem. sec. adversary A $\operatorname{Adv}_{\operatorname{SS}}[A,E]$  is negligible.</p><p>如果 $\forall$ sem. sec. adversary A, $\exist$ a PRG adversay B s.t.  $A d v_{s s}[A, E] \leq 2 \cdot A d v_{P R G}[B, G]$ 不等式成立，则定理得证。</p><p>因为G is a secure PRG, 根据定义，Adv$<em>\text{PRG}$ [A,G]  is negligible，所以左边$\operatorname{Adv}</em>{\operatorname{SS}}[A,E]$  is negligible.</p><h3 id="Proof-intuition"><a href="#Proof-intuition" class="headerlink" title="Proof: intuition"></a>Proof: intuition</h3><p>如果直观上证明Adv$_\text{PRG}$ [A,G]  is negligible，如下图：</p><p><a href="https://imgchr.com/i/Nsu1jf"><img src="https://s1.ax1x.com/2020/06/26/Nsu1jf.md.png" alt="Nsu1jf.md.png"></a> </p><p>（上左上右下左下右：图1234，G(k):PRG 的输出， r: truly random string）</p><p>图1和图2中，由于G is a secure PRG，因此A计算上无法区分用G(k)和 random对m0加密的结果， 即$\ E(m_0,\mathrm{G}(\mathrm{k}))\} \approx_{\mathrm{p}} \{E(m_0,r)\}$  。同理图三图四中，A计算上也无法区分用G(k)和random 对m1加密的结果，即  $\ E(m_1,\mathrm{G}(\mathrm{k}))\} \approx_{\mathrm{p}} \{E(m_1,r)\}$  。</p><p>而图2图4中，是OPT的，即r和任意m异或都是均匀分布，满足 $\left\{\mathrm{E}\left(\mathrm{r}, \mathrm{m}_{0}\right)\right\} =\left\{\mathrm{E}\left(\mathrm{r}, \mathrm{m}_{1}\right)\right\}$  是同分布.(图中应该可以写严格的等号)</p><p>所以可以直观得到  $\ E(m_0,\mathrm{G}(\mathrm{k}))\} \approx_{\mathrm{p}} E(m_1,\mathrm{G}(\mathrm{k}))\}$ ，即$\forall$ sem. sec. adversary A $\operatorname{Adv}_{\operatorname{SS}}[A,E]$  is negligible.</p><h3 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h3><p>Let A be a sem. sec. adversary.</p><p>使用PRG，即$E(m,k) = m\oplus G(k)$ 时，如下图：</p><p><a href="https://imgchr.com/i/NsuEnO"><img src="https://s1.ax1x.com/2020/06/26/NsuEnO.md.png" alt="NsuEnO.md.png"></a> </p><p> for b = 0,1 Wb = [event that b’=1]</p><p>$\operatorname{Adv}_{\operatorname{SS}}[A,E]=|\text{Pr}[W_0]-\text{Pr}[W_1]|$  </p><hr><p>使用OTP时，即 $E(m,r)=m\oplus r$ 时，如下图：</p><p><a href="https://imgchr.com/i/NsuVBD"><img src="https://s1.ax1x.com/2020/06/26/NsuVBD.md.png" alt="NsuVBD.md.png"></a> </p><p>for b = 0,1 Rb = [event that b’=1]</p><p>$\operatorname{Adv}_{\operatorname{SS}}[A,OTP]=|\text{Pr}[R_0]-\text{Pr}[R_1]|=0$ </p><hr><p>如果把Pr[]的关系画在数轴上，如下图：</p><p><a href="https://imgchr.com/i/NsuZHe"><img src="https://s1.ax1x.com/2020/06/26/NsuZHe.md.png" alt="NsuZHe.md.png"></a> </p><ol><li><p>根据OTP的semantic security ，已经得到 $|\text{Pr}[R_0]-\text{Pr}[R_1]|=\operatorname{Adv}_{\operatorname{SS}}[A,OTP]=0$ ，所以Pr[R0]=Pr[R1]。</p></li><li><p>因为G是secure PRG，所以 $\exists$ adversary B满足 $|\text{Pr}[W_b]-\text{Pr}[R_b]|=\operatorname{Adv}_{\operatorname{PRG}}[B,G]$ ,如下图：</p><p><a href="https://imgchr.com/i/Nsu8u8"><img src="https://s1.ax1x.com/2020/06/26/Nsu8u8.md.png" alt="Nsu8u8.md.png"></a> </p></li></ol><p>所以根据数轴的关系，Pr[W0]和Pr[W1]的距离最大为 $2 \cdot \mathrm{Adv}_{\mathrm{PRG}}[\mathrm{B}, \mathrm{G}]$ .</p><p>即  $\Rightarrow\mathrm{Adv}<em>{\mathrm{SS}}[\mathrm{A}, \mathrm{E}]=\left|\operatorname{Pr}\left[\mathrm{W}</em>{0}\right]-\operatorname{Pr}\left[\mathrm{W}<em>{1}\right]\right| \leq 2 \cdot \mathrm{Adv}</em>{\mathrm{PRG}}[\mathrm{B}, \mathrm{G}]$  </p><p>证毕。</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>secure PRG 就是计算上无法区分G(k)的输出和truly random string。</p><p>semantic secure 就是计算上无法区分m0和m1的加密结果。</p><p>而使用secure PRG的流密码是具有semantic security的。</p><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ol start="0"><li><p>Negligible, super-poly, and poly-bounded functions.(Book p28)</p></li><li><p>Advantage Wiki定义：<a href="https://en.wikipedia.org/wiki/Advantage_(cryptography)">https://en.wikipedia.org/wiki/Advantage_(cryptography)</a></p></li><li><p>Yao’82: Theory and application of trapdoor functions:<a href="https://ieeexplore.ieee.org/document/4568378">https://ieeexplore.ieee.org/document/4568378</a></p></li><li><p>Dan Boneh and Victor Shoup: A Graduate Course in Applied Cryptography</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Stream Cipher的第三篇文章。&lt;/p&gt;
&lt;p&gt;文章主要分为两部分，前部分逐步定义Secure PRG的定义，通过引入statistical test（统计测试）和Advantage（优势）得出当且仅当PRG is unpredictable,PRG is secure的结论。&lt;/p&gt;
&lt;p&gt;后部分介绍了密码学中的一个重要概念Semantic Security的定义，通过引入 computationally indistinguishable(计算上不可区分)的概念给出定义，并证明了OTP的语意安全和在安全PRG条件下的流密码的语意安全，得出如果流密码中使用的PRG is secure,那么流密码就具备semantic security。&lt;/p&gt;
&lt;p&gt;文章开头，也简单介绍了密码学中negligible和non-negligible的含义。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cryptography-Dan" scheme="https://f7ed.com/categories/Cryptography-Dan/"/>
    
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="StreamCipher" scheme="https://f7ed.com/tags/StreamCipher/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Recurrent Neural Network（RNN）</title>
    <link href="https://f7ed.com/2020/06/11/rnn/"/>
    <id>https://f7ed.com/2020/06/11/rnn/</id>
    <published>2020-06-10T16:00:00.000Z</published>
    <updated>2020-07-03T08:43:33.524Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中首先从RNN能解决什么问题入手，分析了RNN与一般NN的区别。<br>然后着重讲解了基于RNN的LSTM模型，包括LSTM的细节、和一般NN的区别，以及如何训练LSTM模型。<br>具体阐述了在模型（RNN类模型）训练过程中为什么会遇到剧烈抖动问题和如何解决抖动的工程解决方法。</p><a id="more"></a><h1 id="Example-application"><a href="#Example-application" class="headerlink" title="Example application"></a>Example application</h1><h2 id="Solt-filling"><a href="#Solt-filling" class="headerlink" title="Solt filling"></a>Solt filling</h2><p>先从RNN的应用说起，RNN能做什么？</p><p>RNN可以做智慧系统：</p><p>如下图中，用户告诉订票系统：”I would like to arrive Taipei on November 2nd”.</p><p>订票系统能从这句话中得到Destination: Taipei，time of arrival: November 2nd.</p><p><a href="https://imgchr.com/i/tqZaQJ"><img src="https://s1.ax1x.com/2020/06/11/tqZaQJ.md.png" alt="tqZaQJ.md.png"></a> </p><p>这个过程也就是<strong>Solt Filling</strong> （槽位填充）。</p><p>如果用Feedforward network来解决solt filling问题，输入就是单词，输出是每个槽位（slot）的单词，如下图。</p><p><a href="https://imgchr.com/i/tqZ8oV"><img src="https://s1.ax1x.com/2020/06/11/tqZ8oV.md.png" alt="tqZ8oV.md.png"></a></p><p>上图中，如何将word表示为一个vector？</p><h2 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h2><p>How to represent each word as a vector?</p><h3 id="1-of-N-encoding"><a href="#1-of-N-encoding" class="headerlink" title="1-of-N encoding"></a>1-of-N encoding</h3><p>最简单的方式是1-of-N encoding方式（独热方式）。</p><p>向量维度大小是整个词汇表的大小，每一个维度代表词汇表中的一个单词，如果该维度置1，表示这个维度代表的单词。</p><h3 id="Beyond-1-of-N-encoding"><a href="#Beyond-1-of-N-encoding" class="headerlink" title="Beyond 1-of-N encoding"></a>Beyond 1-of-N encoding</h3><p>对1-of-N encoding方式改进。</p><p>第一种：<u>Dimension for “Other”</u> </p><p><a href="https://imgchr.com/i/tqZNz4"><img src="https://s1.ax1x.com/2020/06/11/tqZNz4.md.png" alt="tqZNz4.md.png"></a> </p><p>在1-of-N的基础上增加一维度——‘other’维度，即当单词不在系统词汇表中，将other维度置1代表该单词。</p><p>第二种：<u>Word hashing</u> </p><p><a href="https://imgchr.com/i/tqZtWF"><img src="https://s1.ax1x.com/2020/06/11/tqZtWF.md.png" alt="tqZtWF.md.png"></a> </p><p>即便是增加了”other”维度，编码vector的维度也很大，用word hashing的方式将大幅减少维度。</p><p>以apple为例，拆成app, ppl, ple三个部分，如上图所示，vector中表示这三个部分的维度置1。</p><p>用这样的word hashing方式，vector的维度只有 $26\times 26\times26$ ，大幅减少词向量的维度。</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>通过encoding的方式，单词用vector来表示，用前馈神经网络来解决solt filling问题。</p><p>如下图.</p><p>input:一个单词（encoding为vector）</p><p>output: input单词中属于该槽位(solts)的概率分布(vector)。</p><p><a href="https://imgchr.com/i/tqZ8oV"><img src="https://s1.ax1x.com/2020/06/11/tqZ8oV.md.png" alt="tqZ8oV.md.png"></a> </p><hr><p>但用普通的前馈神经网络处理solt filling问题会出现下图问题：</p><p><a href="https://imgchr.com/i/tqZJiT"><img src="https://s1.ax1x.com/2020/06/11/tqZJiT.md.png" alt="tqZJiT.md.png"></a> </p><p>上图中，arrive Taipei on November 2nd 和 leave Taipei on November 2nd，将这两句话的每个单词（vector）放入前馈神经网络，得出的dest槽位都应该是Taipei。</p><p>但，通过之前的语意，arrive Taipei的Taipei应该是终点，而leave Taipei的Taipei是起点。</p><p>因此，在处理这种问题时，我们的神经网络应该需要memory，对该输入的上下文有一定的记忆存储。</p><h1 id="Recurrent-Neural-Network-RNN"><a href="#Recurrent-Neural-Network-RNN" class="headerlink" title="Recurrent Neural Network(RNN)"></a>Recurrent Neural Network(RNN)</h1><h2 id="Basic-structure"><a href="#Basic-structure" class="headerlink" title="Basic structure"></a>Basic structure</h2><p>因此，我们对一般的前馈神经网络加入记忆元件a, a 存储hidden layer的输出，同时a也作为下一次计算的输入部分,下图就是最基础的RNN模型。</p><p><a href="https://imgchr.com/i/tqZYJU"><img src="https://s1.ax1x.com/2020/06/11/tqZYJU.md.png" alt="tqZYJU.md.png"></a> </p><p>举一个例子来说明该过程：</p><p>Input sequence: $\begin{bmatrix}1 \ 1 \end{bmatrix}$  $\begin{bmatrix}1 \ 1 \end{bmatrix}$  $\begin{bmatrix}2 \ 2 \end{bmatrix}$ …</p><p>RNN模型如下图所示：所有的weight都是1，没有bias; 所有的神经元的activation function 都是线性的。</p><ol><li><p>input : $\begin{bmatrix}1 \ 1 \end{bmatrix}$, 记忆元件初值 a1=0 a2=0.</p><p><a href="https://imgchr.com/i/tqZ3d0"><img src="https://s1.ax1x.com/2020/06/11/tqZ3d0.md.png" alt="tqZ3d0.md.png"></a> </p><p>记忆元件也作为输入的一部分，hidden layer的输出为 2 2, 更新记忆元件的值.</p><p>output: $\begin{bmatrix}4 \ 4 \end{bmatrix}$ , 记忆元件存储值 a1=2 a2=2.</p></li><li><p>input : $\begin{bmatrix}1 \ 1 \end{bmatrix}$  , 记忆元件存储值 a1=2 a2=2.</p><p><a href="https://sbimg.cn/image/0000D"><img src="https://wx2.sbimg.cn/2020/06/11/2020-06-11-8.42.06.md.png" alt="2020-06-11-8.42.06.md.png"></a> </p></li></ol><p>   记忆元件也作为输入的一部分，hidden layer 的输出为6 6,更新记忆元件的值。</p><p>   output: $\begin{bmatrix}12 \ 12 \end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6.</p><p>   这里可以发现，第一次和第二次的输入相同，但是由于有记忆元件的缘故，两次输出不同。</p><ol start="3"><li><p>input : $\begin{bmatrix}2 \ 2 \end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6.</p><p><a href="https://imgchr.com/i/tqZQLn"><img src="https://s1.ax1x.com/2020/06/11/tqZQLn.md.png" alt="tqZQLn.md.png"></a> </p><p>记忆元件也作为输入的一部分，hidden layer 的输出为16 16,更新记忆元件的值。</p><p>output: $\begin{bmatrix}32 \ 32 \end{bmatrix}$ , 记忆元件存储值 a1=16 a2=16.</p></li></ol><p>RNN中，由于有memory，会和一般前馈模型有两个不同的地方：一是输入相同的vector，输出可能是不同的；二是将一个sequence连续放进RNN模型中，如果sequence中改变顺序，输出也大多不同。</p><hr><p>用这个RNN模型来解决之前的solt filling问题，就可以解决上下文语意不同影响solt的问题。</p><p>将arrive Taipei on November 2nd的每个单词都放入同样的模型中。</p><p><a href="https://imgchr.com/i/tqZAdP"><img src="https://s1.ax1x.com/2020/06/11/tqZAdP.md.png" alt="tqZAdP.md.png"></a> </p><p>因此将RNN展开，如上图，像不同时间点的模型，但其实是不同时间点循环使用同一个模型。</p><p><a href="https://imgchr.com/i/tqZKMj"><img src="https://s1.ax1x.com/2020/06/11/tqZKMj.md.png" alt="tqZKMj.md.png"></a> </p><p>由于左边的前文是arrive，右边的前文是leave，所以存储在memory中的值不同，Taipei作为input的输出（槽位的概率分布）也不同。</p><h2 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h2><p>上文中只是RNN模型中的一种，即Elman Network，记忆元件存储的是上一个时间点hidden layer的输出。</p><p>而Jordan Network模型中,他的记忆元件存储的是上一时间点的output。</p><p>（据说，记忆元件中存储output的值会有较好的performance，因为output是有target vector的，因此能具象的体现放进memory的是什么）</p><p><a href="https://imgchr.com/i/tqZEIf"><img src="https://s1.ax1x.com/2020/06/11/tqZEIf.md.png" alt="tqZEIf.md.png"></a> </p><h2 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h2><p>上文中的RNN模型，记忆元件中存储的都是上文的信息，如果要同时考虑上下文信息，即是bidirectional RNN(双向RNN)。</p><p>模型如下图。</p><p><a href="https://sbimg.cn/image/002sN"><img src="https://wx1.sbimg.cn/2020/06/11/2020-06-11-8.45.55.md.png" alt="2020-06-11-8.45.55.md.png"></a> </p><p>双向RNN的好处是看的范围比较广，当计算输出 $y^t$ 时，上下文的内容都有考虑到。</p><h2 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory(LSTM)"></a>Long Short-term Memory(LSTM)</h2><p>现在最常用的RNN模型是LSTM，Long Short-term Memory，这里的long是相当于上文中的RNN模型，因为上文提到的RNN模型都是short-term,即每一个时间点，都会把memory中的值洗掉，LSTM的long，就是会把memory的值保留的相对于久一些。</p><p>LSTM如下图，与一般NN不同的地方是，他有4个inputs,一个outputs。</p><p><a href="https://imgchr.com/i/tqZmRg"><img src="https://s1.ax1x.com/2020/06/11/tqZmRg.md.png" alt="tqZmRg.md.png"></a> </p><p>LSTM主要有四部分组成：</p><ul><li>Input Gate：输入门，下方箭头是输入，左方箭头是输入信号控制输入门的打开程度，完全打开LSTM才能将输入值完全读入，打开的程度也是NN自己学。</li><li>Output Gate：输出门，上方箭头是输出，左方箭头是输入信号控制输出门的打开程度，同理，打开程度也是NN自己学习。</li><li>Memory Cell：记忆元件。</li><li>Forget Gate：遗忘门，右边的箭头是输入信号控制遗忘门的打开程度，控制将memory cell洗掉的程度。</li></ul><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>更详细的阐述LSTM的内部机制：</p><p>注意：</p><ul><li>$z_o,z_i,z_f$ 是门的signal control,其实就等同于一般NN中neuron的输入z，是scalar。</li><li>gate其实就是一个neuron，通常gate neuron 的activation function f取 sigmod,因为值域在0到1之间，即对应门的打开程度。</li><li>input/forget/output gate的neuron的activation function是f(sigmod function), input neuron的activation function是g。</li></ul><p><a href="https://imgchr.com/i/tqZZi8"><img src="https://s1.ax1x.com/2020/06/11/tqZZi8.md.png" alt="tqZZi8.md.png"></a> </p><ol><li>input gate控制输入:$g(z)f(z_i)$ <ul><li>input:  z $\rightarrow$  $g(z)$ </li><li>input gate signal control:  $z_i \rightarrow f(z_i)$ </li><li>multiply：$g(z)f(z_i)$ </li></ul></li><li>forget gate 控制memory：$cf(z_f)$ <ul><li>forget gate signal control: $z_f\rightarrow f(z_f)$ </li><li>如果 $f(z_f)=1$ ,说明memory里的值保留；如果 $f(z_f)=0$ ,说明memory里的值洗掉。</li></ul></li><li>更新当前时间点的memory(输入+旧的memory值) ：$c’=g(z)f(z_i)+cf(z_f)$ </li><li>output gate 控制输出：$h(c’)f(z_o)$ <ul><li>output: $c’ \rightarrow h(c’)$ </li><li>output gare signal control:  $z_o \rightarrow f(z_o)$ </li><li>multiply: $h(c’)f(z_o)$ </li></ul></li></ol><hr><p>LSTM模型（trained）如下图：</p><p>输入序列为 $\begin{bmatrix}3 \ 1 \ 0 \end{bmatrix}$$\begin{bmatrix}4 \ 1 \ 0 \end{bmatrix}$ $\begin{bmatrix}2 \ 0 \ 0 \end{bmatrix}$ $\begin{bmatrix}1 \ 0 \ 1 \end{bmatrix}$ $\begin{bmatrix}3 \ -1 \ 0 \end{bmatrix}$ </p><p><a href="https://imgchr.com/i/tqZkZt"><img src="https://s1.ax1x.com/2020/06/11/tqZkZt.md.png" alt="tqZkZt.md.png"></a> </p><p>该LSTM activation function: g、h都为linear function（即输出等于输入），f为sigmod.</p><p>通过该LSTM的输出序列为： 0 0 0 7 0 0</p><p>（建议手算一遍）</p><h2 id="Compared-with-Original-Network"><a href="#Compared-with-Original-Network" class="headerlink" title="Compared with Original Network"></a>Compared with Original Network</h2><p>original network如下图：</p><p><a href="https://imgchr.com/i/tqVXa6"><img src="https://s1.ax1x.com/2020/06/11/tqVXa6.md.png" alt="tqVXa6.md.png"></a> </p><p>LSTM 的NN即用LSTM替换原来的neuron，这个neuron有四个inputs，相对于original network也有4倍的参数，如下图：</p><p><a href="https://imgchr.com/i/tqZSRe"><img src="https://s1.ax1x.com/2020/06/11/tqZSRe.md.png" alt="tqZSRe.md.png"></a> </p><hr><p>所以原来RNN的neuron换为LSTM，就是下图：</p><p><a href="https://imgchr.com/i/tqVjIK"><img src="https://s1.ax1x.com/2020/06/11/tqVjIK.md.png" alt="tqVjIK.md.png"></a> </p><p>上图中：</p><p>这里的 $z^f,z^u,z,z^o$ 都是 $x^t \begin{bmatrix} \quad\end{bmatrix}$ 矩阵运算得到的vector, 因为上图中有多个LSTM，因此 $z^i$ 的第k个元素，就是控制第k个LSTM的input signal control scalar。所以，$z^f,z^u,z,z^o$ 的维度等于下一层neuron/LSTM的个数。</p><p>所以这里memory（cell）$c^t$ 也是一个vector，第k个元素是第k个LSTM中cell存储的值。</p><p>向量运算和scalar一样，LSTM细节如下图：</p><p><a href="https://imgchr.com/i/tqVxPO"><img src="https://s1.ax1x.com/2020/06/11/tqVxPO.md.png" alt="tqVxPO.md.png"></a> </p><h2 id="Extension：“peephole”"><a href="#Extension：“peephole”" class="headerlink" title="Extension：“peephole”"></a>Extension：“peephole”</h2><p>上小节的LSTM是simplified，将LSTM hidden layer的输出 $h^t$ 和cell中存储的值 $c^t$  和下一时间点的输入 $x^{t+1}$ 一同作为下一时间点的输入，就是LSTM的扩展版”peephole”。</p><p>如下图：</p><p><a href="https://imgchr.com/i/tqVqq1"><img src="https://s1.ax1x.com/2020/06/11/tqVqq1.md.png" alt="tqVqq1.md.png"></a> </p><h2 id="Multi-layer-LSTM"><a href="#Multi-layer-LSTM" class="headerlink" title="Multi-layer LSTM"></a>Multi-layer LSTM</h2><p>多层的peephole LSTM如下图：</p><p><img src="https://i.loli.net/2020/06/11/sD46OVQpxokjiw8.png" alt="1截屏2020-04-19 下午4.41.50.png"> </p><p>（：wtf 我到底看到了什么</p><p>不要怕：Keras PyTorch等套件都有 “LSTM”，“GUR，”SimpleRNN“ 已实现好的layers.</p><h2 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h2><p>训练RNN时，输入与target如下所示：</p><p><img src="https://i.loli.net/2020/06/11/AK1kSPltUDaqVmR.png" alt="截屏2020-06-11 下午8.52.09.png"> </p><p>估测模型的好坏，计算RNN的Loss时，需要看作一个整体，计算每个时间点RNN输出与target的crossentropy的和。</p><p>训练也可同样用Backpropagation，但考虑到时间点，有一个进阶版的”Backpropogation through time(BPTT)”[1]。</p><p>RNN一般就用BPTT训练。</p><h2 id="How-to-train-well"><a href="#How-to-train-well" class="headerlink" title="How to train well"></a>How to train well</h2><h3 id="not-easy-to-train"><a href="#not-easy-to-train" class="headerlink" title="not easy to train"></a>not easy to train</h3><p>RNN-based network is not always easy to learn.</p><p>但基于RNN的模型往往不太好训练，总是会出现下图中的绿色线情况（即抖动）。</p><p><a href="https://imgchr.com/i/tqVbrR"><img src="https://s1.ax1x.com/2020/06/11/tqVbrR.md.png" alt="tqVbrR.md.png"></a> </p><h3 id="error-surface-is-rough"><a href="#error-surface-is-rough" class="headerlink" title="error surface is rough"></a>error surface is rough</h3><p>error surface，即total loss在参数变化时的函数图。</p><p>会发现基于RNN的模型的error surface会长下图这个样子：有时很平坦(flat)有时很陡峭(steep)</p><p><a href="https://imgchr.com/i/tqZiqI"><img src="https://s1.ax1x.com/2020/06/11/tqZiqI.md.png" alt="tqZiqI.md.png"></a> </p><ul><li><p>橙色点出发：</p><ul><li>起初处在flat的位置。</li><li>随着一次次更新，gradient在变小，learning rate即会变大。</li><li>可能稍微不幸，就会出现跨过悬崖，即出现了剧烈震荡的问题。</li><li>如果刚好当前处在悬崖低，这时的gradient很大，learning rate也很大，step就会很大，飞出去，极可能出现segment fault(NaN).</li></ul></li><li><p>Thomas Mikolv 用工程师的角度来解决这个问题，即当此时的gradient大于某个阈值(threshold)时，就不要让当前的gradient超过这个阈值（通常取15）。</p></li><li><p>这样处在悬崖低的橙色点，（Clipping路线），更新就会到绿色的，继续更新。</p></li></ul><h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><p>为什么RNN模型会出现抖动的情况呢？</p><p>用下图这个简单例子说明（一般activation function用sigmod,而ReLu的performance一般较差）：</p><p><a href="https://imgchr.com/i/tqVh5T"><img src="https://s1.ax1x.com/2020/06/11/tqVh5T.md.png" alt="tqVh5T.md.png"></a> </p><p>上图中，输入序列是1 0 0 0 …，memory连接下一个时间点的权重是w，可以轻易得到最后一个时间点的输出 $y^{1000}=w^{999}$ 。</p><p>上图中，循环输出1000次，如果w变化 $\Delta w$  ，看输出 $y^{1000}$ 的变化，来直观体现gradient 的变化：</p><p><a href="https://imgchr.com/i/tqV5PU"><img src="https://s1.ax1x.com/2020/06/11/tqV5PU.md.png" alt="tqV5PU.md.png"></a> </p><p>上图中，可以看出：</p><ul><li>绿色部分：当w从1变化为1.01时， $y^{1000}$ 的输出变化即大，既有较大的gradient，理应有小的learning rate。</li><li>黄色部分：当w从0.99变化为0.01时， $y^{1000}$ 的输出几乎不变化，即有较小的gradient，理应有大大learning rate.</li><li>在很小的地方（0.01 到 1.01），他的gradient就变化即大，即抖动的出现。</li></ul><p><strong>Reason</strong>：RNN，虽然可以看作不同时间点的展开计算，但始终是同一个NN的权重计算（cell连接到下一个时间点的权重），在不同时间中，反复叠乘，因此会出现这种情况。</p><h3 id="Helpful-Techniques"><a href="#Helpful-Techniques" class="headerlink" title="Helpful Techniques"></a>Helpful Techniques</h3><ol><li>LSTM几乎已经算RNN的一个标准了，为什么LSTM的performance比较好呢。</li></ol><ul><li><p>为什么用LSTM替换为RNN？</p><p>:Can deal with gradient vanishing(not gradient explode).</p><p>可以解决gradient vanish的问题（gradient vanish problem 具体见 <a href="/2020/04/21/tips-for-DL/" title="这篇文章2.1.1">这篇文章2.1.1</a>）</p></li><li><p>为什么LSTM可以解决gradient vanish问题</p><p>：memory and input are added.（LSTM的的输出与输入和memory有关）</p><p>: The influence never disappears unless forget gate is closed.（memory的影响可以很持久）</p></li></ul><ol start="2"><li><p>GRU[2]（Gated Recurrent Unit）：是只有两个Gate，比LSTM简单，参数更少，不容易overfitting</p></li><li><p>玄学了叭</p><p><a href="https://imgchr.com/i/tqVHM9"><img src="https://s1.ax1x.com/2020/06/11/tqVHM9.md.png" alt="tqVHM9.md.png"></a> </p></li></ol><h1 id="More-Applications"><a href="#More-Applications" class="headerlink" title="More Applications"></a>More Applications</h1><p>【待更新】</p><h2 id="Many-to-One"><a href="#Many-to-One" class="headerlink" title="Many to One"></a>Many to One</h2><h2 id="Many-to-Many"><a href="#Many-to-Many" class="headerlink" title="Many to Many"></a>Many to Many</h2><h2 id="Beyond-Sequence"><a href="#Beyond-Sequence" class="headerlink" title="Beyond Sequence"></a>Beyond Sequence</h2><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><h3 id="Auto-encoder-Text"><a href="#Auto-encoder-Text" class="headerlink" title="Auto-encoder-Text"></a>Auto-encoder-Text</h3><h3 id="Auto-encoder-Speech"><a href="#Auto-encoder-Speech" class="headerlink" title="Auto-encoder-Speech"></a>Auto-encoder-Speech</h3><h2 id="Chat-bot"><a href="#Chat-bot" class="headerlink" title="Chat-bot"></a>Chat-bot</h2><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>BPTT</li><li>GRU</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中首先从RNN能解决什么问题入手，分析了RNN与一般NN的区别。&lt;br&gt;然后着重讲解了基于RNN的LSTM模型，包括LSTM的细节、和一般NN的区别，以及如何训练LSTM模型。&lt;br&gt;具体阐述了在模型（RNN类模型）训练过程中为什么会遇到剧烈抖动问题和如何解决抖动的工程解决方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="RNN" scheme="https://f7ed.com/tags/RNN/"/>
    
      <category term="LSTM" scheme="https://f7ed.com/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Convolution Neural Network（CNN）</title>
    <link href="https://f7ed.com/2020/04/25/CNN/"/>
    <id>https://f7ed.com/2020/04/25/CNN/</id>
    <published>2020-04-24T16:00:00.000Z</published>
    <updated>2020-10-17T12:16:20.231Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中首先介绍了为什么要用CNN做图像识别，或者说图像识别问题的特点是什么？<br>文章中也详细介绍了CNN的具体架构，主要包括Convolution、Max Pooling、Flatten。<br>文章最后简要介绍了CNN在诸多领域的应用。</p><a id="more"></a><h1 id="Why-CNN-for-Image"><a href="#Why-CNN-for-Image" class="headerlink" title="Why CNN for Image?"></a>Why CNN for Image?</h1><p>图片本质都是pixels。</p><p>在做图像识别时，本质是对图片中的某些特征像素（properities)识别。</p><p><strong>So Why CNN for image?</strong></p><ol><li><p>Some patterns are much smaller than the whole image.</p><p>A neuron does <strong>not have to see the whole image</strong> to discover the pattern.</p><p>Connecting to small region with <strong>less parameters.</strong></p><p>【很多特征图案的大小远小于整张图片的大小，因此一个neuron不需要为了识别某个pattern而看完整张图片。并且，如果只识别某个小的region，会减少大量参数的数目。】</p><p>如下图，用一个neuron识别红框中的beak，即能大概率认为图片中有bird。</p><img src="https://s1.ax1x.com/2020/04/25/JyitKJ.md.png" alt="JyitKJ.md.png" style="zoom:75%;" /></li><li><p>The same patterns appear in different regions. They can <strong>use the same set of parameters.</strong></p><p>【同样的pattern可能出现在图片的不同位置。pattern几乎相同，因此可以用同一组参数。】</p><p>如下图，两个neuron识别两个不同位置的beak。被识别的beak几乎无差别，因此neuron的参数可以是相同的。</p><img src="https://s1.ax1x.com/2020/04/25/JyiNr9.md.png" alt="JyiNr9.md.png" style="zoom:50%;" /></li><li><p><strong>Subsampling</strong> the pixels will not change the object.</p><p>【一张图片是由许多pixel组成的，如下图，如果去掉图片的所有奇数行偶数列的pixel，图片内容几乎无差别。并且，Subsample pixels，即减少了输入的size，也可以减少NN的参数数量。】</p><img src="https://s1.ax1x.com/2020/04/25/JyiJv4.md.png" alt="JyiJv4.md.png" style="zoom:50%;" /></li></ol><h1 id="The-whole-CNN"><a href="#The-whole-CNN" class="headerlink" title="The whole CNN"></a>The whole CNN</h1><p>CNN的架构如下图。</p><img src="https://s1.ax1x.com/2020/04/25/JyiG2F.md.png" alt="JyiG2F.md.png" style="zoom:67%;" /><p>一张图片经过多次Convolution、Max Pooling得到新的image，再将新的image Flatten（拉直）得到一组提取好的features，将这组features放入前馈神经网络。</p><p>Convolution满足图片识别的：</p><ul><li>Property 1 : Some patterns are much smaller than the whole image.</li><li>Property 2 : The same patterns appear in different regions.</li></ul><p>Max Pooling满足图片识别的：</p><ul><li>Property 3 : Subsamplingthe pixels will not change the object.</li></ul><h2 id="CNN-Convolution"><a href="#CNN-Convolution" class="headerlink" title="CNN-Convolution"></a>CNN-Convolution</h2><p>一张简单的黑白图片如下图，0为白色，1为黑色。</p><img src="https://s1.ax1x.com/2020/04/25/Jyi88U.md.png" alt="Jyi88U.md.png" style="zoom:33%;" /> <p>如果图片是彩色的，即用RGB三原色来表示，用三个matrix分别表示R、G、B的值，如下图：</p><img src="https://s1.ax1x.com/2020/04/25/JyiMEq.md.png" alt="JyiMEq.md.png" style="zoom:50%;" /><p>下文中，以黑白图举例。</p><h3 id="Property-1"><a href="#Property-1" class="headerlink" title="Property 1"></a>Property 1</h3><p>设计Filer matrix满足Property 1，如下图：</p><img src="https://s1.ax1x.com/2020/04/25/Jyi3CT.png" alt="Jyi3CT.png" style="zoom:33%;" /> <p>上图中，filter的大小是3*3，可以检测到小区域的某个pattern。</p><p>每个filter的参数都是NN中的参数，需要learned。</p><p>如果是彩色图片，filter应该是3张3*3matrix组成的，分别代表R、G、B的filter。</p><h3 id="Property-2"><a href="#Property-2" class="headerlink" title="Property 2"></a>Property 2</h3><p>为了满足Property 2，filter可以在图片中移动。设置stride，即每次filter移动的步长。</p><p>filter与覆盖图片的位置做内积，需要走完整张图片，最后得到一张feature map。</p><p>下图为stride=1的convolution结果：</p><img src="https://s1.ax1x.com/2020/04/25/Jyil5V.md.png" alt="Jyil5V.md.png" style="zoom:50%;" /><p>Convolution layer（卷积层）有几个filter，就会得到几张feature maps。</p><h3 id="Convolution-v-s-Fully-Connected"><a href="#Convolution-v-s-Fully-Connected" class="headerlink" title="Convolution v.s. Fully Connected"></a>Convolution v.s. Fully Connected</h3><p><strong>Fully Connected:</strong> </p><p>如果用全连接的方式做图片识别，图片的每一个pixel都要和第一层的所有neurons连接，需要大量参数。</p><p>如下图：</p><img src="https://s1.ax1x.com/2020/04/25/JyinDs.md.png" alt="JyinDs.md.png" style="zoom:70%;" /><hr><p><strong>Convolution:</strong> </p><p>而在Convolution中，把feature map中的每一个值作为neuron的输出，因此图片中只有部分pixels会和第一层的第一个neuron连接，而不是全部pixels。</p><p>对于一个3*3的filter，一个neuron的连接如下：</p><img src="https://s1.ax1x.com/2020/04/25/Jyiubn.md.png" alt="Jyiubn.md.png" style="zoom:70%;" /><p>filter中的值是连接参数，则每一个neuron只需要与3*3个input连接，与全连接相比减少了大量参数。</p><p><strong>shared weights</strong> </p><p>filter在图中移动时，filter的参数不变，即第二个neuron的连接参数和第一个neuron的连接参数是相同的，连接图如下：</p><img src="https://s1.ax1x.com/2020/04/25/Jyimuj.md.png" alt="Jyimudj.md.png" style="zoom:70%;" /><p>通过filter实现了shared weights（参数共享），更大幅度减少了参数数量。</p><h2 id="CNN-Max-Pooling"><a href="#CNN-Max-Pooling" class="headerlink" title="CNN-Max Pooling"></a>CNN-Max Pooling</h2><p>Max Pooling：将convolution layer的neuron作为输入，neuron的activation function其实就是Maxout（Maxout介绍见  的介绍）。</p><p>将convolution layer得到的feature map做Max pooling（池化），即取下图中每个框中的最大值。</p><img src="https://s1.ax1x.com/2020/04/25/JyiZvQ.md.png" alt="JyifZvddQ.md.png" style="zoom:50%;" /><p>如下图，6*6的image经过Convolution layer 和 Max Pooling layer后，得到了new but smaller image，新的image的由两层channel组成，每层channel都是2 * 2的image。</p><img src="https://s1.ax1x.com/2020/04/25/JyiAC8.md.png" alt="JyiAC8.md.png" style="zoom:67%;" /><p>一个image每经过一次Convolution layer 和 Max Pooling layer，都会得到a new image。</p><p>This new image is smaller than the origin image. And the number of channel (of the new image) is the number of filters.</p><p>举个例子：</p><p>Convolution layer有25个filters，再经过Max Pooling，得到的新的image有25 个channel。</p><p>再重复一次Convolution 和Max Pooling，新的Convolution layer也有25个filters，再经过Max Pooling，得到的新的image有多少个channel呢？</p><p>答案是25个channel。</p><p><strong>注意</strong> ：在第二次Convolution中，image有depth，depth=25。因此在convolution中，filter其实是一个cubic，也有depth，depth=image-depth=25，再做内积。</p><p>因此，新的image的channel数是等于filter数的。</p><h2 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h2><p>Flatten很好理解，将最后得到的新的image 拉直（Flatten）为一个vector。</p><img src="https://s1.ax1x.com/2020/04/25/JyiE8S.md.png" alt="Jy8iE8S.md.png" style="zoom:50%;" /><p>拉直后的vector是一组提取好的features，作为 前馈神经网络的输入。</p><h2 id="zero-padding"><a href="#zero-padding" class="headerlink" title="zero padding"></a>zero padding</h2><p>如何让卷积后的图像不变小？</p><p>答案就是zero padding，在原图的padding填0，再做卷积。</p><p>zero-padding后如下图：</p><img src="https://s1.ax1x.com/2020/10/17/0LvkR0.png" alt="0LvdkR0.png" style="zoom:40%;" /><p>卷积后，图像大小不变：</p><img src="https://s1.ax1x.com/2020/10/17/0LvZsU.png" alt="0LvZdsU.png" style="zoom:40%;" /><h1 id="What-dose-CNN-learn"><a href="#What-dose-CNN-learn" class="headerlink" title="What dose CNN learn"></a>What dose CNN learn</h1><p>为什么CNN能够学习pattern，最终达到识别图像的目的？</p><h2 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h2><p>在下图CNN过程中，我们先分析能从Convolution layer的filter能够学到什么？</p><img src="https://s1.ax1x.com/2020/04/25/JyiF4f.md.png" alt="JyiF4f.md.png" style="zoom:33%;" /><p>每个filter本质上是一组shared weights 的neuron。</p><p>因此，定义这组filter的激活程度，即：</p><p> Degree of the activation of the k-th filter: $a^k=\sum_{i=1}^{11}\sum_{j=1}^{11}a_{ij}^{k}$ .</p><p>目标是找到使k-th filter激活程度最大的输入image，即</p><p>$x^{*}=\arg \max _{x} a^{k}$ ，(method :gradient descent).</p><p>部分结果如下图：</p><img src="https://s1.ax1x.com/2020/04/25/JyiVgg.md.png" alt="JyiVgg.md.png" style="zoom:33%;" /> <p>(每一张图都代表一个让filter激活程度最大的 $x$)</p><p>上图中，找到使filter激活程度最大的image，即上图中每个filter可以检测一定的条纹，只有当图像中有该条纹，filter（一组neuron）的激活程度（即输出）才能达到最大。</p><h2 id="Neuron（Hidden-layer）"><a href="#Neuron（Hidden-layer）" class="headerlink" title="Neuron（Hidden layer）"></a>Neuron（Hidden layer）</h2><p>这里的neuron指前馈神经网络中的neuron，如下图的 $a_j$ :</p><img src="https://s1.ax1x.com/2020/04/25/JyiiUP.png" alt="JyiiUP.png" style="zoom:50%;" /><p>目标：找到使neuron的输出最大的输入image，即：</p><p>$x^{*}=\arg \max _{x} a^{j}$ .</p><p>部分结果如下：</p><img src="https://s1.ax1x.com/2020/04/25/JykouQ.md.png" alt="JykouQ.md.png" style="zoom:33%;" /> <p>（每一张图代表一个neuron)</p><p>在上图中，感觉输入像一个什么东西吧emmmm。</p><p>但和filter学到的相比，neuron学到的不仅是图中的小小的pattern（比如条纹、鸟喙等），neuron学的是看整张图像什么。</p><h2 id="Output（Output-layer）"><a href="#Output（Output-layer）" class="headerlink" title="Output（Output layer）"></a>Output（Output layer）</h2><p>再用同样的方法，看看输出层的neuron学到了什么，如下图的 $y_i$  ：</p><img src="https://s1.ax1x.com/2020/04/25/Jyk5jg.png" alt="Jyk5jg.png" style="zoom:33%;" /><p>在手写数字辨识中 $y_i$ 是数字为 $i$ 的概率，因此目标是：找到一个使输出是数字 $i$ 概率最大的输入image，即：</p><p>$x^{*}=\arg \max _{x} y^{i}$ .</p><p>结果如下图：</p><img src="https://s1.ax1x.com/2020/04/25/JyiSud.md.png" alt="JyiSud.md.png" style="zoom:33%;" /> <p>结果和我们期望相差甚远，根本不能辨别以上图片是某个数字。</p><p>这其实也是DNN的一个特点: Deep Neural Networks are Easily Fooled [1]，即NN学到的东西往往和人类学到的东西是不一样的。</p><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>所以CNN到底学到了什么？</p><p>上文中，output 学到的都是一团密密麻麻杂乱的像素点，根本不像数字。</p><p>但是，再考虑手写数字image的特点：图片中应该有少量模式，大片空白部分。</p><p>因此目标改进为：  $x^{*}=\arg \max _{x}\left(y^{i}+\sum_{i, j}\left|x_{i j}\right|\right)$  </p><p>$\sum_{i, j}\left|x_{i j}\right|$ 就像是regularization的限制。</p><p>结果如下：</p><img src="https://s1.ax1x.com/2020/04/25/Jyi9HI.md.png" alt="Jyi9HI.md.png" style="zoom:33%;" /> <p>（注：图中白色为墨水，黑色为空白）</p><h1 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h1><h2 id="Deep-Dream"><a href="#Deep-Dream" class="headerlink" title="Deep Dream"></a>Deep Dream</h2><p>CNN exaggerates what it sees.</p><p>CNN可以夸大图片中他所看到的东西。</p><p>比如：</p><p>可以把下图</p><img src="https://s1.ax1x.com/2020/04/25/JyiPEt.md.png" alt="JyiPEt.md.png" style="zoom:50%;" />  <p>变成下图（emmmm看着有点难受）</p><img src="https://s1.ax1x.com/2020/04/25/JyPxjH.md.png" alt="JyPxjH.md.png" style="zoom:50%;" /> <p>附上生成deep dream image的网站<a href="http://deepdreamgenerator.com/">[2]</a> .</p><h2 id="Deep-Style-3"><a href="#Deep-Style-3" class="headerlink" title="Deep Style[3]"></a>Deep Style<a href="https://arxiv.org/abs/1508.06576">[3]</a></h2><p>Given a photo, make its style like famous paintings.</p><img src="https://s1.ax1x.com/2020/04/25/JyPX9O.md.png" alt="JyPX9O.md.png" style="zoom:50%;" /><p>上图中，用一个CNN学习图中的content，用另一个CNN学习风格图中的style。</p><p>再用一个CNN使得输入的图像content像原图，风格像另一张图。</p><h2 id="Playing-Go"><a href="#Playing-Go" class="headerlink" title="Playing Go"></a>Playing Go</h2><p>CNN 还可以用在下围棋中，如下图，输入是19 * 19的围棋局势（matrix/image），通过CNN，学出下一步应该走哪？</p><img src="https://s1.ax1x.com/2020/04/25/JyPL4K.md.png" alt="JyPL4K.md.png" style="zoom:50%;" /><h3 id="Why-CNN-playing-Go"><a href="#Why-CNN-playing-Go" class="headerlink" title="Why CNN playing Go?"></a>Why CNN playing Go?</h3><p>下围棋满足以下两个property：</p><ol><li><p>Some patterns are much smaller than the whole image.</p><img src="https://s1.ax1x.com/2020/04/25/JyPou9.png" alt="JyPou9.png" style="zoom:50%;" /> <p>（围棋新手，博主只下赢过几次hhh)</p><p>如果白棋棋手，看到上图的pattern，上图的白子只有一口气了，被堵住就会被吃掉，那白棋棋手大概率会救那个白子，下在白棋的下方。</p><p>Alpha Go uese 5 * 5 for first layer.</p></li><li><p>The same patterns appear in different regions.</p><img src="https://s1.ax1x.com/2020/04/25/JyP7H1.md.png" alt="JyP7H1.md.png" style="zoom:50%;" /> </li></ol><hr><p>但如何解释CNN的另一结构——Max Pooling？</p><p>因为围棋的棋谱matrix不像image的pixel，subsample后，围棋的棋谱就和原棋谱完全不像了。</p><p>Alpha Go的论文中：Alpha Go并没有用Max Pooling。</p><img src="https://s1.ax1x.com/2020/04/25/JyPbAx.md.png" alt="JyPbAx.md.png" style="zoom:75%;" /><p>所以，可以根据要训练的东西调整CNN模型。</p><h2 id="Speech"><a href="#Speech" class="headerlink" title="Speech"></a>Speech</h2><p>可以用CNN学习<a href="[https://zh.wikipedia.org/wiki/%E6%97%B6%E9%A2%91%E8%B0%B1](https://zh.wikipedia.org/wiki/时频谱)">Spectrogram</a> ，即识别出这一时段说的是什么话。</p><img src="https://s1.ax1x.com/2020/04/25/JyPqN6.md.png" alt="JyPqN6.md.png" style="zoom:75%;" /><h2 id="Text"><a href="#Text" class="headerlink" title="Text"></a>Text</h2><p>CNN还可以用在文本的情感分析中，对句子中每个word embedding后，通过CNN，学习sentence表达的是negative 还是positive还是neutral的情绪。</p><img src="https://s1.ax1x.com/2020/04/25/JyPTBR.md.png" alt="JyPTBR.md.png" style="zoom:75%;" /><h3 id="More"><a href="#More" class="headerlink" title="More"></a>More</h3><p>（挖坑…生命很漫长，学无止境QAQ）</p><ul><li><p>The methods of visualization in these slides：</p><p> <a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html">https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html</a></p></li><li><p>More about visualization：</p><p><a href="http://cs231n.github.io/understanding-cnn/">http://cs231n.github.io/understanding-cnn/</a></p></li><li><p>Very cool CNN visualization toolkit</p><p><a href="http://yosinski.com/deepvis">http://yosinski.com/deepvis</a></p><p><a href="http://scs.ryerson.ca/~aharley/vis/conv/">http://scs.ryerson.ca/~aharley/vis/conv/</a></p></li><li><p>The 9 Deep Learning Papers You Need To Know About</p><p><a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html</a></p></li><li><p>How to let machine draw an image</p><ul><li><p>PixelRNN</p><p><a href="https://arxiv.org/abs/1601.06759">https://arxiv.org/abs/1601.06759</a></p></li><li><p>Variation Autoencoder (VAE)</p><p><a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></p></li><li><p>Generative Adversarial Network (GAN)</p><p><a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a></p></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>Deep Neural Networks are Easily Fooled： <a href="https://www.youtube.com/watch?v=M2IebCN9Ht4">https://www.youtube.com/watch?v=M2IebCN9Ht4</a></p></li><li><p>deep dream generator: <a href="http://deepdreamgenerator.com/">http://deepdreamgenerator.com/</a></p></li><li><p>A Neural Algorithm of Artistic Style: <a href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中首先介绍了为什么要用CNN做图像识别，或者说图像识别问题的特点是什么？&lt;br&gt;文章中也详细介绍了CNN的具体架构，主要包括Convolution、Max Pooling、Flatten。&lt;br&gt;文章最后简要介绍了CNN在诸多领域的应用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="CNN" scheme="https://f7ed.com/tags/CNN/"/>
    
      <category term="机器学习" scheme="https://f7ed.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Tips for Deep Learning</title>
    <link href="https://f7ed.com/2020/04/21/tips-for-DL/"/>
    <id>https://f7ed.com/2020/04/21/tips-for-DL/</id>
    <published>2020-04-20T16:00:00.000Z</published>
    <updated>2020-07-03T08:45:39.300Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。<br>tips从Training和Testing两个方面展开。<br>在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有RMSprop、Momentum、Adam等）。<br>当在Training中得到好的performance，但在testing中perform bad时，即遇到了overfitting，又该怎么处理呢？文章后半部分详尽介绍了EarlyStopping、Regularization和Dropout三个solution。</p><a id="more"></a><h1 id="Recipe-of-Deep-Learning"><a href="#Recipe-of-Deep-Learning" class="headerlink" title="Recipe of Deep Learning"></a>Recipe of Deep Learning</h1><p>Deep Learning 的三个步骤：</p><p><a href="https://imgchr.com/i/JGW8js"><img src="https://s1.ax1x.com/2020/04/21/JGW8js.md.png" alt="JGW8js.md.png"></a> </p><p>如果在Training Data中没有得到好的结果，需要重新训练Neural Network。</p><p>如果在Training Data中得到好的结果，在Testing Data（这里的Testing Data是指有Label的Data，比如Kaggle的Public Data或者是从Training Data中划分出的Development Data）没有得到的好的结果，说明Overfitting了，需要重新设计Neural Network的结构。</p><h2 id="Do-not-always-blame-Overfitting"><a href="#Do-not-always-blame-Overfitting" class="headerlink" title="Do not always blame Overfitting"></a>Do not always blame Overfitting</h2><p><a href="https://imgchr.com/i/JGW3cj"><img src="https://s1.ax1x.com/2020/04/21/JGW3cj.md.png" alt="JGW3cj.md.png"></a> </p><p>如果在Testing Data中，看到上图，20-layer的error小，56-layer的error大，56-layer一定overfitting了。</p><p>No!!!不要总把原因归咎于Overfitting。</p><p><a href="https://imgchr.com/i/JGW13Q"><img src="https://s1.ax1x.com/2020/04/21/JGW13Q.md.png" alt="JGW13Q.md.png"></a> </p><p>再看Testing Data error之前，先看看Training Data的error。上图中，56-layer的DNN在Training Data的error本来就比20-layer的大，说明56-layer的DNN根本没有train好。</p><p>所以56-layer的DNN在Testing Data上的error大，原因不是overfitting，而是模型根本没有train好。</p><p><strong>注：</strong> Overfitting是在Training Data上error小，但在Testing Data上的error大。</p><p>因此，对于在Training Data上得到不好的结果和在Training Data上得到好的结果但在Testing Data上得到不好的结果这两种情况，需要不同的解决方法。</p><h1 id="Bad-Results-on-Training-Data"><a href="#Bad-Results-on-Training-Data" class="headerlink" title="Bad Results on Training Data"></a>Bad Results on Training Data</h1><p>在不重新设计DNN结构时，如果在Training Data中得到Bad Results，一般有两种方法来改进结果：</p><ul><li>New activation function【neuron换新的激活函数】</li><li>Adaptive Learning Rate</li></ul><h2 id="New-activation-function"><a href="#New-activation-function" class="headerlink" title="New activation function"></a>New activation function</h2><h3 id="Vanishing-Gradient-Problem"><a href="#Vanishing-Gradient-Problem" class="headerlink" title="Vanishing Gradient Problem"></a>Vanishing Gradient Problem</h3><p><a href="https://imgchr.com/i/JGWl9g"><img src="https://s1.ax1x.com/2020/04/21/JGWl9g.md.png" alt="JGWl9g.md.png"></a> </p><p>上图表示，在手写数字辨识中，Deeper layers并不能有好的performance。</p><p>为什么会这样呢？</p><p>因为出现了Vanishing Gradient Problem，即gradient随着deeper layer逐渐消失的问题。</p><p><a href="https://imgchr.com/i/JGWM4S"><img src="https://s1.ax1x.com/2020/04/21/JGWM4S.md.png" alt="JGWM4S.md.png"></a> </p><p>上图中，假设neuron的activation function是sigmod函数。</p><p>靠近Input layer层的参数的变化对Loss的影响很小，所以对Loss function做微分，gradient很小，参数更新慢。</p><p>而靠近Output layer层的参数的编号对Loss的影响更大，所以对Loss function做微分，gradient很大，参数更新快。</p><p>因为靠近Output Layer层的参数更新快，所以很快converge（收敛、趋于稳定）；但靠近Input Layer层的参数更新慢，几乎还处在random（随机）的状态。</p><p>当靠近Output Layer层的参数趋于稳定时，由于靠近Output Layer层的参数对Loss影响大，所以观察到的Loss的值也趋于稳定，于是，你就把training停掉了。</p><p>但是，靠近Input层的参数几乎处在random状态，所以拿模型用在Testing Data上，发现结果几乎是随机的。</p><hr><p>怎么直观理解靠近Input Layer的参数的gradient小呢？</p><p>用微分的直观含义来表示gradient $\partial{l}/\partial{w}$ : </p><p><strong>当 $w$ 增加 $\Delta{w}$ 时，如果 $l$ 的变化 $\Delta{l}$ 变化大，说明 $\partial{l}/\partial{w}$ 大，否则 $\partial{l}/\partial{w}$ 小。</strong></p><p><a href="https://imgchr.com/i/JGWKN8"><img src="https://s1.ax1x.com/2020/04/21/JGWKN8.md.png" alt="JGWKN8.md.png"></a> </p><p>我们在DNN中使用的activation function是sigmod函数，sigmod函数会把值压到0和1之间。</p><p>因此，上图中，其他值不变，只有连接 $x_N$ 的参数 $w$ 增加 $\Delta w$ 时，输入通过neuron的sigmod函数，函数的输出增加的 $\Delta$ 会变小，随着Deeper Layer，neuron的输出的 $\Delta$ 会越变越小，趋至0。</p><p>最后DNN输出的变化对 loss的影响小，即 $\Delta{l}$ 趋至0，即参数的gradient  $\partial{l}/\partial{w}$ 趋至0。（即 Vanishing Gradient）</p><h3 id="ReLu-：Rectified-Linear-Unit"><a href="#ReLu-：Rectified-Linear-Unit" class="headerlink" title="ReLu ：Rectified Linear Unit"></a>ReLu ：Rectified Linear Unit</h3><p>为了防止发生Vanishing Gradient Problem，在DNN中选择使用新的activation function。</p><p>ReLu长下面这个样子：</p><p><a href="https://imgchr.com/i/JGWuAf"><img src="https://s1.ax1x.com/2020/04/21/JGWuAf.md.png" alt="JGWuAf.md.png"></a> </p><p>z: input</p><p>a: output</p><p>当 $z\leq0$ 时， $a=0$ ；当 $z &gt;0$ 时， $a=z$ 。</p><p><u>Reason :</u> </p><ol><li>Fast to compute</li><li>Biological reason【有生物上的原因】</li><li>Infinite sigmod with different biases. 【是无穷个 有不同bias的sigmod函数 的叠加】</li><li><strong>Vanishing gradient problem</strong> 【最重要的是没有vanishing gradient problem】</li></ol><hr><p>为什么ReLu没有vanishing gradient problem</p><p><a href="https://imgchr.com/i/JGWmHP"><img src="https://s1.ax1x.com/2020/04/21/JGWmHP.md.png" alt="JGWmHP.md.png"></a> </p><p>上图DNN中，ReLu在输入是负数时，输出是0。因此这些输出是0的neuron可以去掉。</p><p>就变成了下图这个A Thinner linear network。由于ReLu函数的性质，靠近Input Layer的参数不会有smaller gradient。</p><p><a href="https://imgchr.com/i/JGWeBt"><img src="https://s1.ax1x.com/2020/04/21/JGWeBt.md.png" alt="JGWeBt.md.png"></a> </p><p>这里有一个Q&amp;A: </p><p>Q1: function变成linear的，会不会DNN就变弱了？</p><p>： 当neuron的operation region不变的话，DNN的确是linear的，但是当neuron的operation region改变后，就是unlinear的。</p><p>：即，当input的变化小，operation region不变（即输入不会从大于0变成小于0，小于0变成大于0这种），model还是linear的；但当input的变化大时，很多neuron的operation region都变化了，model其实就是unlinear的。</p><p>Q2: ReLu 怎么微分？</p><p>：ReLu在0点不可微，那就随便指定为0这样（台湾腔QAQ）。</p><h3 id="ReLu-variant"><a href="#ReLu-variant" class="headerlink" title="ReLu - variant"></a>ReLu - variant</h3><p>当 $z\leq 0$ 时，输出为0，就不能更新参数了。于是就有下图变体：</p><p><a href="https://imgchr.com/i/JGWZnI"><img src="https://s1.ax1x.com/2020/04/21/JGWZnI.md.png" alt="JGWZnI.md.png"></a> </p><p>当 $z\leq0$ 时，gradient都为0.01，为什么不能是其他值。于是就有下图变体：其中 $\alpha$ 也是一个需要学习的参数</p><h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>Maxout，如下图，在设计neural network时，会给每一层的neuron分组，成为一个新的neuron。</p><p><a href="https://imgchr.com/i/JGWk1H"><img src="https://s1.ax1x.com/2020/04/21/JGWk1H.md.png" alt="JGWk1H.md.png"></a> </p><p>Maxout也是一个Learnable activation function。</p><p><strong>ReLu是Maxout学出来的一个特例。</strong></p><p><a href="https://imgchr.com/i/JGWAcd"><img src="https://s1.ax1x.com/2020/04/21/JGWAcd.md.png" alt="JGWAcd.md.png"></a> </p><p>上图中，左图是ReLu。 ReLu的输入 $z = wx+b$ ，输出 $a$ 如上图的绿色的线。</p><p>右图是Maxout。Maxout的输入 $z_1 =wx+b,z_2=0$ ，那么输出取max，输出 $a$ 如上图中绿色的线，和左图的ReLu相同。</p><hr><p><strong>Maxout is more than ReLu。</strong> </p><p>当参数更新时，Maxout的函数图像如下图：</p><p><a href="https://imgchr.com/i/JGWPhD"><img src="https://s1.ax1x.com/2020/04/21/JGWPhD.md.png" alt="JGWPhD.md.png"></a> </p><p>DNN中的参数是learnable的，所以Maxout也是一个learnable的activation function。</p><p><u>Reason ：</u></p><ul><li><p>Learnable activation function [Ian J. Goodfellow, ICML’13]</p><ul><li><p>Activation function in maxout network can be any piecewise linear convex function.</p><p>在maxout神经网络中的激活函数可以是任意的分段凸函数。</p></li><li><p>How many pieces depending on how many elements in a group.</p><p>分段函数分几段取决于一组中有多少个元素。</p><p><a href="https://imgchr.com/i/JGWCtO"><img src="https://s1.ax1x.com/2020/04/21/JGWCtO.md.png" alt="JGWCtO.md.png"></a> </p></li></ul></li></ul><h3 id="Maxout-how-to-train"><a href="#Maxout-how-to-train" class="headerlink" title="Maxout : how to train"></a>Maxout : how to train</h3><p>Given a training data x, we know which z would be the max.</p><p>【当给出每笔training data时，我们能知道Maxout neuron中哪一个最大】</p><p><a href="https://imgchr.com/i/JGW9AK"><img src="https://s1.ax1x.com/2020/04/21/JGW9AK.md.png" alt="JGW9AK.md.png"></a> </p><p>如上图，在这笔training data x中，我们只train this thin and linear network 的参数，即max z相连的参数。</p><p>每笔不同的training data x，会得到不同的thin and linear network，最后，会train到每一个参数。</p><h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><h3 id="Review-Adagrad"><a href="#Review-Adagrad" class="headerlink" title="Review Adagrad"></a>Review Adagrad</h3><p>在这篇文章： <a href="/2020/03/01/Gradient/" title="Gradient">Gradient</a> 第一小节讲到一种adaptive learning rate的gradient 算法：Adagrad 算法。在那篇文章中，我们得出的结论是 the best step $\propto$  |First dertivative| / Second derivative.</p><p><a href="https://imgchr.com/i/JGWF9e"><img src="https://s1.ax1x.com/2020/04/21/JGWF9e.md.png" alt="JGWF9e.md.png"></a>  </p><p>在上图中，两个方向，因为蓝色方向的二阶微分更小，所以蓝色方向应该有更大的learning rate。</p><p>因此，在Adagrad中，我们用一阶微分来估量二阶微分的大小：</p>$$w^{t+1}  \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}$$<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>但是，在训练NN时，Error Surface（Total Loss对参数的变化）的图像可能会更复杂，如下图：</p><p><a href="https://imgchr.com/i/JGWS76"><img src="https://s1.ax1x.com/2020/04/21/JGWS76.md.png" alt="JGWS76.md.png"></a> </p><p>因为函数图像过于复杂，可能在同一方向的不同位置，也需要有不同的learning rate。</p><p>RMSProp是Adagrad的进阶版。</p><p><strong>RMSProp过程：</strong> </p><ol><li> $w^{1} \leftarrow w^{0}-\frac{\eta}{\sigma^{0}} g^{0} \quad \sigma^{0}=g^{0}$ </li><li> $w^{2} \leftarrow w^{1}-\frac{\eta}{\sigma^{1}} g^{1} \quad \sigma^{1}=\sqrt{\alpha (\sigma^{0})^2+(1-\alpha)(g^1)^2}$ </li><li> $w^{3} \leftarrow w^{2}-\frac{\eta}{\sigma^{2}} g^{2} \quad \sigma^{2}=\sqrt{\alpha (\sigma^{1})^2+(1-\alpha)(g^2)^2}$ </li><li><p>…</p></li><li> $w^{t+1} \leftarrow w^{t}-\frac{\eta}{\sigma^{t}} g^{t} \quad \sigma^{t}=\sqrt{\alpha (\sigma^{t-1})^2+(1-\alpha)(g^t)^2}$ <p>$\sigma^t$ 也是在算gradients的 root mean squar。</p></li></ol><p>但是在RMSProp中，加入了参数 $\alpha$ (需要手动调节大小的参数)，可以给当前算出来的gradient $g^t$ 更大的权重，即更相信现在gradient的方向，不那么相信以前gradient的方向。</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Momentum，则是引用物理中的惯性。</p><p><a href="https://imgchr.com/i/JGRxn1"><img src="https://s1.ax1x.com/2020/04/21/JGRxn1.md.png" alt="JGRxn1.md.png"></a> </p><p>上图中，当小球到达local minima时，会因为惯性继续往前更新，则有可能到达minima的位置。</p><p>这里的Momentum，就代指上一次前进（参数更新）的方向。</p><p><strong>Vanilla Gradient Descent</strong> </p><p>如果将Gradient的步骤画出图来，就是下图这样：</p><p><a href="https://imgchr.com/i/JGRXc9"><img src="https://s1.ax1x.com/2020/04/21/JGRXc9.md.png" alt="JGRXc9.md.png"></a> </p><p>过程：</p><ol><li><p>Start at position $\theta^0$</p></li><li><p>Compute gradietn at $\theta^0$</p><p>Move to  $\theta^1=\theta^0-\eta\nabla{L(\theta^0)}$ </p></li><li><p>Compute gradietn at $\theta^1$ </p><p>Move to  $\theta^2=\theta^1-\eta\nabla{L(\theta^1)}$ </p></li><li><p>… …Stop until $\nabla{L(\theta^t)}\approx0$</p></li></ol><hr><p><strong>Momentum</strong> </p><p>在Momentum中，参数更新方向是当前Gradient方向和Momentum方向（上一次更新方向）的叠加。</p><p><a href="https://imgchr.com/i/JGRjXR"><img src="https://s1.ax1x.com/2020/04/21/JGRjXR.md.png" alt="JGRjXR.md.png"></a> </p><p>Movement方向：上一次更新方向 - 当前gradient方向。</p><p>过程：</p><ol><li><p>Start at position $\theta^0$</p><p>Movement: $v^0=0$ </p></li><li><p>Compute gradient at $\theta^0$ </p><p>Movement  $v^1=\lambda v^0-\eta\nabla{L(\theta^0)}$  </p><p>Move to  $\theta^1=\theta^0+v^1$ </p></li><li><p>Compute gradient at $\theta^1$  </p><p>Movement  $v^2=\lambda v^1-\eta\nabla{L(\theta^1)}$ </p><p>Move to $\theta^2=\theta^1+v^2$  </p></li><li><p>… …Stop until $\nabla{L(\theta^t)}\approx0$ </p></li></ol><p>和Vanilla Gradient Descent比较，$v^i$ 其实是过去gradient( $\nabla{L(\theta^0)}$ 、$\nabla{L(\theta^1)}$ 、… 、 $\nabla{L(\theta^{i-1})}$  )的加权和。</p><ul><li>迭代过程：<ul><li>$v^0=0$ </li><li> $v^1=-\eta\nabla{L(\theta^0)}$ </li><li> $v^2=-\lambda\eta\nabla{L(\theta^0)}-\eta\nabla{L(\theta^1)}$ </li><li>…</li></ul></li></ul><hr><p>再用那个小球的例子来直觉的解释Momentum：</p><p><a href="https://imgchr.com/i/JGRO1J"><img src="https://s1.ax1x.com/2020/04/21/JGRO1J.md.png" alt="JGRO1J.md.png"></a> </p><p>当小球在local minima时，gradient为0，但是Momentum（即上次移动方向）是继续往前，于是小球可以继续向前更新。</p><h3 id="Adam-RMSProp-Momentum"><a href="#Adam-RMSProp-Momentum" class="headerlink" title="Adam = RMSProp + Momentum"></a>Adam = RMSProp + Momentum</h3><p><a href="https://imgchr.com/i/JGRLp4"><img src="https://s1.ax1x.com/2020/04/21/JGRLp4.md.png" alt="JGRLp4.md.png"></a> </p><p>Algorithm：Adam, our proposed algorithm for stochastic optimization. </p><p>【Adam，是为了优化stochastic gradient】（至于什么是stochastic gradient，建议戳)</p><p>$g_t^2$ indicates the elementwise square $g_t\odot g_t$ .</p><p>【$g_t^2$ 是gradient $g_t$ 向量和 $g_t$ 的元素乘】</p><p>Good default settings for the tested machine learning problems are $\alpha=0.001$ , $\beta_1=0.9$ , $\beta_2=0.999$ and $\epsilon=10^{-8}$ . All operations on vectors are element-wise. With $\beta_1^t$ and $\beta_2^t$ we denote $\beta_1$ and $\beta_2$ to the power t.</p><p>【参数说明：算法默认的参数设置是 $\alpha=0.001$ , $\beta_1=0.9$ , $\beta_2=0.999$ ， $\epsilon=10^{-8}$ 。算法中所有vector之间的操作都是对元素操作。 $\beta_1^t$ 和 $\beta_2^t$ 是 $\beta_1$ 和 $\beta_2$ 的 $t$ 次幂】</p><p><strong>Adam Pseudo Code：</strong> </p><ol start="0"><li><p><strong>Require</strong>：$\alpha$ : Stepsize 【步长/learning rate $\eta$ 】</p><p><strong>Require</strong>：$\beta_1,\beta_2\in\left[0,1\right)$ : Exponential decay rates for the moment estimates.</p><p><strong>Require</strong>：$f(\theta)$ : Stochastic objective function with parameters $\theta$ .【参数 $\theta$ 的损失函数】</p><p><strong>Require</strong>: $\theta_0$ ：Initial parameter vector 【初值】</p></li><li><p>$m_0\longleftarrow 0$ (Initial 1st moment vector) 【 $m$ 是Momentum算法中的更新参数后的方向 $v$ 】</p><p>$v_0\longleftarrow 0$ (Initial 2nd moment vector) 【 $v$ 是RMSprop算法中gradient的root mean square $\sigma$ 】</p><p>$t\longleftarrow 0$ (Initial timestep) 【更新次数】</p></li><li><p><strong>while</strong> $\theta_t$ not concerged <strong>do</strong> 【当 $\theta$ 趋于稳定，即 $\nabla{f(\theta)}\approx0$ 时】</p><ol><li><p>$t\longleftarrow t+1$ </p></li><li> $g_t\longleftarrow \nabla{f_t(\theta_{t-1})}$  (Get gradients w.r.t. stochastic objective at timestep t)<p>【算第t次时 $\theta$ 的gradient】</p></li><li> $m_{t} \leftarrow \beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t}$   (Update biased first momen t estimate)<p>【用Momentum算更新方向】</p></li><li> $v_{t} \leftarrow \beta_{2} \cdot v_{t-1}+\left(1-\beta_{2}\right) \cdot g_{t}^{2}$  (Update biased second raw moment estimate)<p>【RMSprop估测最佳步长（ 和$v$ 负相关） 】</p></li><li> $\widehat{m}_{t} \leftarrow m_{t} /\left(1-\beta_{1}^{t}\right)$ （Comppute bbi. as-corrected first momen t estima te)<p>【算出来的值有bias，论文中有具体解释为什么有。当更新次数增加时， $1-\beta_1^t$ 也趋近于1】</p></li><li> $\widehat{v}_{t} \leftarrow v_{t} /\left(1-\beta_{2}^{t}\right)$  (Compute bias-corrected second raw momen t estimate)<p>【和上同理】</p></li><li> $\theta_{t} \leftarrow \theta_{t-1}-\alpha \cdot \widehat{m}_{t} /(\sqrt{\widehat{v}_{t}}+\epsilon)$ （Update parameters）<p>【 $\widehat{m}<em>t$ 相当于是更准确的gradient的方向，$\sqrt{\widehat{v}</em>{t}}+\epsilon$ 是为了估测最好的步长，调节learning rate】</p></li></ol></li></ol><h3 id="Gradient-Descent-Limitation？"><a href="#Gradient-Descent-Limitation？" class="headerlink" title="Gradient Descent Limitation？"></a>Gradient Descent Limitation？</h3><p>在<a href="/2020/03/01/Gradient/" title="Gradient">Gradient</a>这篇文章中，讲到过Gradient有一些问题不能处理：</p><ul><li>Stuck at local minima</li><li>Stuck at saddle point</li><li>Very slow at the plateau</li></ul><p><a href="https://imgchr.com/i/JG4l9O"><img src="https://s1.ax1x.com/2020/04/21/JG4l9O.md.png" alt="JG4l9O.md.png"></a> </p><p>（李老师说的，不是我说的QAQ）：但是Andrew（吴恩达）在2017年说过，不用太担心这个问题。为什么呢？</p><p>如果要stuck at local minima，前提是每一维度都是local minima。</p><p>如果在一个维度遇到local minima的概率是p，当NN很复杂时，有很多参数时，比如1000，那么遇到local minima的概率是 $p^{1000}$ ，趋近于0了，几乎不会发生。</p><p>：所以不用太担心Gradient Descent的局限性。</p><h1 id="Bad-Results-on-Testing-Data"><a href="#Bad-Results-on-Testing-Data" class="headerlink" title="Bad Results on Testing Data"></a>Bad Results on Testing Data</h1><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p>在更新参数时，可能会出现这样曲线图：</p><p><a href="https://imgchr.com/i/JGRbhF"><img src="https://s1.ax1x.com/2020/04/21/JGRbhF.md.png" alt="JGRbhF.md.png"></a> </p><p>图中，Total Loss在training set中逐渐减小，但在validation set中逐渐增大。</p><p>而我们真正关心的其实是validation set的Loss。</p><p>所以想让参数停在validation set中loss最低时。</p><p>Keras能够实现EarlyStopping功能[1]：click <a href="https://keras.io/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore">here</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line">early_stopping = EarlyStopping(monitor=<span class="string">'val_loss'</span>, patience=<span class="number">2</span>)</span><br><span class="line">model.fit(x, y, validation_split=<span class="number">0.2</span>, callbacks=[early_stopping])</span><br></pre></td></tr></table></figure><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Regularization：Find a set of weight not only minimizing original cost but also close to zero.</p><p>构造一个新的loss function，除了最小化原来的loss function，还能使得参数趋紧0，使得function更平滑。</p><p>function的曲线更平滑，当输入有轻微扰动，不会太影响输出的结果。</p><h3 id="L2-norm-regularization"><a href="#L2-norm-regularization" class="headerlink" title="L2 norm regularization"></a>L2 norm regularization</h3><p><strong>New loss function:</strong> </p>$$\begin{equation}\begin{aligned}\mathrm{L}^{\prime}(\theta)&=L(\theta)+\lambda \frac{1}{2}\|\theta\|_{2}\\ \theta &={w_1,w_2,...}\\ \|\theta\|_2&=(w1)^2+(w_2)^2+...\end{aligned}\end{equation}$$<p>其中用第二范式 $\lambda\frac{1}{2}|\theta|_2$ 作为regularization term。做regularization是为了使函数更平滑，所以一般不考虑bias)</p><p><strong>New gradient:</strong> </p>$$\frac{\partial \mathrm{L}^{\prime}}{\partial w}=\frac{\partial \mathrm{L}}{\partial w}+\lambda w$$<p><strong>New update:</strong> </p>$$\begin{equation}\begin{aligned}w^{t+1} &\longrightarrow w^{t}-\eta \frac{\partial \mathrm{L}^{\prime}}{\partial w}\\ &=w^{t}-\eta\left(\frac{\partial \mathrm{L}}{\partial w}+\lambda w^{t}\right)\\ &=(1-\eta \lambda) w^{t}-\eta \frac{\partial \mathrm{L}}{\partial w}\end{aligned}\end{equation}$$<p>在更新参数时，先乘一个 $(1-\eta\lambda)$ ，再更新。</p><p>weight decay（权值衰减）：由于 $\eta,\lambda$ 都是很小的值，所以 $w^t$ 每次都会先乘一个小于1的数，即逐渐趋于0，实现regularization。但是，因为更新中还有gradient部分，所以不会等于0。</p><h3 id="L1-norm-regularization"><a href="#L1-norm-regularization" class="headerlink" title="L1 norm regularization"></a>L1 norm regularization</h3><p>Regularization除了用第二范式，还可以用其他的，比如第一范式 $|\theta|_1=|w_1|+|w_2|+…$ </p><p><strong>New loss function:</strong> </p>$$\begin{equation}\begin{aligned}\mathrm{L}^{\prime}(\theta)&=L(\theta)+\lambda \frac{1}{2}\|\theta\|_1\\ \theta &={w_1,w_2,...}\\ \|\theta\|_1&=|w_1|+|w_2|+...\end{aligned}\end{equation}$$<p>用sgn()符号函数来表示绝对值的求导。</p><blockquote><p>符号函数：Sgn(number)</p><p>如果number 大于0，返回1；等于0，返回0；小于0，返回-1。</p></blockquote><p><strong>New gradient:</strong> </p>$$\frac{\partial \mathrm{L}^{\prime}}{\partial w}=\frac{\partial \mathrm{L}}{\partial w}+\lambda \text{sgn}(w)$$<p><strong>New update:</strong> </p>$$\begin{equation}\begin{aligned}w^{t+1} &\longrightarrow w^{t}-\eta \frac{\partial \mathrm{L}^{\prime}}{\partial w}\\ &=w^{t}-\eta\left(\frac{\partial \mathrm{L}}{\partial w}+\lambda \text{sgn}(w^t)\right)\\ &=w^{t}-\eta \frac{\partial \mathrm{L}}{\partial w}-\eta \lambda \operatorname{sgn}\left(w^{t}\right)\end{aligned}\end{equation}$$<p>在用第一范式做regularization时，每次 $w^t$ 都要减一个值 $\eta\lambda\text{sgn}(w^t)$ ，和用第二范式做regularization比较，后者每次都要乘一个小于1的值，即使是乘0.99，w下降也很快。</p><blockquote><p>Weight decay（权值衰减）的生物意义：</p><p>Our brain prunes（修剪） out the useless link between neurons.</p><p><a href="https://imgchr.com/i/JGRHtU"><img src="https://s1.ax1x.com/2020/04/21/JGRHtU.md.png" alt="JGRHtU.md.png"></a> </p></blockquote><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Wiki: <strong>Dropout</strong>是Google提出的一种正则化技术，用以在人工神经网络中对抗过拟合。Dropout有效的原因，是它能够避免在训练数据上产生复杂的相互适应。Dropout这个术语代指在神经网络中丢弃部分神经元（包括隐藏神经元和可见神经元）。在训练阶段，dropout使得每次只有部分网络结构得到更新，因而是一种高效的神经网络模型平均化的方法。[2]</p><p>这里讲Dropout怎么做。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p><a href="https://imgchr.com/i/JG4M4K"><img src="https://s1.ax1x.com/2020/04/21/JG4M4K.md.png" alt="JG4M4K.md.png"></a> </p><ul><li><p>Each time before updating the parameters:</p><ul><li><p>Each neuron has p% to dropout. Using the new thin network for training.</p><p>【如上图，每个neuron有p的概率被dropout。于是NN就变成了下图thinner的NN】</p><p><a href="https://imgchr.com/i/JGR5mq"><img src="https://s1.ax1x.com/2020/04/21/JGR5mq.md.png" alt="JGR5mq.md.png"></a> </p></li><li><p>For each mini-batch, we resample the dropout neurons.</p><p>【每次mini-batch，都要重新dropout，更新NN的结构】</p></li></ul></li></ul><h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p><strong>Testing中不做dropout</strong> </p><ul><li><p>If the dropout rate at training is p%, all the weights times 1-p%.</p><p>【如果在training中 dropout rate是 p%，在testing是，每个参数都乘 （1-p%)】</p><p>【比如dropout rate 是0.5。如果train出来的w是 1，那么testing中 w=0.5】</p></li></ul><h3 id="Why-dropout-in-training：Intuitive-Reason"><a href="#Why-dropout-in-training：Intuitive-Reason" class="headerlink" title="Why dropout in training：Intuitive Reason"></a>Why dropout in training：Intuitive Reason</h3><ol><li><p>这是一个比较有趣的比喻：</p><p><a href="https://imgchr.com/i/JGRI00"><img src="https://s1.ax1x.com/2020/04/21/JGRI00.md.png" alt="JGRI00.md.png"></a> </p></li><li><p>这也是一个有趣的比喻hhh:</p><p><a href="https://imgchr.com/i/JGRo7V"><img src="https://s1.ax1x.com/2020/04/21/JGRo7V.md.png" alt="JGRo7V.md.png"></a> </p><p>即，团队合作的时候，如果每个人都认为队友在带我，那每个人都可能划水。</p><p>但是，（training中）如果你知道你的队友在划水，那你可能会做的更好。</p><p>但是，（testing中）发现每个人都有更好地做，都没有划水，那么结果就会很好。</p><p>（hhhh，李老师每次讲Intuitive Reason的时候，都觉得好有道理hhh，科学的直觉orz给我也整一个）</p></li></ol><h3 id="Why-multiply-1-p-in-testing-Intuitive-reason"><a href="#Why-multiply-1-p-in-testing-Intuitive-reason" class="headerlink" title="Why multiply (1-p%) in testing: Intuitive reason"></a>Why multiply (1-p%) in testing: Intuitive reason</h3><p>为什么在testing中 weights要乘（1-p%)?</p><p>用一个具体的例子来直观说明：</p><p><a href="https://imgchr.com/i/JGRf6s"><img src="https://s1.ax1x.com/2020/04/21/JGRf6s.md.png" alt="JGRf6s.md.png"></a> </p><p>上图中，如果dropout rate=0.5，假设只训练一次， $w_2,w_4$ 相连的neuron都被dropout。</p><p>在testing中，因为不对neurondropout，所以如果不改变weight，计算出的结果 $z’\approx 2z$ 。</p><p>因此将所有weight简单地和(1-p%) 相乘，能尽量保证计算出的结果 $z’\approx z$ 。</p><h3 id="Dropout-is-a-kind-of-ensemble"><a href="#Dropout-is-a-kind-of-ensemble" class="headerlink" title="Dropout is a kind of ensemble"></a>Dropout is a kind of ensemble</h3><p>Ensemble(合奏)，如下图，将testing data丢给train好的NN来估计，最后的估计值取所有NN输出的平均，如下图：</p><p><a href="https://imgchr.com/i/JGRhXn"><img src="https://s1.ax1x.com/2020/04/21/JGRhXn.md.png" alt="JGRhXn.md.png"></a> </p><p>为什么说dropout is a kind of ensemble?</p><p><a href="https://imgchr.com/i/JGRRpQ"><img src="https://s1.ax1x.com/2020/04/21/JGRRpQ.md.png" alt="JGRRpQ.md.png"></a> </p><ul><li><p>Using one mini-batch to train one network</p><p>【dropout相当于每次用一个mini-batch来训练一个network】</p></li><li><p>Some parameters in the network are shared</p><p>【有些参数可能会在很多个mini-batch都被train到】</p></li></ul><p>由于每个神经元有 p%的概率被dropout，因此理论上，如果有M个neuron，可能会训练 $2^M$ 个network。</p><p>但是在Ensemble中，将每个network存下来，testing的时候输出取平均，这样的过程太复杂了，结果也不一定会很好。</p><p>所以在testing中，no dropout，对原始network中的每个参数乘 (1-p%)，用这样简单的操作来达到ensemble的目的。</p><p><a href="https://imgchr.com/i/JGRWlj"><img src="https://s1.ax1x.com/2020/04/21/JGRWlj.md.png" alt="JGRWlj.md.png"></a> </p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Keras: <a href="https://keras.io/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore">how can i interrupt training when the validation loss isn’t decresing anymore.</a> </li><li>Dropout-wiki：<a href="https://zh.wikipedia.org/wiki/Dropout">https://zh.wikipedia.org/wiki/Dropout</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。&lt;br&gt;tips从Training和Testing两个方面展开。&lt;br&gt;在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有RMSprop、Momentum、Adam等）。&lt;br&gt;当在Training中得到好的performance，但在testing中perform bad时，即遇到了overfitting，又该怎么处理呢？文章后半部分详尽介绍了EarlyStopping、Regularization和Dropout三个solution。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="DNN" scheme="https://f7ed.com/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Backpropagation</title>
    <link href="https://f7ed.com/2020/04/18/Backpropagation/"/>
    <id>https://f7ed.com/2020/04/18/Backpropagation/</id>
    <published>2020-04-17T16:00:00.000Z</published>
    <updated>2020-07-03T08:39:21.739Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，讲解了Deep Learning中使用的一种高效Gradient Descent的算法：BackPropagation。<br>BackPropagation通过正向传播和反向传播两个阶段，最后能一起算出损失函数对每一个参数的gradient。</p><a id="more"></a><h1 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h1><p>在Neural Network中，参数的更新也是通过Gradient Descent。</p><p>但是当Neural Network层数很深，结构很复杂的时候，会有millions of parapmeters。</p><p>Backpropagation：To compute the gradient efficiently.</p><h2 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h2><p>BP中需要用到的数学知识：微积分中的链式法则。</p><p><a href="https://imgchr.com/i/Jmc7z4"><img src="https://s1.ax1x.com/2020/04/18/Jmc7z4.md.png" alt="Jmc7z4.md.png"></a> </p><h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><p><a href="https://imgchr.com/i/JmcTWF"><img src="https://s1.ax1x.com/2020/04/18/JmcTWF.md.png" alt="JmcTWF.md.png"></a> </p><p>在NN中，定义损失函数 $L(\theta)=\sum_{n=1}^{N} C^{n}(\theta)$ （$\theta$ 代指NN中所有的weight 和bias）</p><p>对某一参数的gradient为  $\frac{\partial L(\theta)}{\partial w}=\sum_{n=1}^{N} \frac{\partial C^{n}(\theta)}{\partial w}$ </p><p><a href="https://imgchr.com/i/JmcoJU"><img src="https://s1.ax1x.com/2020/04/18/JmcoJU.md.png" alt="JmcoJU.md.png"></a> </p><p>在上图NN中，我们先只研究红框部分，即是以下结构：</p><p><a href="https://imgchr.com/i/JmcIiT"><img src="https://s1.ax1x.com/2020/04/18/JmcIiT.md.png" alt="JmcIiT.md.png"></a> </p><p>z：每个activation function的输入。</p><p>根据链式法则， $\frac{\partial C}{\partial w}= \frac{\partial z}{\partial w} \frac{\partial C}{\partial z}$  .</p><p>要计算每个参数的  $\frac{\partial C}{\partial w}$  ，分为两部分。</p><ol><li><u>Forward pass:</u>  compute $\frac{\partial z}{\partial w} $ for all parameters.</li><li><u>Backward pass:</u>  compute $\frac{\partial C}{\partial z} $ for all activation function inputs z.</li></ol><h2 id="BP：Forward-pass"><a href="#BP：Forward-pass" class="headerlink" title="BP：Forward pass"></a>BP：Forward pass</h2><p><strong>Compute $\frac{\partial z}{\partial w} $ for all parameters.</strong></p><p><a href="https://imgchr.com/i/Jmchd0"><img src="https://s1.ax1x.com/2020/04/18/Jmchd0.md.png" alt="Jmchd0.md.png"></a> </p><p>还是只看上图这一部分，可以轻易得出： $\partial{z}/\partial{w_1}=x_1\qquad \partial{z}/\partial{w_2}=x_2$  </p><p>得到结论： $\frac{\partial z}{\partial w} $  等于 the value of the input connected by the weight. </p><p>【$\frac{\partial z}{\partial w} $ 等于 连接w的输入的值】</p><hr><p>那么，如何计算出NN中全部的 $\frac{\partial z}{\partial w} $ ？</p><p><a href="https://imgchr.com/i/Jmc4oV"><img src="https://s1.ax1x.com/2020/04/18/Jmc4oV.md.png" alt="Jmc4oV.md.png"></a> </p><p>：Forward pass.</p><p>用当前参数（w,b)</p><p>从hidden layer的第一层开始，计算出第一层的输出，即第二层的输入。</p><p>依次相前计算，计算出每一层的输出，即下一层的输入，即输入所连接权重的 $\frac{\partial z}{\partial w}$ 。</p><h2 id="BP：Backward-pass"><a href="#BP：Backward-pass" class="headerlink" title="BP：Backward pass"></a>BP：Backward pass</h2><p><strong>Compute $\frac{\partial C}{\partial z} $ for all activation function inputs z.</strong> </p><p><a href="https://imgchr.com/i/JmcfZq"><img src="https://s1.ax1x.com/2020/04/18/JmcfZq.md.png" alt="JmcfZq.md.png"></a> </p><p>z：activation function的 input</p><p>a：activation function的 output</p><p>这里的activation function 是 sigmod函数  $a=\sigma(z)=\frac{1}{1+e^{-z}}$ </p><p>要求  $\frac{\partial C}{\partial z}$  ， 再根据链式法则： $\frac{\partial C}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial C}{\partial a}$ </p><ol><li><p>求  $\frac{\partial{a}}{\partial{z}}$  :   $\frac{\partial{a}}{\partial{z}}=\sigma'(z)=\sigma(z)(1-\sigma(z))$  （是其他activation function 也能轻易求出）</p></li><li><p>求 $\frac{\partial C}{\partial a}$ ：根据链式法则： $\frac{\partial C}{\partial a}=\frac{\partial z^{\prime}}{\partial a} \frac{\partial C}{\partial z^{\prime}}+\frac{\partial z^{\prime \prime}}{\partial a} \frac{\partial C}{\partial z^{\prime \prime}}$  </p><p><a href="https://imgchr.com/i/JmcfZq"><img src="https://s1.ax1x.com/2020/04/18/JmcfZq.md.png" alt="JmcfZq.md.png"></a> </p><ul><li> $\frac{\partial z^{\prime}}{\partial a} =w_3$  ， $\frac{\partial z^{\prime\prime}}{\partial a} =w_4$ </li><li> $\frac{\partial C}{\partial z^{\prime}}$  和 $\frac{\partial C}{\partial z^{\prime\prime}}$ ？假设，已经通过某种方法算出这个值。</li></ul></li><li> $\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_{3} \frac{\partial C}{\partial z^{\prime}}+w_{4} \frac{\partial C}{\partial z^{\prime \prime}}\right]$  <p>这个式子，可以画成一个反向传播的NN，见下图。</p><p><a href="https://imgchr.com/i/JmcRLn"><img src="https://s1.ax1x.com/2020/04/18/JmcRLn.md.png" alt="JmcRLn.md.png"></a> </p> $\frac{\partial C}{\partial z^{\prime}},\frac{\partial C}{\partial z^{\prime\prime}}$  是这个neuron的输入， $w_3,w_4$ 仍然是 neuron的 weight（无bias）。<p>$\sigma’(z)$ 是一个常数，因为在forward pass中每一个activation的输入已经被算出来了。</p><p>和forward pass中的NN的区别是，forward 中是一个activation function，输入z作用于这个函数；</p><p>而在 backward pass中，这更像一个放缩器，将他的输入变小，即乘上一个 $\sigma’(z)$ 。</p></li></ol><hr><p>问题还是如何计算  $\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_{3} \frac{\partial C}{\partial z^{\prime}}+w_{4} \frac{\partial C}{\partial z^{\prime \prime}}\right]$  ？</p><p>分为两种情况讨论， $z’,z’’$ 是否为输出层的输入？</p><ul><li><p>Output Layer：</p><p><a href="https://imgchr.com/i/Jmc2ss"><img src="https://s1.ax1x.com/2020/04/18/Jmc2ss.md.png" alt="Jmc2ss.md.png"></a> </p><p>z’,z’’：activation function的输入。</p><p>y1,y2：actiavtion function（也是NN）的输出。</p><p>C：NN输出和target的cross entropy。</p><p>根据链式法则： $\frac{\partial C}{\partial z^{\prime}}=\frac{\partial y_{1}}{\partial z^{\prime}} \frac{\partial C}{\partial y_{1}} \quad \frac{\partial C}{\partial z^{\prime \prime}}=\frac{\partial y_{2}}{\partial z^{\prime \prime}} \frac{\partial C}{\partial y_{2}}$  </p><p>所以，已知activation function（simod或者其他），可以轻易求出  $\frac{\partial y_{1}}{\partial z^{\prime}}(=\sigma'(z'))$ 和  $\frac{\partial y_{2}}{\partial z^{\prime\prime}}(=\sigma''(z''))$      。</p><p>所以，已知损失函数，也可以轻易求出 $\frac{\partial C}{\partial y_1}$ 和  $\frac{\partial C}{\partial y_2}$  。（  $C\left(y, \hat{y}\right)=-\left[\hat{y} \ln y+\left(1-\hat{y}\right) \ln \left(1-y\right)\right]$ )</p><p>所以，可以直接求出  $\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_{3} \frac{\partial C}{\partial z^{\prime}}+w_{4} \frac{\partial C}{\partial z^{\prime \prime}}\right]$  。</p></li><li><p>Not Output Layer:</p><p><a href="https://imgchr.com/i/JmcsJS"><img src="https://s1.ax1x.com/2020/04/18/JmcsJS.md.png" alt="JmcsJS.md.png"></a> </p><p>上图中，如果我们要计算 $\frac{\partial C}{\partial z’}$ ，必须要已知下一层的 $\frac{\partial C}{\partial z_a}$ ，然后一直递归下去，直到到达最后的输出层，也就是上面一种情况，可以直接计算出，再递归回来，计算当前层的 $\frac{\partial C}{\partial z’}$ 。</p><p>但是，这样计算每个参数的 $\frac{\partial{C}}{\partial{z}}$ 都要一直递归到输出层，效率显然太低了。</p><p><a href="https://imgchr.com/i/JmcyRg"><img src="https://s1.ax1x.com/2020/04/18/JmcyRg.md.png" alt="JmcyRg.md.png"></a>  </p><p>计算方法如上图：</p><p>当我们已知输出层的  $\frac{\partial{C}}{\partial{z'}},\frac{\partial{C}}{\partial{z''}}$  时，再通过上面的步骤3（且的确算出了    $\frac{\partial{C}}{\partial{z'}},\frac{\partial{C}}{\partial{z''}}$ ），画成反向的NN，计算$\frac{\partial{C}}{\partial{z}}$. </p><p>再依次反向传播计算出每一个neuron的输出z （也是正向传播neuron的输入）的 $\frac{\partial{C}}{\partial{z}}$ .</p></li></ul><hr><p><strong>Backforward pass 的做法：</strong></p><p><a href="https://imgchr.com/i/Jmcri8"><img src="https://s1.ax1x.com/2020/04/18/Jmcri8.md.png" alt="Jmcri8.md.png"></a> </p><ol><li>先计算出输出层的 $\frac{\partial{C}}{\partial{z}}$ （也就是上图的  $\frac{\partial{C}}{\partial{z_5}}$ 和 $\frac{\partial{C}}{\partial{z_6}}$  ）</li><li>用反向传播的NN，向后依次计算出每一层每一个neuron的 $\frac{\partial{C}}{\partial{z}}$ 。</li></ol><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><a href="https://imgchr.com/i/JmcgMj"><img src="https://s1.ax1x.com/2020/04/18/JmcgMj.md.png" alt="JmcgMj.md.png"></a> </p><p>公式：  $\frac{\partial z}{\partial w} \frac{\partial C}{\partial z}=\frac{\partial C}{\partial w}$ </p><p>在正向传播NN中，z是neuron的activation function的输入。</p><p>在反向传播NN中，z是neuron的放缩器的输出。</p><p>通过Forward Pass计算出正向传播NN的每一个neuron的 $\frac{\partial z}{\partial w}$ ，等于该层neuron的输入。</p><p>通过Backward Pass计算出反向传播NN的每一个neuron的 $\frac{\partial C}{\partial z}$ 。</p><p>然后，通过相乘，计算出每个参数的 $\frac{\partial C}{\partial w}$。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，讲解了Deep Learning中使用的一种高效Gradient Descent的算法：BackPropagation。&lt;br&gt;BackPropagation通过正向传播和反向传播两个阶段，最后能一起算出损失函数对每一个参数的gradient。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="Backpropagation" scheme="https://f7ed.com/tags/Backpropagation/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Deep Learning-Introduction</title>
    <link href="https://f7ed.com/2020/04/18/DL-introdunction/"/>
    <id>https://f7ed.com/2020/04/18/DL-introdunction/</id>
    <published>2020-04-17T16:00:00.000Z</published>
    <updated>2020-07-03T08:41:15.486Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，介绍了Deep Learning的一般步骤。</p><a id="more"></a><h1 id="Up-and-downs-of-Deep-Learning"><a href="#Up-and-downs-of-Deep-Learning" class="headerlink" title="Up and downs of Deep Learning"></a>Up and downs of Deep Learning</h1><ul><li><p>1958: Perceptron (linear model)</p></li><li><p>1969: Perceptron has limitation</p></li><li><p>1980s: Multi-layer perceptron </p><p>​            Do not have significant difference from DNN today</p></li><li><p>1986: Backpropagation</p><p>​            Usually more than 3 hidden layers is not helpful</p></li><li><p>1989: 1 hidden layer is “good enough”, why deep?</p></li><li><p>2006: RBM initialization (breakthrough) </p></li><li><p>2009: GPU</p></li><li><p>2011: Start to be popular in speech recognition【语音辨识】</p></li><li><p>2012: win ILSVRC image competition 【图像识别】</p></li></ul><h1 id="Step-1-Neural-Network"><a href="#Step-1-Neural-Network" class="headerlink" title="Step 1: Neural Network"></a>Step 1: Neural Network</h1><p>在将Regression 和 Classification时，Step 1 是确定一个function set。</p><p>在Deep Learning中，也是相同的，只是这里的function set就是一个neural network的结构。</p><p><a href="https://imgchr.com/i/JmFnne"><img src="https://s1.ax1x.com/2020/04/18/JmFnne.md.png" alt="JmFnne.md.png"></a> </p><p>上图中，一个Neuron就是如上图所示的一个unit，neuron之间不同的连接方式构成不同的Neural Network。</p><h2 id="Fully-Connect-Feedforward-Network"><a href="#Fully-Connect-Feedforward-Network" class="headerlink" title="Fully Connect Feedforward Network"></a>Fully Connect Feedforward Network</h2><p><a href="https://imgchr.com/i/JmFV1K"><img src="https://s1.ax1x.com/2020/04/18/JmFV1K.md.png" alt="JmFV1K.md.png"></a> </p><p>这是一个Fully Connect Feedforward Network【全连接反馈网络】，其中每个neuron的activation function都是一个sigmod函数。</p><p><a href="https://imgchr.com/i/JmFeXD"><img src="https://s1.ax1x.com/2020/04/18/JmFeXD.md.png" alt="JmFeXD.md.png"></a>  </p><p>为什么说neural network其实就是一个function呢？上面两张图中，输入是一个vector，输出也是一个vector，可以用下面函数来表示。</p>$$f\left(\left[\begin{array}{c}1 \\ -1\end{array}\right]\right)=\left[\begin{array}{c}0.62 \\ 0.83\end{array}\right] f\left(\left[\begin{array}{l}0 \\ 0\end{array}\right]\right)=\left[\begin{array}{l}0.51 \\ 0.85\end{array}\right]$$<p><a href="https://imgchr.com/i/JmFZ6O"><img src="https://s1.ax1x.com/2020/04/18/JmFZ6O.md.png" alt="JmFZ6O.md.png"></a> </p><p>上图为全连接网络的一般形式，第一层是Input Layer，最后一层是Output Layer，中间的其他层称为Hidden Layer。</p><p>而Deep Learning中的Deep的含义就是Many hidden layers的意思。</p><h2 id="Matrix-Operation"><a href="#Matrix-Operation" class="headerlink" title="Matrix Operation"></a>Matrix Operation</h2><p><a href="https://imgchr.com/i/JmFkfx"><img src="https://s1.ax1x.com/2020/04/18/JmFkfx.md.png" alt="JmFkfx.md.png"></a> </p><p>上图的全连接网络中，第一个hidden layer的输出可以写成矩阵和向量的形式：</p>$$\sigma\left(\left[\begin{array}{cc}1 & -2 \\ -1 & 1\end{array}\right]\left[\begin{array}{c}1 \\ -1\end{array}\right]+\left[\begin{array}{c}1 \\ 0\end{array}\right]\right)=\left[\begin{array}{c}0.98 \\ 0.12\end{array}\right]$$<p><a href="https://imgchr.com/i/JmFEp6"><img src="https://s1.ax1x.com/2020/04/18/JmFEp6.md.png" alt="JmFEp6.md.png"></a> </p><p>更为一般的公式，用W表示权重，b代表bias，a表示hidden layer的输出。输出vector y可以写成 $y = f(x)$ 的形式，即： $y= f(x)=$</p><p><a href="https://imgchr.com/i/JmFFt1"><img src="https://s1.ax1x.com/2020/04/18/JmFFt1.md.png" alt="JmFFt1.md.png"></a> </p><p>转换为矩阵运算的形式，就可以使用并行计算的硬件技术（GPU）来加速矩阵运算，这也是为什么用GPU来训练Neural Network 更快的原因。</p><h2 id="Output-Layer"><a href="#Output-Layer" class="headerlink" title="Output Layer"></a>Output Layer</h2><p>在 <a href="/2020/04/01/Classification2/" title="Logistic Regression">Logistic Regression</a>中第4节讲到Logistic Regression有局限，消除局限的一种方法是Feature Transformation。</p><p>但是Feature Transformation需要人工设计，不太“机器学习”。</p><p>在下图全连接图中，把Output Layer换成一个Multi-class Classifier（SoftMax），而其中Hidden Layers的作用就是Feature extractor，从feature x提取出新的feature，也就是 output layer的输入。</p><p><a href="https://imgchr.com/i/JmFpm4"><img src="https://s1.ax1x.com/2020/04/18/JmFpm4.md.png" alt="JmFpm4.md.png"></a> </p><p>这样就不需要人工设计Feature Transformation/Feature engineering，可以让机器自己学习：如何将原来的feature转换为更好分类的feature。</p><hr><p><strong>Handwriting Digit Recognition</strong> </p><p><a href="https://imgchr.com/i/Jmix6U"><img src="https://s1.ax1x.com/2020/04/18/Jmix6U.md.png" alt="Jmix6U.md.png"></a> </p><p>在手写数字辨别中，输出是一个16*16的image（256维的vector），输出是一个10维的vector，每一维表示是该image是某个数字的概率。</p><p>在手写数字辨别中，需要设计neural network的结构来提取输入的256维feature。</p><p><a href="https://imgchr.com/i/JmFC79"><img src="https://s1.ax1x.com/2020/04/18/JmFC79.md.png" alt="JmFC79.md.png"></a> </p><h1 id="Step-2-Goodness-of-function"><a href="#Step-2-Goodness-of-function" class="headerlink" title="Step 2: Goodness of function"></a>Step 2: Goodness of function</h1><p>之前我们已经使用过的最小二乘法和交叉熵作为损失函数。</p><p>一般在Neural Network中，使用output vector 和target vector的交叉熵作为Loss。</p><h1 id="Step-3-Pick-the-best-function"><a href="#Step-3-Pick-the-best-function" class="headerlink" title="Step 3: Pick the best function"></a>Step 3: Pick the best function</h1><p>在NN中，也使用Gradient Descent。</p><p><a href="https://imgchr.com/i/JmF90J"><img src="https://s1.ax1x.com/2020/04/18/JmF90J.md.png" alt="JmF90J.md.png"></a> </p><p>但是，Deep Neural Network中，参数太多了，计算结构也很复杂。</p><p>Backpropagation：an efficient way to compute $\partial{L}/\partial{w}$ in neural network.</p><p>Backpropagation本质也是Gradient Descent，只是一种更高效进行Gradient Descent的算法。</p><p>在很多 toolkit（TensorFlow，PyTorch ，Caffe等）中都实现了Backpropgation。</p><p>Backpropagation部分，见<a href="/2020/04/18/Backpropagation/" title="下一篇博客">下一篇博客</a>。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，介绍了Deep Learning的一般步骤。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：HW2-Binary Income Predicting</title>
    <link href="https://f7ed.com/2020/04/15/ml-lee-hw2/"/>
    <id>https://f7ed.com/2020/04/15/ml-lee-hw2/</id>
    <published>2020-04-14T16:00:00.000Z</published>
    <updated>2020-07-03T08:42:48.419Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。<br>包括对数据集的处理，训练模型，可视化，预测等。<br>有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的<a href="https://github.com/f1ed/ML-HW2">GitHub</a></p><a id="more"></a><h1 id="Task-introduction-and-Dataset"><a href="#Task-introduction-and-Dataset" class="headerlink" title="Task introduction and Dataset"></a>Task introduction and Dataset</h1><p> Kaggle competition: <a href="https://www.kaggle.com/c/ml2020spring-hw2">link</a> </p><p><strong>Task: Binary Classification</strong></p><p>Predict whether the income of an individual exceeds $50000 or not ?</p><p>*<em>Dataset: *</em> Census-Income (KDD) Dataset</p><p>(Remove unnecessary attributes and balance the ratio between positively and negatively labeled data)</p><h1 id="Feature-Format"><a href="#Feature-Format" class="headerlink" title="Feature Format"></a>Feature Format</h1><ul><li><p>train.csv, test_no_label.csv【都是没有处理过的数据，可作为数据参考和优化参考】</p><ul><li><p>text-based raw data</p></li><li><p>unnecessary attributes removed, positive/negative ratio balanced.</p></li></ul></li><li><p>X_train, Y_train, X_test【已经处理过的数据，可以直接使用】</p><ul><li><p>discrete features in train.csv =&gt; one-hot encoding in X_train (education, martial state…)</p></li><li><p>continuous features in train.csv =&gt; remain the same in X_train (age, capital losses…).</p></li><li><p>X_train, X_test : each row contains one 510-dim feature represents a sample.</p></li><li><p>Y_train: label = 0 means “&lt;= 50K” 、 label = 1 means “ &gt;50K ”</p></li></ul></li></ul><p>注：数据集超大，用notepad查看比较舒服；调试时，也可以先调试小一点的数据集。</p><h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><p>Logistic Regression 原理部分见<a href="/2020/04/01/Classification2/" title="这篇博客">这篇博客</a>。</p><h2 id="Prepare-data"><a href="#Prepare-data" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>本文直接使用X_train Y_train X_test 已经处理好的数据集。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_Train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./logistic_output/output_logistic.csv'</span></span><br><span class="line">fpath = <span class="string">'./logistic_output/logistic'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br></pre></td></tr></table></figure><p>统计一下数据集：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In logistic model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of Training set: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of development set: &#123;&#125;\n'</span>.format(dev_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n'</span>.format(data_dim))</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">In</span> logistic model:</span><br><span class="line">Size of Training set: <span class="number">48830</span></span><br><span class="line">Size of development set: <span class="number">5426</span></span><br><span class="line">Size of test set: <span class="number">27622</span></span><br><span class="line">Dimension of <span class="keyword">data</span>: <span class="number">510</span></span><br></pre></td></tr></table></figure><h3 id="normalize"><a href="#normalize" class="headerlink" title="normalize"></a>normalize</h3><p>normalize data.</p><p>对于train data，计算出每个feature的mean和std，保存下来用来normalize test data。</p><p>代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br></pre></td></tr></table></figure><h3 id="Development-set-split"><a href="#Development-set-split" class="headerlink" title="Development set split"></a>Development set split</h3><p>在logistic regression中使用的gradient，没有closed-form解，所以在train set中划出一部分作为development set 优化参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_dev_split</span><span class="params">(X, Y, dev_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br></pre></td></tr></table></figure><h2 id="Useful-function"><a href="#Useful-function" class="headerlink" title="Useful function"></a>Useful function</h2><h3 id="shuffle-X-Y"><a href="#shuffle-X-Y" class="headerlink" title="_shuffle(X, Y)"></a>_shuffle(X, Y)</h3><p>本文使用mini-batch gradient。</p><p>所以在每次epoch时，以相同顺序同时打乱X_train,Y_train数组，再mini-batch。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br></pre></td></tr></table></figure><h3 id="sigmod-z"><a href="#sigmod-z" class="headerlink" title="_sigmod(z)"></a>_sigmod(z)</h3><p>计算 $\frac{1}{1+e^{-z}}$ ，注意：防止溢出，给函数返回值规定上界和下界。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br></pre></td></tr></table></figure><h3 id="f-X-w-b"><a href="#f-X-w-b" class="headerlink" title="_f(X, w, b)"></a>_f(X, w, b)</h3><p>是sigmod函数的输入，linear part。</p><ul><li>输入：<ul><li>X：shape = [size, data_dimension]</li><li>w：weight vector, shape = [data_dimension, 1]</li><li>b: bias, scalar</li></ul></li><li>输出：<ul><li>属于Class 1的概率（Label=0，即收入小于$50k的概率）</li></ul></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br></pre></td></tr></table></figure><h3 id="predict-X-w-b"><a href="#predict-X-w-b" class="headerlink" title="_predict(X, w, b)"></a>_predict(X, w, b)</h3><p>预测Label=0？（0或者1，不是概率）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br></pre></td></tr></table></figure><h3 id="accuracy-Y-pred-Y-label"><a href="#accuracy-Y-pred-Y-label" class="headerlink" title="_accuracy(Y_pred, Y_label)"></a>_accuracy(Y_pred, Y_label)</h3><p>计算预测出的结果（0或者1）和真实结果的正确率。</p><p>这里使用 $1-\overline{error}$ 来表示正确率。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure><h3 id="cross-entropy-loss-y-pred-Y-label"><a href="#cross-entropy-loss-y-pred-Y-label" class="headerlink" title="_cross_entropy_loss(y_pred, Y_label)"></a>_cross_entropy_loss(y_pred, Y_label)</h3><p>计算预测出的概率（是sigmod的函数输出）和真实结果的交叉熵。</p><p>计算公式为： $\sum_n {C(y_{pred},Y_{label})}=-\sum[Y_{label}\ln{y_{pred}}+(1-Y_{label})\ln(1-{y_{pred}})]$ </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="gradient-X-Y-label-w-b"><a href="#gradient-X-Y-label-w-b" class="headerlink" title="_gradient(X, Y_label, w, b)"></a>_gradient(X, Y_label, w, b)</h3><p>和Regression的最小二乘一样。（严谨的说，最多一个系数不同）</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">def</span> <span class="string">_gradient(X, Y_label, w, b):</span></span><br><span class="line"><span class="comment">    # This function calculates the gradient of cross entropy</span></span><br><span class="line"><span class="comment">    # X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    <span class="attr">y_pred</span> = <span class="string">_f(X, w, b)</span></span><br><span class="line">    <span class="attr">pred_error</span> = <span class="string">Y_label - y_pred</span></span><br><span class="line">    <span class="attr">w_grad</span> = <span class="string">- np.dot(X.T, pred_error)</span></span><br><span class="line">    <span class="attr">b_grad</span> = <span class="string">- np.sum(pred_error)</span></span><br><span class="line">    <span class="attr">return</span> <span class="string">w_grad, float(b_grad)</span></span><br></pre></td></tr></table></figure><h2 id="Training-Adagrad"><a href="#Training-Adagrad" class="headerlink" title="Training (Adagrad)"></a>Training (Adagrad)</h2><p>初始化一些参数。</p><p><strong>这里特别注意</strong> :</p><p>由于adagrad的参数更新是 $w \longleftarrow w-\eta \frac{gradient}{ \sqrt{gradsum}}$ .</p><p><strong>防止除0</strong>，初始化gradsum的值为一个较小值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.float(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.float(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br></pre></td></tr></table></figure><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Aagrad具体原理见<a href="/2020/03/01/Gradient/" title="这篇博客">这篇博客</a>的1.2节。</p><p>迭代更新时，每次epoch计算一次loss和accuracy，以便可视化更新过程，调整参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> range(int(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br></pre></td></tr></table></figure><h3 id="Loss-amp-accuracy"><a href="#Loss-amp-accuracy" class="headerlink" title="Loss &amp; accuracy"></a>Loss &amp; accuracy</h3><p>输出最后一次迭代的loss和accuracy。</p><p>结果如下：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Training loss: <span class="number">0.2933570286596322</span></span><br><span class="line">Training accuracy: <span class="number">0.8839238173254147</span></span><br><span class="line">Development loss: <span class="number">0.31029505347634456</span></span><br><span class="line">Development accuracy: <span class="number">0.8336166253549906</span></span><br></pre></td></tr></table></figure><p>画出loss 和 accuracy的更新过程：</p><p>loss：</p><p><a href="https://imgchr.com/i/JPCjx0"><img src="https://s1.ax1x.com/2020/04/15/JPCjx0.png" alt="JPCjx0.png"></a> </p><p>accuracy：</p><p><a href="https://imgchr.com/i/JPCxMV"><img src="https://s1.ax1x.com/2020/04/15/JPCxMV.png" alt="JPCxMV.png"></a> </p><p>由于Feature数量较大，将权重影响最大的feature输出看看：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Other Rel &lt;<span class="number">18</span> spouse of subfamily RP: [<span class="number">7.11323764</span>]</span><br><span class="line"> Grandchild &lt;<span class="number">18</span> ever marr not <span class="keyword">in</span> subfamily: [<span class="number">6.8321061</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> ever marr RP of subfamily: [<span class="number">6.77322397</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> ever marr RP of subfamily: [<span class="number">6.76688406</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> never married RP of subfamily: [<span class="number">6.37488958</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> spouse of subfamily RP: [<span class="number">5.97717831</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.53932651</span>]</span><br><span class="line"> Grandchild <span class="number">18</span>+ spouse of subfamily RP: [<span class="number">5.42948497</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.41543809</span>]</span><br><span class="line"> Mexico: [<span class="number">4.79920763</span>]</span><br></pre></td></tr></table></figure><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>完整数据集、代码等，欢迎光临小透明<a href="https://github.com/f1ed/ML-HW2">GitHub</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################</span></span><br><span class="line"><span class="comment"># Data:2020-04-05</span></span><br><span class="line"><span class="comment"># Author: Fred Lau</span></span><br><span class="line"><span class="comment"># ML-Lee: HW2 : Binary Classification</span></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################</span></span><br><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_Train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./logistic_output/output_logistic.csv'</span></span><br><span class="line">fpath = <span class="string">'./logistic_output/logistic'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_dev_split</span><span class="params">(X, Y, dev_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In logistic model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of Training set: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of development set: &#123;&#125;\n'</span>.format(dev_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################</span></span><br><span class="line"><span class="comment"># useful function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to calculate probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - (<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This is the logistic function, parameterized by w and b</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [batch_size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       predict probability of each row of X being positively labeled, shape = [batch_size, 1]</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This fucntion returns a truth value prediction for each row of X by logistic regression</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradient</span><span class="params">(X, Y_label, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the gradient of cross entropy</span></span><br><span class="line">    <span class="comment"># X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    y_pred = _f(X, w, b)</span><br><span class="line">    pred_error = Y_label - y_pred</span><br><span class="line">    w_grad = - np.dot(X.T, pred_error)</span><br><span class="line">    b_grad = - np.sum(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, float(b_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#######################################</span></span><br><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.float(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.float(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> range(int(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'Training loss: &#123;&#125;\n'</span>.format(train_loss[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Training accuracy: &#123;&#125;\n'</span>.format(train_acc[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Development loss: &#123;&#125;\n'</span>.format(dev_loss[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Development accuracy: &#123;&#125;\n'</span>.format(dev_acc[<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###################</span></span><br><span class="line"><span class="comment"># Plotting Loss and accuracy curve</span></span><br><span class="line"><span class="comment"># Loss curve</span></span><br><span class="line">plt.plot(train_loss, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(dev_loss, label=<span class="string">'dev'</span>)</span><br><span class="line">plt.title(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'./logistic_output/loss.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(train_acc, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(dev_acc, label=<span class="string">'dev'</span>)</span><br><span class="line">plt.title(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'./logistic_output/acc.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id, label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> id, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;, &#123;&#125;\n'</span>.format(id, label[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line"><span class="comment"># Output the weights and bias</span></span><br><span class="line">ind = (np.argsort(np.abs(w), axis=<span class="number">0</span>)[::<span class="number">-1</span>]).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>, <span class="number">0</span>: <span class="number">10</span>]:</span><br><span class="line">       f.write(<span class="string">'&#123;&#125;: &#123;&#125;\n'</span>.format(content[i], w[i]))</span><br></pre></td></tr></table></figure><h1 id="Generative-Model"><a href="#Generative-Model" class="headerlink" title="Generative Model"></a>Generative Model</h1><p>Generative Model 原理部分见 <a href="/2020/03/21/Classification1/" title="这篇博客">这篇博客</a></p><h2 id="Prepare-data-1"><a href="#Prepare-data-1" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>这部分和Logistic regression一样。</p><p>只是，因为generative model有closed-form solution，不需要划分development set。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./generative_output/output_&#123;&#125;.csv'</span></span><br><span class="line">fpath = <span class="string">'./generative_output/generative'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In generative model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of training data: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n\n'</span>.format(data_dim))</span><br></pre></td></tr></table></figure><h2 id="Useful-functions"><a href="#Useful-functions" class="headerlink" title="Useful functions"></a>Useful functions</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="公式再推导"><a href="#公式再推导" class="headerlink" title="公式再推导"></a>公式再推导</h3><p>计算公式： </p>$$\begin{equation}\begin{aligned}P\left(C_{1} | x\right)&=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}\\&=\frac{1}{1+\frac{P\left(x | C_{2}\right) P\left(C_{2}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)}}\\&=\frac{1}{1+\exp (-z)} =\sigma(z)\qquad(z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}\end{aligned}\end{equation}$$<p>计算z的过程：</p><ol><li>首先计算Prior Probability。</li><li>假设模型是Gaussian的，算出 $\mu_1,\mu_2 ,\Sigma$  的closed-form solution 。</li><li>根据 $\mu_1,\mu_2,\Sigma$ 计算出 $w,b$ 。</li></ol><hr><ol><li><p><strong>计算Prior Probability。</strong> </p><p>程序中用list comprehension处理较简单。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br></pre></td></tr></table></figure></li><li><p>计算 $\mu_1,\mu_2 ,\Sigma$ （Gaussian）</p><p>$\mu_0=\frac{1}{C0} \sum_{n=1}^{C0} x^{n} $  (Label=0)</p><p>$\mu_1=\frac{1}{C1} \sum_{n=1}^{C1} x^{n} $  (Label=0)</p><p>$\Sigma_0=\frac{1}{C0} \sum_{n=1}^{C0}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$  (<strong>注意</strong> ：这里的 $x^n,\mu$ 都是行向量，注意转置的位置）</p><p>$\Sigma_1=\frac{1}{C1} \sum_{n=1}^{C1}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$ </p><p>$\Sigma=(C0 \times\Sigma_0+C1\times\Sigma_1)/(C0+C1)$   (shared covariance) </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>计算 $w,b$ </p><p>在 <a href="/2020/03/21/Classification1/" title="这篇博客">这篇博客</a>中的第2小节中的公式推导中， $x^n,\mu$ 都是列向量，公式如下：</p>   $$   z=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} x-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}   $$     $w^T=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} \qquad b=-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}$ <hr><p><strong>但是</strong> ，一般我们在处理的数据集，$x^n,\mu$ 都是行向量。推导过程相同，公式如下：</p><p><font color=#f00> <strong>（主要注意转置和矩阵乘积顺序）</strong> </font></p>   $$   z=x\cdot \Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  -\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}   $$     $w=\Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  \qquad b=-\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}$ </li></ol><hr><p><font color=#f00>但是，协方差矩阵的逆怎么求呢？ </font> </p><p>numpy中有直接求逆矩阵的方法(np.linalg.inv)，但当该矩阵是nearly singular，是奇异矩阵时，就会报错。</p><p>而我们的协方差矩阵（510*510）很大，很难保证他不是奇异矩阵。</p><p>于是，有一个 <del>牛逼</del> 强大的数学方法，叫SVD(singular value decomposition, 奇异值分解) 。</p><p>原理步骤我……还没有完全搞清楚QAQ（先挖个坑）[1]</p><p>利用SVD，可以将任何一个矩阵（即使是奇异矩阵），分界成 $A=u s v^T$ 的形式：其中u,v都是标准正交矩阵，s是对角矩阵。（numpy.linalg.svd方法实现了SVD）</p><p><font color=#f00>可以利用SVD求矩阵的伪逆 </font> </p><ul><li>$A=u s v^T$<ul><li>u,v是标准正交矩阵，其逆矩阵等于其转置矩阵</li><li>s是对角矩阵，其”逆矩阵“<strong>（注意s矩阵的对角也可能有0元素）</strong> 将非0元素取倒数即可。</li></ul></li><li>$A^{-1}=v s^{-1} u$</li></ul><p>计算 $w,b$ 的代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><p> accuracy结果：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">0.8756450899439694</span></span><br></pre></td></tr></table></figure><p>也将权重较大的feature输出看看：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">age: [-<span class="number">0.51867291</span>]</span><br><span class="line"> Masters degree(MA MS MEng MEd MSW MBA): [-<span class="number">0.49912643</span>]</span><br><span class="line"> Spouse of householder: [<span class="number">0.49786805</span>]</span><br><span class="line">weeks worked <span class="keyword">in</span> year: [-<span class="number">0.44710924</span>]</span><br><span class="line"> Spouse of householder: [-<span class="number">0.43305697</span>]</span><br><span class="line">capital gains: [-<span class="number">0.42608727</span>]</span><br><span class="line">dividends from stocks: [-<span class="number">0.41994666</span>]</span><br><span class="line"> Doctorate degree(PhD EdD): [-<span class="number">0.39310961</span>]</span><br><span class="line">num persons worked <span class="keyword">for</span> employer: [-<span class="number">0.37345994</span>]</span><br><span class="line"> Prof school degree (MD DDS DVM LLB JD): [-<span class="number">0.35594107</span>]</span><br></pre></td></tr></table></figure><h2 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h2><p>具体数据集和代码，欢迎光临小透明<a href="https://github.com/f1ed/ML-HW2">GitHub</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment">##############################################</span></span><br><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./generative_output/output_&#123;&#125;.csv'</span></span><br><span class="line">fpath = <span class="string">'./generative_output/generative'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In generative model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of training data: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n\n'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment">########################</span></span><br><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"><span class="comment">#######################</span></span><br><span class="line"><span class="comment"># Generative Model: closed-form solution, can be computed directly</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute accuracy on training set</span></span><br><span class="line">Y_train_pred = <span class="number">1</span> - _predict(X_train, w, b)</span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'\nTraining accuracy: &#123;&#125;\n'</span>.format(_accuracy(Y_train_pred, Y_train)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = <span class="number">1</span> - _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath.format(<span class="string">'generative'</span>), <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id, label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;, &#123;&#125;\n'</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output the most significant weight</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">ind = np.argsort(np.abs(np.concatenate(w)))[::<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>)<span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>:<span class="number">10</span>]:</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;: &#123;&#125;\n'</span>.format(content[i], w[i]))</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>SVD原理，待补充</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。&lt;br&gt;包括对数据集的处理，训练模型，可视化，预测等。&lt;br&gt;有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的&lt;a href=&quot;https://github.com/f1ed/ML-HW2&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="Classification" scheme="https://f7ed.com/tags/Classification/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:HW1-Predict PM2.5</title>
    <link href="https://f7ed.com/2020/04/06/ml-lee-hw1/"/>
    <id>https://f7ed.com/2020/04/06/ml-lee-hw1/</id>
    <published>2020-04-05T16:00:00.000Z</published>
    <updated>2020-07-03T08:42:35.767Z</updated>
    
    <content type="html"><![CDATA[<p>在本篇文章中，用手刻Adagrad完成了「机器学习-李宏毅」的HW1-预测PM2.5的作业。其中包括对数据的处理，训练模型，预测，并使用sklearn toolkit的结果进行比较。<br>有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的<a href="https://github.com/f1ed/ML-HW1">GitHub</a>  </p><a id="more"></a><h1 id="Task-Description"><a href="#Task-Description" class="headerlink" title="Task Description"></a>Task Description</h1><p><a href="https://www.kaggle.com/c/ml2020spring-hw1">kaggle link</a> </p><p>从中央气象局网站下载的真实观测资料，必须利用linear regression或其他方法预测PM2.5的值。</p><p>观测记录被分为train set 和 test set, 前者是每个月前20天所有资料；后者是从剩下的资料中随机取样出来的。</p><p>train.csv: 每个月前20天的完整资料。</p><p>test.csv: 从剩下的10天资料中取出240笔资料，每一笔资料都有连续9小时的观测数据，必须以此观测出第十小时的PM2.5.</p><h1 id="Process-Data"><a href="#Process-Data" class="headerlink" title="Process Data"></a>Process Data</h1><p>train data如下图，每18行是一天24小时的数据，每个月取了前20天（时间上是连续的小时）。</p><p><a href="https://imgchr.com/i/GyqsyR"><img src="https://s1.ax1x.com/2020/04/06/GyqsyR.md.png" alt="GyqsyR.md.png"></a> </p><p>test data 如下图，每18行是一笔连续9小时的数据，共240笔数据。</p><p><a href="https://imgchr.com/i/Gyqcex"><img src="https://s1.ax1x.com/2020/04/06/Gyqcex.md.png" alt="Gyqcex.md.png"></a> </p><hr><ol><li><p><strong>最大化training data size</strong></p><p>每连续10小时的数据都是train set的data。为了得到更多的data，应该把每一天连起来。即下图这种效果：</p><p><a href="https://imgchr.com/i/GyqyO1"><img src="https://s1.ax1x.com/2020/04/06/GyqyO1.md.png" alt="GyqyO1.md.png"></a> </p><p>每个月就有： $20*24-9=471$ 笔data</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dictionary: key:month value:month data</span></span><br><span class="line">month_data = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># make data timeline continuous</span></span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    temp = np.empty(shape=(<span class="number">18</span>, <span class="number">20</span>*<span class="number">24</span>))</span><br><span class="line">    <span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        temp[:, day*<span class="number">24</span>: (day+<span class="number">1</span>)*<span class="number">24</span>] = data[(month*<span class="number">20</span>+day)*<span class="number">18</span>: (month*<span class="number">20</span>+day+<span class="number">1</span>)*<span class="number">18</span>, :]</span><br><span class="line">    month_data[month] = temp</span><br></pre></td></tr></table></figure></li><li><p><strong>筛选需要的Features</strong> :</p><p>这里，我就只考虑前9小时的PM2.5，当然还可以考虑和PM2.5等相关的氮氧化物等feature。</p><p><strong>training data</strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># x_data v1: only consider PM2.5</span><br><span class="line">x_data &#x3D; np.empty(shape&#x3D;(12*471, 9))</span><br><span class="line">y_data &#x3D; np.empty(shape&#x3D;(12*471, 1))</span><br><span class="line">for month in range(12):</span><br><span class="line">    for i in range(471):</span><br><span class="line">        x_data[month*471+i][:] &#x3D; month_data[month][9][i: i+9]</span><br><span class="line">        y_data[month*471+i] &#x3D; month_data[month][9][i+9]</span><br></pre></td></tr></table></figure><p><strong>testing data</strong> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Testing data features</span></span><br><span class="line">test_x = np.empty(shape=(<span class="number">240</span>, <span class="number">9</span>))</span><br><span class="line"><span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">240</span>):</span><br><span class="line">    test_x[day, :] = test_data[<span class="number">18</span>*day+<span class="number">9</span>, :]</span><br><span class="line">test_x = np.concatenate((np.ones(shape=(<span class="number">240</span>, <span class="number">1</span>)), test_x), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Normalization</strong> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature scale: normalization</span></span><br><span class="line">mean = np.mean(x_data, axis=<span class="number">0</span>)</span><br><span class="line">std = np.std(x_data, axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x_data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(x_data.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[j] != <span class="number">0</span>:</span><br><span class="line">            x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(test_data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(test_data.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[j] != <span class="number">0</span>:</span><br><span class="line">            test_data[i][j] = (test_data[i][j] - mean[j])/std[j]</span><br></pre></td></tr></table></figure></li></ol><h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><p>手刻Adagrad 进行training。（挖坑：RMSprop、Adam[1]</p><p><strong>Linear Pseudo code</strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Declare weight vector, initial lr ,and # of iteration</span><br><span class="line">for i_th iteration :</span><br><span class="line"> y’ &#x3D; the product of train_x  and weight vector</span><br><span class="line"> Loss &#x3D; y’ - train_y</span><br><span class="line"> gradient &#x3D; 2*np.dot((train_x)’, Loss )</span><br><span class="line">   weight vector -&#x3D; learning rate * gradient</span><br></pre></td></tr></table></figure><p>其中的矩阵操作时，注意求gradient时矩阵的维度。可参考下图。</p><p><a href="https://imgchr.com/i/Gyqgw6"><img src="https://s1.ax1x.com/2020/04/06/Gyqgw6.md.png" alt="Gyqgw6.md.png"></a> </p><p><strong>Adagrad Pseudo code</strong></p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Declare weight vector, initial lr ,and <span class="comment"># of iteration</span></span><br><span class="line">Declare prev_gra storing gradients <span class="keyword">in</span> every previous iterations</span><br><span class="line"> <span class="keyword">for</span> i_th iteration :</span><br><span class="line">  y’ = the inner product of train_x  and weight vector</span><br><span class="line">  Loss = y’ - train_y</span><br><span class="line">  gradient = <span class="number">2</span>*np.dot((train_x)’, Loss )</span><br><span class="line">        prev_gra += gra**<span class="number">2</span></span><br><span class="line"> ada = np.sqrt(prev_gra)</span><br><span class="line">    weight vector -= learning rate * gradient / ada</span><br></pre></td></tr></table></figure><p>注：代码实现时，将bias存在w[0]处，x_data的第0列全1。因为w和b可以一同更新。（当然，也可以分开更新）</p><p><strong>Adagrad training</strong> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train-adagrad</span></span><br><span class="line"></span><br><span class="line">batch = x_data.shape[<span class="number">0</span>]  <span class="comment"># full batch</span></span><br><span class="line">epoch = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># some parameters for training</span></span><br><span class="line">dim = x_data.shape[<span class="number">1</span>]+<span class="number">1</span></span><br><span class="line">w = np.zeros(shape=(dim, <span class="number">1</span>))  <span class="comment"># concatenate bias = w[0]</span></span><br><span class="line">lr = np.full((dim, <span class="number">1</span>), <span class="number">0.8</span>)  <span class="comment"># learning rate</span></span><br><span class="line">grad = np.empty(shape=(dim, <span class="number">1</span>))  <span class="comment"># gradient of loss to every para</span></span><br><span class="line">gradsum = np.zeros(shape=(dim, <span class="number">1</span>))  <span class="comment"># sum of gradient**2</span></span><br><span class="line"></span><br><span class="line">x_data = np.concatenate((np.ones(shape=(x_data.shape[<span class="number">0</span>], <span class="number">1</span>)), x_data), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss_his = np.empty(shape=(epoch, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> T <span class="keyword">in</span> range(epoch):</span><br><span class="line">    L = y_data - np.dot(x_data, w)</span><br><span class="line">    loss_his[T] = np.sum(L**<span class="number">2</span>) / x_data.shape[<span class="number">0</span>]</span><br><span class="line">    grad = (<span class="number">-2</span>)*np.dot(np.transpose(x_data), L)</span><br><span class="line">    gradsum = gradsum + grad**<span class="number">2</span></span><br><span class="line">    w = w - lr*grad/(gradsum**<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><h1 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">answer = np.dot(test_x, w)</span><br></pre></td></tr></table></figure><h1 id="Draw-and-Analysis"><a href="#Draw-and-Analysis" class="headerlink" title="Draw and Analysis"></a>Draw and Analysis</h1><p>在每次迭代更新时，我将Loss的值存了下来，以便可视化Loss的变化和更新速度。</p><p>Loss的变化如下图：(红色的是sklearn toolkit的loss结果)</p><p><a href="https://imgchr.com/i/Gyqrl9"><img src="https://s1.ax1x.com/2020/04/06/Gyqrl9.png" alt="Gyqrl9.png"></a> </p><p>此外，在源代码中，使用sklearn toolkit来比较结果。</p><p>结果如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">v1: only consider PM2<span class="number">.5</span></span><br><span class="line"></span><br><span class="line">Using sklearn</span><br><span class="line">LinearRegression(copy_X=<span class="literal">True</span>, fit_intercept=<span class="literal">True</span>, n_jobs=<span class="literal">None</span>, normalize=<span class="literal">False</span>)</span><br><span class="line">bias= [<span class="number">21.37402689</span>]</span><br><span class="line">w= [[ <span class="number">0.00000000e+00</span>]</span><br><span class="line"> [<span class="number">-5.54801503e-01</span>]</span><br><span class="line"> [<span class="number">-4.32873874e-01</span>]</span><br><span class="line"> [ <span class="number">3.63669814e+00</span>]</span><br><span class="line"> [<span class="number">-3.99037687e+00</span>]</span><br><span class="line"> [<span class="number">-9.07364636e-01</span>]</span><br><span class="line"> [ <span class="number">8.83495803e+00</span>]</span><br><span class="line"> [<span class="number">-9.51785135e+00</span>]</span><br><span class="line"> [ <span class="number">1.32734655e-02</span>]</span><br><span class="line"> [ <span class="number">1.81886444e+01</span>]]</span><br><span class="line"></span><br><span class="line">In our model</span><br><span class="line">bias= [<span class="number">19.59387132</span>]</span><br><span class="line">w= [[<span class="number">-0.14448468</span>]</span><br><span class="line"> [ <span class="number">0.39205748</span>]</span><br><span class="line"> [ <span class="number">0.26897134</span>]</span><br><span class="line"> [<span class="number">-1.02415371</span>]</span><br><span class="line"> [ <span class="number">1.21151411</span>]</span><br><span class="line"> [ <span class="number">2.21925424</span>]</span><br><span class="line"> [<span class="number">-5.48242478</span>]</span><br><span class="line"> [ <span class="number">4.01080346</span>]</span><br><span class="line"> [<span class="number">13.56369122</span>]]</span><br></pre></td></tr></table></figure><p>发现参数有一定差异，于是我在testing时，也把sklearn的结果进行预测比较。</p><p>一部分结果如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'id'</span>, <span class="string">'value'</span>, <span class="string">'sk_value'</span>]</span><br><span class="line">[<span class="string">'id_0'</span>, <span class="number">3.551092352912313</span>, <span class="number">5.37766865368331</span>]</span><br><span class="line">[<span class="string">'id_1'</span>, <span class="number">13.916795471648756</span>, <span class="number">16.559245678900034</span>]</span><br><span class="line">[<span class="string">'id_2'</span>, <span class="number">24.811333478647043</span>, <span class="number">23.5085950470451</span>]</span><br><span class="line">[<span class="string">'id_3'</span>, <span class="number">5.101440436158914</span>, <span class="number">6.478306159981166</span>]</span><br><span class="line">[<span class="string">'id_4'</span>, <span class="number">26.7374726797937</span>, <span class="number">27.207516152986663</span>]</span><br><span class="line">[<span class="string">'id_5'</span>, <span class="number">19.43735346531517</span>, <span class="number">21.916809502961648</span>]</span><br><span class="line">[<span class="string">'id_6'</span>, <span class="number">22.20460696285646</span>, <span class="number">24.751295357256392</span>]</span><br><span class="line">[<span class="string">'id_7'</span>, <span class="number">29.660872382552682</span>, <span class="number">30.24344042612033</span>]</span><br><span class="line">[<span class="string">'id_8'</span>, <span class="number">17.5964527734513</span>, <span class="number">16.64242443764712</span>]</span><br><span class="line">[<span class="string">'id_9'</span>, <span class="number">56.58017426943178</span>, <span class="number">59.760988216575115</span>]</span><br><span class="line">[<span class="string">'id_10'</span>, <span class="number">13.767504260132299</span>, <span class="number">10.808372404511037</span>]</span><br><span class="line">[<span class="string">'id_11'</span>, <span class="number">11.743000466164233</span>, <span class="number">11.526958393801682</span>]</span><br><span class="line">[<span class="string">'id_12'</span>, <span class="number">59.509878887026105</span>, <span class="number">64.201008247897</span>]</span><br><span class="line">[<span class="string">'id_13'</span>, <span class="number">53.19824337746267</span>, <span class="number">54.3856368053018</span>]</span><br><span class="line">[<span class="string">'id_14'</span>, <span class="number">21.97191108867921</span>, <span class="number">24.530720709840974</span>]</span><br><span class="line">[<span class="string">'id_15'</span>, <span class="number">10.833283625735444</span>, <span class="number">14.350345549104446</span>]</span><br></pre></td></tr></table></figure><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><p>有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的<a href="https://github.com/f1ed/ML-HW1">GitHub</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########################</span></span><br><span class="line"><span class="comment"># Date: 2020-4-4</span></span><br><span class="line"><span class="comment"># Author: FredLau</span></span><br><span class="line"><span class="comment"># HW1: predict the PM2.5</span></span><br><span class="line"><span class="comment">##########################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#####################</span></span><br><span class="line"><span class="comment"># process data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># process train data</span></span><br><span class="line">raw_data = np.genfromtxt(<span class="string">'data/train.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">data = raw_data[<span class="number">1</span>:, <span class="number">3</span>:]</span><br><span class="line">data[np.isnan(data)] = <span class="number">0</span>  <span class="comment"># process nan</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dictionary: key:month value:month data</span></span><br><span class="line">month_data = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># make data timeline continuous</span></span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    temp = np.empty(shape=(<span class="number">18</span>, <span class="number">20</span>*<span class="number">24</span>))</span><br><span class="line">    <span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        temp[:, day*<span class="number">24</span>: (day+<span class="number">1</span>)*<span class="number">24</span>] = data[(month*<span class="number">20</span>+day)*<span class="number">18</span>: (month*<span class="number">20</span>+day+<span class="number">1</span>)*<span class="number">18</span>, :]</span><br><span class="line">    month_data[month] = temp</span><br><span class="line"></span><br><span class="line"><span class="comment"># x_data v1: only consider PM2.5</span></span><br><span class="line">x_data = np.empty(shape=(<span class="number">12</span>*<span class="number">471</span>, <span class="number">9</span>))</span><br><span class="line">y_data = np.empty(shape=(<span class="number">12</span>*<span class="number">471</span>, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> month <span class="keyword">in</span> range(<span class="number">12</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">471</span>):</span><br><span class="line">        x_data[month*<span class="number">471</span>+i][:] = month_data[month][<span class="number">9</span>][i: i+<span class="number">9</span>]</span><br><span class="line">        y_data[month*<span class="number">471</span>+i] = month_data[month][<span class="number">9</span>][i+<span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># process test data</span></span><br><span class="line">test_raw_data = np.genfromtxt(<span class="string">'data/test.csv'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">test_data = test_raw_data[:, <span class="number">2</span>:]</span><br><span class="line">test_data[np.isnan(test_data)] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># feature scale: normalization</span></span><br><span class="line">mean = np.mean(x_data, axis=<span class="number">0</span>)</span><br><span class="line">std = np.std(x_data, axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x_data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(x_data.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[j] != <span class="number">0</span>:</span><br><span class="line">            x_data[i][j] = (x_data[i][j] - mean[j]) / std[j]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(test_data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(test_data.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> std[j] != <span class="number">0</span>:</span><br><span class="line">            test_data[i][j] = (test_data[i][j] - mean[j])/std[j]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing data features</span></span><br><span class="line">test_x = np.empty(shape=(<span class="number">240</span>, <span class="number">9</span>))</span><br><span class="line"><span class="keyword">for</span> day <span class="keyword">in</span> range(<span class="number">240</span>):</span><br><span class="line">    test_x[day, :] = test_data[<span class="number">18</span>*day+<span class="number">9</span>, :]</span><br><span class="line">test_x = np.concatenate((np.ones(shape=(<span class="number">240</span>, <span class="number">1</span>)), test_x), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">################################</span></span><br><span class="line"><span class="comment"># train-adagrad</span></span><br><span class="line"></span><br><span class="line">batch = x_data.shape[<span class="number">0</span>]  <span class="comment"># full batch</span></span><br><span class="line">epoch = <span class="number">400</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># some parameters for training</span></span><br><span class="line">dim = x_data.shape[<span class="number">1</span>]+<span class="number">1</span></span><br><span class="line">w = np.zeros(shape=(dim, <span class="number">1</span>))  <span class="comment"># concatenate bias = w[0]</span></span><br><span class="line">lr = np.full((dim, <span class="number">1</span>), <span class="number">0.8</span>)  <span class="comment"># learning rate</span></span><br><span class="line">grad = np.empty(shape=(dim, <span class="number">1</span>))  <span class="comment"># gradient of loss to every para</span></span><br><span class="line">gradsum = np.zeros(shape=(dim, <span class="number">1</span>))  <span class="comment"># sum of gradient**2</span></span><br><span class="line"></span><br><span class="line">x_data = np.concatenate((np.ones(shape=(x_data.shape[<span class="number">0</span>], <span class="number">1</span>)), x_data), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss_his = np.empty(shape=(epoch, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> T <span class="keyword">in</span> range(epoch):</span><br><span class="line">    L = y_data - np.dot(x_data, w)</span><br><span class="line">    loss_his[T] = np.sum(L**<span class="number">2</span>) / x_data.shape[<span class="number">0</span>]</span><br><span class="line">    grad = (<span class="number">-2</span>)*np.dot(np.transpose(x_data), L)</span><br><span class="line">    gradsum = gradsum + grad**<span class="number">2</span></span><br><span class="line">    w = w - lr*grad/(gradsum**<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">f = open(<span class="string">'output/v1.csv'</span>, <span class="string">'w'</span>)</span><br><span class="line">sys.stdout = f</span><br><span class="line">print(<span class="string">'v1: only consider PM2.5\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line"><span class="comment"># train by sklearn linear model</span></span><br><span class="line">print(<span class="string">'Using sklearn'</span>)</span><br><span class="line">reg = linear_model.LinearRegression()</span><br><span class="line">print(reg.fit(x_data, y_data))</span><br><span class="line">print(<span class="string">'bias='</span>, reg.intercept_)</span><br><span class="line">print(<span class="string">'w='</span>, reg.coef_.transpose())</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In our model</span></span><br><span class="line">print(<span class="string">'In our model'</span>)</span><br><span class="line">print(<span class="string">'bias='</span>, w[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'w='</span>, w[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment">###########################</span></span><br><span class="line"><span class="comment"># draw change of loss</span></span><br><span class="line">plt.xlim(<span class="number">0</span>, epoch)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$iteration$'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.ylabel(<span class="string">'$Loss$'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">iteration = np.arange(<span class="number">0</span>, epoch)</span><br><span class="line">plt.plot(iteration, loss_his/<span class="number">100</span>, <span class="string">'-'</span>, ms=<span class="number">3</span>, lw=<span class="number">2</span>, color=<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sk_w = reg.coef_.transpose()</span><br><span class="line">sk_w[<span class="number">0</span>] = reg.intercept_</span><br><span class="line">sk_loss = np.sum((y_data - np.dot(x_data, sk_w))**<span class="number">2</span>) / x_data.shape[<span class="number">0</span>]</span><br><span class="line">plt.hlines(sk_loss/<span class="number">100</span>, <span class="number">0</span>, epoch, colors=<span class="string">'red'</span>, linestyles=<span class="string">'solid'</span>)</span><br><span class="line">plt.legend([<span class="string">'adagrad'</span>, <span class="string">'sklearn'</span>])</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># plt.savefig('output/v1.png')</span></span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">##############</span></span><br><span class="line"><span class="comment"># test (sklearn vs our adagrad</span></span><br><span class="line">f = open(<span class="string">'output/v1test.csv'</span>, <span class="string">'w'</span>)</span><br><span class="line">sys.stdout = f</span><br><span class="line"></span><br><span class="line">title = [<span class="string">'id'</span>, <span class="string">'value'</span>, <span class="string">'sk_value'</span>]</span><br><span class="line">answer = np.dot(test_x, w)</span><br><span class="line">sk_answer = np.dot(test_x, sk_w)</span><br><span class="line">print(title)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(test_x.shape[<span class="number">0</span>]):</span><br><span class="line">    content = [<span class="string">'id_'</span>+str(i), answer[i][<span class="number">0</span>], sk_answer[i][<span class="number">0</span>]]</span><br><span class="line">    print(content)</span><br><span class="line"></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>待完成</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在本篇文章中，用手刻Adagrad完成了「机器学习-李宏毅」的HW1-预测PM2.5的作业。其中包括对数据的处理，训练模型，预测，并使用sklearn toolkit的结果进行比较。&lt;br&gt;有关HW1的相关数据、源代码、预测结果等，欢迎光临小透明的&lt;a href=&quot;https://github.com/f1ed/ML-HW1&quot;&gt;GitHub&lt;/a&gt;  &lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="Regression" scheme="https://f7ed.com/tags/Regression/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:Classification-Logistic Regression</title>
    <link href="https://f7ed.com/2020/04/01/Classification2/"/>
    <id>https://f7ed.com/2020/04/01/Classification2/</id>
    <published>2020-03-31T16:00:00.000Z</published>
    <updated>2020-07-03T08:40:06.165Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中，讲解了怎么用Generative Model做分类问题。<br>这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。<br>文章主要有两部分：<br>第一部分讲解了Logistic Regression的三个步骤。<br>第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。</p><a id="more"></a><h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><h2 id="Step1-Function-Set"><a href="#Step1-Function-Set" class="headerlink" title="Step1: Function Set"></a>Step1: Function Set</h2><p>在文章末尾，我们得出 $P_{w, b}\left(C_{1} | x\right)=\sigma(w\cdot x+b)$ 的形式，想跳过找 $\mu_1,\mu_2,\Sigma$ 的过程，直接找 $w,b$ 。</p><p>因此Function Set: $f_{w, b}(x)=P_{w, b}\left(C_{1} | x\right)$ 。值大于0.5，则属于C1类，否则属于C2类。</p><p><a href="https://imgchr.com/i/G8TlKf"><img src="https://s1.ax1x.com/2020/04/01/G8TlKf.md.png" alt="G8TlKf.md.png"></a> </p><h2 id="Step2-Goodness-of-a-Function"><a href="#Step2-Goodness-of-a-Function" class="headerlink" title="Step2: Goodness of a Function"></a>Step2: Goodness of a Function</h2><p>使用极大似然的思想（在前一篇机率模型/生成模型中有讲）</p><p>估计函数是 ：$L(w, b)=f_{w, b}\left(x^{1}\right) f_{w, b}\left(x^{2}\right)\left(1-f_{w, b}\left(x^{3}\right)\right) \cdots f_{w, b}\left(x^{N}\right)$ </p><p>目标：  $ w^{*}, b^{*}=\arg \max _{w, b} L(w, b)$ </p><p>由于在之前的Regression中，我们都是找极小值点，为了方便处理，将估计函数转换为如下形式的<strong>损失函数：</strong> </p>$$\begin{equation}\begin{aligned}Loss &= -\ln L(w, b)=\ln f_{w, b}\left(x^{1}\right)+\ln f_{w, b}\left(x^{2}\right)+\ln \left(1-f_{w, b}\left(x^{3}\right)\right) \cdots \\ &=\sum_{n}-\left[\hat{y}^{n} \ln f_{w, b}\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f_{w, b}\left(x^{n}\right)\right)\right]\end{aligned}\end{equation}$$<p><strong>目标</strong> ： $w^{*}, b^{*}=\arg \min _{w, b} L(w, b)$ </p><blockquote><p><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy</a>（交叉熵）</p><p>上式中的 $\left[\hat{y}^{n} \ln f_{w, b}\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f_{w, b}\left(x^{n}\right)\right)\right]$ 其实是两个Bernoulli distribution的交叉熵。</p><p>交叉熵是什么？ 简单来说，交叉熵是评估两个distribution 有多接近。所以当两个分布的交叉熵为0时，表明这两个分布一模一样。</p><p>对于 $\hat{y}^{n} \ln f_{w, b}\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f_{w, b}\left(x^{n}\right)\right)$ ：</p><p> Distribution p: p(x = 1) = $\hat{y}^n$ ; p( x = 0 ) = 1 - $\hat{y}^n$ </p><p>Distribution q: q(x = 1 ) =  $f(x^n)$ ; q(x = 0 ) = 1 - $f(x^n)$ </p><p>交叉熵  $H(p,q)=-\Sigma_xp(x)\ln(q(x))$ </p></blockquote><p>因此，这个损失函数的表达式其实也是输出分布和target分布的交叉熵，即：</p><p>$L(f)=\sum_{n} C\left(f\left(x^{n}\right), \hat{y}^{n}\right)$ </p><p>（ $C\left(f\left(x^{n}\right), \hat{y}^{n}\right)=-\left[\hat{y}^{n} \ln f\left(x^{n}\right)+\left(1-\hat{y}^{n}\right) \ln \left(1-f\left(x^{n}\right)\right)\right]$  ）</p><hr><p>和Linear Regression不同，为什么Logistic Regression不用square error，而要使用cross entropy。</p><p>在1.4小节会给出解释。</p><h2 id="Step3-Find-the-best-function"><a href="#Step3-Find-the-best-function" class="headerlink" title="Step3: Find the best function"></a>Step3: Find the best function</h2><p>在第三步，同样使用Gradient来寻找最优函数。</p><p>推导过程：</p><ul><li><p>$\left.\frac{-\ln L(w, b)}{\partial w_{i}}=\sum_{n}-\left[\hat{y}^{n} \frac{\ln f_{w, b}\left(x^{n}\right)}{\partial w_{i}}+\left(1-\hat{y}^{n}\right) \frac{\ln \left(1-f_{w, b}\left(x^{n}\right)\right.}{\partial w_{i}}\right)\right]$ </p><ul><li><p>$\frac{\partial \ln f_{w, b}(x)}{\partial w_{i}}=\frac{\operatorname{\partial\ln} f_{w, b}(x)}{\partial z} \frac{\partial z}{\partial w_{i}}$ </p><ul><li>$\frac{\partial \ln \sigma(z)}{\partial z}=\frac{1}{\sigma(z)} \frac{\partial \sigma(z)}{\partial z}=\frac{1}{\sigma(z)} \sigma(z)(1-\sigma(z))$  </li><li>$\frac{\partial z}{\partial w_{i}}=x_{i}$ </li></ul></li><li><p>$\frac{\partial \ln \left(1-f_{w, b}(x)\right)}{\partial w_{i}}=\frac{\operatorname{\partial\ln}\left(1-f_{w, b}(x)\right)}{\partial z} \frac{\partial z}{\partial w_{i}}$ </p><ul><li>$\frac{\partial \ln (1-\sigma(z))}{\partial z}=-\frac{1}{1-\sigma(z)} \frac{\partial \sigma(z)}{\partial z}=-\frac{1}{1-\partial(z)} \sigma(z)(1-\sigma(z))$ </li><li>$\frac{\partial z}{\partial w_{i}}=x_{i}$ </li></ul></li><li><p>$\frac{\partial \sigma(z)}{\partial z}=\sigma(z)\cdot(1-\sigma(z))$ </p><p><a href="https://imgchr.com/i/G8TMxP"><img src="https://s1.ax1x.com/2020/04/01/G8TMxP.png" alt="G8TMxP.png"></a> </p></li></ul></li><li><p>注：$f_{w, b}(x)=\sigma(z)$   ; $z=w \cdot x+b=\sum_{i} w_{i} x_{i}+b$  </p></li><li> $$  \begin{equation}  \begin{aligned}  \frac{-\ln L(w, b)}{\partial w_{i}}&=\sum_{n}-\left[\hat{y}^{n}\left(1-f_{w, b}\left(x^{n}\right)\right) x_{i}^{n}-\left(1-\hat{y}^{n}\right) f_{w, b}\left(x^{n}\right) x_{i}^{n}\right]  \\&=\sum_{n}-\left(\hat{y}^{n}-f_{w, b}\left(x^{n}\right)\right) x_{i}^{n}  \end{aligned}  \end{equation}  $$ </li></ul><p>因此Logistic Regression的损失函数的导数和Linear Regression的一样。</p><p>迭代更新： $w_{i} \leftarrow w_{i}-\eta \sum_{n}-\left(\hat{y}^{n}-f_{w, b}\left(x^{n}\right)\right) x_{i}^{n}$ </p><h2 id="与Linear-Regression-的对比"><a href="#与Linear-Regression-的对比" class="headerlink" title="与Linear Regression 的对比"></a>与Linear Regression 的对比</h2><p><a href="https://imgchr.com/i/G8Tu8I"><img src="https://s1.ax1x.com/2020/04/01/G8Tu8I.md.png" alt="G8Tu8I.md.png"></a> </p><p>如图所示。</p><h2 id="If-Logistic-Square-Error"><a href="#If-Logistic-Square-Error" class="headerlink" title="If : Logistic + Square Error"></a>If : Logistic + Square Error</h2><p>前面一小节我们提到，在Logistic Regression中使用cross entropy判别一个函数的好坏,那为什么不使用square error来judge the goodness？</p><p>如果使用 Square Error的方法，步骤如下：</p><p><a href="https://imgchr.com/i/G8TnPA"><img src="https://s1.ax1x.com/2020/04/01/G8TnPA.md.png" alt="G8TnPA.md.png"></a> </p><p>来看Step 3: 损失函数的导数是 $2\left(f_{w, b}(x)-\hat{y}\right) f_{w, b}(x)\left(1-f_{w, b}(x)\right) x_{i}$ </p><p>考虑 $\hat{y}^n=1$ （即我们的target是1）：</p><ul><li>如果  $f_{w,b}(x^n)=1$ , 即预测值接近 target, 算出来的 $\partial{L}/\partial{w_i}=0$ 是期望的。</li><li>如果  $f_{w,b}(x^n)=0$ , 即预测值原理 target, 算出来的 $\partial{L}/\partial{w_i}=0$ 是不期望的。</li></ul><p>同理，当考虑 $\hat{y}^n=0$ 情况时，也是如此。</p><p>更直观的看：</p><p><a href="https://imgchr.com/i/G8Te5d"><img src="https://s1.ax1x.com/2020/04/01/G8Te5d.md.png" alt="G8Te5d.md.png"></a> </p><p>上图中，画出了两种损失函数的平面，中心的最低点是我们的target。</p><p>但在Square Error中，远离target的蓝色点，也处在很平坦的位置，其导数小，参数的更新会很慢。</p><p>因此在Cross Entropy中，离target越远，其导数更大，更新更快。</p><p>所以Cross Entropy的效果比Square Error更快，效果更好。</p><h1 id="Discriminative-V-S-Generative"><a href="#Discriminative-V-S-Generative" class="headerlink" title="Discriminative V.S. Generative"></a>Discriminative V.S. Generative</h1><p>这篇文章中的Logistic Regression是Discriminative Model。</p><a href="/2020/03/21/Classification1/" title="上篇文章">上篇文章</a>中Classification是Generative Model。<p>有什么区别呢？</p><p><a href="https://imgchr.com/i/G8TVVe"><img src="https://s1.ax1x.com/2020/04/01/G8TVVe.md.png" alt="G8TVVe.md.png"></a> </p><p>上图中，Generative Model做了假设（脑补），假设它是 Gaussian Distribution，假设它是Bernoulli Distribution。然后去找这些分布的参数，在求出 $w,b$。</p><p>而在Discriminative Model中，没有做任何假设，直接找 $w,b$ 参数。</p><p>所以，这两种Model经过training找出来的参数一样吗？</p><p>答案是不一样的。</p><p>The same model(function set), but different function is selected by the same training data.</p><p>在上篇Pokemon的例子中，比较两种方法的结果差异。</p><p><a href="https://imgchr.com/i/G8TAbD"><img src="https://s1.ax1x.com/2020/04/01/G8TAbD.md.png" alt="G8TAbD.md.png"></a> </p><p>可见，在Pokemon的例子总，Discriminative的效果比Generative的效果好一些。</p><hr><p>但是Generative Model就不好吗？</p><p><strong>Benefit of generative model</strong></p><ul><li><p>With the assumption of probability distribution, less training data is needed.</p><p>【训练生成模型所需数据更少】 </p></li><li><p>With the assumption of probability distribution, more robust to the noise.</p><p>【生成模型对noise data更兼容】</p></li><li><p>Priors and class-dependent probabilities can be estimated from different sources.</p><p>【生成模型中的 先验概率Priors 和 基于类别的分布概率不同】</p><p>比如，做语音辨识系统，整个系统是generative的。</p><p>因为Prior（某一句话的概率）并不需要从data中知道，可以直接在网络上爬虫统计。</p><p>而class-dependent probabilities（这段语音是这句话的概率）需要data进行训练才能得知。</p></li></ul><h1 id="Multi-class-classification"><a href="#Multi-class-classification" class="headerlink" title="Multi-class classification"></a>Multi-class classification</h1><h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2><p><a href="https://imgchr.com/i/G8TkDO"><img src="https://s1.ax1x.com/2020/04/01/G8TkDO.md.png" alt="G8TkDO.md.png"></a> </p><p>假设有三个类别：C1、C2、C3 。模型已经得到，参数分别是 w、b。</p><p>对于输入x, 判断x属于哪一个类别。</p><ol start="0"><li>通过每个类别的 w、b求出 $z^i=w^i\cdot x+b_i$  </li></ol><p>Softmax的步骤：</p><ol><li>exponential：每个z值得到 $=e^z$ .</li><li>sum：将指数化后的值加起来$=\Sigma_{j=1}^3e^{z_j}$ </li><li>output: 每个类别的输出 $y_i=e^{z_1}/\Sigma_{j=1}^3e^{z_j}$  ，即x属于类别i的概率。</li></ol><p>求出的 $1&gt;y_i&gt;0$ 且 $\Sigma_iy_i=1$ 。</p><p>通过Softmax，得到 $y_i=P(C_i|x)$ 。</p><h2 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h2><p>（手写笔记，略倾斜，原来不切一切还不知道自己歪的这么厉害 泪）</p><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1:"></a>Step 1:</h3><p><a href="https://imgchr.com/i/G8TFKK"><img src="https://s1.ax1x.com/2020/04/01/G8TFKK.md.png" alt="G8TFKK.md.png"></a> </p><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2:"></a>Step 2:</h3><p><a href="https://imgchr.com/i/G8TPv6"><img src="https://s1.ax1x.com/2020/04/01/G8TPv6.md.png" alt="G8TPv6.md.png"></a> </p><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3:"></a>Step 3:</h3><p><a href="https://imgchr.com/i/G8TCgx"><img src="https://s1.ax1x.com/2020/04/01/G8TCgx.md.png" alt="G8TCgx.md.png"></a> </p><p>使用Stochastic Gradient（即每个样本更新一次）的话：</p><p>data: [x, $\hat{y}$ ] , $\hat{y}_i=1$ </p><p>更新 $w^j$ :</p><ol><li><p>$j=i$ :</p><p>$w^j \leftarrow w^j-\eta\cdot (y_i-1)\cdot x$ </p></li><li><p>$j\neq i$ :</p><p>$w^j \leftarrow w^j-\eta\cdot y_i\cdot x$  </p></li></ol><p>(下次一定，笔记写直一点！)</p><p>更为规范的推导见[1]</p><h1 id="Limitation-of-Logistic-Regression"><a href="#Limitation-of-Logistic-Regression" class="headerlink" title="Limitation of Logistic Regression"></a>Limitation of Logistic Regression</h1><p><a href="https://imgchr.com/i/G8oLuT"><img src="https://s1.ax1x.com/2020/04/01/G8oLuT.md.png" alt="G8oLuT.md.png"></a> </p><p>对于如上情况，Logistic Regression并不能进行分类，因为他的boundary 应该是线性的。</p><h2 id="Feature-Transforming"><a href="#Feature-Transforming" class="headerlink" title="Feature Transforming"></a>Feature Transforming</h2><p>如果对feature做转换后，就可以用Logistic Regression处理。</p><p>重定义feature， $x_1’$ :定义为到[0,0]的距离， $x_2’$ :定义为到[1,1]的距离。</p><p>于是图变成下图，即可用Logistic Regression进行分类。</p><p><a href="https://imgchr.com/i/G8oxUJ"><img src="https://s1.ax1x.com/2020/04/01/G8oxUJ.md.png" alt="G8oxUJ.md.png"></a> </p><p>但这样的做法，就不像人工智能了，因为Feature Transformation需要人来设计，而且较难设计。</p><h2 id="Cascading-logistic-regression-models"><a href="#Cascading-logistic-regression-models" class="headerlink" title="Cascading logistic regression models"></a>Cascading logistic regression models</h2><p>另一种做法是，将logistic regression连接起来。</p><p><a href="https://imgchr.com/i/G8TpCR"><img src="https://s1.ax1x.com/2020/04/01/G8TpCR.md.png" alt="G8TpCR.md.png"></a></p><p>上图中，左边部分的两个logistic regression就相当于在做Feature Transformation，右边部分相当于在做Classification。</p><p>而通过这种形式，将多个model连接起来，也就是大热的Neural Network。</p><p><a href="https://imgchr.com/i/G8T981"><img src="https://s1.ax1x.com/2020/04/01/G8T981.md.png" alt="G8T981.md.png"></a> </p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Multi-class Classification推导：Bishop，P209-210</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上篇文章中，讲解了怎么用Generative Model做分类问题。&lt;br&gt;这篇文章中，讲解了做Classification的另一种Discriminative的方式，也就是Logistic Regression。&lt;br&gt;文章主要有两部分：&lt;br&gt;第一部分讲解了Logistic Regression的三个步骤。&lt;br&gt;第二个部分讲解了multi-class多分类的三个步骤，以及softmax是如何操作的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="Classification" scheme="https://f7ed.com/tags/Classification/"/>
    
      <category term="Logistic Regression" scheme="https://f7ed.com/tags/Logistic-Regression/"/>
    
      <category term="Softmax" scheme="https://f7ed.com/tags/Softmax/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:Classification-Generative Model</title>
    <link href="https://f7ed.com/2020/03/21/Classification1/"/>
    <id>https://f7ed.com/2020/03/21/Classification1/</id>
    <published>2020-03-20T16:00:00.000Z</published>
    <updated>2020-07-03T08:39:45.986Z</updated>
    
    <content type="html"><![CDATA[<p>Classification 有Generative Model和Discriminative Model。<br>这篇文章主要讲述了用生成模型来做分类的原理及过程。</p><a id="more"></a><h1 id="What-is-Classification"><a href="#What-is-Classification" class="headerlink" title="What is Classification?"></a>What is Classification?</h1><p>分类是什么呢？分类可以应用到哪些场景呢？</p><ul><li><p>Credit Scoring【贷款评估】</p><ul><li><p>Input: income, savings, profession, age, past financial history ……</p></li><li><p>Output: accept or refuse</p></li></ul></li><li><p>Medical Diagnosis【医疗诊断】</p><ul><li><p>Input: current symptoms, age, gender, past medical history ……</p></li><li><p>Output: which kind of diseases</p></li></ul></li><li><p>Handwritten character recognition【手写数字辨别】</p><ul><li>Input：<a href="https://imgchr.com/i/8hsTEV"><img src="https://s1.ax1x.com/2020/03/21/8hsTEV.png" alt="8hsTEV.png"></a> </li><li>Output：金</li></ul></li><li><p>Face recognition 【人脸识别】</p><ul><li>Input: image of a face</li><li>output: person</li></ul></li></ul><h2 id="Classification：Example-Application"><a href="#Classification：Example-Application" class="headerlink" title="Classification：Example Application"></a>Classification：Example Application</h2><p>【图】</p><p>如上图，<del>Pokemon又来啦！</del> Pokemon有很多属性，比如皮卡丘是电属性，杰尼龟是水属性之类。</p><p>关于Pokemon的Classification：Predict the “type” of Pokemon based on the information</p><p>Input：Information of Pokemon (数值化）<a href="https://imgchr.com/i/8hsfjs"><img src="https://s1.ax1x.com/2020/03/21/8hsfjs.md.png" alt="8hsfjs.md.png"></a> </p><p>Output：the type</p><p>Training Data: ID在前400的Pokemon<a href="https://imgchr.com/i/8hsWcj"><img src="https://s1.ax1x.com/2020/03/21/8hsWcj.md.png" alt="8hsWcj.md.png"></a> </p><p>Testing Data: ID在400后的Pokemon</p><h3 id="Classification-as-Regression"><a href="#Classification-as-Regression" class="headerlink" title="Classification as Regression?"></a>Classification as Regression?</h3><p><strong>1. 简化问题，只考虑二分类：Class 1 ， Class2。</strong></p><p>如果把分类问题当作回归问题，把类别数值化。</p><p>在Training中： Class 1 means the target is 1; Class 2 means the target is -1.</p><p>在Testing中：如果Regression的函数值接近1，说明是class 1；如果函数值接近-1，说明是class 2.</p><hr><p>Regression：输入信息只考虑两个特征。</p><p>Model：$y=w_1x_1+w_2x_2+b$ </p><p><a href="https://imgchr.com/i/8hs29g"><img src="https://s1.ax1x.com/2020/03/21/8hs29g.md.png" alt="8hs29g.md.png"></a> </p><p>当Training data的分布如上图所示时，得到的（最优函数）分界线感觉很合理。</p><p><a href="https://imgchr.com/i/8hsR3Q"><img src="https://s1.ax1x.com/2020/03/21/8hsR3Q.md.png" alt="8hsR3Q.md.png"></a></p><p>但当Training data在右下角也有分布时（如右图），训练中为了减少error，训练得到的分界线会变成紫色的那一条。</p><p>所以，如果用Regression来做Classification：<strong>Penalize to the examples that are “too correct”</strong> .[1]</p><p>训练中会因为惩罚一些“过于正确”（即和我们假定的target离太远）的example，得到的最优函数反而have bad performance.</p><p><strong>2. 此外，如果用Regression来考虑多分类。</strong></p><p>Multiple class: Class 1 means the target is 1; Class 2 means the target is 2; Class 3 means the target is 3……</p><p>如果用上面这种假设，可以认为Class 3和Class 2 的关系更近，和Class 1的关系更远一些。但实际中，可能这些类别<strong>have no relation</strong>。</p><h3 id="Classification-Ideal-Alternatives"><a href="#Classification-Ideal-Alternatives" class="headerlink" title="Classification: Ideal Alternatives"></a>Classification: Ideal Alternatives</h3><p>在上面，我们假设二元分类每一个类别都有一个target，结果不尽人意。</p><p><a href="https://imgchr.com/i/8hs6N8"><img src="https://s1.ax1x.com/2020/03/21/8hs6N8.md.png" alt="8hs6N8.md.png"></a> </p><p>如上图所示，将模型改为以上形式，也可以解决分类问题。（挖坑）[2]</p><h1 id="Generative-Model-生层模型"><a href="#Generative-Model-生层模型" class="headerlink" title="Generative Model(生层模型)"></a>Generative Model(生层模型)</h1><h2 id="Estimate-the-Probabilities"><a href="#Estimate-the-Probabilities" class="headerlink" title="Estimate the Probabilities"></a>Estimate the Probabilities</h2><p>用<strong>概率的知识来考虑分类</strong>这个问题，如下图所示，有两个两个类别，C1和C2。</p><p><a href="https://imgchr.com/i/8hsyAf"><img src="https://s1.ax1x.com/2020/03/21/8hsyAf.md.png" alt="8hsyAf.md.png"></a> </p><p>在Testing中，如果任给一个x，属于C1的概率是（贝叶斯公式）</p>$$P\left(C_{1} | x\right)=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}$$<p>所以在Training中知道这些： $P(C_1),P(x|C_1),P(C_2),P(x|C_2)$ </p><p>P(C1)和P(C2)很容易得知。</p><p>而P(x|C1)和P(x|C2)的概率应该如何得知呢？</p><p>如果能假设：类别是C1中的变量x服从某种分布，如高斯分布等，即可以得到任意P(x|C1)的值。</p><p>所以Generative Model：是对examples假设一个分布模型，在training中调节分布模型的参数，使得examples出现的概率最大。（极大似然的思想）</p><h2 id="Prior-Probabilities（先验概率）"><a href="#Prior-Probabilities（先验概率）" class="headerlink" title="Prior Probabilities（先验概率）"></a>Prior Probabilities（先验概率）</h2><p>先只考虑Water和Normal两个类别。</p><p>先验概率：即通过过去资料分析得到的概率。</p><p>在Pokemon的例子中，Training Data是ID&lt;400的水属性和一般属性的Pokemon信息。</p><p>Training Data：79 Water，61 Normal。</p><p>得到的先验概率 P(C1)=79/(79+61)=0.56, P(C2)=61/(79+61)=0.44。</p><h2 id="Probability-from-Class"><a href="#Probability-from-Class" class="headerlink" title="Probability from Class"></a>Probability from Class</h2><p>先只考虑Defense和SP Defense这两个feature。</p><p>如果不考虑生成分布模型，在testing中直接计算P(x|Water)的概率，如下图右下角的那只龟龟，在training data中没有出现过，那值为0吗？显然不对。</p><p><a href="https://imgchr.com/i/8hsDBt"><img src="https://s1.ax1x.com/2020/03/21/8hsDBt.md.png" alt="8hsDBt.md.png"></a> </p><p>假设：上图中<strong>water type的examples是从Gaussian distribution（高斯分布）中取样</strong>出来的。</p><p>因此在training中通过training data得到最优的Gaussian distribution的参数，计算样本中没有出现过的P(x|Water)也就迎刃而解了。</p><h3 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h3><p>多维的高斯分布（高斯分布就是正态分布啦）的联合概率密度：</p>$$f_{\mu, \Sigma}(x)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right\}$$<p>D: 维数</p><p>$\mu$ : mean</p><p>$\Sigma$ :covariance matrix(协方差矩阵)</p><blockquote><p>协方差： $ cov(X,Y)=E[[X-E(X)][Y-E(Y)]]=E(XY)-E(X)E(Y)$ </p><p>具体协方差性质，查阅概率论课本吧。</p></blockquote><p>x: vector,n维随机变量</p><p><strong>高斯分布的性质只和 $\mu$ 和 $\Sigma$ 有关。</strong></p><p> $\Sigma$ 一定时，$\mu$ 不同，如下图：</p><p><a href="https://imgchr.com/i/8hsBnI"><img src="https://s1.ax1x.com/2020/03/21/8hsBnI.md.png" alt="8hsBnI.md.png"></a> </p><p>$\mu$ 一定， $\Sigma$ 不同时，如下图：</p><p><a href="https://imgchr.com/i/8hswjA"><img src="https://s1.ax1x.com/2020/03/21/8hswjA.md.png" alt="8hswjA.md.png"></a> </p><h3 id="Maximum-Likelihood（极大似然）"><a href="#Maximum-Likelihood（极大似然）" class="headerlink" title="Maximum Likelihood（极大似然）"></a>Maximum Likelihood（极大似然）</h3><p>样本分布如下图所示，假设这些样本是从Gaussian distribution中取样，那如何在训练中得到高斯分布的 $\mu$ 和 $\Sigma$ 呢？</p><p>极大似然估计。</p><p>考虑Water，有79个样本，估计函数  $L(\mu, \Sigma)=f_{\mu, \Sigma}\left(x^{1}\right) f_{\mu, \Sigma}\left(x^{2}\right) f_{\mu, \Sigma}\left(x^{3}\right) \ldots \ldots f_{\mu, \Sigma}\left(x^{79}\right)$ </p><p>极大似然估计，即找到  $f_{\mu, \Sigma}(x)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right\}$  中的 $\mu$ 和$\Sigma$ 使得估计函数最大（使得取出这些样本的概率最大化）。</p><hr>$\mu^{*}, \Sigma^{*}=\arg \max _{\mu, \Sigma} L(\mu, \Sigma)$ <ol><li><p>求导计算（过于复杂，但也不是不能做是吧）</p></li><li><p>背公式[3]</p> $\mu^{*}=\frac{1}{79} \sum_{n=1}^{79} x^{n} \qquad \Sigma^{*}=\frac{1}{79} \sum_{n=1}^{79}\left(x^{n}-\mu^{*}\right)\left(x^{n}-\mu^{*}\right)^{T}$ </li></ol><p>得到Water和Normal的高斯分布，如下图:</p><p><a href="https://imgchr.com/i/8hsa1H"><img src="https://s1.ax1x.com/2020/03/21/8hsa1H.md.png" alt="8hsa1H.md.png"></a>  </p><h2 id="Do-Classification-different-Sigma"><a href="#Do-Classification-different-Sigma" class="headerlink" title="Do Classification: different $\Sigma$"></a>Do Classification: different $\Sigma$</h2><h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p>Testing：  $P\left(C_{1} | x\right)=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}$  </p><p>P(x|C1)由训练得出的Water的高斯分布计算出，P(x|C2)由Normal的高斯分布计算出。（如下图，过于难打）</p><p><a href="https://imgchr.com/i/8hsJAK"><img src="https://s1.ax1x.com/2020/03/21/8hsJAK.md.png" alt="8hsJAK.md.png"></a> </p><p>如果P(C1|x)&gt;0.5，说明x 属于Water(Class 1)。</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>如果只考虑两个feature（Defense和SP Defense），下图是testing data的样本图，蓝色属于Water，红色属于Normal。</p><p><a href="https://imgchr.com/i/8hsthD"><img src="https://s1.ax1x.com/2020/03/21/8hsthD.md.png" alt="8hsthD.md.png"></a> </p><p>用训练得出的模型，Testing Data: 47% accuracy。（结果如下图）</p><p><a href="https://imgchr.com/i/8hcrH1"><img src="https://s1.ax1x.com/2020/03/21/8hcrH1.md.png" alt="8hcrH1.md.png"></a> </p><p>如果考虑全部features(7个)，重新训练出的模型，结果：Testing Data：54% accuracy。（结果如下图）</p><p><a href="https://imgchr.com/i/8hsYtO"><img src="https://s1.ax1x.com/2020/03/21/8hsYtO.md.png" alt="8hsYtO.md.png"></a> </p><p>结果并不好。参数过多，模型过于复杂，有些过拟合了。</p><h2 id="Modifying-Model：same-Sigma"><a href="#Modifying-Model：same-Sigma" class="headerlink" title="Modifying Model：same $\Sigma$"></a>Modifying Model：same $\Sigma$</h2><p>模型中的参数有两个的Gaussian Distribution中的 $\mu^<em>$ 和 $\Sigma^</em>$ ，其中协方差矩阵的大小等于feature的平方，所以让<strong>不同的class share 同一个 $\Sigma$ ，以此来减少参数，简化模型</strong>。</p><p><a href="https://imgchr.com/i/8hsMc9"><img src="https://s1.ax1x.com/2020/03/21/8hsMc9.md.png" alt="8hsMc9.md.png"></a> </p><p>极大似然估计的估计函数：</p>$$L\left(\mu^{1}, \mu^{2}, \Sigma\right)=f_{\mu^{1}, \Sigma}\left(x^{1}\right) f_{\mu^{1}, \Sigma}\left(x^{2}\right) \cdots f_{\mu^{1}, \Sigma}\left(x^{79}\right)\times f_{\mu^{2}, \Sigma}\left(x^{80}\right) f_{\mu^{2}, \Sigma}\left(x^{81}\right) \cdots f_{\mu^{2}, \Sigma}\left(x^{140}\right)$$<p>公式推导:[3]</p><p>$\mu$ 的公式不变。</p>$\Sigma=\frac{79}{140} \Sigma^{1}+\frac{61}{140} \Sigma^{2}$  ,即是原 $\Sigma^1\ \Sigma^2$的加权平均。<h3 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h3><p>当只考虑两个features，用同样的协方差参数，结果如下图：</p><p><a href="https://imgchr.com/i/8hsQXR"><img src="https://s1.ax1x.com/2020/03/21/8hsQXR.md.png" alt="8hsQXR.md.png"></a> </p><p>可以发现，用了同样的协方差矩阵参数后，边界变成了线性的，所以这也是一个线性模型。</p><p>再考虑7个features，用同样的协方差矩阵参数，模型也是线性模型，但由于在高维空间，人无法直接画出其boundary，这也是机器学习的魅力所在，能解决一些人无法解决的问题。</p><p>结果：从之前的54% accuracy增加到 73% accurancy.</p><p>结果明显变好了。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Three Steps：</p><ol><li><p>Function Set（Model）：<a href="https://imgchr.com/i/8hs30x"><img src="https://s1.ax1x.com/2020/03/21/8hs30x.md.png" alt="8hs30x.md.png"></a> </p></li><li><p>Goodness of a function:</p><p>The mean µ and convariance $\Sigma$ that maximizing the likelihood(the probability of generating data)</p></li><li><p>Find the best function:easy(公式)</p></li></ol><h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="为什么要选择Gaussian-Distribution"><a href="#为什么要选择Gaussian-Distribution" class="headerlink" title="为什么要选择Gaussian Distribution"></a>为什么要选择Gaussian Distribution</h2><p><strong>You can always use the distribution you like.</strong> </p><p>可以选择你喜欢的任意分布，t分布，开方分布等。</p><p>（老师说：如果我选择其他分布，你也会问这个问题，哈哈哈）</p><h2 id="Naive-Bayes-Classifier"><a href="#Naive-Bayes-Classifier" class="headerlink" title="Naive Bayes Classifier"></a>Naive Bayes Classifier</h2><p>If you assume all the dimensions are independent, then you are using <em>Naive Bayes Classifier</em>.</p><p>如果假设features之间互相独立， $P\left(x | C_{1}\right)=P\left(x_{1} | C_{1}\right) P\left(x_{2} | C_{1}\right) \quad \ldots \ldots \quad P\left(x_{k} | C_{1}\right) $   。</p><p>xi是x第i维度的feature。</p><p>对于每一个 P(xi|C1)，可以假设其服从一维高斯分布。如果是binary features（即feature取值只有两个），也可以假设它服从Bernoulli distribution(贝努利分布)。</p><h2 id="Posterior-Probability（后验概率）"><a href="#Posterior-Probability（后验概率）" class="headerlink" title="Posterior Probability（后验概率）"></a>Posterior Probability（后验概率）</h2><p>Posterior Probability后验概率，即使用贝叶斯公式，已知结果，寻找最优可能导致它发生的原因。</p><p>对  $P\left(C_{1} | x\right)=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}$ 进行处理。</p><p>得到：</p>$$\begin{equation}\begin{aligned}P\left(C_{1} | x\right)&=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}\\&=\frac{1}{1+\frac{P\left(x | C_{2}\right) P\left(C_{2}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)}}\\&=\frac{1}{1+\exp (-z)} =\sigma(z)\qquad(z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}\end{aligned}\end{equation}$$<p><strong><font color=#f00>Worning of Math</font></strong> </p><ul><li> $z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}=\ln \frac{P\left(x | C_{1}\right)}{P\left(x | C_{2}\right)}+\ln \frac{P\left(C_{1}\right)}{P\left(C_{2}\right)}$  <ul><li> $\ln \frac{P\left(C_{1}\right)}{P\left(C_{2}\right)}=\frac{\frac{N_{1}}{N_{1}+N_{2}}}{\frac{N_{2}}{N_{1}+N_{2}}}=\frac{N_{1}}{N_{2}}$  </li><li> $P\left(x | C_{1}\right)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\Sigma 1|^{1 / 2}} \exp \left\{-\frac{1}{2}\left(x-\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1}\left(x-\mu^{1}\right)\right\}$ </li><li> $P\left(x | C_{2}\right)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{\left|\Sigma^{2}\right| 1 / 2} \exp \left\{-\frac{1}{2}\left(x-\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1}\left(x-\mu^{2}\right)\right\}$ </li></ul></li><li> $\ln \frac{\left|\Sigma^{2}\right|^{1 / 2}}{\left|\Sigma^{1}\right|^{1 / 2}}-\frac{1}{2}\left[\left(x-\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1}\left(x-\mu^{1}\right)-\left(x-\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1}\left(x-\mu^{2}\right)\right]$  <ul><li> $\left(x-\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1}\left(x-\mu^{1}\right)=x^{T}\left(\Sigma^{1}\right)^{-1} x-2\left(\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1} x+\left(\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1} \mu^{1}$ </li><li> $\left(x-\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1}\left(x-\mu^{2}\right)=x^{T}\left(\Sigma^{2}\right)^{-1} x-2\left(\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1} x+\left(\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1} \mu^{2}$ </li></ul></li><li> $\begin{aligned} z=& \ln \frac{\left|\Sigma^{2}\right|^{1 / 2}}{\left|\Sigma^{1}\right|^{1 / 2}}-\frac{1}{2} x^{T}\left(\Sigma^{1}\right)^{-1} x+\left(\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1} x-\frac{1}{2}\left(\mu^{1}\right)^{T}\left(\Sigma^{1}\right)^{-1} \mu^{1} \\ &+\frac{1}{2} x^{T}\left(\Sigma^{2}\right)^{-1} x-\left(\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1} x+\frac{1}{2}\left(\mu^{2}\right)^{T}\left(\Sigma^{2}\right)^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}} \end{aligned}$  </li></ul><hr><p>简化模型后， $\Sigma^1=\Sigma^2=\Sigma$ :</p>$$z=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} x-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}$$<p>令  $w^T=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} \qquad b=-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}$ </p><p>当简化模型后，z是线性的，这也是为什么在之前的结果中边界是线性的原因。</p><p>最后模型变成这样： $P\left(C_{1} | x\right)=\sigma(w \cdot x+b)$ .</p><p>在生成模型中，我们先估计出  $\mu_1\ \mu_2\ N_1\ N_2\ \Sigma$  的值，也就得到了 $w\ b$ 的值。</p><p>那，我们能不能跳过  $\mu_1\ \mu_2\ N_1\ N_2\ \Sigma$  ，直接估计 $w\ b$ 呢？</p><p>在下一篇博客[4]中会继续Classification。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>Classification as Regression: Bishop, P186.</p></li><li><p>挖坑：Classification：Perceptron，SVM.</p></li><li><p>Maximum likelihood solution：Bishop chapter4.2.2</p></li><li></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Classification 有Generative Model和Discriminative Model。&lt;br&gt;这篇文章主要讲述了用生成模型来做分类的原理及过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="Classification" scheme="https://f7ed.com/tags/Classification/"/>
    
  </entry>
  
</feed>
