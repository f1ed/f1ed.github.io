<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>fred&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://f7ed.com/"/>
  <updated>2021-01-16T07:13:27.910Z</updated>
  <id>https://f7ed.com/</id>
  
  <author>
    <name>f1ed</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>「Math」:Entropy, Cross-Entropy and DL-Divergence</title>
    <link href="https://f7ed.com/2021/01/16/entropy-and-more/"/>
    <id>https://f7ed.com/2021/01/16/entropy-and-more/</id>
    <published>2021-01-15T16:00:00.000Z</published>
    <updated>2021-01-16T07:13:27.910Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习中，常用cross-entropy来作为模型的损失函数，这篇文章将阐述信息学中的entropy（熵）是什么，cross-entropy（交叉熵）又是什么，KL-Divergence和entropy、cross-entropy的关系是什么？</p><p>如何具象的理解这些概念？</p><p>在开始阅读这篇文章之前，先提及一下香农对bit的定义，香农认为bit是用来消除信息的不确定性的。</p><p>bit：uncertainty divided by 2.</p><p><a href="https://www.youtube.com/watch?v=ErfnhcEV1O8">原视频</a> 讲的很好，本文只是在此基础上对一些总结，方便理解物质化（马原.jpg）。</p><a id="more"></a><h1 id="公式总概"><a href="#公式总概" class="headerlink" title="公式总概"></a>公式总概</h1><p>bit：用来消除信息的不确定性</p><p>Entropy（熵）： $H(p)=-\sum_i p_i\log(p_i)$ </p><p>度量概率分布的平均信息量（即不确定性）。值越大，不确定性越大。</p><p>Cross-Entropy（交叉熵）： $H(p,q)=-\sum_i p_i\log(q_i)$ </p><p>度量两个分布的相似程度（一般 $p$ 为真实分布，$q$为预测分布），值越大，两个分布越不相似。</p><p>KL-Divergence（KL散度，也叫相对熵） ：$D_{KL}(p|q)=H(p,q)-H(q)$ </p><p>度量交叉熵超过熵的那一部分。</p><h1 id="Entropy-熵"><a href="#Entropy-熵" class="headerlink" title="Entropy-熵"></a>Entropy-熵</h1><p><strong>实例1：</strong></p><p>sunny和rainy的发生的概率都是0.5，天气预报预测明天的天气为sunny，将sunny消息发给用户。</p><p>该条消息不管多长，有用的信息其实只有1个比特，即uncertainty divided by 2.</p><p><strong>实例2：</strong></p><p>有八种不同的天气，发生的概率相同，当天气预报将预测消息发送给用户时。</p><p>该条消息能使得uncertainty divided By 8.即有用信息为3个比特。</p><p><strong>实例3:</strong></p><p>sunny发生的概率为0.75，rainy的概率为0.25，如果天气预测明天的天气：</p><blockquote><p>将这个例子理解为抽球游戏，盒子里有3个红球（表示sunny天气），1个白球（rainy天气）。</p><p>事件 $X$ 表示为在盒子里抽中球的颜色，可得知抽中红球的概率为0.75，抽中白球的概率是0.25。</p><p>抽中哪个球是不确定的，即uncertainty 如果原来是4，即不知道将抽中这四个球中的哪一个。</p><ul><li><p>如果抽中白球，那该信息表示：就是那4个球中的唯一一个白球，uncertainty 从原来的4变为1，即 uncertainty divided by 4.表示该信息，需要有用比特， $\log_2(4)=\log_2(1/0.25)=2$ 个比特来表示。即抽打白球的情况的不确定性更大，需要更多的比特来消除不确定性，来表示白球的发生。所以该条信息中只有2个比特是useful information.</p></li><li><p>如果抽中红球，该信息表示为：是那3个红球中的一个，uncertainty 从原来的4变为3</p><p>（如果和抽中白球的情况统一，最后的确定发生的uncertainty都表示为1，即在没有抽之前，抽到红球的uncertainty为 $1/0.75=4/3$ ）</p><p>即uncertainty divided by 4/3.表示该信息需要有用 $\log_2(4/3)=\log_2(1/0.75)=-\log_2(0.75)=0.41$ 比特来表示。即抽到红球的情况不确定性没有那么大，只需要较少比特即可消除不确定性，来表示红球的发生。所以该条信息中只有0.41个比特是useful information.</p></li></ul><p>这里也可以看出，如果一个事件的发生的概率越小（越不可能发生），即对该事件发生的不确定性越大，但一旦发生了，所携带的信息量就会很大，因为需要用更多的比特来消除不确定性。</p></blockquote><p>回到本例子：</p><ul><li><p>如果预测天气为rainy，将预测消息发给用户，则该条消息包含2比特（$\log_2(1/0.25)=-\log_2(0.25)=2$）的有用信息，即对rainy天气发送的不确定性更大，需要更多的比特来消除不确定性。</p></li><li><p>如果预测天气为sunny，因为在预测之前，用户对sunny发生的可能性就没有那么大，因此只需要0.41比特（$\log_2(1/0.75)=-\log_2(0.75)$)来消除不确定性。</p></li></ul><p>那平均下来，气象局发送的平均信息量为 $0.75\times 0.41+0.25\times2=0.81$ bits.</p><hr><p>因此我们用 $\log_2(1/p)=-\log_2p$ 来表示事件发生时所携带的信息量。（或者说需要这么多信息量来消除事件发生的不确定性）</p><p>用 $-\sum_i{p_i}\log_2{p_i}$ 来表示该事件的平均信息量（概率分布的不确定性），这就是信息熵（Entropy）。</p><p>Entropy：<br>$$<br>H(p)=-\sum_i pi\log_2(p_i)<br>$$<br>熵越大，说明携带的平均信息量越多，即不确定性越强，需要越多的比特来消除不确定性。所以熵是用来衡量不确定性的量。</p><blockquote><p>和化学中衡量混乱程度的熵，是类似的。</p></blockquote><h1 id="Cross-Entropy-交叉熵"><a href="#Cross-Entropy-交叉熵" class="headerlink" title="Cross-Entropy-交叉熵"></a>Cross-Entropy-交叉熵</h1><p><strong>例1：</strong></p><p>从上面的实例2来看，即8中天气发生概率相同，对天气表示进行信息编码，为下图：</p><img src="https://s3.ax1x.com/2021/01/16/sDloKf.png" alt="例1" style="zoom:33%;" /><p>entropy为3bits，而cross-entropy（交叉熵），也就是消息（比特流）的平均长度，为3bits.</p><p><strong>例2：</strong></p><p>但如果8种天气发生的可能性为下图：</p><img src="https://s3.ax1x.com/2021/01/16/sDl5xP.png" alt="例2" style="zoom:33%;" /><p>算出来的entropy为2.23bits，即平均信息量为2.23bits。</p><p>如果仍是用这样的编码，cross-entropy为3bits，就多出一些冗余信息量。</p><p><strong>例3:</strong></p><p>如果换一种编码方式：</p><img src="https://s3.ax1x.com/2021/01/16/sD12LT.png" alt="例3" style="zoom:33%;" /><p>算出来的cross-entropy为 $0.35 \times2+0.35\times2+0.1\times3+…+0.01\times5=2.42$ bits，就非常接近entropy=2.23bits。</p><p>说明这种编码方式冗余量很小，非常接近真实的概率分布所包含的平均信息。</p><p><strong>例4:</strong></p><p>如果天气的概率分布变为下图，entropy不变仍然为2.23bits：</p><img src="https://s3.ax1x.com/2021/01/16/sDl42t.png" alt="例4" style="zoom:33%;" /><p>那么算出来的cross-entropy为 $0.01\times2+0.01\times 2+0.04\times3+…+0.35\times5=4.58$ bits，远大于entropy的值。说明这种编码方式冗余量很大。</p><hr><p>换一种角度看例4，把信息编码认为是预测的概率分布，例4点编码表示的分布如下：</p><img src="https://s3.ax1x.com/2021/01/16/sD1rJs.png" alt="cross-entropy" style="zoom:33%;" /><p>Cross-Entropy：<br>$$<br>H(p,q)=-\sum_i p_i\log(q_i)<br>$$<br>所以cross-entropy可以理解为<strong>信息/比特流的平均长度。</strong></p><p>如果预测的概率分布非常接近真实的概率分布，那比特流的平均长度也会非常接近原分布的平均信息量。</p><p>如果预测的概率分布 $q$ 和真实分布 $p$ 完全一样，那么cross-entropy等于entropy。</p><p>所以cross-entropy可以用来衡量两个概率分布的相似程度。</p><p>用在机器学习中用作评判模型好坏的损失函数，度量模型预测分布和真实分布的相似程度。</p><h1 id="KL-Divergence-KL散度"><a href="#KL-Divergence-KL散度" class="headerlink" title="KL-Divergence-KL散度"></a>KL-Divergence-KL散度</h1><p>而如果预测的概率分布和真实分布不同，那么cross-entropy的值就会大于entropy的值，超过的部分就叫做relative entropy（相对熵），也就是KL-Divergence（Kullback-Leibler Divergence，KL散度）</p><p>即可以得到等式：$\text{Cross-Entropy = Entropy+KL-Divergence}$ </p><p>则KL-Divergence：<br>$$<br>\begin{align}<br>D_{KL}(p|q)=H(p,q)-H(p) &amp;= -\sum_i p_i\ln q_i - \sum_i p_i\ln p_i  \<br>&amp;= -\sum_i p_i \ln \frac{p_i}{q_i} \<br>&amp;= \sum_i p_i \ln \frac{q_i}{p_i}<br>\end{align}<br>$$</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>视频链接：<a href="https://www.youtube.com/watch?v=ErfnhcEV1O8">https://www.youtube.com/watch?v=ErfnhcEV1O8</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在机器学习中，常用cross-entropy来作为模型的损失函数，这篇文章将阐述信息学中的entropy（熵）是什么，cross-entropy（交叉熵）又是什么，KL-Divergence和entropy、cross-entropy的关系是什么？&lt;/p&gt;
&lt;p&gt;如何具象的理解这些概念？&lt;/p&gt;
&lt;p&gt;在开始阅读这篇文章之前，先提及一下香农对bit的定义，香农认为bit是用来消除信息的不确定性的。&lt;/p&gt;
&lt;p&gt;bit：uncertainty divided by 2.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ErfnhcEV1O8&quot;&gt;原视频&lt;/a&gt; 讲的很好，本文只是在此基础上对一些总结，方便理解物质化（马原.jpg）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Information-Theory" scheme="https://f7ed.com/categories/Information-Theory/"/>
    
    
      <category term="Math" scheme="https://f7ed.com/tags/Math/"/>
    
      <category term="Information-Theory" scheme="https://f7ed.com/tags/Information-Theory/"/>
    
  </entry>
  
  <entry>
    <title>「政治」:马克思主义</title>
    <link href="https://f7ed.com/2021/01/16/Marxism/"/>
    <id>https://f7ed.com/2021/01/16/Marxism/</id>
    <published>2021-01-15T16:00:00.000Z</published>
    <updated>2021-01-16T08:09:53.258Z</updated>
    
    <content type="html"><![CDATA[<p>这学期马原毛概并举，政治知识储备达到巅峰qwq</p><p>看着徐涛老师的政治课复习，做了些许笔记，做个总结分享出来。</p><p>「哲学们只是用不同的方式解释世界，而问题在于改变世界。——卡尔·马克思」</p><p>马克思主义真是大智慧！！！</p><a id="more"></a><h1 id="马克思主义哲学"><a href="#马克思主义哲学" class="headerlink" title="马克思主义哲学"></a>马克思主义哲学</h1><h2 id="马克思主义导论"><a href="#马克思主义导论" class="headerlink" title="马克思主义导论"></a>马克思主义导论</h2><h3 id="1-马克思主义的内涵"><a href="#1-马克思主义的内涵" class="headerlink" title="1.马克思主义的内涵"></a>1.马克思主义的内涵</h3><ul><li><p>马克思主义是什么</p><ol><li>（创造者）马克思主义是马克思和恩格斯创立并为后继者所不断发展的科学理论体系，</li><li>（内容）是关于自然、社会、人类思维发展的一般规律的学说。，</li><li>（目的）是关于社会主义必然代替资本主义，最终实现共产主义的学说，</li><li>（立场）是关于无产阶级解放、全人类解放，和每个人自由而全面发展的学说，</li><li>（作用）是指引人民创造美好生活的行动指南。</li></ol></li><li><p>马克思主义的构成</p><ol><li>（方法）马克思主义哲学</li><li>（主体）马克思主义政治经济学</li><li>（归属）科学社会主义</li></ol><blockquote><p>用了马克思主义哲学的方法，创立了马克思主义政治经济学，得出了科学社会主义的结论。</p></blockquote></li></ul><h3 id="2-马克思主义基本原理"><a href="#2-马克思主义基本原理" class="headerlink" title="2.马克思主义基本原理"></a>2.马克思主义基本原理</h3><p><em>马克思主义基本原理是对马克思主义立场、观点、方法的集中概括</em> </p><ol><li><p>基本立场</p><ul><li>（人的解放）马克思主义以无产阶级的解放和全人类的解放为己任，</li><li>（人的发展）以人的全面发展为目标，</li><li>（人为中心）以人民为中心，一切为了人民，一切依靠人民。</li></ul></li><li><p>基本观点</p><ul><li>（科学认识）是对自然、社会、人类思维发展一般规律的科学认识</li><li>（科学总结）是对人类思想成果和社会实践经验的科学总结。</li></ul></li><li><p>基本方法</p><ul><li><p>辩证唯物主义和历史唯物主义的世界观和方法论。</p><p><em>实事求是的方法、辩证分析的方法、社会基本矛盾和主要矛盾分析的方法、阶级分析的方法、群众路线的方法</em> </p></li></ul></li></ol><h3 id="3-马克思主义的来源"><a href="#3-马克思主义的来源" class="headerlink" title="3.马克思主义的来源"></a>3.马克思主义的来源</h3><ul><li>第一个无产阶级同盟：正义者同盟</li><li>第一个无产阶级政党：共产主义者同盟</li><li>1844年马恩《德法年鉴》：完成了唯心主义向唯物主义的、从革命民主主义向共产主义的转变，为创立马克思主义奠定思想基础。</li><li>1844年马恩《德意志形态》：首次系统阐述了历史唯物主义的基本观点，历史观的伟大变革。</li><li>1848年马恩《共产党宣言的发表》标志马克思主义的公开问世。</li><li>马克思《资本论》：系统阐述剩余价值学说，揭示资本主义生产关系的密码</li></ul><blockquote><p>马克思一生的伟大发现：唯物史观和剩余价值学说</p></blockquote><ul><li>马克思《法兰西内战》：高度赞扬巴黎工人的伟大创举，科学总结了巴黎公社的历史经验</li><li>马克思《哥达纲领批判》：丰富科学社会主义学说</li><li>恩格斯《反杜林论》：全面阐述马克思主义理论体系</li></ul><h3 id="4-马克思主义的产生条件"><a href="#4-马克思主义的产生条件" class="headerlink" title="4.马克思主义的产生条件"></a>4.马克思主义的产生条件</h3><p><em>马克思主义的产生具有深刻的社会根源、阶级基础和思想渊源</em> </p><ol><li><p>社会渊源：马克思、恩格斯生活的时代，资本主义生产方式在西欧已经有相当的发展。</p><blockquote><p>资本主义一方面带来社会化大生产的迅猛发展，一方面又造成严重的社会灾难：</p><ol><li>社会两极化，工人极端困苦</li><li>周期性经济危机频繁爆发</li></ol></blockquote></li><li><p>阶级基础/实践基础：无产阶级在反抗资产阶级剥削和压迫的斗争中，逐渐走向自觉，并迫切渴望科学的理论指导。</p><blockquote><p>工人运动：法、英、德</p><ul><li>1831年法国里昂工人</li><li>1836年英国工人运动——宪章运动：集中反映工人的政治需求</li><li>1844年德国西里西亚纺织工人举行起义：标志现代无产阶级作为独立的政治力量登上历史舞台</li></ul></blockquote></li><li><p>思想渊源：德国古典哲学、英国古典政治经济学、英法两国空想社会主义</p><blockquote><ul><li>德国古典哲学：辩证法</li><li>英国古典政治经济学：资本主义生产关系的分析和关于劳动创造价值的思想</li><li>空想社会主义：对资本主义社会的批判和对未来新社会的发展</li><li>19世纪三大科学发现：细胞学说、能量守恒和能量转化定律、生物进化论</li></ul></blockquote></li></ol><h3 id="5-马克思主义的特征"><a href="#5-马克思主义的特征" class="headerlink" title="5.马克思主义的特征"></a>5.马克思主义的特征</h3><blockquote><p>考：特征和解释对应，一段材料反映什么样的特征。</p></blockquote><ul><li><p>科学性：马克思主义是对自然、社会和人类思维发展本质和规律的<strong>正确</strong>反映 。</p></li><li><p>革命性：马克思主义的革命性集中表现为<strong>彻底的批判精神</strong>和<strong>鲜明的无产阶级立场</strong>。 </p></li><li><p>实践性：马克思主义是从实践中来，到实践中去，在实践中接受检验，并随实践不断发展的学说。</p><blockquote><p>实践性是马克思主义独有的特征，可以区分其他派别。</p></blockquote></li><li><p>人民性：人民至上是马克思主义的政治立场。</p><p><em>马克思主义的人民性是以阶级性为深刻基础的，是无产阶级先进性的的体现</em> </p><blockquote><p>以阶级性为基础，表示只要你是无产阶级，你就代表人民</p></blockquote></li><li><p>发展性：马克思是不断发展的学说，具有与时俱进的品质。马克思主义理论体系是开放的，不断吸收人类最新的文明成果来充实和发展自己。</p></li></ul><h2 id="哲学基本问题"><a href="#哲学基本问题" class="headerlink" title="哲学基本问题"></a>哲学基本问题</h2><h3 id="6-哲学的基本问题"><a href="#6-哲学的基本问题" class="headerlink" title="6.哲学的基本问题"></a>6.哲学的基本问题</h3><ul><li><p>哲学的基本问题：思维和存在的问题。（意识/精神和物质的问题。）</p><ol><li><p>存在和思维何者是第一性，何者是第二性。</p><p><em>该问题的回答，构成了划分唯物主义和唯心主义的标准</em></p></li><li><p>物质和意识是否具有同一性。</p><p><em>即思维能否正确认识存在的问题</em> </p></li></ol></li></ul><h3 id="6-哲学的不同流派"><a href="#6-哲学的不同流派" class="headerlink" title="6.哲学的不同流派"></a>6.哲学的不同流派</h3><p><em>通过对哲学基本问题的回答来划分哲学的流派</em></p><ul><li><p>存在和思维何者是第一性，何者是第二性。</p><ol><li><p>唯物主义：物质第一性</p><p><em>什么才是物质？划分唯物主义中的流派</em> </p><ul><li><p>古代朴素（古典）唯物主义：物质是某一种或者几种</p><blockquote><p>五行；火；一生二，二生三，三生万物。</p></blockquote></li><li><p>近代形而上学唯物主义：物质是原子或粒子</p><blockquote><p>其中机械唯物主义是近代形而上学唯物主义</p><p>近代形而上学唯物主义&gt;机械唯物主义</p></blockquote></li><li><p>现代辩证唯物主义：一切客观存在的都是物质</p><blockquote><p>把历史作为物质。</p><p>唯物史观：把历史作为物质来分析，得出的观点和结论。</p><p>=马克思主义</p></blockquote></li></ul></li><li><p>唯心主义：意识第一性</p><p><em>什么才是意识</em> </p><ul><li><p>主观唯心主义：世界的本原是人的意识。</p></li><li><p>客观唯心主义：世界的本原是独立于“我”之上的。</p><blockquote><p>比如上帝、道、理、缘说等</p></blockquote></li></ul></li></ol></li><li><p>物质和意识是否具有同一性</p><ol><li><p>可知论：意识可以认识物质。</p><blockquote><p>唯物主义和唯心主义都属于可知论，只是唯物主义是物质决定意识，顺序不同。</p></blockquote></li><li><p>不可知论：意识不可以或者不能完全认识物质。</p><blockquote><p>二元论：认为物质和意识都是世界的本原。</p><p>二元论属于不完全的唯心主义。</p></blockquote></li></ol></li></ul><h3 id="6-哲学的重要问题"><a href="#6-哲学的重要问题" class="headerlink" title="6.哲学的重要问题"></a>6.哲学的重要问题</h3><ul><li><p>哲学的重要问题：世界是什么样的。</p><blockquote><p>先回答哲学的基本问题：唯物主义或唯心主义</p><p>再回答哲学的重要问题：形而上学或辩证法。</p></blockquote><ol><li><p>形而上学：世界是孤立的、片面的、静止的、无矛盾的</p></li><li><p>辩证法：世界是联系的、全面的、发展的、矛盾的</p><blockquote><p>辩证法是一种观点，回答世界是什么样的观点。</p></blockquote></li></ol></li><li><p>哲学流派的划分</p><p><em>先回答哲学的基本问题，再回答哲学的重要问题</em></p><ol><li>形而上学的唯物主义</li><li>形而上学的唯心主义</li><li>辩证唯心主义</li></ol><p><em>在马克思主义出现之前，没有辩证唯物主义的组合</em> </p><ol start="4"><li>辩证唯物主义</li></ol></li><li><p>马克思在哲学的两大贡献</p><ol><li>创立了历史唯物主义（唯物史观）：将历史作为物质来研究得出的观点和结论。</li><li>形成了辩证唯物主义：将辩证法的观点和唯物主义结合。</li></ol></li></ul><h3 id="马克思主义的当代价值"><a href="#马克思主义的当代价值" class="headerlink" title="马克思主义的当代价值"></a>马克思主义的当代价值</h3><ol><li>观察当代世界变化的工具</li><li>指引当代中国发展的行动指南</li><li>引领人类社会进步的科学真理</li><li>自觉学习和运用马克思主义</li></ol><h2 id="唯物论-物质观"><a href="#唯物论-物质观" class="headerlink" title="唯物论-物质观"></a>唯物论-物质观</h2><p>框架：</p><ul><li>物质范畴：什么是物质</li><li>物质和运动</li><li>运动和静止</li><li>物质运动和时空</li></ul><h3 id="7-物质范畴：什么是物质"><a href="#7-物质范畴：什么是物质" class="headerlink" title="7.物质范畴：什么是物质"></a>7.物质范畴：什么是物质</h3><p><em>马克思批判了旧唯物主义对物质世界的直观、消极理解，强调要从能动的实践出发去把握客观世界的意义</em></p><ul><li><p>恩格斯的物质概念：物、物质是各种物的总和，这个概念是从这个总和中<strong>抽象</strong>而来的。</p><blockquote><p>考点：物质和具体物的关系：抽象和具体的关系</p></blockquote></li><li><p>列宁的定义：</p><p><em>定义方式：通过物质和意识的关系定义</em></p><ol><li>（客观实在）物质是标志客观实在的哲学范畴，</li><li>（人的感觉）这种客观实在是人通过感觉感知的，不依赖于人的感觉而存在，为我们的感觉所复写、摄影、反映。</li></ol></li><li><p>物质的唯一特性（最本质的特性）：客观实在性</p><p><em>马克思的物质范畴从客观存在的物质世界抽象出万物的共同特性：客观实在性</em> </p></li><li><p>马克思主义的物质范畴理论的意义：（两个坚持两个体现）</p><ol><li><p>坚持唯物主义一元论，同唯心主义一元论和二元论划清界限。</p><p><em>马克思主义是从物质与意识的对立统一关系中把握和规定物质的，物质最本质的特征是客观实在性，这就指明了物质对于意识的独立性、根源性，以及意识对于物质的依赖性、派生性。因为意识不过是物质的反映，不能离开物质而独立存在，所以意识不可能成为另一种本原</em></p></li><li><p>坚持能动反映论和可知论，批判不可知论。</p><p><em>世界上还有很多事物未被人类认识，但这并不意味着它们不可认识</em> </p></li><li><p>体现了唯物论和辩证法的统一，克服了形而上学唯物主义的缺陷</p></li><li><p>体现了唯物主义自然观和唯物主义历史观的统一</p></li></ol></li></ul><h3 id="8-物质和运动"><a href="#8-物质和运动" class="headerlink" title="8.物质和运动"></a>8.物质和运动</h3><ul><li><p>物质的定义：不依赖于人的意识而存在，并能为人类的意识所反映的客观存在。</p></li><li><p>运动的定义：运动是标志一切事物和现象的变化及其过程的哲学范畴</p></li><li><p>物质和运动的关系：</p><ul><li><p>物质和运动的关系：不可分割</p><blockquote><p>不可分割：我是你的我，你是我的你</p></blockquote><p>物质是运动着的物质，运动是物质在运动。</p></li><li><p>物质的根本属性（固有属性）是运动。</p><blockquote><p>物质的唯一特性：客观实在性。</p><p>物质的根本属性：运动。</p></blockquote></li><li><p>物质的存在方式是运动。</p></li></ul></li><li><p>批判两种错误的观点：运动——物质</p><ul><li>脱离物质谈运动，则是唯心主义。</li><li>脱离运动谈物质，则是形而上学。</li></ul></li></ul><h3 id="9-运动和静止"><a href="#9-运动和静止" class="headerlink" title="9.运动和静止"></a>9.运动和静止</h3><ul><li><p>相对静止的定义：静止是物质运动在一定条件下的稳定状态，包括两种状态：空间的相对位置暂时不变和事物的根本性质暂时不变。</p></li><li><p>运动和静止的关系：对立统一</p><blockquote><p>对立统一：相互区别；相互联系。（相同词性）</p><p>不可分割：我是你的我，你是我的你。（不同词性）</p></blockquote><ul><li><p>相互区别：运动是绝对的、无条件性的；静止是相对的、有条件性的。</p></li><li><p>相互联系：运动和静止是相互依赖、相互渗透，“动中有静，静中有动“。</p><blockquote><p>鸟鸣山更幽，风定花犹落。</p></blockquote></li></ul></li><li><p>批判两种错误的观点：运动——静止</p><ul><li><p>夸大静止，否定运动，导致形而上学。</p></li><li><p>夸大运动，否定静止，导致诡辩论。</p><blockquote><p>人不能两次踏入同一条河流。（正确）</p><p>人没有一次能踏入同一条河流。（错误）</p></blockquote></li></ul></li></ul><h3 id="10-物质运动和时空"><a href="#10-物质运动和时空" class="headerlink" title="10.物质运动和时空"></a>10.物质运动和时空</h3><ul><li><p>时空的概念：时间是物质运动的持续性、顺序性，特点是一维性，即时间一去不复返。空间是物质运动的广延性、伸展性，特点是三维性。</p></li><li><p>时空和物质运动的关系：</p><ul><li><p>不可分割：时空是物质运动的时空，物质运动是时空中的物质运动。</p></li><li><p>物质运动的存在形式是时空。</p><blockquote><p>物质运动的存在形式是时空。</p><p>时间是物质运动的存在形式。正确</p><p>空间是物质运动的存在形式。正确</p></blockquote></li></ul></li><li><p>时空的特点：（五个特点）</p><blockquote><p>考察：给一段材料，体现了时空的什么特点</p></blockquote><ul><li><p>客观性：不以人的意志为转移。</p><p><em>物质运动与时空不可分割，证明时间和空间的客观性</em></p></li><li><p>绝对性：时空是绝对存在的。</p></li><li><p>相对性：当速度突破极限，时空会发生变化。</p></li><li><p>有限性：具体物质形态的时空是有限的。</p></li><li><p>无限性：整个物质世界的时空是无限的。</p></li></ul></li></ul><h2 id="唯物论-意识观"><a href="#唯物论-意识观" class="headerlink" title="唯物论-意识观"></a>唯物论-意识观</h2><p>框架：</p><ul><li>实践：物质世界和意识世界的桥梁</li><li>意识的起源</li><li>意识到本质</li><li>意识到作用</li><li>物质和意识的辩证关系</li></ul><h3 id="11-实践：是自然存在和社会存在区分和统一的基础"><a href="#11-实践：是自然存在和社会存在区分和统一的基础" class="headerlink" title="11.实践：是自然存在和社会存在区分和统一的基础"></a>11.实践：是自然存在和社会存在区分和统一的基础</h3><p><em>人类的产生使自然界的运动变化发生了新的飞跃，并通过人的实践形成了自然界与人类社会的区别</em></p><ul><li><p>社会生活的本质是实践。</p><p><em>全部社会生活的本质是实践。</em> </p></li><li><p>从实践出发理解社会生活的本质</p><ol><li><p>实践是使物质世界分化为自然界和人类社会的历史前提，又是使自然界与人类社会统一起来的现实基础。</p><blockquote><p>物质世界：一切客观存在。</p><p>在没有人之前，物质世界是自然界，有人之后，人的实践使其分为自然界和人类社会。</p><p>也通过实践，两部分相互转换，统一。</p></blockquote><p><em>实践活动过程中，物质世界被区分为自然界和人类社会两大领域</em></p><p><em>劳动是人的存在方式，也是人类社会存在与发展的基础。通过劳动实践，人不再是单纯的自然存在物，更主要是社会存在物。</em></p></li><li><p>实践是人类社会的基础，是理解和解释一切社会现象的钥匙。</p><blockquote><p>人类社会的本质是实践，把社会生活当作实践去理解。</p></blockquote><ul><li><p>社会生活的实践性体现在（为什么说实践是社会生活的本质）</p><p><em>社会生活是对人民各种社会活动的总称</em> </p><ol><li><p>实践是社会关系形成的基础。</p><p><em>包含了全部社会关系，是社会关系的发源地</em>。</p></li><li><p>实践形成了社会生活的基本领域。</p><p><em>即社会的物质生活、政治生活和精神生活领域。</em> </p></li><li><p>实践 <strong>构成了</strong> 社会发展的动力。</p><blockquote><p>考：构成动力，而不是 是动力，社会发展的动力是社会基本矛盾。</p></blockquote></li></ol></li></ul></li></ol></li><li><p>实践是自然存在和社会存在区分和统一的基础。</p><blockquote><p>自然存在是自然界，社会存在是人类社会。</p></blockquote></li></ul><h3 id="12-意识的起源"><a href="#12-意识的起源" class="headerlink" title="12.意识的起源"></a>12.意识的起源</h3><ul><li><p>意识的概念：</p><ol><li>（物质的产物）物质世界长期发展的产物，</li><li>（本质）意识是人脑这样一种特殊物质的机能和属性，</li><li>（最本质）是物质/客观世界的主观映像。</li></ol></li><li><p>意识的起源：</p><ol><li><p>由一切事物所具有的反映特性到低等生物所具有的刺激感应性，</p><blockquote><p>一切事物的物理性质，化学性质，到低等生物的应激性反应。</p></blockquote></li><li><p>再到高等动物的感觉和心理，</p><blockquote><p>高等动物的开心、难过等情绪和疼痛等感觉</p></blockquote></li><li><p>最终发展为人的意识。</p><blockquote><p>意识是人所特有的</p></blockquote></li></ol></li><li><p>意识形成过程中的影响因素：</p><p><em>意识是自然界长期发展的产物，也是社会历史发展的产物</em> </p><p><em>意识有很多影响因素，其中</em> </p><ul><li><p>劳动：社会实践，特别是劳动，在意识到产生和发展中起<strong>决定作用</strong>。</p></li><li><p>语言：人们在劳动和交往中形成的语言，<strong>促进</strong>了意识的发展。</p><ul><li><p>语言是意识的物质外壳。</p><blockquote><p>语言是物质，语言的含义是意识。</p></blockquote></li></ul></li></ul></li></ul><h3 id="12-意识的本质"><a href="#12-意识的本质" class="headerlink" title="12.意识的本质"></a>12.意识的本质</h3><ul><li><p>意识的本质：意识是客观世界的主观映像，是客观内容和主观形式的统一。</p><p><em>意识在内容上客观的，在形式上主观的，是客观内容和主观形式的统一</em></p><p><em>意识是物质的产物，又不是物质本身，因此在内容上是反应物质的，是客观存在的，在形式上是主观的映像。</em> </p></li></ul><h3 id="12-意识的作用"><a href="#12-意识的作用" class="headerlink" title="12.意识的作用"></a><font color='red'>12.意识的作用</font></h3><ul><li><p>意识的作用：能动作用</p><p><em>物质决定意识，意识对物质具有反作用，这种反作用就是意识的能动作用</em> </p></li><li><p>意识到能动作用表现在：</p><ol><li><p>意识活动具有自觉性，具有目的性和计划性。</p><blockquote><p>人认识世界时，根据一定的目的和要求去确定反映什么、不反映什么、怎么反应，表现出主体的选择性。</p></blockquote></li><li><p>意识活动具有创造性。</p><blockquote><p>意识是对所见的东西，在头脑中进行加工想象。</p></blockquote></li><li><p>意识具有指导实践改造客观世界的作用。</p><p><em>意识到能动性不限于从实践中形成思想，形成活动的目的、计划、方法等观念的东西，更重要的是，将这些观念作为指导，通过实践使之成为客观现实。</em> </p><blockquote><p>可以创造出世界上原本没有的东西。</p></blockquote></li><li><p>意识具有指导、控制人的行为和生理活动的作用。</p><p><em>意识、心理对人的行为和健康有重要影响，”笑一笑十年少，愁一愁，白了头。</em>  </p><blockquote><p>想做的事，能通过意识不做这件事。</p></blockquote></li></ol></li></ul><h3 id="13-物质和意识的辩证关系"><a href="#13-物质和意识的辩证关系" class="headerlink" title="13.物质和意识的辩证关系"></a>13.物质和意识的辩证关系</h3><blockquote><p>两大关系：不可分割和对立统一，对立统一即辩证关系。</p></blockquote><ul><li><p>物质和意识相互区别：</p><ol><li><p>物质是本原，意识是派生。</p></li><li><p>物质不是意识，意识不是物质。</p><blockquote><p>大脑的分泌物是意识。错误。</p></blockquote></li><li><p>物质不能代替意识，意识不能代替物质。</p></li></ol></li><li><p>物质和意识相互联系：</p><ol><li><p>物质可以转换为意识，意识可以转换为物质。</p></li><li><p>意识对物质既有依赖性，又有相对独立性。</p><blockquote><p>把物质作为强，意识作为弱，但没有那么弱，弱的要依赖于强的，但也要有自己的独立性。</p></blockquote></li><li><p>物质决定意识，意识反作用于物质。</p><blockquote><p>强的决定弱的。</p></blockquote></li></ol></li><li><p>主观能动性和客观规律性的统一</p><p><em>正确认识和处理物质和意识的辩证统一，还需要处理好主观能动性和客观规律性的关系</em> </p><ul><li><p>主观能动性和客观规律性的关系（物质世界）</p><ul><li>一方面，尊重客观规律是正确发挥主观能动性的前提。</li><li>另一方面，只有充分发挥主观能动性，才能正确认识和利用客观规律。</li></ul></li><li><p>正确发挥人的主观能动性的前提和条件：</p><ol><li>前提：从实际出发是发挥人的主观能动性的前提。</li><li>基本途径：实践是正确发挥人的主观能动性的基本途径。</li><li>正确发挥人的主观能动性，还需要依赖一定的物质条件和物质手段。</li></ol><blockquote><p>类比考研：从实际出发，实践，依赖一定的条件和手段</p></blockquote></li><li><p>主观能动性和客观规律性的关系（人类历史的角度：社会领域）</p><ul><li><p>社会历史领域，主观能动性和客观规律的辩证关系具体表现在社会历史趋向与主体选择的关系   </p><blockquote><p>社会历史趋向，即，客观规律。</p></blockquote></li></ul><blockquote><p>主体选择，即，主观能动。</p></blockquote><p>  <em>社会历史趋向是指社会历史规律的客观性和必然性</em> </p><p>  <em>主体选择是指社会主体在社会发展中的能动性和选择性</em> </p><p>  <em>社会历史规律的客观性和必然性规定了人的活动要受到规律的限制，但又不能否认人作为历史主体的能动性和选择性</em> </p></li></ul></li></ul><h3 id="14-世界的物质统一性原理"><a href="#14-世界的物质统一性原理" class="headerlink" title="14.世界的物质统一性原理"></a>14.世界的物质统一性原理</h3><blockquote><p> 总结性，命题的方向</p><ol><li>世界是统一的，即世界的本原只有一个。（批判二元论）</li><li>世界的统一性在于他的物质性，即世界统一的基石是物质。（批判唯心主义）</li><li>物质世界的统一性是多样性的统一，而不是单一的无差别的统一。（批判旧唯物主义，即马哲之前的唯物主义）</li></ol></blockquote><ul><li><p>马克思主义哲学与唯心主义的比较</p><ul><li><p>联系：都是可知论。</p><blockquote><p>都是一元论，认为世界的本原只有一个。</p></blockquote></li><li><p>区别：</p><ol><li><p>（本原）马克思主义哲学认为物质是本原，唯心主义认为意识是本原。</p></li><li><p>（认识论）马克思主义哲学在认识问题上坚持能动反映论，唯心主义坚持先验论。</p></li><li><p>（辩证法）马克思主义哲学坚持彻底的辩证法，唯心主义部分坚持辩证法。</p><blockquote><p>唯心主义中也有形而上学的唯心主义和辩证唯心主义。</p></blockquote></li><li><p>（历史观）马克思主义哲学在历史观上是唯物的，唯心主义在历史观上是唯心的。</p></li></ol></li></ul></li><li><p>马克思主义与旧唯物主义的比较。</p><ul><li><p>联系：都是唯物主义，认为物质是世界的本原。</p></li><li><p>区别：</p><ol><li><p>（认识论）马克思主义哲学在认识论上坚持能动反映论，旧唯物主义坚持机械反映论。</p></li><li><p>（辩证论）马克思主义哲学彻底坚持辩证法，旧唯物主义完全不认同辩证法，坚持形而上学。</p></li><li><p>（历史观）马克思主义哲学在历史观上唯物，旧唯物主义在历史观上唯心。</p></li><li><p>（实践）马克思主义哲学坚持实践的观点，旧唯物主义没有实践的观点。</p><blockquote><p>实践是马克思独有的观点</p></blockquote></li></ol></li></ul></li><li><p>世界统一于物质的观点</p><ol><li><p>（意识）世界的物质统一性首先体现在，意识统一于物质。</p><blockquote><p>意识是物质发展的产物。</p></blockquote></li><li><p>（人类社会）世界的物质统一性还体现在，人类社会也统一于物质。</p><blockquote><p>人类社会（历史）是物质。</p></blockquote></li></ol><ul><li><p>人类社会的物质性</p><ol><li><p>（物质世界）人类社会是物质世界的组成部分。</p><blockquote><p>物质世界包含人类社会。</p></blockquote></li><li><p>（物质资料的获取）人类获取物质生活资料的实践活动，虽然是意识指导，但仍然是物质性的活动。</p><blockquote><p>实践活动是物质：</p><p>社会生活的本质是实践。</p><p>社会生活/人类社会是物质的，实践是物质。</p></blockquote></li><li><p>（物质资料的生产）物质资料的生产方式，是人类存在和发展的基础，集中体现人类社会的物质性。</p><blockquote><p>生产方式是物质：</p><p>实践是社会关系的基础，包含所有社会关系。</p><p>实践是物质的，物质资料的生产方式也是物质的。</p></blockquote></li></ol></li></ul></li></ul><h3 id="14-世界的物质统一性原理的意义"><a href="#14-世界的物质统一性原理的意义" class="headerlink" title="14.世界的物质统一性原理的意义"></a><font color='red'>14.世界的物质统一性原理的意义</font></h3><p><em>世界的物质统一性原理的作用</em> </p><ol><li><p>（理论上）世界的物质统一性是马克思主义的基石。</p></li><li><p>（实践上：一切从实际出发）在认识和改造世界过程中，坚持实事求是，一切从实际出发，是世界的物质统一原理体现在现实生活中和实际工作中的生动体现。</p><blockquote><p>考：材料关于一切从实际出发，问题问这段材料体现什么哲学原理</p><p>：世界的物质统一性原理</p></blockquote></li></ol><h2 id="辩证法-两大总特征"><a href="#辩证法-两大总特征" class="headerlink" title="辩证法-两大总特征"></a>辩证法-两大总特征</h2><ul><li><p>两大总特征</p><ol><li>普遍联系</li><li>永恒发展</li></ol></li><li><p>五对范畴：联系和发展环节上的逻辑</p><blockquote><p>怎样的联系。</p><p>怎么发展</p></blockquote></li><li><p>三大规律</p><blockquote><p>事物怎么联系</p></blockquote><ol><li>对立统一</li><li>质量互变</li><li>否定之否定</li></ol></li></ul><h3 id="15-唯物辩证法两大总特征"><a href="#15-唯物辩证法两大总特征" class="headerlink" title="15.唯物辩证法两大总特征"></a>15.唯物辩证法两大总特征</h3><p><em>唯物辩证法的总观点和总特征：联系和发展</em> </p><ul><li><p>两大总特征：</p><ol><li>普遍联系</li><li>永恒发展</li></ol></li><li><p>普遍联系</p><ul><li><p>联系的概念：联系是指事物内部各要素之间 <strong>和</strong> 事物之间相互影响、相互制约、相互作用的关系。</p><blockquote><p>联系是以区别为前提。</p></blockquote></li><li><p>联系的特点：</p><blockquote><p>考：材料体现联系什么特点</p></blockquote><ul><li><p>客观性：联系是事物本身固有的，不是主观臆想的。</p></li><li><p>普遍性：</p><ol><li>（事物内部）任何事物内部的不同部分和要素是相互联系，即任何事物都有内部结构性。</li><li>（事物间）任何事物都不能孤立存在，都同其他事物处于一定的相互联系中中。</li><li>（世界是一个联系网）整个世界是相互联系的统一整体，每个事物都是世界普遍联系中的一个成分或环节，并通过这个联系之网体现出联系的普遍性。事物的普遍联系是通过中介实现的，是通过中间性的联系和过渡性环节而实现的。</li></ol></li><li><p>多样性：世界上的事物是多样的，事物之间的联系也是多样的。</p></li><li><p>条件性</p><p><em>条件是对事物存在和发展发生作用的诸要素的总和。</em> </p><ol><li><p>（支持或制约）条件对事物发展和人的活动具有<strong>支持或制约</strong>作用。</p><blockquote><p>有利条件促进发展，不利条件抑制发展。</p></blockquote></li><li><p>条件是可以改变的。</p><blockquote><p>将不利条件转化为有利条件。</p></blockquote></li><li><p>改变和创造条件不是任意的，必需尊重事物发展的客观规律。</p></li></ol></li></ul></li></ul></li><li><p>永恒发展</p><ul><li><p>发展的概念：概括一切形式的变化是运动，运动变化的趋势是发展。</p><blockquote><p>运动=变化 &gt; 发展</p><p>运动：标志一切事物和现象变化及其过程的哲学范畴。</p><p>运动可以是前进或者后退的方向，变化也是可以往好的和不好的方向。</p><p>但发展是运动变化的趋势，是前进的，是上升的。</p><p>因此运动是绝对的无条件的，发展不是，发展是永恒的。</p></blockquote></li><li><p>发展的实质：前进的上升的运动，新事物的产生和旧事物的灭亡。</p><ul><li>新事物：合乎历史前进方向、具有远大前途的东西。</li><li>旧事物：丧失历史必然性、日趋灭亡的东西。</li></ul><blockquote><p>新旧事物的区分是：通过历史趋势来看的。而与时间的先后无关。</p></blockquote><ul><li><p>新事物是不可战胜的：</p><ol><li><p>新事物具有新结构，适应新环境。</p><p><em>新事物之所以新，是因为新事物有新的要素、结构和功能，它适应已经变化了的环境和条件；旧事物之所以旧，是因为他的各种要素和功能已经不适应环境和客观条件的变化</em></p></li><li><p>新事物是旧事物的改良，吸收了旧的优点。</p><p><em>新事物否定了旧事物中消极腐朽的东西，又保留了旧事物中合理的、适应新条件的因素，并添加了旧事物所不能容纳的新内容</em> </p></li><li><p>新事物符合群众利益，受到群众拥护。</p></li></ol></li></ul></li></ul></li><li><p>过程的观点</p><ul><li><p>恩格斯指出：“世界不是既成事物的集合体，而是过程的集合体。”</p><p><em>事物的发展是一个过程，只有经过一定的过程，事物才能实现自身的发展。</em> </p><p><em>事物发展的过程，从形式上看，是事物在时间上持续性和空间上广延性的交替；从内容上看，是事物在运动形式、形态、结构、功能和关系上的更替。</em> </p></li><li><p>一切在历史上产生的都要在历史上灭亡。</p></li><li><p>任何事物都有它的过去、现在和将来。</p></li></ul></li></ul><h2 id="辩证法-五对范畴"><a href="#辩证法-五对范畴" class="headerlink" title="辩证法-五对范畴"></a>辩证法-五对范畴</h2><p><em>联系和发展是通过一系列基本环节得以实现的。</em> </p><p>五对范畴：联系和发展环节上的逻辑问题</p><ol><li>原因与结果</li><li>必然与偶然</li><li>可能与现实</li><li>现象与本质</li><li>内容与形式</li></ol><blockquote><p>概念、概念、方法论展开阐述</p></blockquote><h3 id="16-原因和结果"><a href="#16-原因和结果" class="headerlink" title="16.原因和结果"></a>16.原因和结果</h3><ul><li><p>原因和结果：是揭示事物前后相继、彼此制约的关系范畴</p><p><em>原因和结果是揭示事物引起和被引起关系的一对范畴</em> </p><blockquote><p>前后相继，则是因果，错误。</p><p>发生在你之前的不一定是原因。比如雷电不是因果。</p></blockquote><ul><li>原因：引起某种现象的现象</li><li>结果：被某种现象引起的现象</li></ul></li><li><p>辩证关系：对立统一</p><ol><li><p>（确定又不确定）原因和结果的区分既是确定的，又是不确定。</p><blockquote><p>在某一种具体因果关系中，原因和结果是相互区别的，原因就是原因，结果就是结果。</p><p>但整体来说，一种现象在一种联系中是原因，在另一种现在中可能是结果。</p></blockquote></li><li><p>（互为因果）原因和结果相互作用，原因产生结果，结果反影响原因。互为因果。</p></li><li><p>（互相表达）原因和结果互相渗透，结果存在在原因中，原因出现在结果之中。</p><blockquote><p>知道原因，可以预测结果，知道结果，可以反推原因。原因结果互相表达。</p></blockquote></li><li><p>（关系复杂）原因和结果的关系复杂多样，有一因多果、同因异果、一果多因、异因同果、多因多果、复因多果。</p><blockquote><p>有其因，必有其果。错误。原因结果的关系复杂，有一因多果的情况。</p><p>有因必有果。正确。</p></blockquote></li></ol></li><li><p>方法论：凡事预则立不预则废。</p><blockquote><p>做什么事都要先有计划和准备。有因，结果自然就有了。</p><p>考：故事体现什么哲理。</p><p>如果故事讲的是凡事预则立不预则废，为因果关系哲理，因为该方法论是因果范畴的出的启示。</p></blockquote></li></ul><h3 id="17-必然和偶然"><a href="#17-必然和偶然" class="headerlink" title="17.必然和偶然"></a>17.必然和偶然</h3><ul><li><p>必然和偶然：解释客观事物产生、发展和衰亡过程中不同趋势的一对范畴。</p><ul><li><p>事物的发展既包含必然的方面，也包含偶然的方面。</p><blockquote><p>不是说这件事是必然，这件事是偶然。每一件事都有必然和偶然的成分。</p></blockquote></li><li><p>必然：事物联系与发展中一定要发生、确定不移的趋势。</p></li><li><p>偶然：事物联系和发展过程中并非确定出现，可以出现，可以不出现，可以这样出现，也可以那样出现，具有不确定的趋势。</p></li></ul></li><li><p>关系：对立统一</p><ul><li><p>相互区别：</p><ol><li>产生和形成的原因不同：必然是内部原因；偶然是外部原因。</li><li>他们表现的形式不同：必然是稳定的；偶然是不稳定的。</li><li>他们在事物发展中地位和作用不同：必然起决定作用，偶然起影响作用。</li></ol></li><li><p>相互联系：</p><ol><li><p>一方面，没有脱离偶然的必然（都有偶然的一面，必然是通过多次偶然表现的）。现实事物的发展，不通过偶然而只表现为纯粹的必然的情况是不存在的。必然伴随着偶然，必然要通过偶然表现出来。</p><blockquote><p>我对芒果过敏，但如果我永远不吃芒果，这件事情就不会发生，不会有我过敏的偶然事件发生。</p></blockquote></li><li><p>另一方面，没有脱离必然的偶然（都有必然的一面）。似乎是偶然起支配作用的地方，实际上是必然起决定作用，制约着偶然的作用形式及其变化。</p><blockquote><p>我对芒果过敏，但我不知道。我吃了一次，过敏了，我以为是偶然，我又吃了一次，又过敏了，又以为是偶然。</p><p>虽然是偶然，但其实是我芒果过敏，因此是必然的。</p></blockquote></li></ol><ul><li><p>必然与偶然相互转化。</p><p><em>对于某一过程来说是必然的东西，对另一过程就可能成为偶然的东西。</em> </p><blockquote><p>在生物进化中，某个基因变异会导致新物种的产生，这是偶然转化为必然。</p><p>旧物种的基本性状在新物种中表现为返祖现象，这是必然转化为偶然。</p></blockquote></li></ul></li></ul></li></ul><ul><li>方法论：重视事物发展的必然，把握事物发展的总趋势，又要善于从偶然中发现必然，有利于事物发展的机遇。</li></ul><h3 id="18-现实与可能"><a href="#18-现实与可能" class="headerlink" title="18.现实与可能"></a>18.现实与可能</h3><ul><li><p>现实和可能：揭示事物过去、现在和将来的相互关系的一对范畴。</p><ul><li>可能（潜在的）：事物发展过程中<strong>潜在的东西</strong>，包含在事物中，是事物发展前途的种种趋势。</li><li>现实（实际存在）：相互联系着的<strong>实际存在</strong>的事物的综合。</li></ul></li><li><p>关系：对立统一</p><ul><li><p>区别：一个潜在的，一个实际存在的。</p><p><em>现实是当下的客观存在，标志着事物的当前状况；可能是事物潜在的趋势，标志着事物的发展方向。</em> </p></li><li><p>联系：</p><ol><li>（现实包含可能、现实产生新的可能）一方面，现实蕴藏着未来的发展方向，会不断<strong>产生新的可能</strong>。</li><li>（可能 包含发展为现实的依据，条件成熟可以转化为现实）另一方面，可能包含着发展为现实的因素依据，一旦主客观条件成熟，可能就会<strong>转化为现实</strong>。</li></ol></li><li><p>区分可能性和不可能性的根据是：现实中是否有依据。</p><blockquote><p>如果现实有依据，则是可能性。</p><p>如果现实没有依据，则是不可能。</p></blockquote></li><li><p>区分现实的可能和抽象的可能（潜在的可能）的根据是：在现实中是否有充分的依据</p><blockquote><p>如果有充分的依据，则是现实的可能。</p><p>如果没有充分的依据，则是抽象的可能。</p></blockquote></li></ul></li><li><p>方法论：</p><ul><li>（立足现实，分析可能）一方面，要求人民立足现实，展望未来，对可能性作全面的分析和预判。</li><li>（防止坏的可能变成现象，争取好的可能变为现实）另一方面着远长远，防止坏的可能变为现实，同时善于创造条件，促使好的可能获得实现。</li></ul></li></ul><h3 id="19-现象和本质"><a href="#19-现象和本质" class="headerlink" title="19.现象和本质"></a>19.现象和本质</h3><blockquote><p>考试重要</p></blockquote><ul><li><p>现象和本质：解释客观事物外部表现和内在联系的一对范畴。</p><ul><li>本质：事物内在关系和根本性质，只有靠人的理性思维才能把握。</li><li>现象：事物的外部联系和表面特性，通过人的感官直接感知</li></ul></li><li><p>关系：对立统一</p><ul><li><p>区别：</p><ol><li>现象是个别的、具体的，本质是一般的、普遍的。</li><li>现象是多变的，本质是相对稳定的。</li><li>现象是生动的、丰富的，本质是深刻单纯的。</li></ol><ul><li><p>现象有真象和假象之分。</p><p><em>科学的任务就在于准确辨别真象和假象</em></p><ul><li><p>真象和假象都是现象，都是事物外部联系和表面特性，都通过人的感官直接感知。</p><blockquote><p>都是客观存在的反映，没有正确和错误之分。</p></blockquote></li><li><p>真象是正面直接的反映本质，假象是侧面歪曲的反映本质。</p></li></ul></li><li><p>假象和错觉不是一回事。</p><ul><li>假象也是现象，是客观事物的外在表现，而错觉是错误的感觉，是主观的。</li></ul></li></ul></li><li><p>联系：</p><ul><li>本质决定现象，本质总是通过一定的现象表现自己的存在；</li><li>现象表现本质，现象的存在和变化归根到底依赖于本质。假象也是本质的表现。</li><li>不表现为现象的本质和不表现本质的现象是不存在的。是错觉。</li></ul></li></ul><blockquote><p>例：</p><ol><li>错觉一定是被假象迷惑。错误。（错在一定）</li><li>真象是正确的现象，错觉是错误的感觉。错误。（现象是本质的表现，是客观的，没有正确错误之分，后半句正确）</li><li>真象往往隐藏在内部，假象往往外露在外部。错误。（真象、假象都是现象，都是事物本质的外在表现，本质才是在事物内部）</li></ol></blockquote></li><li><p>方法论：</p><ul><li>（现象和本质有联系）现象和本质是<strong>统一</strong>的，所以我们<strong>能够通过</strong>现象认识事物的本质。</li><li>（现象和本质是有区别的）又因为现象和本质是<strong>对立</strong>的，所以要求我们不能停留在现象中，要透过现象，解释本质。</li></ul></li></ul><h3 id="20-内容和形式"><a href="#20-内容和形式" class="headerlink" title="20.内容和形式"></a>20.内容和形式</h3><ul><li><p>内容和形式：从构成要素和表现方式反映事物的一对基本范畴。</p><ul><li>内容：构成事物的一切要素的总和，是事物存在的基础。</li><li>形式：把诸要素统一起来的结构或表现内容的方式。</li></ul></li><li><p>关系：相互依赖、不可分割</p><ul><li><p><strong>任何事物都是内容与形式的统一。</strong>任何事物的内容都有一定的形式，任何形式也都有一定的内容，没有无内容的空洞形式，也没有无形式的纯内容。</p></li><li><p><strong>内容决定形式，形式反作用于内容。</strong>当形式适合内容的时候，对内容的发展起积极的推动作用；当形式不适合内容的时候，对内容的发展起消极的阻碍作用。</p><blockquote><p>生产力是社会生产的内容。生成关系是社会生产的形式。</p><p>当生产关系适应生产力发展时，会推动生产力。当生产关系不适应生产力时，就会阻碍生产力。</p></blockquote></li></ul></li><li><p>方法论：</p><ul><li>（注重内容，反对形式主义）在我们的认识和实践中，根据内容决定形式的原理，注重事物的内容，反对忽视内容、夸大形式作用的形式主义；</li><li>（利用形式促进内容的发展）又要积极利用合适的形式促进内容的发展，不能忽视形式对内容的能动促进作用。</li></ul></li></ul><h2 id="辩证法-三大规律"><a href="#辩证法-三大规律" class="headerlink" title="辩证法-三大规律"></a>辩证法-三大规律</h2><p>三大规律：</p><ol><li>对立统一规律：事物联系的内容和发展的动力<ul><li>唯物辩证法的实质和核心</li><li>同一性和斗争性的辩证关系原理</li><li>同一性和斗争性在事物发展中的作用原理</li><li>普遍性和特殊性的辩证规律关系原理</li><li>矛盾的不平衡发展原理</li><li>矛盾分析法（方法论总结）</li></ul></li><li>质量互变规律：事物发展过程中的状态</li><li>否定之否定规律：回答事物发展的方向和最终归属<ul><li>辩证否定观</li><li>否定之否定规律</li><li>方法论</li></ul></li></ol><h3 id="21-对立统一规律（很重要）"><a href="#21-对立统一规律（很重要）" class="headerlink" title=" 21.对立统一规律（很重要）"></a><font color='red'> 21.对立统一规律（很重要）</font></h3><ul><li><p>唯物辩证法的实质和核心：对立统一规律</p><blockquote><p>为什么怎么说</p></blockquote><ol><li>对立统一规律揭示了事物普遍联系的根本内容和变化发展的内在动力，从根本上回答了事物为什么会发展的问题。</li><li>对立统一规律贯穿质变量变规律、否定之否定规律以及唯物辩证法基本范畴的中心线索。</li><li>对立统一规律提供矛盾分析法，他是对事物辩证认识的实质。</li><li>是否承认对立统一学说是唯物辩证法和形而上学对立的实质。</li></ol></li><li><p>矛盾：反映事物内部<strong>和</strong>事物之间对立统一关系的哲学范畴。</p><ul><li>矛盾的两种基本属性：对立和统一。<ul><li>矛盾的对立属性又称斗争性</li><li>矛盾的统一属性又称同一性</li></ul></li></ul></li></ul><h4 id="（1）同一性和斗争性的辩证关系原理"><a href="#（1）同一性和斗争性的辩证关系原理" class="headerlink" title="（1）同一性和斗争性的辩证关系原理"></a>（1）同一性和斗争性的辩证关系原理</h4><blockquote><p>同一性；斗争性；关系；方法论</p></blockquote><ul><li><p>矛盾的同一性：矛盾双方<strong>相互依存、相互贯通</strong>的性质和趋势。</p><ul><li><p>相互依存：对立面相互依存，互为存在的前提，共处一个统一体中。</p><blockquote><p>比如，没有上，就没有下。没有强就没有弱。</p><p>上下、强弱都是相互存在的前提。</p></blockquote></li><li><p>相互贯通：对立面相互贯通，<strong>在一定条件下可以相互转化</strong>。</p></li></ul></li><li><p>矛盾的斗争性：矛盾的对立面<strong>相互排斥、相互分离</strong>的性质和趋势。</p><ul><li><p>矛盾的性质不同，矛盾的斗争形式也不同</p><ul><li>对抗性矛盾</li><li>非对抗性矛盾</li></ul><blockquote><p>资本主义终将灭亡。</p><p>因为资本主义中间存在需要对抗性的激烈的矛盾，在对抗性矛盾运动中，资本主义走向灭亡。</p><p>而社会主义中间也存在矛盾，但是是非对抗性的人民内部的矛盾，可以通过民主集中制解决，不会走向灭亡。</p></blockquote></li></ul></li><li><p>同一性和斗争性的关系：对立统一</p><blockquote><p>矛盾的同一性和斗争性的辩证关系不是时而同一性时而斗争性的关系，而是每时每刻既同一又斗争的关系。</p></blockquote><p><em>矛盾的同一性和斗争性相结合，构成了事物的矛盾运动，推动着事物的变化发展</em> </p><ul><li><p>相互联系：矛盾的同一性和斗争性相互联结相辅相成。没有斗争性就没有同一性，没有同一性就没有斗争性，斗争性寓于同一性之中，同一性通过斗争性来体现。</p><blockquote><p>现实中，越相似的两个事物，同一性越强，斗争性就会越强。</p><p>因此，想要减少斗争性，那就要消除同一性。</p></blockquote></li><li><p>相互区别：在事物的矛盾中，矛盾的同一性是有条件的、相对的，矛盾的斗争性是无条件的、绝对的。</p></li></ul></li><li><p>方法论的意义：</p><blockquote><p>矛盾的同一性和斗争性是同时存在的，因此事物总是具有两面性，这要求我 们看待事物时要做到“一分为二”。例如，对待传统文化要“批判地继承”，对待外来文化应该 “批判地吸收”。</p></blockquote><ol><li>看问题要一分为二。（矛盾的同一性和斗争性是同时存在的）</li><li>求同存异（承认事物存在斗争性，追求同一性）</li><li>批判的继承（事物都有两面性）</li><li>事物之间会相互转化。（同一性和斗争性是相互转化的）</li></ol><blockquote><p>考：材料说明了同一性和斗争性的辩证关系原理。</p></blockquote></li></ul><h4 id="（2）同一性和斗争性在事物发展中的作用原理"><a href="#（2）同一性和斗争性在事物发展中的作用原理" class="headerlink" title="（2）同一性和斗争性在事物发展中的作用原理"></a>（2）同一性和斗争性在事物发展中的作用原理</h4><blockquote><p>同一性的作用；斗争性的作用；方法论</p></blockquote><ul><li><p>矛盾的同一性在事物发展中的作用：</p><ol><li><p>（相互发展）由于矛盾双方相互依存、互为存在的条件，矛盾双方可以利用对方的发展使自己得到发展。</p><blockquote><p>你好我也好。</p></blockquote></li><li><p>（各自发展）同一性可以使矛盾双方相互吸取有利于自身的因素，在相互作用中各自得到发展。</p><blockquote><p>相互吸取优点，各自发展的更好。</p></blockquote></li><li><p>（转化发展）由于矛盾双方彼此互通，矛盾双方可以向彼此的对立面转化而得到发展，并规定着事物的发展方向。</p><blockquote><p>你强我弱，变成我强你弱，向对立面转化了，但我的强不是原来你的强了，而是更强，因此是发展。</p></blockquote></li></ol></li><li><p>矛盾的斗争性在发展中的作用：</p><ol><li><p>（量变）矛盾双方的斗争促进矛盾双方力量的变化，竞长争高，此消彼长，造成事物的量变。</p></li><li><p>（质变）矛盾双方的斗争，促使矛盾双方的的地位和性质发生变化，实现事物的质变。</p><p><em>矛盾双方的斗争是一种矛盾统一体向另一种矛盾统一体过渡的决定力量</em> </p></li></ol></li><li><p>方法论：</p><ul><li><p>事物的发展不仅表现为“相辅相成”，而且表现为“相反相成”。</p><blockquote><p>矛盾的斗争性处于主要方面：“相反相成”是从事物的对立面也能达到统一，达到想要的效果。即逆向思维。</p><p>矛盾的同一性处于主要方面：“相辅相成”是从事物的同一性达到统一，达到想要的效果。</p></blockquote></li><li><p>学会从事物的对立面把握事物的统一，逆向思考；</p></li><li><p>正确把握和谐对事物发展的作用。和谐是对立统一，而不是无差别的一致。</p><p><em>和谐是矛盾的一种特殊表现形式，体现着，矛盾双方的相互依存、相互促进、共同发展，和谐并不意味着矛盾的绝对同一。</em> </p></li></ul></li></ul><h4 id="（3）矛盾的普遍性和特殊性的辩证关系原理"><a href="#（3）矛盾的普遍性和特殊性的辩证关系原理" class="headerlink" title="（3）矛盾的普遍性和特殊性的辩证关系原理"></a>（3）矛盾的普遍性和特殊性的辩证关系原理</h4><ul><li><p>矛盾的普遍性：（矛盾无处不在，矛盾无时不有）矛盾的普遍性是指矛盾存在与一切事物中，存在一切事物发展过程的始终，旧的矛盾解决了，新的矛盾又产生，事物始终在矛盾运动。</p></li><li><p>矛盾的特殊性：各个具体事物的矛盾、每一个矛盾的各个方面在发展的不同阶段上各有特点。</p><ol><li>不同事物的矛盾各有特点。</li><li>同一事物的矛盾在不同发展过程和发展阶段各有不同。</li><li>构成事物的 诸多矛盾 以及 每一矛盾的不同方面 各有不同的性质、地位和作用。</li></ol></li><li><p>关系：</p><ul><li>相互区别：<ul><li>矛盾的共性，即矛盾的普遍性，是无条件的、绝对的。</li><li>矛盾的个性，即矛盾的特殊性，是条件的、相对的。</li></ul></li><li>相互联系：任何现实存在的事物的矛盾都是共性和个性的有机统一，共性寓于个性之中，没有离开个性的共性，也没有离开共性的个性。</li></ul></li><li><p>方法论：</p><blockquote><p>考：材料体现矛盾的普遍性和特殊性的辩证关系原理</p></blockquote><ul><li><p>“具体问题具体分析“，对症下药，量体裁衣，因材施教。</p><p><em>只有具体分析矛盾的特殊性，才能认清事物的本质和发展规律，并采取正确的方法和措施去解决矛盾，推动事物的发展。</em> </p></li></ul></li></ul><h4 id="（4）矛盾的不平衡发展原理"><a href="#（4）矛盾的不平衡发展原理" class="headerlink" title="（4）矛盾的不平衡发展原理"></a>（4）矛盾的不平衡发展原理</h4><p><em>事物是由多种矛盾构成的。</em></p><ul><li><p>主要矛盾：主要矛盾是矛盾体系中处于支配地位，对事物发展、对事物发展起决定性作用的矛盾。</p></li><li><p>次要矛盾：次要矛盾是矛盾体系中处于从属地位、对事物的发展起次要作用的矛盾。</p><ul><li>每一对矛盾：<ul><li>主要方面：有一方处于支配地位，起主导地位，是矛盾的主要方面。</li><li>次要方面：被支配的一方则是矛盾的次要方面。</li></ul></li></ul></li><li><p>原理：事物的性质主要是由主要矛盾的主要方面决定的。</p></li><li><p>方法论：“两点论”与“重点论”相结合；看问题既要全面的看，又要抓关键，看主流。</p><ul><li>“两点论”（面面俱到）：分析事物的矛盾，不仅要看的矛盾双方的对立，而且还要看到矛盾双方的统一；还要看到矛盾体系中存在主要矛盾、矛盾的主要方面，而且还有看到次要矛盾、矛盾的次要方面。</li><li>“重点论”（突出重点）：把握主要矛盾、矛盾的主要方面，并以此作为解决问题的出发点。</li></ul><blockquote><p>考：材料体现矛盾的不平衡发展原理。</p></blockquote></li></ul><h4 id="矛盾分析法"><a href="#矛盾分析法" class="headerlink" title="矛盾分析法"></a>矛盾分析法</h4><blockquote><p>与矛盾有关的所有原理的所有方法论。</p><p>考：根据题干材料判断矛盾分析法。</p></blockquote><ul><li>从事物的对立面把握事物的统一，反向思考，逆向思维。2<ul><li>矛盾的同一性和斗争性在事物发展中的作用原理。</li></ul></li><li>物极必反；否极泰来；福祸相依的对立面把握事物的统一。1<ul><li>矛盾的同一性和斗争性的辩证关系原理。</li></ul></li><li>中庸、和谐不走极端的思考方法和态度。1<ul><li>矛盾的同一性和斗争性的辩证关系原理。</li></ul></li><li>具体问题具体分析，对症下药，量体裁衣。3<ul><li>矛盾的普遍性和特殊性的辩证关系的原理。</li></ul></li><li>求同存异；差异中谋求共识。1<ul><li>矛盾的同一性和斗争性的辩证关系原理。</li></ul></li><li>两点论和重点论；抓关键，看主流。4<ul><li>矛盾的不平衡发展原理。</li></ul></li></ul><h3 id="22-量变质变规律"><a href="#22-量变质变规律" class="headerlink" title="22.量变质变规律"></a>22.量变质变规律</h3><blockquote><p>概念；关系；方法论</p></blockquote><ul><li><p>质：事物成为其自身并区别于其他事物的内在规定性。</p><p>考点：</p><ul><li>认识质是<strong>认识和实践的起点和基础</strong>。</li><li>只有认识质，才能区别事物。</li></ul></li><li><p>量：事物的规模、程度、速度 以及 它的构成成分在空间上的排列组合等可以用数量关系表示的规定性。</p><blockquote><p>量：两种情况，一种是数量上的，一种是空间上的排列组合。</p></blockquote><ul><li><p>考点：量的意义：</p><ol><li><p>认识事物的量是认识的深化和精确化。</p><blockquote><p>只认识质是不够的，量才能深化认识，准确认识。</p></blockquote></li><li><p>只有正确了解事物的量，才能正确估计事物在实践中的地位和作用。</p></li></ol></li></ul></li><li><p>度：保持事物 质的稳定性的数量界限，即事物的限度、幅度和范围，度的两端叫关节点或临界点。</p><blockquote><p>度是一个区间，关节点/临界点时端点。</p></blockquote></li><li><p>量变：事物 数量的增减 <strong>和</strong> 次序的变动 ，是保持事物的质的相对稳定性的<strong>不显著变化</strong>，体现了事物渐进过程的<strong>连续性</strong>。 </p><blockquote><p>量变有两种情况，一种是数量上的变化，一种是空间排列次序的变动。</p></blockquote></li><li><p>质变：事物性质的 <strong>根本变化</strong> ，是事物由一种质态向另一种质态的飞跃，体现了事物渐进过程和连续性的<strong>中断</strong>。 </p></li><li><p>关系：对立统一</p><ul><li><p>相互区别：</p><ul><li>量变是事物质相对稳定的不显著变化，而质变是事物性质的根本变化。</li><li>量变是事物渐进过程的连续性，而质变是事物渐进过程和连续性的中断。</li></ul></li><li><p>相互联系:</p><ol><li><p>（必要准备）量变是质变的<strong>必要</strong>准备。</p><p><em>任何事物的变化都有一个量变的积累过程，没有量变的积累，质变就不会发生。</em> </p><blockquote><p>想要质变，就必须要量变。</p></blockquote><ul><li>激变论：夸大质变，否定量变，认为可以不通过量变产生质变。</li></ul></li><li><p>（必要结果）质变是量变的<strong>必然</strong>结果。</p><p><em>单纯的量变不会永远持续下去，量变达到一定的程度必然引起质变。</em> </p><blockquote><p>持续的量变一定会引起质变。</p></blockquote><ul><li>庸俗进化论：夸大量变，否定质变，认为可以一直量变下去，而没有质变。</li></ul></li><li><p>量变和质变是相互渗透的。</p><blockquote><p>事物的总量变认为是一个连续性过程，在某些点是阶段性的质变。</p><p>这些点的前后是旧质在量上的收缩和新质在量上的扩张。</p></blockquote><ul><li>一方面，在总的量变过程中有阶段性和局部性的部分质变。</li><li>另一方面，在质变过程中也有旧质在量上的收缩和新质在量上的扩张。</li></ul></li></ol></li><li><p>方法论：</p><ul><li><p>理论上的方法论：</p><blockquote><p>可以用来批判其他理论观点。</p></blockquote><ul><li>激变论：夸大质变，否定量变，认为可以不通过量变产生质变。</li><li>庸俗进化论：夸大量变，否定质变，认为可以一直量变下去，而没有质变。</li></ul></li><li><p>实践中的方法论：</p><blockquote><p>指导实践</p></blockquote><ul><li><p>适度原则。</p><blockquote><p>否则会引起质变。</p></blockquote></li><li><p>对社会主义初级阶段的认识。</p><blockquote><p>社会主义是质，初级阶段是量。</p></blockquote></li><li><p>改革、发展和稳定。</p><blockquote><p>改革、发展追求的是速度，但也要把握好度，保持稳定。</p></blockquote></li></ul></li></ul></li></ul></li></ul><h3 id="23-否定之否定规律"><a href="#23-否定之否定规律" class="headerlink" title="23.否定之否定规律"></a>23.否定之否定规律</h3><ul><li><p>概念</p><blockquote><p>万事万物都同时存在着肯定因素和否定因素</p></blockquote><ul><li><p>肯定因素：维持现存事物存在的因素。</p><blockquote><p>存在的原因是因为事物身上有肯定因素。</p></blockquote></li><li><p>否定因素：促使现存事物灭亡的因素。</p><blockquote><p>灭亡的原因是因为事物身上有否定的因素。</p></blockquote></li></ul></li><li><p><strong>辩证否定观</strong>：</p><ol><li><p>否定是事物的<strong>自我否定</strong>。</p><p><em>是事物内部矛盾运动的结果</em>。</p><blockquote><p>形而上学：认为是外在力量对事物进行消灭，错误的。</p></blockquote></li><li><p>否定是事物发展的环节。</p><p><em>是旧事物向新事物的转变，是从旧质到新质到飞跃。只有经过否定，旧事物才能向新事物转变</em> </p></li><li><p>否定是新旧事物联系的环节。</p><p><em>新事物孕育产生于旧事物，新旧事物是通过否定环节联系起来的。</em> </p></li><li><p>辩证否定的实质是 <strong>扬弃</strong> 。</p><p><em>新事物对旧事物既批判又继承，既克服其消极因素又保留其积极因素</em> </p><blockquote><p>形而上学：认为要么肯定一切，要么否定一切。错误的。</p></blockquote></li></ol><ul><li><p>对比：</p><table><thead><tr><th>辩证否定法观</th><th>形而上学否定观</th></tr></thead><tbody><tr><td>“自我否定”</td><td>外在力量对事物进行否定和消灭</td></tr><tr><td>”扬弃“</td><td>”要么肯定一切，要么否定一切“</td></tr></tbody></table></li></ul></li><li><p>否定之否定规律</p><ul><li><p>事物的辩证发展经过肯定-否定-否定之否定三个阶段。</p><ul><li>第一次否定：使矛盾初步解决，而处于否定阶段的事物仍然有片面性。</li><li>第二次否定：还需要经过再次否定，即否定之否定，实现对立面的统一，使矛盾得到根本解决。</li></ul><blockquote><p>每一个事物都是由许多矛盾组成的，一对矛盾，A方面和B方面组成这对矛盾。</p><p>否定是事物发展的环节，最开始事物处于A的方面，第一次否定，A - &gt; B 。</p><p>第二次发展，否定之否定，是更高形态的A，变成A‘。</p></blockquote></li><li><p>事物的辩证发展就是经过两次否定、三个阶段，形成一个周期。其中，否定之否定阶段仿<strong>佛是向原来出发点的“回复”，但这是在更高阶段的“回复”，是“扬弃”的结果。</strong> </p></li><li><p>事物的发展呈现出周期性，不同周期的交替使事物的发展呈现<strong>波浪式前进或螺旋式上升的趋势</strong>。</p></li></ul></li><li><p>方法论</p><ul><li><p>理论上的方法论：</p><ul><li>循环论：只看到回归，没有看到发展。</li><li>直线论：只看到发展，看不到回归。</li></ul></li><li><p>实践中的方法论：</p><p><em>否定之否定规律揭示了事物发展的前进性和曲折性</em> </p><ul><li><p>事物发展的前进性：前途是光明的</p><p><em>每一次否定都是质变，都把事物推进到新阶段；每一个周期都是开放的，前一个周期的终点是下一个周期的起点，不存在不被否定的终点。</em> </p></li><li><p>事物发展的曲折性：道路是曲折的</p><p><em>曲折性体现在回复性上，其中有暂时的停顿甚至是倒退。但是，曲折性终将为事物的发展开辟道路。表明，事物的发展是螺旋式上升的，而不是直线式前进的</em> </p></li></ul></li></ul></li></ul><h3 id="24-客观辩证法与主观辩证法"><a href="#24-客观辩证法与主观辩证法" class="headerlink" title="24.客观辩证法与主观辩证法"></a>24.客观辩证法与主观辩证法</h3><ul><li><p>客观辩证法：客观事物或客观存在的辩证法。</p><p><em>客观事物以相互作用、相互联系的形式呈现出的各种物质形态的辩证运动和发展规律。</em> </p><blockquote><p>大自然本身就存在的辩证法。</p></blockquote><ul><li>客观辩证法采取<strong>外部必然性形式</strong>  ，不以人的意志为转移，是物质世界本身的联系和发展。</li></ul></li><li><p>主观辩证法：人类认识和思维运动的辩证法。</p><p><em>以概念作为思维细胞的辩证思维运动和发展规律</em> </p><blockquote><p>是人头脑中思维的辩证法。</p></blockquote><ul><li>主观辩证法则是采取<strong>观念的、逻辑的形式</strong>，是同人类思维的自觉活动相联系的，是以概念为基础的辩证思维规律，是辩证法的科学体系。</li></ul></li></ul><blockquote><p>考：</p><p>客观辩证法是唯物的，主观辩证法是唯心的。（错误）</p><p>主观和客观辩证法是反映和被反映的关系，没有正确错误之分。</p><p>而唯物是正确的，唯心是错误的。</p></blockquote><h3 id="25-辩证思维方法"><a href="#25-辩证思维方法" class="headerlink" title="25.辩证思维方法"></a>25.<em>辩证思维方法</em></h3><ol><li>归纳与演绎</li><li>分析与综合</li><li>抽象与具体</li><li>历史与逻辑</li></ol><h2 id="认识论"><a href="#认识论" class="headerlink" title="认识论"></a>认识论</h2><p>框架：</p><ol><li>认识的来源和本质<ul><li>来源：实践</li><li>本质：主体在实践的基础上对客体的能动反映</li></ul></li><li>认识的过程和规律<ul><li>两次飞跃：感性认识到理性认识到飞跃；理论到实践的飞跃。</li><li>规律：反复性和无限性。</li></ul></li><li>认识的结果和检验标准</li></ol><p>物质和意识：物质决定意识，意识反作用于物质，意识对物质具有依赖性，又有相对独立性。</p><p>实践和认识：实践决定认识，认识反作用于实践，认识对实践具有依赖性，又有相对独立性。</p><p>实践是物质性活动。</p><p>认识是意识活动。</p><h3 id="26-实践的本质和特征"><a href="#26-实践的本质和特征" class="headerlink" title="26.实践的本质和特征"></a>26.实践的本质和特征</h3><ul><li><p>错误的实践观：</p><ul><li><p>中国古代哲学：实践被称为“践行””实行“或”行“与”知“相对于，（知行合一的行是实践），但主要是指<strong>道德伦理行为</strong>。</p><blockquote><p>错在道德伦理，太局限。不是道德伦理行为也是实践。</p></blockquote></li><li><p>康德：把实践看作<strong>理性自主</strong>的<strong>道德活动</strong>。</p><blockquote><p>错在道德活动。</p><p>错在理性自主，只强调主体，忽略客体。</p></blockquote></li><li><p>黑格尔：把实践理解为<strong>主体改造客观对象</strong>的创造性的<strong>精神活动</strong>。</p><blockquote><p>和康德相比，主体改造客观对象。</p><p>错在精神活动，实践是物质性活动，具有客观性。</p></blockquote></li><li><p>费尔巴哈：把实践与<strong>物质的活动</strong>联系起来，但他所理解的实践仅限于日常生活活动，并将实践等同于生物适应环境的活动。</p><blockquote><p>和黑格尔相比，认为实践是物质的活动。</p><p>错在日常活动，生理活动。实践只能是人的行为，动物没有实践。</p></blockquote></li></ul></li><li><p>马克思正确的实践观：实践是<strong>感性的</strong>、<strong>对象性</strong>的<strong>物质活动</strong>。</p><p><em>马克思科学阐明了人类实践的本质及其在认识世界和改造世界中的作用，创立了科学的实践观。</em> </p><p><em>他在《关于费尔巴哈的提纲》阐明了实践是感性的、对象性的物质活动，提出全部社会生活的本质是实践，并鲜明指出哲学家们只是用不同的方式解释世界，而问题在于改变世界。</em> </p><blockquote><p>感性的：是人的意识指导的，具有目的性和计划性。</p><p>对象性：实践有客体。</p><p>物质活动：实践是物质性活动，具有客观性。</p></blockquote></li><li><p>实践的本质含义：实践是<strong>人类</strong> <strong>能动地</strong>改造世界的社会性<strong>的物质活动</strong>。</p><blockquote><p>人类：实践是人独有的活动，动物没有实践。</p><p>能动的：和感性的相同，是由意识指导的，意识的反作用。</p></blockquote></li><li><p>实践的基本特征：直接现实性、自觉能动性/主体能动性、社会历史性。</p><blockquote><p>考：材料体现实践的特点。</p></blockquote><ul><li><p>直接现实性（实践最本质的特性）：实践具有将人脑中观念的存在变为现实的可能。</p><p><em>实践是改造世界的物质活动，不是纯粹的精神活动，是以感性事物为对象的现实的物质活动。</em> </p><p><em>实践所具有的直接现实性也就是实践活动的客观实在性。（物质的唯一特性）</em> </p><blockquote><p>能将头脑中的想法变成现实存在。</p></blockquote></li><li><p>自觉能动性/主体能动性：实践受意识的指导，体现主体的目的性。</p><p><em>与动物的本能的、被动的适应活动不同，人的实践活动是一种<strong>有意识、有目的</strong>的活动。</em> </p><blockquote><p>和动物的本能活动不同（包括人的本能活动：吃饭/睡觉），实践是有目的的。</p><p>因此人的活动中，本能活动不属于实践。</p></blockquote></li><li><p>社会历史性：不同历史阶段的实践内涵不同。</p><p><em>实践的内容、性质、范围、水平以及方式都收到一定社会条件的制约，随着一定社会历史条件的变化而变化。</em> </p><blockquote><p>比如现在人们可以通过互联网实践，虚拟实践。</p></blockquote></li></ul></li></ul><h3 id="27-实践的基本结构和形式"><a href="#27-实践的基本结构和形式" class="headerlink" title="27.实践的基本结构和形式"></a>27.实践的基本结构和形式</h3><ul><li><p>实践的基本结构：</p><ul><li><p>实践主体：指具有<strong>一定的主体能力、从事现实社会实践活动的人</strong>，是实践活动中自主性和能动性的因素，担负着设定实践目的、操作实践中介、改造实践客体的任务。 </p><blockquote><p>实践主体只能是人，但不是所有人都是实践主体，只有具有一定主体能力、从事现实实践活动的人才是主体。</p></blockquote><ul><li>实践主体的能力：自然能力和精神能力。<ul><li>自然能力：如力量大。</li><li>精神能力：知识性因素和非知识性因素。<ul><li>知识性因素：对理论知识的掌握，对经验知识的掌握。<ul><li>知识性因素是<strong>首要的</strong>实践主体的能力</li></ul></li><li>非知识性因素：情感和意志因素。</li></ul></li></ul></li><li>实践主体的基本形态：个体主体、群体主体、人类主体。</li></ul></li><li><p>实践客体：指<strong>实践活动所指向</strong>的对象。</p><p>实践客体和客观存在的事物不完全等同，客观事物只有在被纳入主体实践活动的范围之内，为主体实践活动所指向并与主体相互作用时才成为现实的实践客体 。</p><blockquote><p>实践客体是客观存在的事物，但不是所有客观存在的物都是实践客体，必需是实践活动所指向的，而像外太空、深海未被人所认知到的物都不是实践的客体。</p></blockquote></li><li><p>实践中介：各种形式的工具、手段 以及 运用、操作这些工具、手段的程序和方法。</p><p>实践的中介系统：</p><p><em>正是依赖这些中介系统，实践的主体和客体才能够<strong>相互作用</strong>。</em></p><ol><li><p>作为人的肢体延长、感官延伸、体能放大的物质性工具系统。</p><blockquote><p>火车、电脑、雷达分别是对人的腿、脑、眼功能的延伸和放大。</p></blockquote></li><li><p>语言符号工具系统。</p><p><em>语言符号是主体思维活动的现实形式，也是人们社会交往得以进行的中介。</em>  </p></li></ol></li></ul></li><li><p>主体和客体相互作用的关系：实践关系、认识关系和价值关系。</p><ul><li><p>实践关系是<strong>最根本</strong>的关系。</p><ul><li>实践的主体和客体与认识的主体和客体在本质上是一致的。</li></ul><blockquote><p>认识关系和价值关系都是基于实践关系。</p></blockquote></li><li><p>实践结构的变化</p><p><em>主体客体化和客体主体化的双向运动是人类实践活动两个不可分割的方面。</em> </p><ul><li><p><strong>主体客体化</strong>：人通过实践使自己的本质力量作用于客体，使其按照主体的需要发生结构和功能上的变化，形成了世界上本来不存在的对象。</p><p><em>实际上，人类一切实践活动的结果都是主体客体化的结果。</em> </p><blockquote><p>比如：人通过实践，使树变成纸张和筷子。</p></blockquote></li><li><p><strong>客体主体化</strong>：客体从客观对象的存在形式转化为主体生命结构的因素 或 本质力量的因素，客体失去客体性的形式，变成主体的一部分。</p><blockquote><p>比如：</p><p>主体把物质工具如电脑、汽车等作为自己身体器官的延长 包括 在主体的活动 属于客体主体化。</p><p>把作为精神性客体的精神产品、先进理念和思想 转化为 主体意识的一部分，属于客体主体化。</p></blockquote></li></ul></li></ul></li><li><p>实践的形式：</p><p><em>人类实践的具体形式日益多样化，从内容上看，实践可分为三种基本形式</em> </p><blockquote><p>注意：人的活动分为本能活动和实践活动，本能活动不属于实践活动，只有下列的劳动、搞关系、探索才算实践活动。</p></blockquote><ul><li><p>物质生产实践（<strong>最基本</strong>的实践活动）</p><blockquote><p>物质生成实践：就是劳动。</p></blockquote></li><li><p>社会政治实践：人们之间的社会交往和政治活动。</p><blockquote><p>社会政治实践：人和人之间搞关系。</p></blockquote></li><li><p>科学文化实践：创造精神文化产品的实践。</p><blockquote><p>科学社会实践：探索创新，比如科学、艺术、教育等。</p></blockquote></li><li><p>虚拟实践：实践活动的<strong>派生形式</strong>，具有<strong>相对独立性</strong>。</p><blockquote><p>虚拟实践是基于人类实践的三种形式：物质生产实践、社会政治实践和科学文化实践，只是实践的载体发生了变化，变成了网络世界。</p><p>虚拟实践是一般实践活动的派生。</p></blockquote><ul><li><p>特点：交互性、开放性、间接性。</p><blockquote><p>交互性：人与人之间。</p><p>开放性：互联网是共享的。</p><p>间接性：通过数字中介</p></blockquote></li></ul></li></ul></li></ul><h3 id="28-实践决定认识"><a href="#28-实践决定认识" class="headerlink" title="28.实践决定认识"></a><font color='red'>28.实践决定认识</font></h3><p><em>辩证唯物主义认为，在实践和认识之间，实践是认识的基础，实践在认识活动中起着决定性的作用。</em> </p><ul><li><p>实践在认识活动中的决定作用（实践决定认识的原因）</p><ol><li><p>实践是认识的来源（是唯一来源）</p><p>形成认识的因素中：</p><ul><li>实践是决定因素</li><li>天赋（生理因素）和间接经验等起影响作用，是重要因素</li></ul><blockquote><p>认识的形成是多因素的，实践是决定因素。</p><p>类似意识的形成：</p><p>意识也是多因素形成的，实践特别是劳动，对意识的形成起决定作用，语言也促进意识的形成。</p></blockquote></li><li><p>实践是认识的动力</p><blockquote><p>恩格斯：社会一旦有技术上的需要，这种需要就会比十所大学更能把科学推向前进。</p></blockquote></li><li><p>实践是认识的目的</p><p><em>不是为了认识而认识，其最终目的是为实践服务，指导实践</em> </p></li><li><p><strong>实践</strong>是检验认识真理性的唯一标准</p></li></ol></li></ul><blockquote><p>考：</p><p>认识总是滞后于实践。错在总是。</p><p>实践是认识的先导。错在先导，实践指导认识不对，实践是一种行为，应该是认识/意识指导实践。</p><p>实践高于认识。正确，实践决定认识，高于认识。</p><p>实践和认识是合一的。知行合一。正确。</p><p>（但王阳明的知行合一，是作为的，王阳明的行局限于道德伦理行为）</p></blockquote><h3 id="29-认识的本质"><a href="#29-认识的本质" class="headerlink" title="29.认识的本质"></a>29.认识的本质</h3><ul><li><p>各流派的认识论</p><ul><li><p>唯心主义先验论：从思想和感觉到物</p><ul><li>认识不是对事物的反映，而是先于事物的存在。</li></ul></li><li><p>唯物主义反映论：从物到思想和感觉</p><ul><li>先有客观事物，才有我们的认识，认识是对客观事物的反映</li><li>旧唯物主义机械反映论：直观的、消极被动的反映论</li><li>辩证唯物主义能动反映论：反映是一个能动的过程</li></ul><blockquote><p>区别在于机械反映论只有反应，即物到人的大脑中的直观反映，是什么就是什么，没有加工创造。</p><p>而辩证唯物主义能动反映是，先反映，再能动创造加工。</p></blockquote></li></ul></li><li><p>认识的本质：主体<strong>在实践基础上</strong>对客体的<strong>能动</strong>反映。</p><p>能动反映：</p><ol><li><p>认识的<strong>反映特性</strong>：具有反映客体内容的反映性特征</p><p>认识的反映特性指人的认识必然要  以客观事物为原型和摹本  ，在思维中再现或摹写客观事物的状态、属性和本质。</p><blockquote><p>（错误）虚幻的观念也是对事物本质的反映。错在本质，一切观念，即意识，都是对事物的反映，但不一定都是对事物的本质的反映，可能是对事物的现象、属性的反映。</p><p>回顾：本质和现象，现象反映本质，真象和假象都是对本质的反映。</p></blockquote></li><li><p>认识的能动反映具有<strong>创造性</strong> ：具有实践的主体能动的、创造的特征</p><p>认识是一种在思维中能动的、创造性的活动，而不是主观对客观对象简单、直接的描摹或照镜子式的原物映现。</p><blockquote><p>（错误）一切观念都是现实的模仿。错在模仿，认识是具有能动的创造性。</p></blockquote></li></ol><ul><li><p>认识的反映特性和能动的创造特性之间的关系：<strong>不可分割</strong></p><p><em>人的认识是反映性或摹写性与创造性的统一</em> </p><blockquote><p>反映是能动创造的反映，创造是在基于反映创造。</p></blockquote><ul><li>旧唯物主义直观反映论：只坚持认识的反映特性，看不到认识的能动和创造性。</li><li>唯心主义和不可知论：只坚持认识能动的创造性，使创造脱离反映论的前提。</li></ul></li></ul></li><li><p>能动反映论的两个优点/特点：</p><ol><li><p>把<strong>实践</strong>的观点引入认识论。</p></li><li><p>把<strong>辩证法</strong>应用于反映论考察认识的发展过程。</p><p><em>科学揭示认识过程中多方面的辩证关系，把认识看成一个由不知到可知、由浅入深的充满矛盾的能动的认识过程，全面揭示了认识过程的辩证特征。</em> </p></li></ol></li></ul><h3 id="30-认识过程的两次飞跃"><a href="#30-认识过程的两次飞跃" class="headerlink" title=" 30.认识过程的两次飞跃"></a><font color='red'> 30.认识过程的两次飞跃</font></h3><ol><li><p>从感性认识到理性认识（第一次飞跃）</p><blockquote><p>比第二次飞跃考的多</p></blockquote><ul><li><p>感性认识（通过感觉感官直接感受）：人们在实践基础上，由感觉器官直接感受到的关于事物的现象、事物的外部联系、事物的各个方面的认识。</p><ul><li>对象：事物的现象、事物的外部联系、事物的各个方面</li><li>形式：感觉、知觉和表象<ul><li>感觉：对客观事物的个别属性、个别方面的直接反映。</li><li>知觉：对客观事物外部特征的整体反映。</li><li>表象：人脑对过去感觉和知觉的回忆</li></ul></li><li>特点：直接性、具体性</li></ul></li><li><p>理性认识（需要抽象、归纳、总结）：人们借助抽象思维，在概括整理大量感性材料的基础上达到关于事物的本质、全体、内部联系和事物自身的规律性的认识。</p><ul><li>对象：事物的本质、全体、内部联系和事物自身的规律性的认识。</li><li>形式：概念、判断、推理<ul><li>概念：同类事物的一般特性和本质属性的概括和反映</li><li>判断：展开了的概念，是对事物之间的联系和关系的反映，是什么或不是什么</li><li>推理：形式上表现为判断和判断之间的关系</li></ul></li><li>特点：间接性和抽象性</li></ul></li><li><p>感性认识和理性认识的辩证关系：对立统一</p><ol><li><p>（感性到理性）感性认识有待发展和深化为理性认识</p></li><li><p>（理性依赖感性）理性认识依赖于感性认识</p></li><li><p>（相互渗透）感性认识和理性认识相互渗透、相互包含。</p><p><em>感性中有理性，理性中有感性，具有交融性。</em> </p></li></ol><ul><li><p>感性认识和理性认识的辩证统一关系是在实践的基础上形成的，也需要在实践中发展。</p><p>如果割裂辩证统一关系：</p><ul><li>教条主义-唯理论：否认感性认识而片面夸大理性认识。（否定实践）</li><li>经验主义：否认理性认识而片面夸大感性认识。</li></ul></li></ul><blockquote><p>考：选择题：材料中是认为<strong>感性认识更重要还是理性认识更重要</strong></p><ol><li>尽信书，则不如无书。感性认识。</li><li>饱经风霜的老人与缺乏阅历的少年对同一句格言的理解不同。感性认识。</li></ol><p>感性认识，即要实践。</p></blockquote></li><li><p>感性认识上升到理性认识到的条件：</p><ol><li>（实践）投身实践，深入调查，获取十分丰富和合乎实际的感性材料。</li><li>（思考能动性）必须经过思考的作用，运用理论思维和科学抽象，将丰富的感性材料加工制作。</li></ol></li></ul></li><li><p>从认识到实践（理性认识到实践的飞跃）</p><p><em>从认识到实践，是认识过程的第二个阶段，是第二次能动的飞跃，也是<strong>认识过程中</strong>最重要的一次飞跃</em> </p><p>重要性和必要性：一是认识世界的目的是为了改造世界。而是认识的真理性只有在实践中才能得到检验和发展。</p></li></ol><h3 id="31-认识过程中的理性因素和非理性因素"><a href="#31-认识过程中的理性因素和非理性因素" class="headerlink" title="31.认识过程中的理性因素和非理性因素"></a><font color='red'>31.认识过程中的理性因素和非理性因素</font></h3><blockquote><p>即形成认识，获得认识中的影响因素</p></blockquote><ol><li><p>理性因素：人的理性直观、理性思维的能力。</p><ul><li><p>在认识活动中的作用：指导作用、解释作用和预见作用。</p><blockquote><p>知识</p></blockquote></li></ul></li><li><p>非理性因素（感性因素）：人的情感和意志</p><ul><li><p>在认识过程中的作用：激活、驱动和控制作用</p><blockquote><p>联想、想象、猜测、直觉、灵感</p></blockquote></li></ul></li></ol><blockquote><p>区分感性认识和理性认识｜感性因素和理性因素：</p><p>感性认识是理性认识是认识过程已经结束，已经获得了认识。</p><p>感性因素和理性因素是在认识过程中的影响因素，还没有获得认识。</p><p>在感性认识/理性认识中都有感性因素和理性因素在起作用。</p></blockquote><h3 id="32-认识的规律"><a href="#32-认识的规律" class="headerlink" title="32.认识的规律"></a><font color='red'>32.认识的规律</font></h3><ul><li>认识过程的反复性（反复循环）：人们对于一个复杂事物的认识往往要经过由感性认识到理性认识、再由理性认识到实践的多次反复才能完成。<ul><li>原因：<ul><li>客观：事物暴露有一个过程</li><li>主观：主体认识能力提高有个过程</li></ul></li></ul></li><li>认识过程的无限性（无限发展）：事物发展过程的推移来说，人类的认识永无止境，无限发展，表现为“实践、认识、再实践、再认识”，由低级阶段向高级阶段不断推移的永无止境的前进运动。<ul><li>认识的无限发展过程：形式上是<strong>循环往复</strong>，实质上是<strong>前进上升</strong>。</li></ul></li><li>认识是一个波浪式前进和螺旋式上升的过程。（否定之否定在认识论中）</li></ul><h3 id="33-认识与实践的具体的历史统一性"><a href="#33-认识与实践的具体的历史统一性" class="headerlink" title="33.认识与实践的具体的历史统一性"></a><em>33.认识与实践的具体的历史统一性</em></h3><p><em>在实践和认识的辩证运动中，主观必须统一于客观，认识必须统一于实践</em> </p><ul><li><p>方法论</p><ol><li><p>实践超前于认识：冒进主义（左）</p><blockquote><p>比如跑步进入中国特色社会主义道德的观点，左派。</p></blockquote></li><li><p>实践落后于认识：保守主义（右）</p><blockquote><p>先学西方搞几百年资本主义，再搞社会主义的观点，右派。</p></blockquote></li></ol></li></ul><h2 id="认识的结果"><a href="#认识的结果" class="headerlink" title="认识的结果"></a>认识的结果</h2><h3 id="34-真理及其特点"><a href="#34-真理及其特点" class="headerlink" title="34.真理及其特点"></a><font color='red'>34.真理及其特点</font></h3><ul><li><p>错误的真理观：</p><ul><li><p>马赫主义：真理是“思想形式”，是社会性组织起来的经验，凡事大多数人承认的就是真理。</p><blockquote><p>错在，大多数人。</p></blockquote></li><li><p>实用主义：“有用即真理”，把真理的有用性与真理本身等同起来。</p><blockquote><p>错在，真理和有用等同起来。</p></blockquote></li></ul></li><li><p>正确的真理观：（马克思）真理是标志主观和客观相符合的哲学范畴，是对客观事物的<strong>正确</strong>反映。</p><blockquote><p>谬误：同客观事物及其发展规律相违背的认识（主客观不相符合），是对客观事物的错误/歪曲反映。</p></blockquote></li><li><p>真理的特点：</p><ol><li><p>客观性（<strong>真理的本质属性</strong>）：真理的内容是对客观事物及其规律的正确反映，真理中包含着不依赖人和人的意识的客观内容。</p><ul><li><p>客观性体现在：真理的内容和检验标准上</p><ul><li>真理的内容是客观的：对物质世界的正确反映</li><li>真理的检验标准是客观的：实践是物质性的活动。</li></ul></li><li><p>真理的形式是主观的：通过感觉、知觉、表象、概念、判断、推理等主观形式表达出来</p><blockquote><p>真理的内容是客观的，形式是主观的。真理是对客观事物及其规律的正确反映。</p><p>意识到内容是客观的，形式是主观的。意识是对物质世界的主观反映。</p></blockquote></li><li><p>真理的客观性决定了真理的<strong>一元性</strong>：在同一条件下对特定的认识客体的真理性认识只有一个，而不可能是多个。</p></li></ul></li><li><p>绝对性：真理<strong>主客观统一的确定性</strong>和<strong>发展的无限性</strong>。</p><ul><li><p>绝对性体现在：</p><blockquote><p>今天认为正确的东西，主客观的符合，是真理。</p><p>但明天认为昨天的不对，新的主客观符合，也是真理。</p></blockquote><ul><li>（承认了真理的客观性就是承认了真理的绝对性）任何真理都<strong>标志主观和客观的符合</strong>，包含着不依赖于人和人的意识的客观内容，都同谬误有原则的界限。这一点是绝对的、无条件的。</li><li>（承认世界的可知性，承认人能够获得关于无限发展的物质世界的正确认识，也就是承认了真理的绝对性）人类认识按其本性来说，能够正确认识无限发展着的物质世界，认识每前进一步，都是对无限发展着物质世界的接近，这一点也是绝对的、无条件的。</li></ul></li></ul></li><li><p>相对性：人们在一定条件下对客观事物及其本质和发展规律的正确认识总是有限度的、不完善的。</p><ul><li><p>相对性体现在：</p><blockquote><p>今天认为正确的东西，明天可能不对了。</p></blockquote><ul><li>（客观世界在发展，认识的广度有待扩展）客观世界的整体来看，任何真理都只是对客观世界的某一阶段、某一部分的正确认识，人类已经达到的认识的广度总是有限度的，<strong>认识有待扩展</strong>。</li><li>（一定程度，认识反映事物的深度有限）特定事物而言，任何真理只是对客观对象一定方面、一定层次和一定程度的正确认识，<strong>认识反映事物的深度是有限的</strong> </li></ul></li></ul></li></ol></li><li><p>真理的绝对性和相对性的关系：辩证统一</p><blockquote><p>辩证统一不是这个真理是绝对的，这个真理是相对的，而是每一个真理都有绝对性和相对性的一面。 </p></blockquote><ul><li><p>相互依存</p><blockquote><p>真理的绝对性是基于相对性的（一定条件下的），真理的相对性也是基于绝对性的（主观和客观的相符合）。</p></blockquote></li><li><p>相互包含：</p><p><em>没有离开绝对真理的相对真理，也没有离开相对真理的绝对真理</em></p><ul><li><p>真理的绝对性寓于真理的 相对性之中。</p><p><em>任何真理所包含的客观内容都只能是人们在特定条件下所把握到的，都是对客观世界及其事物的一定范围、一定程度的正确反映。</em> </p></li><li><p>真理的相对性必然包含并表现着真理的绝对性。</p><p><em>真理都是对无限发展着的物质世界的正确认识，包含着正确的客观内容</em> </p></li><li><p><strong>无数相对的真理的总和，就是绝对的真理。</strong> </p></li></ul></li><li><p>真理永远处在由相对向绝对的转化和发展中，是真理的相对性走向绝对性、接近绝对性的过程。</p><blockquote><p>真理发展的规律就是真理的相对性无限接近绝对性。</p></blockquote></li><li><p>真理的绝对性与相对性根源于人认识世界的能力的无限性（至上性）与有限性（非至上性）、绝对性与相对性的矛盾。</p><blockquote><p>总体来看，人是能够完全认识世界的，是人认识世界的能力的无限性，对应真理的绝对性。</p><p>但现在的人囿于一定条件还没有完全认识世界，是人认识世界的能力的有限性，对应真理的相对性。</p></blockquote></li></ul></li><li><p>方法论：</p><ul><li><p>教条主义：只看到真理的绝对性，忽视相对性。</p></li><li><p>诡辩论（怀疑主义）：只看到真理的相对性，忽视绝对性。</p><blockquote><p>在运动和静止中，诡辩论也是夸大运动，忽视静止。认为在一直运动。</p></blockquote></li></ul></li></ul><h3 id="35-真理与谬误"><a href="#35-真理与谬误" class="headerlink" title="35.真理与谬误"></a><font color='red'>35.真理与谬误</font></h3><ul><li><p>真理与谬误的关系：对立统一</p><p><em>真理和谬误的对立只是在非常有限的范围内才具有绝对的意义，超出这个范围，二者的对立就是相对的。</em> </p><ul><li>相互区别：在<strong>确定的对象和范围内</strong>，真理与谬误的对立是绝对的，与对象相符合的认识就是真理，与对象不相符合的认识就是谬误。在确定条件下，真理和谬误存在着原则界限。</li><li>相互联系：在一定条件下，真理和谬误能相互转化。<ul><li>真理和谬误在一定范围内的对立是绝对的，但超出一定范围，真理和谬误就会相互转化，真理变成谬误，谬误变成真理。</li></ul></li></ul></li></ul><h3 id="36-真理的检验标准"><a href="#36-真理的检验标准" class="headerlink" title="36.真理的检验标准"></a><em>36.真理的检验标准</em></h3><ul><li><p>实践是检验真理的唯一标准这是由真理的本性和实践的特点决定的。</p><ul><li><p>真理的本性：主观和客观相符合。</p><p><em>真理是人们对客观事物及其发展规律的正确反应，他的本性在于主观和客观相符合。</em> </p><blockquote><p>实践是连接物质和意识的桥梁，只有通过实践才能判断主客观是否相符合。</p></blockquote></li><li><p>实践的本质特点：直接现实性。</p><p><em>实践是人们改造世界的客观的物质性活动，具有直接现实性的特点。</em> </p></li></ul></li><li><p>实践是标准，并不排斥逻辑证明的作用。</p><blockquote><p>不排斥，但实践是唯一标准。</p></blockquote></li><li><p>实践标准的确定性和不确定性</p><blockquote><p>实践标准的确定性和不确定性是说实践标准既有确定性又有不确定性，而不是这个实践标准是确定的，另一个实践标准是不确定的。</p></blockquote><ul><li>实践标准的确定性<ol><li>检验真理的唯一标准</li><li>不可推翻</li><li>即使当前不能检验，但最终能裁决</li></ol></li><li>实践标准的不确定性<ol><li>一定时期的实践受到主客观的制约具有局限性，不能完全证明或驳倒一切</li><li>实践检验真理不是一次完成的</li><li>已被检验的仍需接受再检验。</li></ol></li></ul></li></ul><h3 id="37-真理与价值的辩证统一"><a href="#37-真理与价值的辩证统一" class="headerlink" title="37.真理与价值的辩证统一"></a>37.真理与价值的辩证统一</h3><ul><li><p>价值：价值是指在实践基础上形成的主体和客体之间的意义关系，是客体对个人、群体乃至整个社会的生活和活动所具有的积极意义。</p><blockquote><p>价值：客体对…的意义</p></blockquote></li><li><p>价值的特点</p><blockquote><p>考：材料体现了价值的什么特点</p></blockquote><ul><li><p>客观性：客体对于主义的意义不依赖于主体的主观意识存在。</p><blockquote><p>所有的客观性：不依赖于人的意识存在</p></blockquote></li><li><p>主体性：主体不同，价值不同</p><p><em>主体性是指价值直接同主体相联系，始终以主体为中心</em></p><blockquote><p>主体性不等同于主观性。</p></blockquote></li><li><p>多维性：维度不同，价值不同。</p></li><li><p>社会历史性：历史时期不同，价值不同。</p></li></ul></li><li><p>价值评价（价值判断）：主体对客体的价值以及价值大小所作的评判或判断。</p><blockquote><p>认识分为两种，一种是知识性认识，一种是评价性认识。</p><p>知识性认识：以客体为对象，主要是是什么。</p><p>评价性认识：以主客体的关系为对象，比如主体喜不喜欢客体等。</p></blockquote></li><li><p>价值评价的特点：</p><ol><li><p>评价以主客体的价值关系为认识对象。</p></li><li><p>评价结果与评价主体直接相关。</p></li><li><p>评价结果的正确与否 依赖于对客体状况和主体需要的认识。</p><blockquote><p>评价性认识依赖于知识性认识。</p><p>即知识越多，评价越好，评价认识越充分。</p><p>（充分了解后，再来说喜欢与否。）</p></blockquote></li></ol><ul><li><p>价值评价的特点表明，评价并不是一种主观随意性的认识，而是<strong>具有客观性的认识活动。</strong> </p><p><em>评价作为一种价值评判活动，虽具有主观性，但不是一种主观随意性的认识，只有正确反映价值关系的评价才是正确认识。</em> </p><p><em>对于任何价值评价的主体而言，其价值评价只有与人类整体的要求或理由相一致，才是正确的价值评价。</em> </p></li></ul></li><li><p>真理和价值在实践中的辩证统一</p><ul><li><p>人们的实践活动总是受着真理尺度和价值尺度的制约。</p><ul><li><p>实践的真理尺度是指在实践中人们必须遵循正确反映客观事物本质和规律的真理。</p></li><li><p>实践的价值是指在实践中人们都是按照自己的尺度和需要去认识世界和改造世界。</p><p><em>这一尺度体现了人的活动的目的性</em> </p></li></ul></li><li><p>任何实践活动都是在这两种尺度共同制约下进行的，任何<strong>成功的实践都是真理尺度和价值尺度的统一。</strong></p></li><li><p>紧密联系、不可分割的辩证统一关系</p><ol><li><p>价值尺度必须以真理为前提。真理又必然是具有价值的。</p><blockquote><p>真理必然有价值，有价值的不一定是真理。</p><p>真理一定有用，有用的不一定是真理。</p></blockquote></li><li><p>人类自身需要的内在尺度，推动着人们不断发现新的真理。</p></li></ol></li></ul></li></ul><h3 id="38-认识世界和改造世界必须勇于创新"><a href="#38-认识世界和改造世界必须勇于创新" class="headerlink" title="38.认识世界和改造世界必须勇于创新"></a><em>38.认识世界和改造世界必须勇于创新</em></h3><h3 id="39-自由和必然"><a href="#39-自由和必然" class="headerlink" title="39.自由和必然"></a>39.自由和必然</h3><blockquote><p>自由：人想怎样就怎样。</p><p>必然：客观规律，该怎样就怎样。</p></blockquote><ul><li><p>自由：标示人的活动状态的范畴，是指人在活动中 通过认识和利用必然 所表现出的一种自觉自主的状态。<strong>自由是对必然的认识和对客观世界的改造。</strong></p></li><li><p>必然：必然性即规律性，指的是不依赖于人的意识而存在的自然和社会发展所固有的客观规律。</p></li><li><p>认识必然和争取自由，是人类认识世界和改造世界的根本目标，是一个历史性的过程。</p><p>由必然到自由的表现为人类不断从必然王国向自由王国发展的 历史。</p><blockquote><p>以前只能顺应自然的一切规律。是必然王国。</p><p>现在通过认识和利用必然，能做一些人类想做的事，是向自由王国的发展。</p></blockquote></li><li><p>自由是有条件的：</p><ol><li>认识条件：要有对客观事物的正确认识，最主要的是对客观事物运动发展规律性和必然性的正确认识。</li><li>实践条件：能够将获得的规律性认识运用于指导实践，实现改造世界的目的，才是真正的自由。</li></ol></li></ul><h2 id="唯物史观"><a href="#唯物史观" class="headerlink" title="唯物史观"></a>唯物史观</h2><p>人类社会历史</p><ol><li>社会发展的物质动因</li><li>人民群众创造历史</li></ol><h3 id="40-唯物史观和唯心史观的对立"><a href="#40-唯物史观和唯心史观的对立" class="headerlink" title="40.唯物史观和唯心史观的对立"></a>40.唯物史观和唯心史观的对立</h3><ul><li>唯心史观的缺陷：<ul><li>只看到历史发展背后的精神力量，没有看到精神力量背后的 <strong>物质动因</strong> 和 <strong>经济根源</strong> 。</li><li>只看到历史发展中少数英雄人物的力量，而没有看到人们群众在社会历史发展中的决定作用。</li></ul></li></ul><h3 id="41-社会存在和社会意识及其辩证关系"><a href="#41-社会存在和社会意识及其辩证关系" class="headerlink" title="41.社会存在和社会意识及其辩证关系"></a>41.社会存在和社会意识及其辩证关系</h3><blockquote><p>社会存在和社会意识是社会历史领域的物质和意识</p></blockquote><ul><li><p>社会存在：社会物质生活条件</p><ul><li><p>自然地理环境：影响因素，<strong>非决定力量</strong></p></li><li><p>人口因素：影响因素，<strong>非决定力量</strong></p><blockquote><p>考：是影响因素，而非决定力量</p><p>低级概念，夸大说是决定力量，拉低说是没有作用</p></blockquote></li><li><p>物质生产方式（物质生活的生产方式，生产方式）：<strong>决定力量</strong></p><p><em>是指人民为获取物质生活资料而进行的生产活动的方式，它是生产力和生产关系的统一体。</em></p><ul><li><p>生产力 + 生产关系 = 生产方式</p></li><li><p>生产力</p><ul><li>劳动资料：生产工具是生产力发展水平的标准</li><li>劳动对象：与劳动资料合成为生产资料</li><li>劳动者：生产力中最活跃的因素</li></ul></li><li><p>生产关系</p><ul><li>生产资料所有制关系：最基本内容</li><li>生产中人与人的关系</li><li>产品分配关系</li></ul></li></ul></li></ul></li><li><p>社会意识：社会生活的精神方面，是社会存在的反映</p><p><em>根据不同的层次：</em></p><ul><li><p><strong>社会心理</strong>：低层次的社会意识，是自发的、不系统的、不定型的社会意识</p><ul><li>表现为：人民的感知、情绪、情感、心态、习俗</li></ul></li><li><p><strong>社会意识形式</strong>：高层次的社会意识，是自觉的、系统的、定型的社会意识</p><p><em>社会意识形式以社会心理为基础，并对社会心理起指导和影响作用。</em> </p><ul><li>包括：政治法律思想、道德、艺术、宗教、哲学、科学，以理性认识为主。</li></ul><blockquote><p>社会意识形态是和阶级有关的，不同的阶级社会意识形态不同。</p><p>非社会意识形态和阶级无关，不同的阶级非社会意识形态相同。</p></blockquote><ul><li><strong>社会意识形态</strong>：反映社会的经济关系、阶级关系的社会意识形式<ul><li>包括：政治法律思想、道德、艺术、宗教、哲学</li></ul></li><li><strong>非社会意识形态</strong>：不具有社会经济形态和政治制度的性质<ul><li>包括：自然科学、语言学、形式逻辑、心理学</li></ul></li></ul></li></ul></li><li><p>社会存在和社会意识的辩证关系</p><p><em>社会存在和社会意识是辩证统一的。社会存在决定社会意识，社会意识是社会存在的反映。</em> </p><ul><li><p>社会存在决定社会意识</p><ol><li><p>社会存在是社会意识内容的客观来源，社会意识是社会物质生活过程及其条件的主观反映。</p><blockquote><p>物质是意识内容的客观来源，意识是物质的主观反映。</p></blockquote></li><li><p>社会意识是人们进行社会物质交换的产物。</p><p><em>社会意识同语言一样，是在生产中由于交往活动的需要而产生的。</em> </p></li><li><p>随着社会存在的发展，社会意识也相应地<strong>或早或迟地</strong>发生变化和发展。</p><blockquote><p>考：将或早或迟改为一定立刻</p></blockquote><blockquote><p>社会存在发展，社会意识也在变，但不一定是一致的、同步的变化，而是不一致不同步。</p></blockquote></li></ol></li><li><p>社会意识反作用与社会存在</p><p>社会意识的相对独立性：</p><ol><li><p>社会意识与社会存在发展的不完全同步性和不平衡性</p><blockquote><p>社会意识和社会存在的发展不同步、不平衡，有些经济发展水平较高的国家社会意识水平未必很高。</p></blockquote></li><li><p>社会意识内部各形式之间的相互作用 及 各自具有的历史继承性</p><blockquote><p>社会意识形式中各要素相互作用社会意识。</p><p>但有些社会意识还具有历史继承性，比如一些封建残余观念。</p></blockquote></li><li><p>社会意识对社会存在能动的反作用。（双向）</p><p><em>先进的社会意识促进社会发展，落后的社会意识阻碍社会发展。</em> </p></li></ol></li></ul></li></ul><h3 id="42-生产力与生产关系矛盾运动的规律"><a href="#42-生产力与生产关系矛盾运动的规律" class="headerlink" title="42.生产力与生产关系矛盾运动的规律"></a>42.生产力与生产关系矛盾运动的规律</h3><ul><li><p><strong>生产力</strong>：人们解决社会同自然矛盾的实际能力，是人类在生产实践中形成的改造和影响自然以使其适合社会需要的<strong>物质力量</strong> 。</p><ul><li><p>它表示<strong>人和自然</strong>的关系</p></li><li><p>生产力基本要素：</p><ul><li><strong>劳动资料</strong>（劳动手段）：人们在劳动过程中所运用的物质资料或物质手段<ul><li>最重要的是<strong>生产工具</strong>，他是生产力发展水平的<strong>客观尺度</strong>，是区分<strong>社会经济时代</strong>的<strong>客观依据</strong>。</li></ul></li><li><strong>劳动对象</strong><ul><li>劳动资料+劳动对象 = 生产资料 </li></ul></li><li><strong>劳动者</strong> 劳动者是生产力中<strong>最活跃</strong>的因素：</li></ul></li><li><p>科学技术日益成为生产发展的<strong>决定性因素</strong> </p><blockquote><p>注意：</p><p>科学技术属于生产力，但不是生产力的独立的基本要素，科学技术是与生产力的三个基本要素结合，发挥作用。</p><p>考：问生产力的因素还是生产力的独立要素/基本要素。</p><p>（正确）生产力在生产劳动中起决定作用。</p><p>（错误）生产力在社会历史中起决定作用。</p></blockquote><ul><li>科学技术是先进生产力的集中体现和主要标志，是第一生产力</li></ul></li></ul></li><li><p>生产关系：人们在物质生产过程中形成的<strong>不以人的意志</strong>为转移的经济关系。</p><blockquote><p>不以人的意志为转移，生产关系是物质</p></blockquote><p><em>生产关系是社会关系中最基本的关系，政治关系、家庭关系、宗教关系等其他社会关系，都受生产关系的支配和制约。</em> </p><ul><li><p>包括：</p><ul><li><p><strong>生产资料所有制关系</strong></p><ul><li><p>生产资料所有制是<strong>最基本、具有决定</strong>意义的方面。</p></li><li><p>它构成全部社会关系<strong>的基础</strong>，是区分不同生产方式、判定<strong>社会经济结构</strong>的<strong>客观依据</strong>。</p><blockquote><p>生产工具：区分社会经济时代的客观依据。</p><p>生产资料所有制关系：判定社会经济结构的客观依据。</p></blockquote></li></ul></li><li><p><strong>生产中人与人的关系</strong></p></li><li><p><strong>产品分配关系</strong></p></li></ul></li><li><p>生产关系是一种<strong>人和人的关系</strong>，但他在物质生产过程中结成的关系，是不以人的意志为转移。 </p><blockquote><p>生产力：人和自然的关系，是物质。</p><p>生产关系：人和人的关系，是物质。</p></blockquote></li></ul></li><li><p>生产力和生产关系的关系：不可分割</p><p><em>生产力和生产关系是社会生产不可分割的两个方面</em> </p><ul><li><p>在社会生产中：生产力是生产的<strong>物质内容</strong>，生产关系是生产的<strong>社会形式</strong>，二者的有机结合统一构成<strong>社会的生产方式</strong>。</p><blockquote><p>在辩证法的内容和形式的范畴中：</p><p>内容决定形式，形式反作用与内容，当形式适合内容，对内容发展起积极作用，当形式不适合内容是，对内容发展起消极作用。</p></blockquote><ul><li><p>生产力决定生产关系</p></li><li><p>生产关系反作用与生产力（双向）</p><p><em>当生产关系适合生产力发展的客观要求时，对生产力的发展起推动作用；当生产关系不适合生产力发展的客观要求时，就会阻碍生产力的发展。</em> </p></li></ul></li></ul></li><li><p><strong>社会发展第一规律：生产关系一定要适应生产力发展状况的规律</strong></p><blockquote><p>形式要适应内容。</p></blockquote></li></ul><h3 id="43-经济基础与上层建筑矛盾运动的规律"><a href="#43-经济基础与上层建筑矛盾运动的规律" class="headerlink" title="43.经济基础与上层建筑矛盾运动的规律"></a>43.经济基础与上层建筑矛盾运动的规律</h3><ul><li><p>经济基础：是指 由社会一定发展阶段的生产力  <strong>所决定的生产关系</strong>的总和</p><blockquote><p>经济基础是生产关系。</p><p>经济基础和生产关系是同级概念，生产力决定生产关系，生产力也决定 经济基础。</p></blockquote></li><li><p>上层建筑：建立在一定的经济基础之上的  <strong>意识形态</strong>  以及  <strong>相应的制度、组织和设施</strong> </p><ul><li><p>组成</p><blockquote><p>区分：</p><p>观念上层建筑是无形的，政治上层建筑是有形的。</p></blockquote><ul><li><p><strong>意识形态</strong>（<strong>观念上层建筑</strong>）</p><ul><li><p>包括政治法律思想、道德、意识、宗教、哲学等思想观念。</p><blockquote><p>辨析：社会意识中的意识形态和上层建筑中的意识形态</p><p>社会意识中的意识形态和上层建筑中的意识形态是同一个东西</p><ul><li>社会意识：<ul><li>社会心理</li><li>社会意识形式<ul><li><strong>社会意识形态</strong></li><li>非社会意识形态</li></ul></li></ul></li><li>上层建筑：<ul><li><strong>意识形态（观念上层建筑）</strong></li><li>政治上层建筑</li></ul></li></ul><p><strong>属于上层建筑的意识形态是社会意识形态。</strong> </p></blockquote></li></ul></li><li><p><strong>政治法律制度及设施 和 政治组织</strong>（<strong>政治上层建筑</strong>）</p><ul><li><p>包括：国家政治制度、立法司法制度和行政制度，以及国家政权机构、政党、军队、警察、法庭、监狱等政治组织形态和设施</p></li><li><p>国家</p><p><em>国家不是从来就有的，而是社会发展到一定历史阶段的产物。</em> </p><p><em>这种从社会中产生但又自居于社会之上并且日益同社会相异化的力量，就是国家。</em> </p><ul><li><p>产生：</p><ul><li>国家是按照地域来划分国民 的，而不再以血缘关系来划分。</li><li>国家依靠强制性或暴力手段以及征收赋税来维系。</li><li>国家是阶级矛盾不可调和的产物</li></ul></li><li><p>实质：一个阶级统治另一个阶级的工具，具有政治统治和社会管理职能的有组织的力量。</p><ul><li>它是经济上占支配地位的阶级为维护其根本利益而建立起来的强制性暴力机关，以保证其在政治上也成为统治阶级。</li><li>国家和社会完全统一之日，也就是国家消亡之时。</li></ul></li><li><p>国体：社会各阶级在国家的地位，国家政权掌握在哪个阶级手中。</p></li><li><p>政体：统治阶级实现其阶级统治的具体组织形式，统治阶级采取什么样的形式去组织自己的政权，实现自己的统治。</p></li><li><p>国体决定政体，政体服务于国体。</p></li></ul></li></ul></li></ul></li><li><p>上层建筑中，政治上层建筑占<strong>主导地位</strong>，国家政权是<strong>核心</strong>。 </p><blockquote><p>政治上层建筑可以主导 观念上层建筑，决定法律思想等。</p></blockquote></li></ul></li><li><p>经济基础和上层建筑的关系中</p><blockquote><p>辩证统一关系与生产力和生产关系的矛盾相同</p></blockquote><ul><li><p>经济基础<strong>决定</strong>上层建筑</p></li><li><p>上层建筑对经济基础具有反作用</p><ul><li>反作用集中表现在 为自己的经济基础服务。上层建筑的反作用是巨大的，但不是无限的，它可以<strong>影响</strong>社会性质和历史进程，但<strong>不能决定</strong>历史发展的总趋势。</li></ul><blockquote><p>生产力 决定 生产关系，经济基础 决定 上层建筑。</p><p>生产关系和经济基础是同级概念，因此，生产力 可以 决定 上层建筑， 生产关系可以决定上层建筑。</p></blockquote><ul><li><p>上层建筑反作用的性质，取决于它所服务的经济基础的性质，归根到底取决于它是否有利于生产力发展</p><p><em>上层建筑这种反作用的后果可能有两种：当他 为适合生产力发展要求的经济基础服务时，就成为推动社会发展的进步力量；反之，当它为落后的经济基础服务时，就成为阻碍社会发展的消极力量。</em> </p><blockquote><p>辨析：</p><p>判断生产关系是否先进：看生产关系是否适应生产力的发展。</p><p>判断上层建筑是否先进：看上层建筑 所服务的经济基础 是否适应生产力的 发展。</p><p>如果生产关系适应生产力的发展，生产力是先进的，经济基础也适应生产力发展，是先进的，那上层建筑服务于先进的经济基础，上层建筑就是先进的，推动社会发展。</p><p>反之，上层建筑 为落后的经济基础服务，即经济基础不适应于生产力，上层建筑阻碍社会发展。</p></blockquote></li></ul></li></ul></li><li><p><strong>社会发展的第二规律：上层建筑一定要适应经济基础发展状况的规律。</strong></p></li></ul><h3 id="44-社会形态更替的一般规律和特殊形式"><a href="#44-社会形态更替的一般规律和特殊形式" class="headerlink" title="44.社会形态更替的一般规律和特殊形式"></a>44.社会形态更替的一般规律和特殊形式</h3><p><em>马克思、恩格斯揭示的生产力与生产关系矛盾运动的规律和经济基础与上层建筑矛盾运动的规律，是<strong>人类社会发展的一般规律</strong>。</em> </p><ul><li><p>社会形态：关于社会运动的具体形式、发展阶段和不同质态的范畴，是 同生产力发展一定阶段相适应的 经济基础与上层建筑的统一体。</p><ul><li><p>社会形态 = 经济基础 + 上层建筑</p><blockquote><p>总结：</p><p>生产方式 = 生产力 + 生产关系</p><p>社会形态 = 经济基础 + 上层建筑</p><p>生产资料 = 劳动资料 + 劳动对象</p></blockquote></li><li><p>社会形态包括：社会的<strong>经济形态</strong>、<strong>政治形态和意识形态</strong> </p><blockquote><p>社会形态 = 经济基础 + 上层建筑</p><p>经济基础 对应 社会的经济形态，上层建筑（观念上层建筑 和 政治上层建筑） 对应政治形态和意识形态。</p></blockquote></li><li><p>马克思主义的<strong>社会形态范畴</strong> 深刻揭示了人类社会的本质结构及其发展的客观规律。</p><ul><li><p>全面的：</p><p>社会形态的内容是<strong>全面的</strong>，既包括经济基础，又包括上层建筑，两者缺一不可，犹如“骨骼”和“血肉”。<strong>经济基础是社会的“骨骼系统</strong>”，<strong>上层建筑是社会的“血肉系统</strong>”，上层建筑不过是经济基础的政治和思想的表现形态。</p><blockquote><p>骨骼 决定 血肉</p></blockquote></li><li><p>具体的：</p><p>社会形态是<strong>具体的</strong>，不是抽象的。</p></li><li><p>历史的：</p><p>社会形态是<strong>历史的</strong>，有它产生、发展和灭亡的过程。</p></li></ul></li></ul></li><li><p>社会形态更替的<strong>统一性和多样性</strong> </p><ul><li><p>社会形态更替的统一性：</p><p><em>社会历史可以划分为五种社会形态：原始社会、奴隶社会、封建社会、资本主义社会和共产主义社会（第一阶段是社会主义社会）</em> </p><ul><li>这五种社会形态的依次更替，是社会历史运动的一般过程和一般规律，表现社会形态更替的统一性。</li></ul></li><li><p>社会形态更新的多样性：</p><ul><li>有些国家在发展中经历了几种社会形态依次更替的典型过程，也有些国家在发展中超越了一个甚至几个社会形态而跨越式向前发展，甚至多种社会形态特征交叉渗透。</li><li>即使是同一种社会形态，在不同国家也会显现不同特点。</li></ul></li><li><p>社会形态更替统一性与多样性辩证关系</p><p><em>列宁指出：世界历史发展的一般规律，不仅丝毫不排斥个别发展阶段在发展的形式或顺序上表现出的特殊性，反而是以此为前提的。</em> </p></li></ul></li><li><p>社会形态更替的<strong>必然性</strong>与人们的<strong>历史选择性</strong> </p><blockquote><p>必然性，即客观必然性，是社会历史的客观规律。</p><p>人们的历史选择性，是能动的选择。</p></blockquote><ul><li>社会形态更替的必然性：社会形态依次更替的过程和规律是客观的，发展趋势是确定不移的。<ul><li>社会形态更替归根结底是社会基本矛盾运动的结果。</li><li>生产力与生产关系矛盾运动的规律性，从根本上规定了社会形态更替的客观必然性。</li></ul></li><li>人们的历史选择性：<ul><li>社会发展的客观必然性造成了一定历史阶段社会发展的基本趋势，为人们的历史选择提供了基础、范围和可能性空间。</li><li>社会形态更替的过程也是一个主观能动性与客观规律性相统一的过程。</li><li>人们的历史选择性归根结底社会人们群众的选择性，取决于人们群众的根本利益：<ul><li>取决于民族利益</li><li>取决于交往</li><li>取决于对历史必然性以及本民族特点的把握程度</li></ul></li></ul></li></ul></li><li><p>社会形态更替的前进性与曲折性</p><blockquote><p>否定之否定在社会形态更替上的应用。</p></blockquote><p><em>社会形态的更替还表现为历史的前进性与曲折性、顺序性与跨越性的统一。</em> </p><ul><li>社会形态更替的前进性、顺序性：五种社会形态依次演进的基本趋势，其历史过程是一个“扬弃”的过程。<ul><li>表明社会发展的总趋势是前进的。</li></ul></li><li>社会形态的曲折性、跨越性：每一次社会制度的变革，无不经过曲折反复的斗争。</li></ul></li></ul><blockquote><p>例题：</p><p>（错误）社会发展过程与自然界演变过程一样都是自觉的。</p><p>社会发展过程是自觉的，能动的，有人们的历史选择性。而自然界演变过程，没有人的参与，是自发的。</p><p>辨析：自觉和自发</p><p>自觉和自发是标志人们行为活动的觉悟程度的一对范畴。</p><p>同动物相比，人类一切有意思的活动都是自觉的。自然界的规律都是自发的。</p><p>而就人的活动本身而言，自发是指人们在社会活动中盲目地为历史必然性所支配。</p><p>（正确）人类总体历史进程是不可超越的。</p><p>总体，是站在人类历史发展规律的角度，即社会更替的统一性，是不可超越的。</p></blockquote><h3 id="45-社会基本矛盾在历史发展中的作用：社会发展的根本动力"><a href="#45-社会基本矛盾在历史发展中的作用：社会发展的根本动力" class="headerlink" title="45.社会基本矛盾在历史发展中的作用：社会发展的根本动力"></a>45.社会基本矛盾在历史发展中的作用：社会发展的根本动力</h3><ul><li><p>社会基本矛盾：生产力和生产关系、经济基础和上层建筑的矛盾</p></li><li><p>社会基本矛盾是社会发展的<strong>根本动力</strong> </p><p>她在历史发展中的作用主要表现在：</p><ol><li>生产力是社会基本矛盾运动中<strong>最基本的动力因素</strong>，是人类社会发展和进步的最终决定力量。 </li><li>社会基本矛盾特别是生产力和生产关系的矛盾，是“一切历史冲突的根源”，决定着社会中其他矛盾的存在和发展。</li><li>社会基本矛盾具有不同<strong>的表现形式</strong>和解决方式，并从根本上影响和促进社会形态的变化和发展。</li></ol></li></ul><h3 id="46-阶级斗争在阶级社会发展中的作用"><a href="#46-阶级斗争在阶级社会发展中的作用" class="headerlink" title="46.阶级斗争在阶级社会发展中的作用"></a><em>46.阶级斗争在阶级社会发展中的作用</em></h3><ul><li>阶级斗争是社会基本矛盾在阶级社会中的直接表现，是阶级社会发展的<strong>直接动力</strong>。 </li></ul><h3 id="47-社会革命在阶级社会发展中的作用"><a href="#47-社会革命在阶级社会发展中的作用" class="headerlink" title="47.社会革命在阶级社会发展中的作用"></a><em>47.社会革命在阶级社会发展中的作用</em></h3><ul><li>革命是实现社会形态更替的<strong>重要手段</strong> </li></ul><h3 id="48-改革的性质及其在社会发展中的作用"><a href="#48-改革的性质及其在社会发展中的作用" class="headerlink" title="48.改革的性质及其在社会发展中的作用"></a><em>48.改革的性质及其在社会发展中的作用</em></h3><ul><li>改革是推动社会发展的又一<strong>重要动力</strong></li></ul><h3 id="49-科学技术在社会发展中的作用"><a href="#49-科学技术在社会发展中的作用" class="headerlink" title="49.科学技术在社会发展中的作用"></a>49.科学技术在社会发展中的作用</h3><ul><li><p>科学技术革命是社会动力体系的一种<strong>重要动力</strong> </p><blockquote><p>考：夸大科学技术的作用，说根本动力，贬低科学技术的作用，没有影响。</p></blockquote><ol><li>现代科技革命推动<strong>生产方式</strong>的变革</li><li>现代科技革命推动<strong>生活方式</strong>的变革</li><li>现代科技革命推动<strong>思维方式</strong>的变革</li></ol></li></ul><h3 id="50-关于历史创造者的问题"><a href="#50-关于历史创造者的问题" class="headerlink" title="50.关于历史创造者的问题"></a><em>50.关于历史创造者的问题</em></h3><p><em>唯物史观和唯心史观的对立，在历史创造者问题上表现为群众史观和英雄史观的对立。</em> </p><ul><li><p>英雄史观：唯心史观从社会意识决定社会存在的基本前提，否认物质资料生产方式是社会发展的决定力量，抹杀人民群众的历史作用，宣扬少数英雄人物创作历史。</p></li><li><p>唯物史观考察历史创造者问题的原则：</p><ol><li><p>唯物史观立足于 <strong>现实的人</strong>及其本质 来把握历史的创造者。</p></li><li><p>唯物史观立足 <strong>整体的社会历史过程</strong>来探究历史创造者问题。</p></li><li><p>唯物史观从社会历史发展的<strong>必然性</strong>入手来考察和说明历史创造者及其活动。</p></li><li><p>唯物史观从人与历史关系的<strong>不同层次</strong> 考察谁是历史的创造者。</p><p><em>人与历史的关系具有类与历史、群体与历史、个体与历史三层关系。</em> </p></li></ol></li></ul><h3 id="51-现实的人及其本质"><a href="#51-现实的人及其本质" class="headerlink" title="51.现实的人及其本质"></a>51.现实的人及其本质</h3><ul><li><p>唯物史观认为：人不是 抽象的而<strong>是现实的</strong>，现实的人，不是处在某种虚幻的离群索居和固定不变状态的人，而是处在现实的、<strong>可以通过经验观察到的</strong>、在一定条件下进行的发展过程的人。</p></li><li><p><strong>人与动物</strong>相区别的层次上，<strong>人的本质在于劳动。</strong></p></li><li><p>从<strong>人与人</strong>层相区别的层次上，人的本质是一切社会关系的总和。</p><p><em>马克思指出：人的本质不是单个人所固有的抽象物，在其现实性上，它是一切社会关系的总和。</em> </p><blockquote><p>有许多社会关系，而我是我的原因是，我是这些社会关系的这个节点。</p></blockquote><ul><li>人的本质是社会属性，而不是自然属性。</li></ul></li></ul><h3 id="52-人民群众创造历史原理"><a href="#52-人民群众创造历史原理" class="headerlink" title="52.人民群众创造历史原理"></a>52.人民群众创造历史原理</h3><ul><li><p>人民群众：是一个历史范畴。</p><ul><li><p>从质上看，人民群众是指<strong>一切对社会历史发展起推动作用的人</strong>，从量上看，人民群众是指<strong>社会人口中的绝大多数</strong>。</p><blockquote><p>人民群众：推动顺应社会历史发展。</p><p>反动派：阻碍社会历史发展。</p></blockquote></li><li><p>人民群众的最稳定的主体部分始终是<strong>从事物质资料生产的劳动群众及其知识分子</strong>。</p></li></ul></li><li><p>人民群众创造历史的原因（为什么说人民群众创造历史，起决定作用）</p><ol><li><p>人民群众是社会<strong>物质财富</strong>的创造者。</p><p><em>人类社会赖以存在和发展的基础是物质资料的生产方式。</em></p></li><li><p>人民群众是社会<strong>精神财富</strong>的创造者。</p></li><li><p>人民群众是<strong>社会变革的决定力量</strong>。</p><p><em>人民群众在创造社会财富的同时，也创造并改造着社会关系。生产关系的变革，社会制度的更替，最终取决于生产力的发展。但不会随着生产力的发展自发地实现和完成，而必须借助人民群众的力量。</em> </p></li><li><p>人民群众既是先进生产力和先进文化的<strong>创造主体</strong>，也是实现自身利益的<strong>根本力量。</strong> </p></li></ol></li><li><p>人民群众创造历史的活动收到一定社会历史条件的制约</p><ul><li>经济条件：根本制约因素</li><li>政治条件</li><li>精神文化条件</li></ul></li><li><p>人民群众创造历史原理的方法论：</p><ul><li>马克思主义群众观点：坚信人民群众自己解放自己的观点，全心全意为人民服务的观点，一切向人民群众负责的观点，虚心向群众学习的观点。</li><li>群众路线：一切为了群众，一切依靠群众，从群众中来，到群众中去。</li></ul><blockquote><p>为什么要一切为了群众，一切依靠群众，从群众中来，到群众中去？</p><p>考：方法论后面依靠的原理，即因为人民群众创造历史。</p></blockquote></li></ul><h3 id="53-个人在社会历史中的作用"><a href="#53-个人在社会历史中的作用" class="headerlink" title="53.个人在社会历史中的作用"></a><em>53.个人在社会历史中的作用</em></h3><p><em>唯物史观从人民群众创造历史这一基本前提出发，既明确了人民群众是历史的创造者，也不否认个人在历史上的作用。</em> </p><ul><li><p>历史人物对历史发展的具体过程始终起着一定的作用，有时甚至对历史事件的进程和结局发生决定性的影响，但不能决定历史发展的基本趋势。</p><blockquote><p>个人是与人民群众一起创造历史。</p><p>反动派不能创造历史（历史是哲学范畴的历史）</p></blockquote></li></ul><h1 id="政治经济学"><a href="#政治经济学" class="headerlink" title="政治经济学"></a>政治经济学</h1><ul><li>简单商品经济（54-60）<ul><li>价值是什么</li><li>价值如何衡量</li><li>价值如何表现</li><li>价值有何规律</li></ul></li><li>发达商品经济（资本主义以后的商品经济）<ul><li><strong>自由竞争阶段</strong> （重点）（61-70）</li><li>垄断阶段</li></ul></li></ul><h2 id="简单商品经济"><a href="#简单商品经济" class="headerlink" title="简单商品经济"></a>简单商品经济</h2><p><em>马克思主义不仅揭示了人类社会发展的一般规律，而且揭示了资本主义社会发展的特殊规律。特别是马克思的劳动价值论和剩余价值论，科学揭示了资本主义生产方式的本质和资本主义剥削的秘密。</em> </p><h3 id="54-资本主义生产关系的产生和生产方式的形成"><a href="#54-资本主义生产关系的产生和生产方式的形成" class="headerlink" title="54.资本主义生产关系的产生和生产方式的形成"></a><em>54.资本主义生产关系的产生和生产方式的形成</em></h3><p><em>资本主义生产关系的产生和资本主义生产方式的形成的过程，与商品经济的发展有着密不可分的关系。</em> </p><ul><li>商品经济的发展<ul><li>简单商品经济：以生产资料私有制和个体劳动力为基础</li><li>资本主义商品经济：以生产资料私有制和雇佣劳动为基础，是商品经济的高级发达形态。</li></ul></li><li>资本主义<strong>产生的途径</strong> <ul><li>从小商品经济分化出来</li><li>从商人和高利贷者转化而成</li></ul></li><li>资本<strong>原始积累</strong>的主要途径<ul><li>用暴力手段剥夺农民<strong>土地</strong></li><li>用暴力手段掠夺货币<strong>财富</strong>  </li></ul></li><li>通过资本主义革命和产业革命，最终建立起资本主义生产方式</li></ul><h3 id="55-价值是什么"><a href="#55-价值是什么" class="headerlink" title="55.价值是什么"></a>55.价值是什么</h3><ul><li><p><strong>商品经济</strong>：是<strong>以交换为目的</strong>而进行生产的经济形式</p><ul><li>商品经济产生的<strong>历史条件</strong>：<ul><li>存在社会分工</li><li>生产资料和劳动产品属于不同的所有者</li></ul></li></ul></li><li><p>商品的二因素：商品的使用价值和价值</p><ul><li><p>商品：用来交换、能满足人的某种需要的劳动产品</p></li><li><p><strong>使用价值</strong>：商品能满足人民某种需要的属性，即商品的有用性。</p><ul><li><p>反映<strong>人与自然</strong>之间的<strong>物质关系。</strong></p></li><li><p>是商品的<strong>自然属性</strong> </p></li><li><p>是一切劳动产品共有的属性</p><blockquote><p>劳动产品一定有使用价值，而有些劳动产品，不是为了交换，不是商品，比如为了自己用，也具有使用价值。</p><p>任何有用的物品都具有使用价值，诸如空气，也具有使用价值。</p></blockquote></li><li><p>使用价值构成社会财富的物质内容。</p></li></ul></li><li><p><strong>价值</strong>： 凝结在商品中的无差别的一般人类<strong>劳动</strong>，即人的脑力和体力的耗费。</p><blockquote><p>价值的本质 是 劳动，是藏在商品里的抽象劳动。</p><p>因此，没有蕴含劳动的物品就没有价值，比如空气，没有劳动蕴含其中，本身存在的，就没有价值。</p></blockquote><ul><li><p>价值是商品特有的<strong>社会属性</strong> </p></li><li><p>本质上体现了生产者之间的社会关系</p><p><em>商品的价值是劳动创造的，实质是凝结在商品中的无差别的一般人类劳动，商品交换实际上是商品生产者之间互相交换劳动的关系，商品价值的本质体现了生产者之间一定的社会关系。</em> </p></li></ul></li><li><p>交换价值：首先<strong>表现为</strong>一种使用价值同另一种使用价值想交换的量的关系或比例，<strong>决定</strong>商品交换的比例的不是商品的使用价值，<strong>而是价值</strong>。</p><blockquote><p>注意：商品的二因素只有使用价值和价值，交换价值是另一个概念。</p><p>交换价值，表面上是有用性比例的关系，但其实是商品背后蕴藏的劳动的比例关系。</p></blockquote><ul><li>价值是交换价值的<strong>基础</strong>，交换价值是价值的<strong>表现形式</strong>。 </li></ul></li><li><p>商品的价值和使用价值的关系：<strong>对立统一</strong> </p><ul><li><p>对立性：商品的使用价值和价值是相互排斥的，二者<strong>不可兼得</strong>。</p><p><em>要获得商品的价值，就必须放弃商品的使用价值；要得到商品的使用价值，就不能得到商品的价值。</em> </p></li><li><p>统一性：作为商品，必须同时具有使用价值和价值两个因素，二者<strong>缺一不可</strong>。 </p></li></ul></li></ul></li><li><p>劳动的二重性</p><p><em>商品是劳动产品，生产商品的劳动可区分为具体劳动和抽象劳动。</em> </p><ul><li><p>具体劳动：生产一定使用价值的<strong>具体形式</strong>的劳动，即有用劳动。</p><ul><li>形成商品的<strong>使用价值</strong></li></ul></li><li><p>抽象劳动：撇开一切具体形式的、无差别的一般人类劳动，即<strong>人的体力和脑力的消耗</strong>。 </p><ul><li>形成商品的<strong>价值实体</strong> </li></ul></li><li><p>具体劳动和抽象劳动的关系：对立统一</p><p><em>具体劳动和抽象劳动是同一劳动的两种规定。任何一种劳动，<strong>既是</strong>特殊的具体劳动，<strong>又是</strong>一般的抽象劳动，这就是劳动的二重性。</em> </p><p><em>正是劳动的二重性决定了商品的二因素。</em> </p><ul><li><p>一方面，具体劳动和抽象劳动在时间上和空间上是统一的，是商品生产者同一劳动过程的两个方面。</p></li><li><p>另一方面，具体劳动和抽象劳动又分别反映劳动的不同属性</p><ul><li><p>具体劳动反映的是<strong>人与自然</strong>的关系，是劳动的<strong>自然属性</strong></p><blockquote><p>使用价值反映的是人与自然的物质关系，是商品的自然属性。</p><p>具体劳动决定商品的使用价值。</p></blockquote></li><li><p>抽象劳动反映的是<strong>商品生产者的社会关系</strong>，是劳动的<strong>社会属性</strong></p><blockquote><p>价值本质体现了生产者之间的社会关系，是商品的社会属性。</p><p>抽象劳动决定商品的价值</p><p>因此，价值 就是 藏在商品里的抽象劳动。</p></blockquote></li></ul></li></ul></li></ul></li></ul><h3 id="56-价值如何衡量"><a href="#56-价值如何衡量" class="headerlink" title="56.价值如何衡量"></a>56.价值如何衡量</h3><ul><li><p>决定商品价值量的不是生产商品的个别劳动时间，而只能是<strong>社会必要劳动时间</strong>。 </p><ul><li><p>社会必要劳动时间：在现有的社会正常的生产条件下，在<strong>社会平均</strong>的劳动熟练程度和劳动强度下制造的某种使用价值所需要的劳动时间。</p><blockquote><p>是社会平均劳动时间。</p></blockquote></li></ul></li><li><p>商品的价值量与生产商品所耗费的<strong>劳动时间成正比</strong>，<strong>与劳动生产率成反比</strong>。</p><blockquote><p> 与生产商品所耗费的劳动时间成正比，指的是社会必要劳动时间。</p><p> 与劳动生产率成反比，劳动生产率是劳动者生产使用价值的效率，生产效率越高，所耗费的劳动时间越少，商品价值量也就越低。</p></blockquote><ul><li><p>影响劳动生产率的因素包括：</p><p>劳动者的平均熟练程度、科学技术的发展程度及其在生产中的应用、生产过程的社会结合、生产过程的社会结合、生产资料的规模和效能以及自然条件等。</p><blockquote><p>生产过程的社会结合，就是分工</p></blockquote></li><li><p><strong>劳动生产率的变化</strong>对商品价值总量的影响</p><blockquote><p>商品价值总量 = 单位商品对价值量 * 相同时间生产商品的数量</p></blockquote><table><thead><tr><th></th><th>单位商品的价值量</th><th>相同时间生产商品数量</th><th>商品价值总量</th></tr></thead><tbody><tr><td>社会劳动生产率增加</td><td>减少</td><td>增多</td><td>不变</td></tr><tr><td>个别劳动生产率增加</td><td>不变</td><td>增多</td><td>增多</td></tr></tbody></table></li></ul></li><li><p>商品价值量同简单劳动与复杂劳动有密切的关系</p><ul><li><p>简单劳动：不需要经过专门训练的培养的一般劳动者都能从事的劳动</p></li><li><p>复杂劳动：需要经过专门训练和培养，具有一定文化知识和技术专长的劳动者所从事的劳动</p></li><li><p>形成商品价值量的劳动是以<strong>简单劳动为尺度</strong>计量的，<strong>复杂劳动</strong>等于<strong>自乘的或多倍</strong>的简单劳动。</p><p><em>也就是说，少量的复杂劳动等于多量的简单劳动。在相同的劳动时间里，复杂劳动创造的价值大于简单劳动创造的价值。</em></p><ul><li>在以私有制为基础的商品经济条件下，复杂劳动转化为简单劳动，不是商品生产者自觉计算出来的，而是在商品交换过程中<strong>自发</strong>实现的。</li></ul></li></ul></li></ul><h3 id="57-价值如何表现"><a href="#57-价值如何表现" class="headerlink" title="57.价值如何表现"></a>57.价值如何表现</h3><blockquote><p>通过交换表现价值</p></blockquote><ul><li><p>商品的价值形式的发展经历了四个阶段：</p><ol><li><p>简单的或偶然的价值形式：1只绵羊 = 2把石斧</p></li><li><p>总和的或扩大的价值形式</p></li><li><p>一般的价值形式</p></li><li><p>货币形式</p></li></ol><p><em>金银天然不是货币，货币天然是金银。</em> </p><blockquote><p>金银只是大自然之物，而货币天生就应该是金银来充当，因为金银具备了充当货币的优良特点。</p><p>其他东西充当一般等价物时，不能叫货币，只有金银充当一般等价物时才能叫货币。</p></blockquote></li><li><p>货币：在长期交换过程中形成的固定地充当一般等价物的<strong>商品</strong>。</p></li><li><p>货币的五种基本职能：</p><ul><li><p><strong>价值尺度（最基本职能）</strong>：货币衡量和表现一切商品价值大小的作用</p><ol><li><p>原因：货币也是商品，也有价值，可以衡量其他商品的价值。</p></li><li><p>特点：可以是观念上的货币。</p><blockquote><p>衡量其他商品价值时，不一定有这么多货币，可以是头脑中想象的。</p></blockquote></li></ol></li><li><p><strong>流通手段（最基本职能）</strong> ：货币作为商品交换的媒介</p><blockquote><p>用货币可以交换其他商品。</p></blockquote><ol><li><p>必须是现实的货币</p><blockquote><p>现实的货币，电子钱包也是现实的货币。</p></blockquote></li><li><p>可以不足值</p><blockquote><p>金银，可能由于磕碰，其价值可能不足它代表的值，但还是按照它代表的值流通。</p><p>经济现象：劣币驱逐良币</p><p>因为货币作为流通手段时，可以不足值，所以衍生出了纸币，纸币 = 0价值（忽略纸币的价值），纸币是一个工具，代替货币作为流通手段。</p><p>但纸币没有价值，所以纸币不能执行价值尺度的功能。</p></blockquote></li></ol></li></ul><ol start="3"><li>储藏手段：货币推出流通领域作为社会财富的一般代表被保存起来的职能</li></ol><ul><li><p>支付手段：货币被用来<strong>清偿债务</strong>或<strong>支付赋税</strong>、<strong>租金</strong>、<strong>工资</strong></p><blockquote><p>辨析：</p><p>支付手段 vs 流通手段</p><p>流通手段：现货交易，与商品交换。</p><p>支付手段：没有现货。</p></blockquote></li><li><p>世界货币</p></li></ul></li><li><p>货币的产生使整个商品世界分化为两级：</p><ul><li>一极是各种各样的<strong>具体商品</strong>，分别代表不同的使用价值。</li></ul></li><li><p>一极是<strong>货币</strong>，只代表商品的价值。</p></li><li><p>这样就使商品 内在的使用价值和价值的矛盾 发展成为 外在的<strong>商品</strong>和<strong>货币</strong>的矛盾。</p><p>一切商品<strong>只要转化为货币</strong>，商品使用价值和价值的矛盾就能得到解决，从而使<strong>商品的价值得到实现</strong>。</p><ul><li>货币的出现有利于商品交换的困难性，但不能解决也不可能解决商品经济的基本矛盾。</li></ul></li></ul><h3 id="58-价值规律及其作用"><a href="#58-价值规律及其作用" class="headerlink" title="58.价值规律及其作用"></a>58.价值规律及其作用</h3><ul><li><p>价值规律的<strong>基本内容</strong>：</p><ul><li>商品的价值量由生产商品的社会必要劳动时间决定，</li><li>商品交换以价值量为基础，按照<strong>等价交换</strong>的原则进行。</li></ul></li><li><p>价值规律的<strong>表现形式</strong>：是商品的价格围绕商品的价值自发波动。</p><blockquote><p>决定商品价格的因素：</p><ul><li>供给：影响因素</li><li>币值：影响因素</li><li>价值：决定因素</li></ul></blockquote></li><li><p>价值规律的作用：</p><ul><li><p>积极作用：</p><ol><li><strong>自发地</strong>调节 生产资料和劳动力在社会各生产部门之间的分配比例。</li><li>自发地刺激社会生产力的发展</li><li>自发地调节社会收入的分配</li></ol></li><li><p>消极后果：</p><ol><li><p>导致社会资源浪费。</p><p>（价值规律自发调节社会资源在社会生产各部门的配置，可能出现比例失调的情况）</p></li><li><p>阻碍技术的进步。</p><p>（垄断的发生）</p></li><li><p>导致收入两极分化。</p></li></ol></li></ul></li></ul><h3 id="59-私有制基础上商品经济的基本矛盾"><a href="#59-私有制基础上商品经济的基本矛盾" class="headerlink" title="59.私有制基础上商品经济的基本矛盾"></a>59.私有制基础上商品经济的基本矛盾</h3><ul><li><p><strong>私人劳动</strong>和<strong>社会劳动</strong>的矛盾构成 <strong>私有制商品经济的基本矛盾</strong>（简单商品经济的基本矛盾）。</p><p><em>在以私有制为基础的商品经济中，商品生产者的劳动具有两重性，既是具有社会性质的社会劳动，又是具有私人性质的私人劳动。</em></p><blockquote><p>是每一种劳动，既是私人劳动，又是社会劳动，角度不同。</p><p>从生产资料私有制来看，是私人劳动，从社会分工的角度看，又是社会劳动。</p><p>在商品经济条件下，每个生产者的劳动本身是私人劳动，而私人劳动要转化为社会劳动，就必须用自己的产品去同别人的产品交换。</p><p><strong>交换</strong>是解决私人劳动和社会劳动之间矛盾的唯一途径。</p></blockquote></li><li><p>私人劳动和社会劳动之间的<strong>矛盾</strong> 在资本主义制度下，进一步发展成<strong>资本主义的基本矛盾</strong>，即生产资料资本主义私人占有 和 生产社会化之间的矛盾，正是这一<strong>矛盾不断运动</strong>，才使资本主义制度最终被社会主义制度所代替具有了客观必然性。</p></li></ul><h3 id="60-马克思劳动价值论的意义"><a href="#60-马克思劳动价值论的意义" class="headerlink" title="60.马克思劳动价值论的意义"></a>60.马克思劳动价值论的意义</h3><ul><li><p>马克思<strong>继承</strong>了古典政治经济学<strong>劳动创造价值理论</strong>的同时，<strong>创立</strong>了<strong>劳动二重性</strong>理论。 </p></li><li><p>劳动二重性理论称为理解政治经济学的<strong>枢纽</strong> 。</p></li><li><p>深化对马克思劳动价值论的认识</p><p><em>走进21世纪，面对新的情况，必须深化对马克思劳动价值论的认识，根据变化了的实践在继承的基础上有所创新、有所前进。</em> </p><ul><li>深化对创造价值的劳动的认识，对<strong>生产性劳动</strong>作出新的界定。</li><li>深化对<strong>科技人员、经营管理人员</strong>在社会生产和价值创造中所起的作用的认识</li><li><del>深化对科技、知识、信息等新的生产要素在财富和价值创造中的作用等认识。</del></li><li>深化对<strong>价值创造</strong>和<strong>价值分配</strong>关系对认识。</li></ul></li></ul><h2 id="发达商品经济-自由竞争阶段"><a href="#发达商品经济-自由竞争阶段" class="headerlink" title="发达商品经济-自由竞争阶段"></a>发达商品经济-自由竞争阶段</h2><blockquote><p>简单商品经济和发达商品经济的区别：劳动力成为商品</p></blockquote><h3 id="61-劳动力成为商品与货币转化为资本"><a href="#61-劳动力成为商品与货币转化为资本" class="headerlink" title="61.劳动力成为商品与货币转化为资本"></a>61.劳动力成为商品与货币转化为资本</h3><ul><li><p>劳动力：指人的劳动能力，是人的体力和脑力的总和。劳动力的使用即<strong>劳动</strong>。</p><blockquote><p>劳动者：人。</p><p>劳动力：体力和脑力的总和。</p></blockquote></li><li><p><strong>劳动力成为商品</strong>的基本条件：</p><ol><li>劳动者是自由人，能够把自己的劳动力当作自己的商品来支配。</li><li>劳动者没有别的商品可以出卖，自由得一无所有，没有任何实现自己的劳动力所必需的物质条件。</li></ol></li><li><p>劳动力的<strong>价值</strong>：是由生产、发展、<strong>维持和延续劳动力</strong>所必需的生活必需品的价值决定的。</p><p>包括三个部分：</p><ol><li><p>维持劳动者<strong>本人</strong>生存所必需的生活资料的价值</p></li><li><p>维持劳动者<strong>家属</strong>的生存所必需的生活资料的价值</p></li><li><p>劳动者接受教育和训练所支出的费用</p></li></ol><ul><li><p>劳动力价值的构成包含一个历史的和道德的因素。</p><blockquote><p>劳动力价值和历史时期、地区有关。</p></blockquote></li></ul></li><li><p>劳动力商品在使用价值上有一个很大的特点：它的<strong>使用价值是劳动</strong>，而<strong>劳动又是普通商品价值的源泉</strong>。</p><blockquote><p>劳动创造商品的价值。</p><p>而劳动力商品在消费的过程，就是在劳动的过程，就能够创造新的商品的价值。</p></blockquote><ul><li><p>货币所有者购买到这种特殊商品，<strong>能够增值，货币也就变成了资本</strong>。</p><p><em>劳动力商品在消费过程中能够创造新的价值，而且这个新的价值比劳动力本身的价值更大。</em> </p><p><em>正是由于这一特点，货币所有者购买到劳动力以后，在消费过程中，不仅能够收回他在购买这种商品时支付的价值，还能得到一个增值的价值即<strong>剩余价值</strong>。而<strong>一旦货币购买的劳动力带来剩余价值</strong>，<strong>货币也就变成了资本</strong>。</em> </p></li></ul></li><li><p>劳动力成为商品，货币转化为资本。</p></li></ul><h3 id="62-资本主义所有制"><a href="#62-资本主义所有制" class="headerlink" title="62.资本主义所有制"></a><em>62.资本主义所有制</em></h3><ul><li>资本家凭借对生产资料的占有，在等价交换原则的掩盖下，雇佣工人从事劳动，占有雇佣工人的剩余价值，这就是资本主义所有制的实质。</li></ul><h3 id="63-剩余价值的生产"><a href="#63-剩余价值的生产" class="headerlink" title="63.剩余价值的生产"></a>63.剩余价值的生产</h3><ul><li><p><strong>资本主义生产过程</strong> 是<strong>劳动过程</strong>和<strong>价值增殖</strong>过程的统一。</p><p><em>剩余价值是在资本主义的生产过程中生产出来的。</em> </p><p><em>资本主义的生产过程具有两重性，一方面是生产物质资料的劳动过程，另一方面是生产剩余价值的过程，即价值增殖过程。资本主义生产过程是劳动过程和价值增殖过程的统一。</em> </p><blockquote><p>工人在劳动，资本家在获得增值价值。</p></blockquote></li><li><p>样例：</p><ul><li><p>有一个做包子的资本家。</p><ul><li><p>情况一：</p><p>生产资料：40元的面</p><p>工人工资：20元</p><p>工人将40元的面生产为60元的包子，需要花费4个小时。</p><p>此时，剩余价值m = 0</p></li><li><p>情况二：</p><p>生产资料：80元的面</p><p>工人工资：20元</p><p>工人将80元的面生产为120元的包子，需要花费8个小时。</p><p>此时，<strong>剩余价值m = 20元</strong></p><blockquote><p>注意，工人的工资，即工人劳动力的价值是由生产、发展、<strong>维持和延续劳动力</strong>所必需的生活必需品的价值决定的，即上述1、2、3点，与工人的工作时间、强度无关。</p><p>所以工人的工资仍然为20元。</p></blockquote></li></ul></li></ul></li><li><p>剩余价值的生产——从劳动的方面来看</p><blockquote><p>劳动：</p><p>具体劳动产生商品的使用价值，抽象劳动产生商品的价值</p></blockquote><ul><li><p><strong>具体劳动</strong>的任务：</p><ol><li><p>转移“面粉”的价值，到包子的“价值上”</p><blockquote><p>将面的80元价值，转移到包子的价值中。</p></blockquote></li><li><p>生产包子的使用价值</p></li></ol></li><li><p>抽象劳动的任务：生产新价值</p><blockquote><p>情况二：</p><p>本来只有80元价值的面，20元价值的劳动力，经过工人的劳动，20元价值的劳动力不变，但产生了120元价值的包子。</p><p>120元的价值中有具体劳动时转移面的80元价值，还有抽象劳动生产的40元的新的价值。</p></blockquote></li></ul></li><li><p>剩余价值的生产——从资本的方面</p><p>全部预付资本100元：</p><ul><li><p>购买面粉的80元，借助<strong>具体劳动</strong>转移到最终产品中，不会增值。称为<strong>不变资本</strong>（C）。</p><ul><li><p><strong>不变资本</strong>：以<strong>生产资料形态</strong>存在的资本。</p><p><em>生产资料的价值通过工人的具体劳动被转移到新产品中，其转移到价值量不会大于它原有价值量，不发生增殖。</em>  </p></li></ul><blockquote><p>具体劳动的任务之一，转移生产资料的价值到商品的价值中去。</p></blockquote></li><li><p>购买工人的20元。由工人的劳动再创造出来，并能够增值。称之为<strong>可变资本</strong>（V）。并能够带来剩余价值（M）</p><ul><li><p><strong>可变资本</strong>：用来<strong>购买劳动力</strong>的那部分资本。</p><p><em>可变资本的价值在生产过程中不是被转移到新产品中去，而是由工人的劳动再生产出来。</em> </p><p><em>在生产过程中，工人所创造的新价值，不仅包括相当于劳动力价值的价值，而且还包括一定量的剩余价值。</em> </p></li></ul><blockquote><p>商品价值中除了生产资料转移过来的价值，还有工人抽象劳动产生的新价值。</p><p>可变成本V = 20元。</p><p>剩余价值M = 20元</p></blockquote></li><li><p>剩余价值率<strong>M‘ = M/V</strong>。衡量剥削程度。</p><blockquote><p>剩余价值率M’ = 100%</p></blockquote></li></ul></li><li><p>剩余价值的生产——从时间的方面</p><p>全天工作8小时</p><blockquote><p>8个小时工人创造40元的新价值，前4个小时创造20元，即工人的工资，后4个小时创造20元，为资本家劳动。</p></blockquote><ul><li><p>前4个小时。为自己劳动，创造劳动力价值，即工资，称之为<strong>必要劳动时间。</strong></p><ul><li><strong>必要劳动时间</strong>：用来产生生产劳动力价值或可变资本的价值的时间。</li></ul><blockquote><p>辨析：必要劳动时间和社会必要劳动时间。</p><p>社会必要劳动时间决定商品的价值量。</p><p>必要劳动时间是工人为自己劳动，创造工人劳动力价值的时间。</p></blockquote></li><li><p>后4个小时。为资本家劳动，创造剩余价值，称之为<strong>剩余劳动时间。</strong></p><ul><li><strong>剩余劳动时间</strong>：生产剩余价值的劳动时间。</li></ul></li></ul></li><li><p>剩余价值既不是由全部资本创造的，也不是由不变资本创造的，而是由<strong>可变资本</strong>雇佣的劳动者创造的。</p><ul><li>雇佣劳动者的<strong>剩余劳动</strong>是剩余价值的唯一源泉。</li><li>资本家对工人的剥削程度m’ = m/v = 剩余劳动/必要劳动 = 剩余劳动时间/必要劳动时间</li></ul></li><li><p>资本家提高对工人剥削程度的方法：绝对剩余价值的方法和相对剩余价值的生产</p><ul><li><p>绝对剩余价值：<strong>必要劳动时间不变</strong>的条件下，由于<strong>延长工作日的长度和提高劳动强度</strong>而生产的剩余价值。</p><blockquote><p>在相同长的劳动时间内比以前消耗更多的脑力和体力，这和延长工作日并没有本质区别，因此，由提高劳动强度而产生剩余价值的方法是绝对剩余价值生产方法。</p></blockquote></li><li><p>相对剩余价值：在<strong>工作日长度不变</strong>的条件下，通过<strong>缩短必要劳动时间</strong>而<strong>相对延长剩余劳动</strong>时间所生产的剩余价值。</p><blockquote><p>资本家在调整必要劳动时间与剩余劳动时间的比例上下功夫，通常缩短必要劳动时间、相对延长剩余劳动时间的方法，增加剩余价值的生产。</p></blockquote><ul><li><p>缩短必要劳动时间是通过<strong>全社会劳动生产率</strong>的提高实现的。</p><blockquote><p>社会生产率的提高（科学技术的革新），缩短了必要劳动时间，相对延长了剩余劳动时间。</p></blockquote></li></ul><blockquote><p>辨析：绝对剩余价值 和 相对剩余价值</p><p>都延长剩余劳动时间。</p><p>绝对剩余价值的生产没有缩短必要劳动时间，相对剩余价值的生产由缩短必要劳动时间。</p><p>绝对剩余价值没有科学技术革新，相对剩余价值有科学技术的革新。</p></blockquote></li><li><p>超额剩余价值：企业由于提高劳动生产率而使商品的个别价值低于社会价值的超额。</p><ul><li>全社会劳动生产率的提高是资本家追逐超额剩余价值的结果。</li><li>相对剩余价值是资本家追逐超额剩余价值的结果。</li></ul><p><em>单个资本家改进技术、改善管理的主动动机是追求超额剩余价值，但其客观后果则是整个社会各个生产部门的劳动生产率普遍提高，导致生活资料的价值下降和补偿劳动力价值的必要劳动时间缩短，而剩余劳动时间相对延长，整个资本家阶级普遍获得更多的相对剩余价值。</em> </p></li></ul></li><li><p>资本主义条件下的生产自动化是资本家<strong>获取超额剩余价值</strong>的手段，而<strong>雇佣工人的剩余劳动</strong>仍然是这种剩余价值的唯一源泉。</p><blockquote><p> 生产自动化：首先是不存在绝对的无人，其次，这些机器上也蕴含着资本家对制作机器工人的剥削。</p></blockquote></li></ul><h3 id="64-资本的积累"><a href="#64-资本的积累" class="headerlink" title="64.资本的积累"></a>64.资本的积累</h3><ul><li><p>资本<strong>积累</strong>：把剩余价值转化为资本，或者说，剩余价值的资本化。</p></li><li><p>资本主义再生产的特点：<strong>扩大再生产。</strong></p><ul><li>资本主义<strong>扩大再生产</strong>的<strong>源泉</strong>：资本积累</li><li>资本主义<strong>简单再生产</strong>：资本家获得剩余价值后，如果将其全部用于消费，则生产就在原有规模的基础上重复进行。</li></ul></li><li><p>资本积累的<strong>本质</strong>：资本家不断利用无偿占有的<strong>工人创造的剩余价值</strong>，来扩大自己的资本规模，进一步扩大和加强对工人的剥削和统治。</p><ul><li>资本积累的<strong>源泉</strong>：剩余价值</li></ul></li><li><p>资本积累规模的大小取决于：对工人的剥削程度、劳动生产率的高低、所用资本和所费资本之间的差额（投资的钱和花掉的钱的差额）以及资本家垫付资本的大小。</p></li><li><p><strong>资本的技术构成</strong>：由生产的技术水平所决定<strong>生产资料</strong>和<strong>劳动力</strong>之间的比例。</p><blockquote><p>在买包子例子中，资本的技术构成为4斤面：1个工人</p></blockquote></li><li><p><strong>资本的价值构成</strong>：资本分为不变资本和可变资本，这两部分资本价值之间的比例。</p><blockquote><p>在买包子的例子中，资本的价值构成为80元：20元</p></blockquote><blockquote><p>资本的不变资本，用来买生产资料。</p><p>资本的可变资本，用来买劳动力。</p></blockquote></li><li><p><strong>资本的有机构成</strong>：由资本技术构成决定并反映技术构成变化的<strong>资本价值构成</strong>。通常用c: v来表示。</p><blockquote><p>资本的有机构成是 资本的价值构成。</p><p><strong>前提条件</strong> 是资本的价值构成反映资本技术构成，即资本的技术构成改变，引起资本的价值构成改变。</p><p>如果资本的技术构成不变，但由于其他原因，资本的价值构成改变了，此时资本的价值构成就不是资本的有机构成。</p></blockquote></li><li><p>在资本主义生产过程中，资本有机构成呈现<strong>不断提高</strong>趋势。</p><blockquote><p>c: v，资本不断积累，c增大。</p></blockquote><ul><li><p>失业：资本的有机构成提高，<strong>可变资本</strong>相对量减少，资本对劳动力的需求日益相对地减少，结果就不可避免地造成大批工人失业，形成相对过剩人口。</p><blockquote><p>资本的积累，造成失业。</p></blockquote></li></ul></li><li><p>资本积累的历史趋势是资本主义制度的必然灭亡和社会主义制度的必然胜利</p><blockquote><p>资本积累 -&gt; 资本有机构成提高 -&gt; 相对剩余人口过剩（失业） -&gt; 贫富差距拉大（两级分化）-&gt;  资本主义灭亡</p></blockquote></li></ul><h3 id="65-剩余价值的循环"><a href="#65-剩余价值的循环" class="headerlink" title="65.剩余价值的循环"></a>65.剩余价值的循环</h3><ul><li><p>产业资本在循环过程中要经历三个不同阶段，与此联系的是资本依次执行三种不同职能：</p><ol><li>购买阶段，即生产资料和劳动力的购买阶段：产业资本执行的是货币资本的职能。</li><li>生产阶段，生产资料与劳动者相结合在一起从事资本主义生产的阶段：产业资本执行的是生产资本的职能。</li><li>售卖阶段，商品资本向货币资本转化的阶段：产业资本执行的是商品资本的职能。</li></ol></li><li><p>产业资本运动的两个基本条件：</p><ol><li><p>产业资本的三种职能形式必须在空间上并存。</p><p><em>产业资本必须按照一定比例同时存在于货币资本、生产资本和商品资本三种形式中。</em> </p></li><li><p>产业资本的三种职能形式必须在时间上继起。</p><p><em>产业资本循环的三种职能形式的转化必须保持时间上的依次连续性。</em> </p></li></ol></li><li><p>资本的<strong>周转</strong>：资本是在运动中增殖的，资本周而复始、不断反复的循环。</p><ul><li>影响资本周转快慢的因素有很多，关键的因素：<ol><li>资本周转的时间</li><li>生产资本中固定资本和流动资本的构成<ul><li>固定资本：资本家的资本分多次转移到最终的产品中，比如机器。</li><li>流动资本：资本家的资本一次转移到最终的产品中，比如做包子的面。</li></ul></li></ol></li></ul></li><li><p>总结：划分资本</p><blockquote><p>考：按照不同的维度判断该资本。</p></blockquote><table><thead><tr><th></th><th>内容</th><th>依据</th></tr></thead><tbody><tr><td>第一次划分</td><td>不变资本、可变资本</td><td>是否能增值（是否能产生剩余价值）</td></tr><tr><td>第二次划分</td><td>货币资本、生产资本、货币资本</td><td>资本执行的不同职能形式</td></tr><tr><td>第三次划分</td><td>固定资本、流动资本</td><td>资本的周转方式</td></tr></tbody></table></li><li><p>社会再生产的<strong>核心问题</strong>是<strong>社会总产品的实现问题</strong>，即社会总产品的<strong>价值补偿</strong>和<strong>实物补偿</strong>问题</p><ul><li>价值补偿：东西卖出去后，能收回钱。</li><li>实物补偿：东西卖出去后，为了后面的生产，需要补偿原材料。</li></ul></li><li><p>马克思将社会总产品在物质上划分为两大类，在价值上划分为三个组成部分。</p><blockquote><p>马克思，真的是大智慧啊！！！！</p></blockquote><ul><li><p>社会总产品：社会在一定时期（通常为一年）所生产的全部物质资料的总和。</p></li><li><p>社会总产品的<strong>物质形态</strong>上，根据其最终用途可以区分为用于生产消费的生产资料和用于生活消费的消费资料</p><ul><li>第一部类（Ⅰ）：由生产 <strong>生产资料</strong>的部门构成，其产品进入生产领域。</li><li>第二部类（Ⅱ）：由生产 <strong>消费资料</strong>的部门构成，其产品进入生活消费领域。</li></ul></li><li><p>社会总产品在<strong>价值形态</strong>上，又叫<strong>社会总价值</strong>(商品价值构成) = c + v + m</p><ul><li>产品中的生产资料的转移价值(c) </li><li>凝结在产品中的由工人必要劳动时间创造的价值(v)</li><li>凝结在产品中的由工人在剩余劳动时间创造的价值(m)</li></ul></li><li><p>第一部类的社会总价值 = Ⅰ(c) + <u>Ⅰ(v) + Ⅰ(m)</u></p><ul><li>其中Ⅰ(c)的来源可以从第一部类中自我解决</li><li>而 Ⅰ(v) + Ⅰ(m) ，一个是工人生活必需品的消耗，一个是资本家的生活必需品的消耗（家属资本家赚到的钱都去用了），Ⅰ(v) + Ⅰ(m) 需要从第二部类中来。</li></ul></li><li><p>第二部类的社会总价值 = <u>Ⅱ(c)</u> + Ⅱ(v) + Ⅱ(m)</p><ul><li>其中Ⅱ(c)的来源需要从第一部类中来</li><li>Ⅱ(v) + Ⅱ(m)，则可以从第二部类中自我解决</li></ul></li><li><p>需要满足Ⅰ(v) + Ⅰ(m) = Ⅱ(c)的平衡，这需要两部类的交换，当Ⅰ(v) + Ⅰ(m) &gt; Ⅱ(c)，就会造成资源浪费。</p><p><em>这两部类的生产都是在价值规律和剩余价值规律的作用下自发进行的，具有严重的盲目性，这就导致了这两大部类生产在规模上和结构上经常处于失衡状态。</em></p><p><em>这种失衡和脱节经常表现为生产过剩，以至于社会总产品的实现，即实物替换和价值补偿难以顺利进行，最严重的就是引发经济危机。</em> </p></li><li><p>经济危机的发生，实际上是资本主义条件下<strong>以强制的方式</strong> 解决社会再生产的实现问题的途径。</p><p>这种强制性地恢复平衡，是以社会经济生活的严重混乱以及社会资源和财富的极大浪费为代价的。</p></li></ul></li></ul><h3 id="66-工资与剩余价值的分配"><a href="#66-工资与剩余价值的分配" class="headerlink" title="66.工资与剩余价值的分配"></a>66.工资与剩余价值的分配</h3><ul><li><p>在资本主义制度下，工人工资：劳动力的价值或价格</p><ul><li><p>资本主义<strong>工资的本质</strong>：劳动力的价值或价格</p></li><li><p>工资<strong>表现为</strong>：“劳动的价格”或工人全部劳动的报酬</p><p><em>这就模糊了工人必要劳动和剩余劳动的界限，掩盖了资本主义的剥削关系。</em>  </p></li></ul></li><li><p>概念</p><ul><li><p>生产成本（成本价格）：不变资本和可变资本构成</p></li><li><p>利润：= 剩余价值</p><p><em>资本家并不把剩余价值看作可变资本的产物，而是把它看作全部垫付资本的产物或增长额。</em> </p></li><li><p>平均利润率：利润平均化形成的 社会的平均利润率（行业间的）</p><p><em>资本主义生产的目的是获得利润。为了得到尽可能高的利润率和尽可能多的利润，不同生产部门的资本家之间必然展开激烈的竞争，大量资本必然从利润率低的部门转投到利润率高的部门，从而<strong>导致利润率平均化</strong>。</em> </p></li><li><p>平均利润：按照平均利润率计算和获得的利润</p><p><em>在利润率平均化的过程中，形成了<strong>社会的平均利润率</strong>，按照平均利润率计算和获得的利润，叫做平均利润。</em> </p></li><li><p>生产价格：商品价值的转化形式， = 生产成本 + 平均利润</p><p><em>在价值转化为生产价格的条件下，价值规律作用的形式发生了变化：商品不再以价值而是<strong>以生产价格为基础进行交换</strong>，市场价格的变动不再以价值为中心，而是<strong>以生产价格为中心</strong>。</em>  </p><p><em>从全社会看，整个资本家阶级获得的利润总额与雇佣工人所创造的剩余价值总额是相等的；从个别部门看，商品的生产价格同价值不一致，但从全社会来看，商品的生产价格总额和价值总额相等</em></p></li><li><p>超额利润： = 超额剩余价值（行业内企业间的竞争）</p><blockquote><p>注意：</p><p>平均利润率是社会中行业间形成的，而超额利润是行业内企业之间形成的。</p><p>平均利润率的形成，并不影响企业革新获取超额利润。</p></blockquote></li></ul></li><li><p>在利润平均化规律作用下，产业资本家获得产业利润，商业资本家获得商业利润，银行资本家获得银行利润，土地所有者获得地租。</p></li></ul><h3 id="67-马克思剩余价值理论的意义"><a href="#67-马克思剩余价值理论的意义" class="headerlink" title="67.马克思剩余价值理论的意义"></a><em>67.马克思剩余价值理论的意义</em></h3><ul><li><p>马克思通过分析剩余价值的生产、积累、流通以及分配，解释了剩余价值的运动规律 ，<strong>创立了</strong>剩余价值理论。</p><blockquote><p>马克思在哲学上的两大成就：创立了唯物史观；形成了唯物辩证统一</p><p>马克思理论上的两大成就：创立了唯物史观；创立了剩余价值理论</p></blockquote></li><li><p>剩余价值理论深刻</p><ul><li><strong>揭露了</strong>资本主义生产关系的<strong>剥削本质</strong>，</li><li><strong>阐明了</strong>资产阶级与无产阶级之间<strong>阶级斗争的经济根源</strong>，</li><li><strong>指出了</strong>无产阶级革命的<strong>历史必然性</strong>。</li></ul></li><li><p>剩余价值理论是马克主义经济理论的基石，是无产阶级反对资产阶级、揭示资本主义制度剥削本质的锐利武器。</p></li><li><p>由于<strong>唯物史观和剩余价值</strong>的发现，社会主义由空想变为科学。</p></li></ul><h3 id="68-资本主义的基本矛盾与经济危机"><a href="#68-资本主义的基本矛盾与经济危机" class="headerlink" title="68.资本主义的基本矛盾与经济危机"></a>68.资本主义的基本矛盾与经济危机</h3><ul><li><p><strong>生产资料资本主义私人占有</strong>和<strong>生产社会化</strong>之间的矛盾，是资本主义的基本矛盾。</p><ul><li>这是生产力和生产关系之间的矛盾在资本主义社会的具体形式。</li></ul></li><li><p><strong>生产相对过剩</strong>是资本主义经济危机的<strong>本质特征</strong>。</p></li><li><p>经济危机的<strong>可能性</strong>是由货币作为<strong>支付手段和流通手段</strong>引起的。</p></li><li><p>资本主义经济危机爆发的<strong>根本原因</strong>是：资本主义的基本矛盾</p><p>这种基本矛盾具体表现为两个方面：</p><ol><li>表现为生产无限扩大的趋势与劳动人民有支付能力的需求相对缩小的矛盾。</li><li>表现为个别企业内部生产的有组织性和整个社会生产的无政府状态之间的矛盾</li></ol></li><li><p>经济危机一般包括四个阶段：危机、萧条、复苏和高涨</p><ul><li>危机是经济危机周期的基本阶段。</li></ul></li></ul><h3 id="69-资本主义的国家、政治制度及其本质"><a href="#69-资本主义的国家、政治制度及其本质" class="headerlink" title="69.资本主义的国家、政治制度及其本质"></a>69.资本主义的国家、政治制度及其本质</h3><ul><li>资本主义国家的<strong>职能</strong>是以服务于资本主义制度和资产阶级利益为根本内容的，是资产阶级进行政治统治的工具。<ul><li>资本主义国家的职能包括对内对外两个基本方面，<ul><li>即<strong>对内</strong>实行政治统治和社会管理</li><li><strong>对外</strong>进行国际交往和维护国家安全及利益</li></ul></li></ul></li><li>资本主义国家的<strong>本质</strong>是：资产阶级进行阶级统治的工具。</li><li>资本主义的<strong>民主制度</strong>：“主权在民”、“天赋人权”、“分权制衡”、“社会契约”、“自由、平等、博爱”</li><li>资本主义<strong>法制</strong>：<strong>宪法</strong>是资本主义国家法律制度的核心<ul><li>依据的基本原则：<ul><li>私有制原则</li><li>“主权在民”原则</li><li>分权与制衡原则</li><li>人权原则</li></ul></li></ul></li><li>资本主义<strong>国家政权</strong>：分权制衡的组织形式，即国家的立法权、行政区、司法权分别由三个权力主义独立行使。</li><li>资本主义政治制度的局限性：<ol><li>资本主义的民主是金钱操纵下的民主，实际是资产阶级精英统治下的民主。</li><li>法律名义上的平等掩盖着事实上的不平等。</li><li>资本主义国家的政党制是一种维护资产阶级统治的政治制度。<ul><li>资本主义多党制仍然是资产阶级选择自己的国家管理者、实现其内部利益平衡的政治机制。</li></ul></li><li>政党恶斗相互掣（che）肘时，决策效率低下，激化社会矛盾。</li></ol></li></ul><h3 id="70-资本主义的意识形态及其本质"><a href="#70-资本主义的意识形态及其本质" class="headerlink" title="70.资本主义的意识形态及其本质"></a><em>70.资本主义的意识形态及其本质</em></h3><ul><li>资本主义国家意识形态的本质：<ul><li>资本主义意识形态是资本主义社会条件下的观念上层建筑，是为资本主义社会形态的经济基础服务的。</li><li>资本主义意识形态是资产阶级的阶级意识的集中体现。</li></ul></li></ul><h2 id="发达商品经济-垄断阶段"><a href="#发达商品经济-垄断阶段" class="headerlink" title="发达商品经济-垄断阶段"></a>发达商品经济-垄断阶段</h2><h3 id="71-资本主义从竞争到垄断"><a href="#71-资本主义从竞争到垄断" class="headerlink" title="71.资本主义从竞争到垄断"></a>71.资本主义从竞争到垄断</h3><ul><li><p>资本主义的发展经历两个阶段：</p><ul><li><p><strong>自由竞争资本主义</strong></p></li><li><p><strong>垄断资本主义</strong></p><p><em>19世纪20世纪初，垄断取代自由竞争在资本主义经济中占据统治地位。</em> </p><ul><li>垄断资本主义的发展包括两种形式：<strong>私人</strong>垄断资本主义和<strong>国家</strong>垄断资本主义</li></ul></li></ul></li><li><p><strong>垄断</strong>：少数资本主义大企业，为了获得高额利润，通过互相<strong>协议或联合</strong>，对一个或几个部门商品的生产、销售和价格进行操作和控制。</p><ul><li><p>垄断的形成方式</p><p><em>自由竞争引起生产集中和资本集中，生产集中和资本集中发展到一定阶段必然引起垄断，这是资本主义发展的客观规律。</em> </p><ul><li><p>生产集中：<strong>生产资料、劳动力和商品</strong>的生产日益集中于少数大企业的过程，其结果是大企业所占的比重不断增加。</p></li><li><p>资本集中：大资本吞并小资本，或由许多小资本合并而成大资本的过程，其结果是越来越多的资本为少数大资本家所支配。</p><blockquote><p>钱集中。</p></blockquote></li></ul></li><li><p>垄断的形成原因：</p><ol><li>获得高额利润</li><li>形成竞争限制</li><li>避免两败俱伤</li></ol></li><li><p>垄断组织的<strong>本质</strong>：通过联合实现独占和瓜分商品生产和销售市场，操作垄断价格，以攫（jue）取高额垄断利润。</p></li></ul></li><li><p>垄断条件下的竞争</p><ul><li>垄断资本主义阶段存在<strong>竞争</strong>的主要原因：<ol><li>垄断没有消除产生竞争的经济条件（私有制）</li><li>垄断必须通过竞争来维持</li><li>不存在由一个垄断组织囊括一切部门、一切社会生产的绝对垄断</li></ol></li><li>垄断条件下的竞争同自由竞争相比，具有的<strong>新特点</strong> ：<ol><li>竞争的目的：<ul><li>自由竞争主要是为获得更多的利润或超额利润，不断扩大资本的积累</li><li>垄断条件下的竞争则是为获取高额垄断利润，并不断巩固、扩大已有的垄断地位</li></ul></li><li>竞争手段：<ul><li>自由竞争主要运用经济手段，如通过改进技术、提高劳动生产率、降低产品成本来战胜对手</li><li>垄断条件下的竞争不仅采取经济手段还采取非经济手段，使经济更加复杂、激烈</li></ul></li><li>竞争范围：<ul><li>自由竞争时期，竞争主要在经济领域，而且主要是国内市场上进行</li><li>垄断时期，国际市场上的竞争规模扩大，范围遍及各个领域和部门，并由国内扩展到国外</li></ul></li></ol></li></ul></li><li><p>金融资本：工业垄断资本和银行垄断资本融合在一起而形成的一种垄断资本</p><ul><li>金融资本形成的主要途径：金融联系、资本参与和人事参与</li></ul></li><li><p>金融寡头：操纵国民经济命脉，并在实际上<strong>控制国家政权</strong>的少数垄断资本家或垄断资本家集团</p><ul><li><p><strong>经济</strong>领域的统治：通过”参与制“实现的。</p><p><em>所谓参与制，即金融寡头通过掌握一定数量的股票来层层控制企业的制度</em> </p></li><li><p><strong>政治</strong>上的统治（金融寡头对国家机器的控制）：通过同政府的”个人联合“来实现的。</p><p>这种联合有多种途径：</p><ul><li>金融寡头直接出马把自己的代理人送进政府或议会，掌握政权，利用政治力量为其垄断统治服务</li><li>收买政府高官或国会议员，让他们在其政治活动中为金融寡头的利益服务</li><li>聘请曾在政府任职的高官到公司担任高级职务</li><li>建立政策咨询机构等方式对政府的政策施加影响，掌握新闻出版、广播电视、科学教育、文化体育等上层建筑的各个领域，左右国家的内政外交及社会生活。</li></ul></li></ul></li><li><p><strong>垄断利润</strong>：垄断资本家凭借其在社会生产和流通中的垄断地位而获得的超过平均利润的高额利润。</p><ul><li><p>垄断利润的来源：归根到底来自无产阶级和其他劳动人民所创造的剩余价值</p><ol><li>来自对<strong>本国无产阶级</strong>和其他劳动人民剥削的加强</li><li>通过控制市场占有<strong>其他企业</strong>特别是非垄断企业的利润</li><li>通过加强对其他国家劳动人民对剥削和掠夺获取的国外利润</li><li>通过资本主义国家政权进行有利于垄断资本的再分配，从而将劳动人民创造的国民收入的一部分变成垄断资本的收入。</li></ol></li><li><p>垄断利润的实现：垄断利润主要是通过垄断组织制定的<strong>垄断价格</strong>实现的 </p></li><li><p>垄断价格：垄断组织在销售或购买商品时，凭借其垄断地位规定的、旨在保证获取最大限度利润的市场价格。</p><ul><li><p><strong>垄断价格 = 成本价格 + 平均利润 + 垄断利润</strong> </p></li><li><p>垄断价格包括垄断高价和垄断低价</p><ul><li>垄断高价：垄断组织出售商品时规定的高于生产价格的价格</li><li>垄断低价：垄断组织在购买非垄断企业所生产的原材料等生产资料时规定的低于生产价格的价格</li></ul></li><li><p>垄断价格的产生并没有否定价值规律，它是价值规律在垄断资本主义阶段作用的具体表现。</p><ul><li><p>商品的价格围绕着商品的垄断价格自发波动</p><blockquote><p>价值规律的基本内容不变</p><p>表现形式：</p><p>简单商品经济：商品的价格围绕着商品的价值自发波动</p><p>自由竞争的资本主义商品经济：商品的价格围绕着商品的生产价格自发波动</p><p>垄断资本主义阶段：商品的价格围绕着商品的垄断价格自发波动</p></blockquote></li></ul></li></ul></li></ul></li></ul><h3 id="72-垄断资本主义的发展"><a href="#72-垄断资本主义的发展" class="headerlink" title="72.垄断资本主义的发展"></a>72.垄断资本主义的发展</h3><ul><li><p><strong>国家垄断资本主义</strong>：<strong>国家政权</strong>和<strong>私人垄断资本</strong>融合在一起的垄断资本主义</p><ul><li><p>国家垄断资本主义的<strong>形成原因</strong>：</p><ul><li>首先，（根本原因）社会生产力的发展，要求资本主义生产资料在更大范围内被支配，从而促进了国家垄断资本主义的产生</li><li>其次，经济波动和经济危机的深化，要求国家垄断资本主义的产生。</li><li>最后，缓和社会矛盾、协调利益关系，也要求国家垄断资本主义的产生。</li></ul></li><li><p>国家垄断资本主义的<strong>主要形式</strong>：</p><ul><li><p>国家所有并直接经营的企业</p></li><li><p>国家与私人共有、合营企业</p></li><li><p>国家通过多种形式参与私人垄断资本的再生产过程，包括国家作为商品和劳务的采购者，向私人垄断企业订货、提供补贴</p></li><li><p><strong>宏观调节</strong>：国家运用财政政策、货币政策等经济手段，对社会总供给和总需求进行调节</p><ul><li>目标：经济快速增长、充分就业、物价稳定和国际收支平衡</li></ul></li><li><p><strong>微观规制</strong> ：国家运用法律手段规范市场秩序，限制垄断，保护竞争、维护社会公众的合法利益</p><ul><li><p>目标：规范市场秩序，限制垄断，保护竞争、维护社会公众的合法利益</p></li><li><p>类型：其一是反托拉斯法（反垄断法）；其二是公共事业规制；其三是社会经济规制</p></li></ul></li></ul></li><li><p>对国家垄断资本主义的评价</p><ul><li>国家垄断资本主义是垄断资本主义的新发展，它对资本主义经济的发展产生了积极的作用</li><li>但是，国家垄断资本主义的出现并没有改变垄断资本主义的性质。</li><li>国家垄断资本主义的出现是资本主义<strong>经济制度内</strong>的经济关系调整，并没有从根本上消除资本主义的基本矛盾。</li></ul></li></ul></li><li><p>金融垄断资本的发展</p><ul><li>金融自由化和金融创新是金融垄断资本得以形成和壮大的重要制度条件</li><li>垄断资本主义的金融化程度不断提高：（体现在）<ol><li>金融业在国民经济中的地位大幅上升</li><li>实体经济的资本利润率下降</li><li>造业就业人数严重减少</li><li>虚拟经济越来越脱离实体经济</li></ol></li><li>金融垄断资本的发展，一方面促进了资本主义的发展，另一方面也造成了经济过度虚拟化，导致金融危机频繁发生，不仅给资本主义经济，也给全球经济带来灾难。</li></ul></li><li><p>垄断资本在世界范围的扩展</p><ul><li><p>垄断资本向世界范围扩展到经济动因：</p><ol><li>将国内过剩的资本输出</li><li>将部分非要害技术转移到国外</li><li>争夺商品销售市场</li><li>确保原材料和能源的可靠来源</li></ol></li><li><p>垄断资本向世界范围扩展到基本形式：</p><ol><li>借贷资本输出</li><li>生产资本输出</li><li>商品资本输出</li></ol><ul><li>输出资本的来源：一是私人资本输出；二是国家资本输出。</li></ul></li><li><p>经济社会后果：对资本输出国来说是有利的，对资本输入国来说是一把双刃剑。</p></li></ul></li><li><p><strong>国际垄断同盟</strong>：在经济上瓜分世界是通过垄断组织间的协议实现的，而协议的订立、瓜分的结果又以经济实力为后盾和基础。</p><ul><li>早期的国际垄断同盟主要是<strong>国际卡特尔</strong>。</li><li>当代的国际垄断同盟的形式以跨国公司和<strong>国家垄断资本主义的国际联盟</strong>为主。<ul><li>国家垄断资本主义的国际联盟：是由一些资本主义国家的政府出面缔结协定所组成的国际经济集团，如西方七国集团、欧盟。</li></ul></li></ul></li><li><p>第二次世界大战后，从事国际经济协调、维护国际经济秩序的<strong>国际性协调组织</strong>主要有三个：</p><ul><li>国际货币基金组织</li><li>世界银行</li><li>世界贸易组织</li></ul></li><li><p>评价：</p><p><em>垄断国际化条件下各种形式的国际垄断组织、国际垄断同盟和国际经济协调机构的发展，在一定程度上促进了经济全球化的发展，但它们从根本上说是<strong>为了维护资产阶级的利益</strong>、为他们攫取高额垄断利润服务的。</em></p></li><li><p><em>垄断资本主义的五个基本特征：</em></p><ol><li>垄断组织在经济生活中起决定作用</li><li>在金融资本的基础上形成金融寡头的统治</li><li>资本输出有了特别重要的意义</li><li>瓜分世界的资本家国际垄断同盟已经形成</li><li>最大资本主义列强已把世界上的领土分割完毕</li></ol></li></ul><h3 id="73-经济全球化及其后果"><a href="#73-经济全球化及其后果" class="headerlink" title="73.经济全球化及其后果"></a>73.经济全球化及其后果</h3><ul><li>经济全球化的表现<ul><li>国际分工进一步分化</li><li>贸易的全球化</li><li>金融的全球化（资本）</li><li>企业经营的全球化</li></ul></li><li>导致经济全球化迅猛发展的因素：<ul><li>科学技术的进步和生产力的发展（根本因素）</li><li>跨国公司的发展</li><li>各国经济体制的变革</li></ul></li><li>经济全球化的影响：“双刃剑”<ul><li>消极后果：<ol><li>发达国家与发展中国家在经济全球化过程中的地位和收益不平等、不平衡</li><li>加剧了发展中国家资源短缺和环境污染恶化</li><li>一定程度上增加经济风险</li></ol></li></ul></li></ul><h1 id="科学社会主义"><a href="#科学社会主义" class="headerlink" title="科学社会主义"></a>科学社会主义</h1><p>略</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这学期马原毛概并举，政治知识储备达到巅峰qwq&lt;/p&gt;
&lt;p&gt;看着徐涛老师的政治课复习，做了些许笔记，做个总结分享出来。&lt;/p&gt;
&lt;p&gt;「哲学们只是用不同的方式解释世界，而问题在于改变世界。——卡尔·马克思」&lt;/p&gt;
&lt;p&gt;马克思主义真是大智慧！！！&lt;/p&gt;
    
    </summary>
    
    
      <category term="政治" scheme="https://f7ed.com/categories/%E6%94%BF%E6%B2%BB/"/>
    
    
      <category term="政治" scheme="https://f7ed.com/tags/%E6%94%BF%E6%B2%BB/"/>
    
      <category term="马克思主义" scheme="https://f7ed.com/tags/%E9%A9%AC%E5%85%8B%E6%80%9D%E4%B8%BB%E4%B9%89/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:Unsupervised-PCA</title>
    <link href="https://f7ed.com/2020/10/31/unsupervised-learning-pca/"/>
    <id>https://f7ed.com/2020/10/31/unsupervised-learning-pca/</id>
    <published>2020-10-30T16:00:00.000Z</published>
    <updated>2020-10-31T14:38:26.963Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章详细讲解了无监督学习（Unsupervised learning）的PCA（主成分分析法）。</p><p>文章开篇从聚类（Clustering）引出Distributed Represention，其中粗略阐述了聚类中K-means和HAC（层次聚类）的思想。</p><p>文章的后半部分具体阐述了PCA的数学细节，PCA的去相关性性质，PCA的另一种解释角度（component的角度），PCA的不足等。</p><a id="more"></a><h1 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h1><p>无监督学习分为两种：</p><ul><li><p>Dimension Reduction：化繁为简。</p><p>function 只有input，能将高维、复杂的输入，抽象为低维的输出。</p><p>如下图，能将3D的折叠图像，抽象为一个2D的表示（把他摊开）。</p><img src="https://s1.ax1x.com/2020/10/31/BaoiAe.png" alt="BaodiAe.png" style="zoom:25%;" /></li><li><p>Generation：无中生有。</p><p>function 只有output。</p><p>（后面的博客会提及）</p></li></ul><h2 id="Dimension-Reduction"><a href="#Dimension-Reduction" class="headerlink" title="Dimension Reduction"></a>Dimension Reduction</h2><p>此前，在<a href="/2020/07/03/semi-supervised/" title="semi-supervised learning">semi-supervised learning</a>的最后，提及过better presentation的思想，Dimension Reduction 其实就是这样的思想：去芜存菁，化繁为简。</p><p>比如，在MNIST中，一个数字的表示是28*28维度的向量（图如左），但大多28 *28维度的向量（图为右）都不是数字。</p><img src="https://s1.ax1x.com/2020/10/31/BaopnK.png" alt="BaopnK.png" style="zoom:33%;" /> <p>因此，在表达下图一众“3”的图像中，根本不需要28*28维的向量表示，1-D即可表示一张图（图片的旋转角度）。28 * 28的图像表示就像左边中老者的头发，1-D的表示就像老者的头，是对头发运动轨迹一种更简单的表达。</p><img src="https://s1.ax1x.com/2020/10/31/Bao90O.png" alt="Bao90O.png" style="zoom:30%;" /><h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><p>在将Dimension Reduction之前，先将一种经典的无监督学习——clustering.</p><p>clustering也是一种降维的表达，将复杂的向量空间抽象为简单的类别，用某一个类别来表示该数据点。</p><p>这里主要讲述cluster的主要思想，算法细节可参考<a href="https://zhuanlan.zhihu.com/p/34168766">其他资料</a> 。（待补充）</p><h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><p>K-means的做法是：</p><ol><li><p>Clustering $X=\left\{x^{1}, \cdots, x^{n}, \cdots, x^{N}\right\}$ into K clusters.</p><p>把所有data分为K个类，K的确定是empirical的，需要自己确定</p></li><li><p>Initialize cluster center $c^i, i=1,2,…,K$ .(K random $x^n$ from $X$)</p><p>初始化K个类的中心数据点，建议从training set $X$ 中随机选K 个点作为初始点。</p><p>不建议直接在向量空间中随机初始化K个中心点，因为很可能随机的中心点不属于任何一个cluster。</p></li><li><p>Repeat：根据中心点标记所属类，再更新新的中心点，重复直收敛。</p><ol><li><p>For all $x^n$ in $X$ : 标记所属类。</p>      $$      b_{i}^{n}\left\{\begin{array}{ll}1 & x^{n} \text { is most "close" to } c^{i} \\ 0 & \text { Otherwise }\end{array}\right.      $$      </li><li><p>Updating all $c^i$ :   $c^{i}=\sum_{x^{n}} b_{i}^{n} x^{n} / \sum_{x^{n}} b_{i}^{n}$  (计算该类中心点)</p></li></ol></li></ol><h3 id="HAC：Hierarchical-Agglomerative-Clustering-HAC"><a href="#HAC：Hierarchical-Agglomerative-Clustering-HAC" class="headerlink" title="HAC：Hierarchical Agglomerative Clustering(HAC)"></a>HAC：Hierarchical Agglomerative Clustering(HAC)</h3><p>另一种clustering的方法是层次聚类（Hierarchical Clustering），这里介绍Agglomerative（自下而上）的策略。</p><img src="https://s1.ax1x.com/2020/10/31/BaIzX6.png" alt="BaIzX6.png" style="zoom:33%;" /><ol><li><p>Build a tree.</p><ol><li>如上图中，计算当前两两数据点（点或组合）的相似度（欧几里得距离或其他）。</li><li>选出最相近的两个合为一组（即连接在同一父子结点上，如最左边的两个）</li><li>重复1-2直至最后合为root。</li></ol><p>该树中，越早分支的点集合，说明越不像。</p></li><li><p>Pick a threshold.</p><p>选一个阈值，即从哪个地方开始划开，比如选上图中红色的线作为阈值，那么点集分为两个cluseter，蓝色、绿色同理。</p></li></ol><p>HAC和K-means相比，HAC不直接决定cluster的数目，而是通过决定threshold的值间接决定cluster的数目。</p><h2 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h2><p>Cluster：an object must belong to one cluster.</p><p>在做聚类时，一个数据点必须标注为某一具体类别。这往往会丢失很多信息，比如一个人可能是70%的外向，30%的内敛，如果做clustering，就将这个人直接归为外向，这样的表示过于粗糙。</p><p>因此仍用vector来表示这个人，如下图。</p><img src="https://s1.ax1x.com/2020/10/31/BaIv11.png" alt="BaIv11.png" style="zoom:33%;" /> <p>Distributed Representation（也叫Dimension Reduction）就是：一个高维的vector通过function，得到一个低维的vector。</p><img src="https://s1.ax1x.com/2020/10/31/BaIOh9.png" alt="BaIOh9.png" style="zoom:33%;" /><p>Distributed的方法有常见的两种：</p><ul><li><p>Feature selection：</p><p>如下图数据点的分布，可以直接选择feature $x_2$ .</p><img src="https://s1.ax1x.com/2020/10/31/BaILtJ.png" alt="BaIdLtJ.png" style="zoom:33%;" /> <p>但这种方法往往只能处理2-D的情况，对于下图这种3-D情况往往不好做特征选择。</p><img src="https://s1.ax1x.com/2020/10/31/Ba70p9.png" alt="Ba70pd9.png" style="zoom:50%;" /> </li><li><p>Principle component analysis（PCA）</p><p>另一种方法就是著名的PCA，主成分分析法。</p><p>PCA中，这个function就是一个简单的linear function（$W$），通过 $z=Wx$ ，将高维的 $x$ 转化为低维的 $z$ .</p></li></ul><h1 id="PCA：Principle-Component-Analysis"><a href="#PCA：Principle-Component-Analysis" class="headerlink" title="PCA：Principle Component Analysis"></a>PCA：Principle Component Analysis</h1><p>PCA的参考资料见Bishop, Chapter12.</p><p>PCA就是要找 $z=Wx$ 中的 $W$ .</p><h2 id="Main-Idea"><a href="#Main-Idea" class="headerlink" title="Main Idea"></a>Main Idea</h2><h3 id="Reduce-1-D"><a href="#Reduce-1-D" class="headerlink" title="Reduce 1-D"></a>Reduce 1-D</h3><p>如果将dimension reduce to 1-D，那么可以得出 $z_1 = w^1\cdot x$ .</p><p>$w^1$ 是vector，$x$ 是vector，做内积。</p><p>如下图，内积即投影，将所有的点 $x$ 投影到 $w^1$ 方向上，然后得到对应的 $z_1$  值。</p><img src="https://s1.ax1x.com/2020/10/31/BaIjpR.png" alt="BaIjpR.png" style="zoom:33%;" /> <p>而对于得到的一系列 $z_1$ 值，我们希望 $z_1$ 的variance越大越好。</p><p>因为 $z_1$ 的分布越大，用 $z_1$ 来刻画数据，才能更好的区分数据点。</p><p>如下图，如果 $w^1$ 的方向是small variance的方向，那么这些点会集中在一起，而large variance方向，$z_1$ 能更好的刻画数据。</p><img src="https://s1.ax1x.com/2020/10/31/BaIH7F.png" alt="BadIH7F.png" style="zoom:33%;" /><p>$z_1$ 的数学表达是： $ \operatorname{Var}\left(z_{1}\right)=\frac{1}{N} \sum_{z_{1}}\left(z_{1}-\overline{z_{1}}\right)^{2} \quad \left\|w^{1}\right\|_{2}=1$  (后文解释为什么要 $w^1$ 的长度为1)</p><h3 id="Reduce-2-D"><a href="#Reduce-2-D" class="headerlink" title="Reduce 2-D"></a>Reduce 2-D</h3><p>同理，如果将dimension reduce to 2-D .</p><p>$z=Wx$ 即</p>$$\left\{ \begin{array}{11}z_1=w^1\cdot x \\ z_2=w^2 \cdot x  \end{array} \right. ,\quad W=\begin{bmatrix}(w^1)^T \\ (w^2)^T \end{bmatrix}$$<ul><li><p>将所有点 $x$ 投影到 $w^1$ 方向，得到对应的 $z_1$ ，且让 $z_1$ 的分布尽可能的大：</p>  $$  \operatorname{Var}\left(z_{1}\right)=\frac{1}{N} \sum_{z_{1}}\left(z_{1}-\overline{z_{1}}\right)^{2} ,\quad \left\|w^{1}\right\|_{2}=1  $$  </li><li><p>将所有点投影到 $w^2$ 方向，得到对应的 $z_2$ ，同样让 $z_2$ 的分布也尽可能大，再加一个约束条件，让 $w^2$ 和 $w^1$ 正交（后文会具体解释为什么）</p>  $$  \operatorname{Var}\left(z_{2}\right)=\frac{1}{N} \sum_{z_{2}}\left(z_{2}-\overline{z_{2}}\right)^{2} ,\quad \left\|w^{2}\right\|_{2}=1 ,\quad w^1\cdot w^2=0  $$  <p>因此矩阵 $W$ 是Orthogonal matrix (正交矩阵)。</p></li></ul><h2 id="Detail-Warning-of-Math"><a href="#Detail-Warning-of-Math" class="headerlink" title="Detail[Warning of Math"></a><font color=#f00>Detail[Warning of Math</font></h2><p><strong>想跳过math部分的，可以直接看Conclusion。</strong> </p><p>1-D中：</p><p>Goal：find $w^1$ to  maximum $(w^1)^T S w^1$  s.t.$(w^1)^Tw^1=1$</p><p><strong>结论：$w^1$ 就是协方差矩阵 $S$ 最大特征值 $\lambda_1 $  对应的特征向量。 s.t.$(w^1)^Tw^1=1$</strong> </p><p>2-D中：</p><p>Goal：find $w^2$ to maximum $(w^2)^TSw^2 $  s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ </p><p><strong>结论：$w^2$ 就是协方差矩阵$S$ 第二大特征值 $\lambda_2 $   对应的特征向量。 s.t.$(w^2)^Tw^2=1$</strong>  </p><p>k-D中：</p><p><strong>结论：$w$ 就是协方差矩阵 $S$ 前 $k$ 大的特征值对应的特征向量。s.t. $W$ 是正交矩阵。</strong></p><h3 id="1-D"><a href="#1-D" class="headerlink" title="1-D"></a>1-D</h3><p><strong>Goal：Find $w^1$  to maximum the variance of $z_1$ .</strong> </p><ul><li> $\operatorname{Var}\left(z_{1}\right)=\frac{1}{N} \sum_{z_{1}}\left(z_{1}-\overline{z_{1}}\right)^{2}$  <ul><li> $z_1=w^1\cdot x  ,\quad \overline{z_{1}}=\frac{1}{N} \sum_{z_{1}}=\frac{1}{N} \sum w^{1} \cdot x=w^{1} \cdot \frac{1}{N} \sum x=w^{1} \cdot \bar{x}$  </li></ul></li><li>   $\operatorname{Var}\left(z_{1}\right)=\frac{1}{N} \sum_{z_{1}}\left(z_{1}-\overline{z_{1}}\right)^{2}=(w^1)^T\operatorname{Cov}(x)w^1$  <ul><li>  $=\frac{1}{N} \sum_{x}\left(w^{1} \cdot x-w^{1} \cdot \bar{x}\right)^{2} $ </li><li>  $=\frac{1}{N} \sum\left(w^{1} \cdot(x-\bar{x})\right)^{2}$ <ul><li><p>$a,b$ 是vector：</p> $(a\cdot b)^2=(a^Tb)^2=a^Tba^Tb$ </li><li><p>$a^Tb$ 是scalar:</p> $(a\cdot b)^2  = (a^Tb)^2=a^Tba^Tb =a^Tb(a^Tb)^T=a^Tbb^Ta$ </li></ul></li><li> $=\frac{1}{N} \sum\left(w^{1}\right)^{T}(x-\bar{x})(x-\bar{x})^{T} w^{1}$ </li><li> $ = \left(w^{1}\right)^{T}\sum\frac{1}{N}(x-\bar{x})(x-\bar{x})^{T} \ w^{1}$ </li><li> $=(w^1)^T\operatorname{Cov}(x)w^1$ </li></ul></li><li><p>令 $S=\operatorname{Cov}(x)$ </p></li></ul><p>之前遗留的两个问题：</p><ol><li>$\left|w^1\right|_2=1$ ?</li><li>$w^1\cdot w^2=1$ ?</li></ol><p>现在来看第一个问题，为什么要 $\left|w^1\right|_2=1$ ？</p><p>现在的目标，变成了 maximum $(w^1)^T S w^1$ ，如果不限制 $\left|w^1\right|_2$ ，让 $\left|w^1\right|_2$ 无穷大，那么 $(w^1)^T S w^1$ 的值也会无穷大，问题无解了。</p><hr><p><strong>Goal：maximum $(w^1)^T S w^1$  s.t. $(w^1)^Tw^1=1$</strong></p><ul><li><p>Lagrange multiplier[挖坑] 求解多元变量在有限制条件下的驻点。</p><p>构造拉格朗日函数： $g\left(w^{1}\right)=\left(w^{1}\right)^{T} S w^{1}-\alpha\left(\left(w^{1}\right)^{T} w^{1}-1\right)$   ，$\alpha\neq 0$ 为拉格朗日乘数</p><ul><li>$\nabla_{w^1}g=0$ 的值为驻点（会单独写一篇博客来讲拉格朗日乘数）</li><li>$\frac{\partial g}{\partial \alpha}=0$ 为限制函数</li></ul></li><li><p>对矩阵微分：详情见<a href="https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors">wiki</a> </p><ul><li><p>scalar-by-vector(scalar对vector微分)</p><img src="https://s1.ax1x.com/2020/10/31/BaIIXV.png" alt="BaIIddXV.png" style="zoom:67%;" /></li><li><p>$S$ 是对称矩阵，不是 $w^1$ 的函数，结果用 $w^1$ 表达：$2Sw^1-2\alpha w^1=0$ </p></li></ul></li><li><p>maximum: $(w^1)^T S w^1=\alpha (w^1)^Tw^1=\alpha$ </p></li></ul><p>*<em>Goal：find $w^1$to maximum $\alpha$   *</em>    </p><ul><li><p>$\alpha$ 满足等式：$Sw^1=\alpha w^1$ </p></li><li><p>$\alpha$ 是 $S$ 的特征向量，$w^1$ 是 $S$ 对应于特征值 $\alpha$  的特征向量。</p><ul><li>关于特征值和特征向量的知识参考：参考下面线代知识</li></ul></li><li><p>$w^1$ is the eigenvector(特征向量) of the covarivance matrix S corresponding to the largest eigenvalue $\lambda_1$ . </p><p><strong>结论：$w^1$ 就是协方差矩阵最大特征值对应的特征向量。</strong> </p></li></ul><hr><blockquote><p><strong>梦回线代</strong>QWQ（自己线代学的太差啦 啊这！</p><ol><li><p>特征向量，特征值定义：</p><p>$A$ 是n阶方阵，如果存在数 $\lambda$ 和n维非零向量 $\alpha$ ，满足 $A\alpha=\lambda \alpha$ ,</p><p>则称 $\lambda$ 为方阵 $A$ 的一个特征值，$\alpha$ 为方阵 $A$ 对应于特征值 $\lambda$ 的一个特征向量。</p></li><li><p>求解特征向量和特征值：</p><p>$A\alpha -\lambda \alpha=(A-\lambda I)\alpha=0$ </p><p>齐次方程有非零解的充要条件是特征方程 $det(A-\lambda I)=0$ （行列式为0）</p><ul><li>根据特征方程先求解出 $\lambda$ 的所有值。</li><li>再根将 $\lambda$ 代入齐次方程，求解齐次方程的解 $\alpha$ ，即为对应 $\lambda$ 的特征向量。</li></ul></li></ol></blockquote><h3 id="2-D"><a href="#2-D" class="headerlink" title="2-D"></a>2-D</h3><p><strong>Goal：find $w^2$ to maximum $(w^2)^TSw^2 $  s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$</strong>  </p><ul><li>构造拉格朗日函数： $g\left(w^{2}\right)=\left(w^{2}\right)^{T} S w^{2}-\alpha\left(\left(w^{2}\right)^{T} w^{2}-1\right)-\beta\left(\left(w^{2}\right)^{T} w^{1}-0\right)$ </li><li>对 $w^2$ 求微分，所求点满足等式： $S w^{2}-\alpha w^{2}-\beta w^{1}=0$ <ul><li>左乘 $(w^1) ^T$： $(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta(w^1)^Tw^1=0$ </li><li>已有： $(w^1)^Tw^2=0, (w^1)^Tw^1=1$  </li><li>证明：$ (w^1)^TSw^2=0$ <ul><li>$\because (w^1)^TSw^2$ 是scalar</li><li> $\therefore (w^1)^TSw^2=((w^1)^TSw^2)^T=(w^2)^TS^Tw^1$  </li><li>$\because S^T=S$ (协方差矩阵是对称矩阵)</li><li>$\because Sw^1=\lambda_1 w^1$ </li><li>$\therefore (w^1)^TSw^2=(w^2)^TSw^1=\lambda_1(w^2)^Tw^1=0$  </li></ul></li><li> $(w^1)^TSw^2-\alpha (w^1)^Tw^2-\beta(w^1)^Tw^1=0-\alpha\cdot 0-\beta \cdot 1=0$ </li><li>$\therefore \beta=0$  </li></ul></li><li>$w^2$ 满足等式：$S w^{2}-\alpha w^{2}=0$ </li><li>和1-D的情况相同：find $w^2$ maximum $(w^2)^TSw^2$ <ul><li>$(w^2)^TSw^2=\alpha$ </li><li>$w^2$  is the eigenvector(特征向量) of the covarivance matrix S corresponding to the largest eigenvalue $\lambda_2$ .</li></ul></li><li>OVER!</li></ul><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>最后解决之前的Q2：$(w^1)^Tw^2=0$ ?</p><ul><li><p>先说明一下$S$ 的性质：</p><p>是对称矩阵，对应不同特征值对应的特征向量都是正交的。</p><p>（参考1，2）</p><p>也是半正定矩阵，其特征值都是非负的。</p><p>（参考4，5，6）</p></li><li><p>其次关于 $W$ 的性质 $ W=\begin{bmatrix}(w^1)^T \\ (w^2)^T \\ ...\end{bmatrix}$ ,易得 $W$ 是orthogonal matrix(正交矩阵)。</p></li><li><p>所以这是一个约束条件，能让PCA的最优化问题转化为求其特征值的问题。</p></li><li><p>（具体见下一小节：PCA-decorrelation）</p><p>其次 $z=Wx$ ，也因为 $W$ 的正交性质，让 $z$ 的各维度（特征）decorrelation，去掉相关性，降维后的特征相互独立，方便后面generative model的假设。</p></li></ul><blockquote><ol><li><p>$S=Cov(x)$ 为实对称矩阵。</p></li><li><p>实对称矩阵的性质：$A$ 是一个实对称矩阵，对于于 $A$ 的不同特征值的特征向量彼此正交。</p></li><li><p>正交矩阵的性质：$W^TW=WW^T=I$ </p></li><li><p>$Var(z)=(w^1)^T S w^1\geq 0$ ，方差一定大于等于0 。</p></li><li><p>半正定矩阵的定义：</p><p>实对称矩阵 $A$ ，对任意非零实向量 $X$ ，如果二次型 $f(X)=X^TAX\geq0$ ，</p><p>则有实对称矩阵 $A$ 是半正定矩阵。</p></li><li><p>半正定矩阵的性质：半正定矩阵的特征值都是非负的。</p></li></ol></blockquote><hr><p>1-D中：</p><p>Goal：find $w^1$ to  maximum $(w^1)^T S w^1$  s.t.$(w^1)^Tw^1=1$</p><p><strong>结论：$w^1$ 就是协方差矩阵 $S$ 最大特征值 $\lambda_1 $  对应的特征向量。 s.t.$(w^1)^Tw^1=1$</strong> </p><p>2-D中：</p><p>Goal：find $w^2$ to maximum $(w^2)^TSw^2 $  s.t. $(w^2)^Tw^2=1, (w^2)^Tw^1=0$ </p><p><strong>结论：$w^2$ 就是协方差矩阵$S$ 第二大特征值 $\lambda_2 $   对应的特征向量。 s.t.$(w^2)^Tw^2=1$</strong>  </p><p>k-D中：</p><p><strong>结论：$w$ 就是协方差矩阵 $S$ 前 $k$ 大的特征值对应的特征向量。s.t. $W$ 是正交矩阵。</strong> </p><h2 id="PCA-decorrelation"><a href="#PCA-decorrelation" class="headerlink" title="PCA-decorrelation"></a>PCA-decorrelation</h2><p>$z=Wx$ </p><p>通过PCA找到的 $W$ ，$x$ 得到新的presentation $z$ ，如下图。</p><img src="https://s1.ax1x.com/2020/10/31/BaITmT.png" alt="BaIdTmT.png" style="zoom:40%;" /><p>可见，经过PCA后，original data变为decorrelated data，各维度（feature）是去相关性的，即各维度是独立的，方便generative model的假设（比如Gaussian distribution).</p><p> $z$ 是docorrelated，即 $Cov(z)=D$ 是diagonal matrix(对角矩阵)</p><p>证明：$Cov(z)=D$ is diagonal matrix</p><ul><li> $W=\begin{bmatrix}(w^1)^T \\ (w^2)^T \\ ...\end{bmatrix}$ ，$S=\operatorname{Cov}(x)$ </li><li> $\operatorname{Cov}(z)=\frac{1}{N} \sum(z-\bar{z})(z-\bar{z})^{T}=W S W^{T}$ </li><li> $=W S\left[\begin{array}{lll}w^{1} & \cdots & w^{K}\end{array}\right]=W\left[\begin{array}{lll}S{w}^{1} & \cdots & S w^{K}\end{array}\right]$ </li><li> $=W\left[\lambda_{1} w^{1} \quad \cdots \quad \lambda_{K} w^{K}\right]=\left[\lambda_{1} W w^{1} \quad \cdots \quad \lambda_{K} W w^{K}\right]$   ($\lambda$ is scalar) </li><li> $=\left[\begin{array}{lll}\lambda_{1} e_{1} & \cdots & \lambda_{K} e_{K}\end{array}\right]=D$ ($W$ is orthogonal matrix) </li></ul><h2 id="PCA-Another-Point-of-View"><a href="#PCA-Another-Point-of-View" class="headerlink" title="PCA-Another Point of View"></a>PCA-Another Point of View</h2><h3 id="Main-Idea-Component"><a href="#Main-Idea-Component" class="headerlink" title="Main Idea: Component"></a>Main Idea: Component</h3><p>PCA看作是一些basic component的组成，如下图，手写数字都是一些基本笔画组成的，记做  $\{u^1,u^2,u^3,...\}$ </p><img src="https://s1.ax1x.com/2020/10/31/BaI4lq.png" alt="BaId4lq.png" style="zoom:25%;" /><p>因此，下图的”7”的组成为 $\{u^1,u^3,u^5\}$ </p><img src="https://s1.ax1x.com/2020/10/31/BaIhpn.png" alt="BadIhpn.png" style="zoom:25%;" /><p>所以原28*28 vector $x$ 表示的图像能近似表示为：</p>$$x \approx c_{1} u^{1}+c_{2} u^{2}+\cdots+c_{K} u^{K}+\bar{x}$$<p>其中  $\{u^1,u^2,u^3,...\}$ 是compoment的vector表示， $\{c^1,c^2,c^3,...\}$ 是component的系数，$\bar{x}$ 是所有images的平均值。</p><p>因此 $\begin{bmatrix}c_1 \\c_2 \\... \\ c_k \end{bmatrix}$ 也能表示一个数字图像。</p><p>现在问题是找到这些component $\{u^1,u^2,u^3,...\}$ , 再得到 他的线形表出 $\begin{bmatrix}c_1 \\c_2 \\... \\ c_k \end{bmatrix}$ 就是我们想得到的better presentation.</p><h3 id="Detail"><a href="#Detail" class="headerlink" title="Detail"></a>Detail</h3><p>要满足：$x \approx c_{1} u^{1}+c_{2} u^{2}+\cdots+c_{K} u^{K}+\bar{x}$  </p><p>即，$x -\bar{x}\approx c_{1} u^{1}+c_{2} u^{2}+\cdots+c_{K} u^{K}$ ，等式两边的误差要尽量小。</p><p>问题变成：找 $\{u^1,u^2,u^3,...\}$ minimize the reconstruction error = $\|(x-\bar{x})-\hat{x}\|_2$ .</p><p>损失函数： $L=\min _{\left\{u^{1}, \ldots, u^{K}\right\}} \sum\left\|(x-\bar{x})-\left(\sum_{k=1}^{K} c_{k} u^{k}\right)\right\|_{2}$ </p><p>而求解PCA的过程就是在minimize损失函数 $L$ ，PCA中求解出的  $\{w^1,w^2,...,w^K\}$ 就是这里的component  $\{u^1,u^2,...,u^K\}$ .(Proof 见Bisho, Chapter 12.1.2)</p><p>*<em>Goal:  minimize the reconstruction error = $\|(x-\bar{x})-\hat{x}\|_2$ *</em>  </p><ul><li><p>$x -\bar{x}\approx c_{1} u^{1}+c_{2} u^{2}+\cdots+c_{K} u^{K}$ </p></li><li><p>每个sample:  $\left\{ \begin{matrix} x^{1}-\bar{x} \approx c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\cdots \\ x^{2}-\bar{x} \approx c_{1}^{2} u^{1}+c_{2}^{2} u^{2}+\cdots \\x^{3}-\bar{x} \approx c_{1}^{3} u^{1}+c_{2}^{3} u^{2}+\cdots \\ ...\end{matrix} \right.$  </p><ul><li><p>下图中 $X=x-\bar{x}$ 矩阵的第一列都和上面的 $x^1-\bar{x}$ 对应：</p><img src="https://s1.ax1x.com/2020/10/31/BaIWfs.png" alt="BaIWfds.png" style="zoom:25%;" /> </li><li><p>而上面的 $c_{1}^{1} u^{1}+c_{2}^{1} u^{2}+\cdots$ 和下图的component矩阵乘系数矩阵的第一列对应：</p><img src="https://s1.ax1x.com/2020/10/31/BaIRYj.png" alt="BaIRYj.png" style="zoom:25%;" /> </li></ul></li><li><p>因此，是要让下图矩阵的结果 minimize error：</p><img src="https://s1.ax1x.com/2020/10/31/Ba7dfJ.png" alt="Ba7dfJ.png" style="zoom:40%;" /> </li><li><p>如何求解: SVD矩阵分解-其实就是最大近似分解（挖坑）</p><p>SVD能将一个任意的矩阵，分解为下面三个矩阵的乘积。</p><img src="https://s1.ax1x.com/2020/10/31/BaIym8.png" alt="BaIym8.png" style="zoom:30%;" /><p>$X = U\Sigma V$ </p><ul><li>$U,V$ 都是orthogonal matrix，$\Sigma$ 是diagonal matrix。</li><li>组成$U$ (M*K) 的K个列向量是 $XX^T$ 矩阵的前K大特征值对应的特征向量。</li><li>组成 $V$ (K*N)的K个行向量是 $X^TX$ 矩阵的前K大特征值对应的特征向量。</li><li>$XX^T$ 和 $X^TX$ 的特征值相同</li><li>$\Sigma$ 的对角值 $\sigma_i=\sqrt{\lambda_i}$ </li></ul></li><li><p>解：$U$ 矩阵作为 component矩阵， $\Sigma V$ 乘在一起作为系数矩阵。</p></li></ul><hr> $U=\{u^1,u^2,u^3,...\}$ 矩阵是$XX^T$ 的特征向量组成正交矩阵。<p>而PCA的解 $W^T=\{w^1,w^2,...,w^K\}$ 也是特征向量组成的正交矩阵。</p><p><strong>所以和PCA的关系：$U$ 矩阵是 $XX^T=Cov(x)$ 的特征向量，所以$U$ 矩阵就是PCA的解。</strong></p><h3 id="PCA-NN：Autoencoder"><a href="#PCA-NN：Autoencoder" class="headerlink" title="PCA-NN：Autoencoder"></a>PCA-NN：Autoencoder</h3><p>上文说到求解PCA的解 $\{w^1,w^2,...,w^K\}$ 就是在最小化restruction error $x -\bar{x}\approx \sum_{k=1}^K c_kw^k$ .</p><p>两者的联系就是PCA的解 $\{w^1,w^2,...,w^K\}$ 就是component $\{u^1,u^2,u^3,...\}$ ,且PCA的表示是 $z$  对应这里的 $c_k$  (第k个image的表示）.</p><p>PCA视角： $z=c_k=(x-\bar{x})\cdot w^k$  </p><p>PCA looks like a neural network with one hidden layer(linear activation function)。</p><p>把PCA视角看作一个NN，如下图，其hidden layer的激活函数是一个简单的线性激活函数。</p><p><img src="https://s1.ax1x.com/2020/10/31/BaIBlt.png" alt="BaIdBlt.png" style="zoom:25%;" /><img src="https://s1.ax1x.com/2020/10/31/BaI0SI.png" alt="BaI0dSI.png" style="zoom:25%;" /></p><p>再看component视角： $\hat{x}=\sum_{k=1}^K c_kw^k\approx x-\bar{x}$ </p><p><img src="https://s1.ax1x.com/2020/10/31/BaID6P.png" alt="BaIdD6P.png" style="zoom:25%;" /><img src="https://s1.ax1x.com/2020/10/31/BaIJeO.png" alt="BaIdJeO.png" style="zoom:25%;" /></p><p>PCA就构成了下面的NN，hidden layer可以是deep，这就是autoencoder(后面的博客会再详细讲)。</p><img src="https://s1.ax1x.com/2020/10/31/BaIdfA.png" alt="BaIdfA.png" style="zoom:30%;" /><p>用Gradient Descent对输入输出做minimize error，hidden layer的输出 $c$ 就是我们想要的编码（降维后的编码）。 </p><p>Q：用PCA求出的结果和用Gradient Descent训练NN的结果一样吗？</p><p>A：当然不一样，PCA的 $w$ 都是正交的，而NN的结果是gradient descent迭代出来的，并且该结果还会于初值有关。</p><p>Q：有了PCA，为什么还要用NN呢？</p><p>A：因为PCA只能处理linear的情况，对前文那种高维的非线形的无法处理，而NN可以是deep的，能较好处理非线形的情况。</p><h2 id="tips-how-many-components"><a href="#tips-how-many-components" class="headerlink" title="tips: how many components?"></a>tips: how many components?</h2><p>比如在对Pokemon进行PCA时，有六个features，如何确定principle component的数目？</p><p>往往在实际操作中，会对每个component计算一个ratio，如图中的公式：</p><img src="https://s1.ax1x.com/2020/10/31/BaIUFH.png" alt="BaIUFH.png" style="zoom:25%;" /> <p>因为每一个component对应一个eigenvector，每个eigenvector对应一个eigenvalue，而这个eigenvalue的值代表了在这个component的维度的variance有多大，越大当然能更好的表示。</p><p>因此计算eigenvalue的ratio，来找出分布较大的component作为主成分。</p><h2 id="More-About-PCA"><a href="#More-About-PCA" class="headerlink" title="More About PCA"></a>More About PCA</h2><p>如果对MNIST做PCA分析，结果如下图，会发现下面eigen-digits这些并不像数字的某个组成部分：</p><img src="https://s1.ax1x.com/2020/10/31/Ba7ym6.png" alt="Ba7dym6.png" style="zoom:35%;" /><p>同样，对face做PCA分析，结果下图：</p><img src="https://s1.ax1x.com/2020/10/31/Ba7BlR.png" alt="Ba7BlR.png" style="zoom:35%;" /><p>为什么呢？</p><p>在MNIST中，一张image的表示如下图：</p><img src="https://s1.ax1x.com/2020/10/31/BaIYwD.png" alt="BaIYwD.png" style="zoom:33%;" /> <p>其中，$\alpha$ 可以是任意实数，那么就有正有负，所以PCA的解包含了一些真正component的adding up and subtracting，所以MNIST的解不像这些数字的一部分。</p><p>如果想得到的解看起来像真正的component，可以规定图像只能是加，即 $\alpha$ 都是非负的。</p><ul><li>Non-negative matrix factorization(NMF)<ul><li>Forcing $\alpha$ be non-negative: additive combination</li><li>Forcing $w$ be non-negative: components more like “parts of digits”</li></ul></li></ul><h2 id="Weakness-of-PCA"><a href="#Weakness-of-PCA" class="headerlink" title="Weakness of PCA"></a>Weakness of PCA</h2><ol><li><p>PCA是unsupervised，因此可能不能区分本来是两个类别的东西。</p><img src="https://s1.ax1x.com/2020/10/31/BaItTe.png" alt="BaItTe.png" style="zoom:33%;" /> <p>如图，PCA的结果可能是上图的维度方向，但如果引入labeled data，更好的表达应该按照下图LDA的维度方向。</p><ul><li>LDA (Linear Discriminant Analysis) 是一种supervised的分析方法。</li></ul></li><li><p>PCA是Linear的，前文已经提及过，除了可以用NN的方式也有很多其他的non-linear的解法。</p><img src="https://s1.ax1x.com/2020/10/31/BaIaYd.png" alt="BaIaYd.png" style="zoom:45%;" /></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>HAC的算法细节待补充完善：<a href="https://zhuanlan.zhihu.com/p/34168766">https://zhuanlan.zhihu.com/p/34168766</a></p></li><li><p>PCA: Bishop, Chapter12. </p></li><li><p>线代知识：特征值、特征向量、实对称矩阵等：</p></li><li><p>拉格朗日乘数：Bishop, Appendix E</p></li><li><p>矩阵微分：<a href="https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors">https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors</a></p></li><li><p>Proof-PCA的过程就是在minimize损失函数 $L$ :Bisho, Chapter 12.1.2</p></li><li><p>SVD：</p><p><a href="https://www.cnblogs.com/pinard/p/6251584.html">https://www.cnblogs.com/pinard/p/6251584.html</a> </p><p><a href="https://www.youtube.com/watch?v=rYz83XPxiZo">https://www.youtube.com/watch?v=rYz83XPxiZo</a></p></li><li><p>NMF：Non-negative matrix factorization</p></li><li><p>LDA：Linear Discriminant Analysis</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章详细讲解了无监督学习（Unsupervised learning）的PCA（主成分分析法）。&lt;/p&gt;
&lt;p&gt;文章开篇从聚类（Clustering）引出Distributed Represention，其中粗略阐述了聚类中K-means和HAC（层次聚类）的思想。&lt;/p&gt;
&lt;p&gt;文章的后半部分具体阐述了PCA的数学细节，PCA的去相关性性质，PCA的另一种解释角度（component的角度），PCA的不足等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="Unsupervised" scheme="https://f7ed.com/tags/Unsupervised/"/>
    
      <category term="PCA" scheme="https://f7ed.com/tags/PCA/"/>
    
  </entry>
  
  <entry>
    <title>「PyTorch」：Tensors Explained And Operations</title>
    <link href="https://f7ed.com/2020/10/21/Tensors/"/>
    <id>https://f7ed.com/2020/10/21/Tensors/</id>
    <published>2020-10-20T16:00:00.000Z</published>
    <updated>2020-10-25T01:19:40.815Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch框架学习。</p><p>本篇文章主要介绍PyTorch中的Tensor及其基本操作，主要分为四个方面：Reshape, Element-wise, Reduction和Access。</p><p>Tensor的具体操作介绍，建议配合Colab笔记使用：</p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/PyTorch%20Tensors%20Explained.ipynb">PyTorch Tensors Explained</a> </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> </p><p> <a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operations: Element-wise</a>  </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a>  </p><p>英文的表达解释都是比较清晰且精确的，所以以英语的形式作为主要记录，文中会夹带一些中文总结语句，方便阅读。</p><a id="more"></a><h1 id="Introducing-Tensors"><a href="#Introducing-Tensors" class="headerlink" title="Introducing Tensors"></a>Introducing Tensors</h1><h2 id="Tensor-Explained-Data-Structures-of-Deep-Learning"><a href="#Tensor-Explained-Data-Structures-of-Deep-Learning" class="headerlink" title="Tensor Explained - Data Structures of Deep Learning"></a>Tensor Explained - Data Structures of Deep Learning</h2><h3 id="What-Is-A-Tensor"><a href="#What-Is-A-Tensor" class="headerlink" title="What Is A Tensor?"></a>What Is A Tensor?</h3><p>A tensor is the primary data structure used by neural networks.</p><p>【Tensor是NN中最主要的数据结构】</p><h4 id="Indexes-Required-To-Access-An-Element"><a href="#Indexes-Required-To-Access-An-Element" class="headerlink" title="Indexes Required To Access An Element"></a>Indexes Required To Access An Element</h4><p>The relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure.</p><p>【以下pairs都是需要同等数量的indexes才能确定特定的元素。】</p><p>【而tensor是generalizations，是一种统一而普遍的定义。】</p><table><thead><tr><th>Indexes required</th><th>Computer science</th><th>Mathematics</th></tr></thead><tbody><tr><td>0</td><td>number</td><td>scalar</td></tr><tr><td>1</td><td>array</td><td>vector</td></tr><tr><td>2</td><td>2d-array</td><td>matrix</td></tr></tbody></table><h3 id="Tensors-Are-Generalizations"><a href="#Tensors-Are-Generalizations" class="headerlink" title="Tensors Are Generalizations"></a>Tensors Are Generalizations</h3><p>When more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language.</p><h4 id="Mathematics"><a href="#Mathematics" class="headerlink" title="Mathematics"></a>Mathematics</h4><p>In mathematics, we stop using words like scalar, vector, and matrix, and we start using the word <em>tensor</em> or nd-tensor. The n tells us the number of indexes required to access a specific element within the structure.</p><p>【数学中，当我们需要用大于两个的indexes才能确定特点元素时，我们使用tensor或者nd-tensor来表示该数据结构，说明需要n个index才能确定该数据结构中的特定元素。】</p><h4 id="Computer-Science"><a href="#Computer-Science" class="headerlink" title="Computer Science"></a>Computer Science</h4><p>In computer science, we stop using words like, number, array, 2d-array, and start using the word <em>multidimensional array</em> or nd-array. The <code>n</code> tells us the number of indexes required to access a specific element within the structure.</p><p>【计算机科学中，我们使用nd-array来表示，因此，nd-array和tensor实则是一个东西。】</p><table><thead><tr><th>Indexes required</th><th>Computer science</th><th>Mathematics</th></tr></thead><tbody><tr><td>n</td><td>nd-array</td><td>nd-tensor</td></tr></tbody></table><p>Tensors and nd-arrays are the same thing!</p><p>One thing to note about the dimension of a tensor is that it differs from what we mean when we refer to the dimension of a vector in a vector space. The dimension of a tensor does not tell us how many components exist within the tensor.</p><p>【需要注意的地方是，tensor中的维度和vector向量空间中的维度不是同一个东西，vector向量空间中的维度表示该vector有多少个元素组成的，而tensor中的维度是下文中rank的含义。】</p><h2 id="Rank-Axes-And-Shape-Explained"><a href="#Rank-Axes-And-Shape-Explained" class="headerlink" title="Rank, Axes, And Shape Explained"></a>Rank, Axes, And Shape Explained</h2><p>【下文会详细解释深度学习tensor的几个重要性质：Rank, Axes, Shape.】</p><p>The concepts of rank, axes, and shape are the tensor attributes that will concern us most in deep learning.</p><ul><li>Rank</li><li>Axes</li><li>Shape</li></ul><h3 id="Rank-And-Indexes"><a href="#Rank-And-Indexes" class="headerlink" title="Rank And Indexes"></a>Rank And Indexes</h3><p>We are introducing the word <em>rank</em> here because it is commonly used in deep learning when referring to the number of dimensions present within a given tensor. </p><p>The rank of a tensor tells us how many indexes are required to access (refer to) a specific data element contained within the tensor data structure.</p><p>A tensor’s rank tells us how many indexes are needed to refer to a specific element within the tensor.</p><p>【这里的rank实则就是tensor的维度。】</p><p>【tensor的rank值告诉我们需要多少个indexes才能确定该tensor中的特定元素。】</p><h3 id="Axes-Of-A-Tensor"><a href="#Axes-Of-A-Tensor" class="headerlink" title="Axes Of A Tensor"></a>Axes Of A Tensor</h3><p>If we have a tensor, and we want to refer to a specific <em>dimension</em>, we use the word <em>axis</em> in deep learning.</p><p>An axis of a tensor is a specific dimension of a tensor.</p><p>Elements are said to exist or run along an axis. This <em>running</em> is constrained by the length of each axis. Let’s look at the length of an axis now.</p><h4 id="Length-Of-An-Axis"><a href="#Length-Of-An-Axis" class="headerlink" title="Length Of An Axis"></a>Length Of An Axis</h4><p>The length of each axis tells us how many indexes are available along each axis.</p><p>【当我们关注tensor的某一具体维度时，在深度学习中我们使用axis来表达。】</p><p>【元素被认为是在某一axie上存在或延伸的，元素延伸的长度取决于axis的长度。】</p><p>【Axis的长度表示在每一维度（axis）上有多少个索引】</p><h3 id="Shape-Of-A-Tensor"><a href="#Shape-Of-A-Tensor" class="headerlink" title="Shape Of A Tensor"></a>Shape Of A Tensor</h3><p>The <em>shape</em> of a tensor is determined by the length of each axis, so if we know the shape of a given tensor, then we know the length of each axis, and this tells us how many indexes are available along each axis.</p><p>The shape of a tensor gives us the length of each axis of the tensor.</p><p>【tensor的shape由每一axis的长度决定，即每一axis的索引数目】</p><p>Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called <em>reshaping</em>.</p><p>Reshaping changes the shape but not the underlying data elements.</p><p>【tensor的常见操作reshape只改变tensor的shape，而不改变底层的数据。】</p><h2 id="CNN-Tensors-Shape-Explained"><a href="#CNN-Tensors-Shape-Explained" class="headerlink" title="CNN Tensors Shape Explained"></a>CNN Tensors Shape Explained</h2><p>CNN的相关介绍，可见 <a href="/2020/04/25/CNN/" title="这篇文章">这篇文章</a></p><p>What I want to do now is put the concepts of rank, axes, and shape to use with a practical example. To do this, we’ll consider an image input as a tensor to a CNN.</p><p>Remember that the shape of a tensor encodes all the relevant information about a tensor’s axes, rank, and indexes, so we’ll consider the shape in our example, and this will enable us to work out the other values. </p><p>【tensor的shape能体现tensor的axes、rank、index所有信息】</p><p>【以CNN为例来说明rank, axes, shape.】</p><h3 id="Shape-Of-A-CNN-Input"><a href="#Shape-Of-A-CNN-Input" class="headerlink" title="Shape Of A CNN Input"></a>Shape Of A CNN Input</h3><p>The shape of a CNN input typically has a length of four. This means that we have a rank-4 tensor with four axes. Each index in the tensor’s shape represents a specific axis, and the value at each index gives us the length of the corresponding axis.</p><p>【CNN的input 是一个rank4-tensor.】</p><p>Each axis of a tensor usually represents some type of real world or logical feature of the input data. If we understand each of these features and their axis location within the tensor, then we can have a pretty good understanding of the tensor data structure overall.</p><p>【tensor的每个axis往往代表着某一个逻辑feature，所以理解features和tensor中axis的位置的关系能帮助我们更好的理解tensor。】</p><h4 id="Image-Height-And-Width"><a href="#Image-Height-And-Width" class="headerlink" title="Image Height And Width"></a>Image Height And Width</h4><p>To represent two dimensions, we need two axes.</p><p>The image height and width are represented on the last two axes.</p><p>【表示图像的height和width，需要2个axes，使用最后两个axes表示。】</p><h4 id="Image-Color-Channels"><a href="#Image-Color-Channels" class="headerlink" title="Image Color Channels"></a>Image Color Channels</h4><p>The next axis represents the color channels. Typical values here are <code>3</code> for RGB images or <code>1</code> if we are working with grayscale images. This color channel interpretation only applies to the input tensor.</p><p>【下一个axis(从右至左)表示图像的color channels（颜色通道，如灰度图像就有1个颜色通道，RGB图像有三个）。】</p><p>【注意：color channel的说法只适用于input tensor。】</p><h4 id="Image-Batches"><a href="#Image-Batches" class="headerlink" title="Image Batches"></a>Image Batches</h4><p>This brings us to the first axis of the four which represents the batch size. In neural networks, we usually work with batches of samples opposed to single samples, so the length of this axis tells us how many samples are in our batch.</p><p>Suppose we have the following shape <code>[3, 1, 28, 28]</code> for a given tensor. Using the shape, we can determine that we have a batch of three images.</p><p>【第一个axis表示batch属性，表明该batch的size。在深度学习中，我们通常使用一批样本，而不是一个单独的样本，所以这一维度表明了我们的batch中有多少样本。】</p><p>tensor：[Batch, Channels, Height, Width]</p><p>Each image has a single color channel, and the image height and width are <code>28 x 28</code> respectively.</p><ol><li>Batch size</li><li>Color channels</li><li>Height</li><li>Width</li></ol><h4 id="NCHW-vs-NHWC-vs-CHWN"><a href="#NCHW-vs-NHWC-vs-CHWN" class="headerlink" title="NCHW vs NHWC vs CHWN"></a>NCHW vs NHWC vs CHWN</h4><p>It’s common when reading API documentation and academic papers to see the <code>B</code> replaced by an <code>N</code>. The <code>N</code> standing for <em>number of samples</em> in a batch.</p><p>【在API文档或学术论文中，N经常会代替代替B，表示the number of samples in a batch。】</p><p>Furthermore, another difference we often encounter in the wild is a <em>reordering</em> of the dimensions. Common orderings are as follows:</p><ul><li><code>NCHW</code></li><li><code>NHWC</code></li><li><code>CHWN</code></li></ul><p>【除此之外，也会经常遇到这些axes的其他顺序。】</p><p>As we have seen, PyTorch uses <code>NCHW</code>, and it is the case that TensorFlow and Keras use <code>NHWC</code> by default (it can be configured). Ultimately, the choice of which one to use depends mainly on performance. Some libraries and algorithms are more suited to one or the other of these orderings.</p><p>【PyTorch 默认使用NCHW，而TensorFlow和Keras使用NHWC】</p><h3 id="Output-Channels-And-Feature-Maps"><a href="#Output-Channels-And-Feature-Maps" class="headerlink" title="Output Channels And Feature Maps"></a>Output Channels And Feature Maps</h3><p>Let’s look at how the interpretation of the color channel axis changes after the tensor is transformed by a convolutional layer.</p><p>Suppose we have three convolutional filters, and lets just see what happens to the channel axis.</p><p>Since we have three convolutional filters, we will have three channel outputs from the convolutional layer. These channels are outputs from the convolutional layer, hence the name output <em>channels opposed</em> to <em>color channels</em>.</p><p>【tensor送入convolutional layer（卷积层）后，color channel 这一axis的长度发生变化。</p><p>【在中解释到，有几个convolutional filters，卷积层输出的tensor就有几个channel（channel代替color channel的表达）。】</p><h4 id="Feature-Maps"><a href="#Feature-Maps" class="headerlink" title="Feature Maps"></a>Feature Maps</h4><p>With the output channels, we no longer have color channels, but modified channels that we call <em>feature maps</em>. These so-called feature maps are the outputs of the convolutions that take place using the input color channels and the convolutional filters.</p><p>Feature maps are the output channels created from the convolutions.</p><p>【卷积层输出tensor的channel维度代替color channels的叫法。】</p><p>【卷积层的输出也叫叫feature maps】</p><h1 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch Tensors"></a>PyTorch Tensors</h1><p>When programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form.</p><p>【数据预处理往往是编写NN的第一步，将原始数据转换为tensor form。】</p><p>Tensor的基本操作见Colab运行笔记链接：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/PyTorch%20Tensors%20Explained.ipynb">PyTorch Tensors Explained</a> </p><p>(不会用的也可以直接看<a href="https://github.com/f1ed/PyTorch-Notebook/blob/master/PyTorch%20Tensors%20Explained.ipynb">github</a> 上的)</p><h2 id="PyTorch-Tensors-Attributes"><a href="#PyTorch-Tensors-Attributes" class="headerlink" title="PyTorch Tensors Attributes"></a>PyTorch Tensors Attributes</h2><ul><li><p>torch.dtype：tensor包含数据类型。</p><p>常见数据类型：</p><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td>torch.float32</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64-bit floating point</td><td>torch.float64</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr><tr><td>16-bit floating point</td><td>torch.float16</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr><tr><td>8-bit integer (signed)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer (signed)</td><td>torch.int16</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32-bit integer (signed)</td><td>torch.int32</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64-bit integer (signed)</td><td>torch.int64</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr></tbody></table></li><li><p>torch.device: tensor数据所分配的设备，如CPU，cuda:0</p></li><li><p>torch.layout: tensor在内存中的存储方式。</p></li></ul><p>As neural network programmers, we need to be aware of the following:</p><ol><li>Tensors contain data of a uniform type (<code>dtype</code>).</li><li>Tensor computations between tensors depend on the <code>dtype</code> and the <code>device</code>.</li></ol><p>【Tensors包含相同类型的数据】</p><p>【Tensors之间的计算取决于他的类型和他所分配的设备】</p><h2 id="Creating-Tensors"><a href="#Creating-Tensors" class="headerlink" title="Creating Tensors"></a>Creating Tensors</h2><p>These are the primary ways of creating tensor objects (instances of the <code>torch.Tensor</code> class), with data (array-like) in PyTorch:</p><p>Creating Tensors with data.</p><p>【四种用数据创建tensor的方式】</p><ol><li><code>torch.Tensor(data)</code></li><li><code>torch.tensor(data)</code></li><li><code>torch.as_tensor(data)</code></li><li><code>torch.from_numpy(data)</code> </li></ol><h3 id="torch-Tensor-Vs-torch-tensor"><a href="#torch-Tensor-Vs-torch-tensor" class="headerlink" title="torch.Tensor() Vs torch.tensor()"></a><code>torch.Tensor()</code> Vs <code>torch.tensor()</code></h3><p>The first option with the uppercase <code>T</code> is the constructor of the <code>torch.Tensor</code> class, and the second option is what we call a <em>factory function</em> that constructs <code>torch.Tensor</code> objects and returns them to the caller.</p><p>However, the factory function <code>torch.tensor()</code> has better documentation and more configuration options, so it gets the winning spot at the moment.</p><p>【<code>torch.Tensor(data)</code> 是 <code>torch.Tensor</code> class的Constructor，而<code>torch.tensor(data)</code> 是生成/返回 torch.Tensor class的函数（factory functions)】</p><p>【因为<code>torch.tensor()</code> 有更多的选项设置，比如可以设置数据类型，所以一般用<code>torch.tensor()</code> 来生成。】</p><h3 id="Default-dtype-Vs-Inferred-dtype"><a href="#Default-dtype-Vs-Inferred-dtype" class="headerlink" title="Default dtype Vs Inferred dtype"></a>Default <code>dtype</code> Vs Inferred <code>dtype</code></h3><p>The difference here arises in the fact that the <code>torch.Tensor()</code> constructor uses the default <code>dtype</code> when building the tensor. The other calls choose a dtype based on the incoming data. This is called <em>type inference</em>. The <code>dtype</code> is inferred based on the incoming data.</p><p>【<code>torch.Tensor()</code> 在生成tensor时，使用的是默认<code>dtype=torch.float32</code> ，而其他三种是使用的引用<code>dtype</code> ，即生成tensor的数据类型和输入的数据类型一致。】</p><h3 id="Sharing-Memory-For-Performance-Copy-Vs-Share"><a href="#Sharing-Memory-For-Performance-Copy-Vs-Share" class="headerlink" title="Sharing Memory For Performance: Copy Vs Share"></a>Sharing Memory For Performance: Copy Vs Share</h3><p><code>torch.Tensor()</code> and <code>torch.tensor()</code> <em>copy</em> their input data while <code>torch.as_tensor()</code> and <code>torch.from_numpy()</code> <em>share</em> their input data in memory with the original input object.</p><p>This sharing just means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the <code>torch.Tensor</code> and the <code>numpy.ndarray</code>.</p><p>Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory.</p><p>【<code>torch.Tensor()</code> 和 <code>torch.tensor()</code> 在根据data创建tensor时，在内存中额外复制数据】</p><p>【<code>torch.as_tensor()</code> 和 <code>torch.from_numpy()</code> 在根据data创建tensor时，是和原输入数据共享的内存，即原numpy.ndarry的数据改变，相应的tensor也会改变。】</p><table><thead><tr><th>Share Data</th><th>Copy Data</th></tr></thead><tbody><tr><td>torch.as_tensor()</td><td>torch.tensor()</td></tr><tr><td>torch.from_numpy()</td><td>torch.Tensor()</td></tr></tbody></table><p>Some things to keep in mind about memory sharing (it works where it can):</p><ol><li><p>Since <code>numpy.ndarray</code> objects are allocated on the CPU, the <code>as_tensor()</code> function must copy the data from the CPU to the GPU when a GPU is being used.</p><p>【在使用GPU时， <code>as_tensor()</code> 也会将ndarray数据从CPU复制到GPU上。】</p></li><li><p>The memory sharing of <code>as_tensor()</code> doesn’t work with built-in Python data structures like lists.</p><p>【<code>as_tensor()</code> 在Python内置数据结构时不会共享内存】</p></li><li><p>The <code>as_tensor()</code> performance improvement will be greater if there are a lot of back and forth operations between <code>numpy.ndarray</code> objects and tensor objects. </p><p>【<code>as_tensor()</code> 在ndarry和tensor之间大量连续操作时能有效提高性能】</p></li></ol><h3 id="torch-as-tensor-Vs-torch-from-numpy"><a href="#torch-as-tensor-Vs-torch-from-numpy" class="headerlink" title="torch.as_tensor() Vs torch.from_numpy()"></a><code>torch.as_tensor()</code> Vs <code>torch.from_numpy()</code></h3><p>This establishes that <code>torch.as_tensor()</code> and <code>torch.from_numpy()</code> both share memory with their input data. However, which one should we use, and how are they different?</p><p>The <code>torch.from_numpy()</code> function only accepts <code>numpy.ndarray</code>s, while the <code>torch.as_tensor()</code> function accepts a wide variety of array-like objects, including other PyTorch tensors. </p><p>【这两个都是和输入数据共享内存，但 <code>torch.from_numpy()</code> 只能接受<code>numpy.ndarrays</code> 类型的数据，而<code>torch.as_tensor()</code> 能接受array-like(像list, tuple)等类型，所以一般<code>torch.as_tensor()</code> 更常用。】</p><p>If we have a <code>torch.Tensor</code> and we want to convert it to a <code>numpy.ndarray</code></p><p>【用<code>torch.numpy()</code> 把tensor转换为ndarray】</p><hr><p>Creating Tensors without data.</p><p>【还有几种创建常见tensor的方式】</p><ol><li><code>torch.eyes(n)</code> : 创建2-D tensor，即n*n的单位向量。</li><li><code>torch.zeros(shape)</code> : 创建shape=shape的全0tensor。</li><li><code>torch.ones(shape)</code> : 创建全1tensor。</li><li><code>torch.rand(shape)</code> : 创建随机值tensor。</li></ol><h1 id="Tensor-Operation"><a href="#Tensor-Operation" class="headerlink" title="Tensor Operation"></a>Tensor Operation</h1><p>关于Tensor 操作的Colab运行笔记。对照使用最佳。如果打不开也可以看<a href="https://github.com/f1ed/PyTorch-Notebook">github</a> </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operations: Element-wise</a> </p><p><a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> </p><p>We have the following high-level categories of operations:</p><ol><li>Reshaping operations</li><li>Element-wise operations</li><li>Reduction operations</li><li>Access operations</li></ol><p>【对tensor的操作主要分为4种：reshape, element-wise, reduction, access】</p><h2 id="Reshape"><a href="#Reshape" class="headerlink" title="Reshape"></a>Reshape</h2><p>As neural network programmers, we have to do the same with our tensors, and usually shaping and reshaping our tensors is a frequent task.</p><p>【reshape在NN编程中是很常见的操作】</p><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">t = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line">t.reshape([<span class="number">2</span>,<span class="number">6</span>])</span><br><span class="line">t.reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>Reshaping changes the tensor’s shape but not the underlying data. Our tensor has <code>12</code> elements, so any reshaping must account for exactly <code>12</code> elements.</p><p>【reshape操作不改变底层的数据，只是改变tensor的shape】</p><p> In PyTorch, the <code>-1</code> tells the <code>reshape()</code> function to figure out what the value should be based on the number of elements contained within the tensor.</p><p>【reshape中传入的-1参数，PyTorch可以自动计算该值，因为PyTorch要保证tensor的元素个数不变】</p><h3 id="Squeezing-And-Unsqueezing"><a href="#Squeezing-And-Unsqueezing" class="headerlink" title="Squeezing And Unsqueezing"></a>Squeezing And Unsqueezing</h3><ul><li><p><em>Squeezing</em> a tensor removes the dimensions or axes that have a length of one.</p><p>【Squeezing操作：移除tensor中axis长度为1的维度】</p></li><li><p><em>Unsqueezing</em> a tensor adds a dimension with a length of one.</p><p>【Unsqueezing操作：增加一个axis长度为1的维度】</p></li></ul><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t.squeeze()</span><br><span class="line">t.squeeze().unsqueeze(dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="Concatenation-Tensors"><a href="#Concatenation-Tensors" class="headerlink" title="Concatenation Tensors"></a>Concatenation Tensors</h3><p>We combine tensors using the <code>cat()</code> function, and the resulting tensor will have a shape that depends on the shape of the two input tensors.</p><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.cat((t1,t2,t3), dim=<span class="number">0</span>)</span><br><span class="line">torch.cat((t1,t2,t3), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h3><p>这里从CNN的例子看Flatten，CNN的相关细节见：<a href="/2020/04/25/CNN/" title="这篇文章">这篇文章</a></p><p>A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input.</p><p>【flatten在卷积层网络很常见，因为输入必须flatten后才能连接到一个全连接网络层】</p><p>对于MNIST数据集中18*18的手写数字，在前文说到CNN的输入是<code>[Batch Size, Channels, Height, Width]</code> ，怎么才能flatten tensor的部分axis，而不是全部维度。</p><p>CNN的输入，需要flatten的axes：(C,H,W)</p><p>从dim1维度开始flatten（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb">Tensor Operations: Reshape</a> ）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.flatten(start_dim=<span class="number">1</span>, end_dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Broadcasting-and-Element-Wise"><a href="#Broadcasting-and-Element-Wise" class="headerlink" title="Broadcasting and Element-Wise"></a>Broadcasting and Element-Wise</h2><p>An <em>element-wise</em> operation operates on corresponding elements between tensors.</p><p>【element-wise操作两个tensor之间对应的元素。】</p><h3 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h3><p>Broadcasting describes how tensors with different shapes are treated during element-wise operations.</p><p>Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors.</p><p>【broadcast描述了不同shape之间的tensor如何进行element-wise操作】</p><p>【broadcast运行我们增加scalars到高维度】</p><p>Let’s think about the <code>t1 + 2</code> operation. Here, the scaler valued tensor is being broadcasted to the shape of <code>t1</code>, and then, the element-wise operation is carried out.</p><p>【在t1+2时，scalar 2实际是先被broadcast到和t1相同的shape, 再执行element-wise操作】</p><p>We have two tensors with different shapes. The goal of broadcasting is to make the tensors have the same shape so we can perform element-wise operations on them.</p><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operations: Element-wise</a> ）</p><h3 id="Broadcasting-Details"><a href="#Broadcasting-Details" class="headerlink" title="Broadcasting Details"></a>Broadcasting Details</h3><p>（具体操作见colab运行笔记本:<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operations: Element-wise</a> ）</p><ul><li><p>Same Shapes: 直接操作</p></li><li><p>Same Rank, Different Shape:</p><ol><li><p>Determine if tensors are compatible（兼容）.</p><p>【两个tensor兼容，才可以对tensor broadcast，再执行element-wise操作】</p><p>We compare the shapes of the two tensors, starting at their last dimensions and working backwards. Our goal is to determine whether each dimension between the two tensors’ shapes is compatible.</p><p>【从最后一个维度向前判断，每个维度是否兼容】</p><p>【判断该维度兼容的条件是满足下面两个条件其一：维度长度相同；或者其中一个为1】</p><p>The dimensions are compatible when either:</p><ul><li>They’re equal to each other.</li><li>One of them is 1.</li></ul></li><li><p>Determine the shape of the resulting tensor.</p><p>【操作的结果是一个新的tensor，结果tensor的每个维度长度是原tensors在该维度的最大值】</p></li></ol></li><li><p>Different Ranks:</p><ol><li><p>Determine if tensors are compatible.(同上)</p><p>When we’re in a situation where the ranks of the two tensors aren’t the same, like what we have here, then we simply substitute a one in for the missing dimensions of the lower-ranked tensor.</p><p>【对低维度的tensor的缺失维度，用1来代替，比如shape为(1,3) 和 ()，低维度的shape变为(1,1)】</p></li><li><p>Determine the shape of the resulting tensor.</p></li></ol></li></ul><h2 id="ArgMax-and-Reduction"><a href="#ArgMax-and-Reduction" class="headerlink" title="ArgMax and Reduction"></a>ArgMax and Reduction</h2><p>A <em>reduction operation</em> on a tensor is an operation that reduces the number of elements contained within the tensor.</p><p>【reduction 操作是能减少tensor元素数量的操作。】</p><p>Reshaping operations gave us the ability to position our elements along particular axes. Element-wise operations allow us to perform operations on elements between two tensors, and reduction operations allow us to perform operations on elements within a single tensor.</p><p>【Reshape操作让我们能沿着某一axis操纵tensor 中的元素位置；Element-wise操作让我们能对tensors之间对应元素进行操作；Reduction操作能让我们对单个tensor间的元素操作。】</p><p>(具体操作见colab笔记本：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t.sum()</span><br><span class="line">t.prod()</span><br><span class="line">t.mean()</span><br><span class="line">t.std()</span><br></pre></td></tr></table></figure><h3 id="Reducing-Tensors-By-Axes"><a href="#Reducing-Tensors-By-Axes" class="headerlink" title="Reducing Tensors By Axes"></a>Reducing Tensors By Axes</h3><p>只需要对这些方法传一个维度对参数。</p><p>(具体操作见colab笔记本：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t.sum(dim=<span class="number">0</span>)</span><br><span class="line">t.sum(dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Argmax"><a href="#Argmax" class="headerlink" title="Argmax"></a>Argmax</h3><p><em>Argmax</em> returns the index location of the maximum value inside a tensor.</p><p>【Argmax返回最大value的index】</p><p>(具体操作见colab笔记本：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.argmax(dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="Aceessing-Elements-Inside-Tensors"><a href="#Aceessing-Elements-Inside-Tensors" class="headerlink" title="Aceessing Elements Inside Tensors"></a>Aceessing Elements Inside Tensors</h2><p>The last type of common operation that we need for tensors is the ability to access data from within the tensor.</p><p>【Access操作能获得tensor中的数据，即将tensor中的数据拿出来放在Python内置的数据结构中】</p><p>(具体操作见colab笔记本：<a href="https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb">Tensor Operation: Reduction and Access</a> )</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.mean().item()</span><br><span class="line">t.mean(dim=<span class="number">0</span>).tolist()</span><br><span class="line">t.mean(dim=<span class="number">0</span>).numpy()</span><br></pre></td></tr></table></figure><h3 id="Advanced-Indexing-And-Slicing"><a href="#Advanced-Indexing-And-Slicing" class="headerlink" title="Advanced Indexing And Slicing"></a>Advanced Indexing And Slicing</h3><p>PyTorch Tensor支持大多数NumPy的index和slicing操作。</p><p>坑：<a href="https://numpy.org/doc/stable/reference/arrays.indexing.html">https://numpy.org/doc/stable/reference/arrays.indexing.html</a></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>挖坑：advanced indexing and slicing: <a href="https://numpy.org/doc/stable/reference/arrays.indexing.html">https://numpy.org/doc/stable/reference/arrays.indexing.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch框架学习。&lt;/p&gt;
&lt;p&gt;本篇文章主要介绍PyTorch中的Tensor及其基本操作，主要分为四个方面：Reshape, Element-wise, Reduction和Access。&lt;/p&gt;
&lt;p&gt;Tensor的具体操作介绍，建议配合Colab笔记使用：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/PyTorch%20Tensors%20Explained.ipynb&quot;&gt;PyTorch Tensors Explained&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AReshape.ipynb&quot;&gt;Tensor Operations: Reshape&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;a href=&quot;https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb&quot;&gt;Tensor Operations: Element-wise&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/f1ed/PyTorch-Notebook/blob/master/Tensor%20Operations%EF%BC%9AElement-Wise.ipynb&quot;&gt;Tensor Operation: Reduction and Access&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;英文的表达解释都是比较清晰且精确的，所以以英语的形式作为主要记录，文中会夹带一些中文总结语句，方便阅读。&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyTorch" scheme="https://f7ed.com/categories/PyTorch/"/>
    
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="DEEPLIZARD" scheme="https://f7ed.com/tags/DEEPLIZARD/"/>
    
      <category term="PyTorch" scheme="https://f7ed.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:Unsupervised Learning：Word Embedding</title>
    <link href="https://f7ed.com/2020/10/11/unsupervised-learning-word-embedding/"/>
    <id>https://f7ed.com/2020/10/11/unsupervised-learning-word-embedding/</id>
    <published>2020-10-10T16:00:00.000Z</published>
    <updated>2020-10-11T15:33:36.817Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要是介绍一种无监督学习——Word Embedding（词嵌入）。</p><p>文章开篇介绍了word编码的1-of-N encoding方式和word class方式，但这两种方式得到的单词向量表示都不能很好表达单词的语义和单词之间的语义联系。</p><p>Word Embedding可以很好的解决这个问题。</p><p>Word Embedding有count based和prediction based两种方法。文章主要介绍了prediction based的方法，包括如何predict the word vector? 为什么这样的模型works？介绍了prediction based的变体；详细阐述了该模型中sharing parameters的做法和其必要性。</p><p>文章最后简单列举了word embedding的相关应用，包括multi-lingual embedding, multi-domain embedding, document embedding 等。</p><a id="more"></a><h1 id="Word-to-Vector"><a href="#Word-to-Vector" class="headerlink" title="Word to Vector"></a>Word to Vector</h1><p>如何把word转换为vector?</p><h2 id="1-of-N-Encoding"><a href="#1-of-N-Encoding" class="headerlink" title="1-of-N Encoding"></a>1-of-N Encoding</h2><p>第一种方法是1-of-N Encoding：</p><p>Vector的维度是单词总数，每一维度都代表一个单词。</p><img src="https://s1.ax1x.com/2020/10/11/0gLLGT.png" alt="0gLLGT.png" style="zoom:25%;" /> <p>1-of-N Encoding的方法简单，但这种向量的表示方式not imformative，即向量表示不能体现单词之间的语义关系。</p><h2 id="Word-Class"><a href="#Word-Class" class="headerlink" title="Word Class"></a>Word Class</h2><p>对1-of-N Encoding方式改进，Word Class采用聚类cluster的方式，根据类别训练一个分类器。</p><img src="https://s1.ax1x.com/2020/10/11/0gLqiV.png" alt="0gLdqiV.png" style="zoom:33%;" /> <p>但这种人为分类的方式，信息是会部分丢失的，即光做clustering是不够的，会丢失单词的部分信息。</p><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>第三种方式是Word Embedding。（词嵌入）</p><p>Word Embedding: Machine learns the meaning of words from reading a lot of documents without supervision.</p><p>Word Embedding，机器通过阅读大量文章学习单词的含义，用vector的形式表示单词的语义。训练时只需要给机器大量文章，不需要label，因此是无监督学习。</p><h1 id="Word-Embedding-1"><a href="#Word-Embedding-1" class="headerlink" title="Word Embedding"></a>Word Embedding</h1><p>如何做Word Embedding呢？</p><h2 id="auto-encoder？"><a href="#auto-encoder？" class="headerlink" title="auto-encoder？"></a>auto-encoder？</h2><p>能否用auto-encoder的方式来做词嵌入呢？</p><p>即用1-of-N encoding的方式对单词编码，作为训练的输入和输出。</p><p>word2vec时，把model中的某一hidden layer的输出作为该单词的向量表示。</p><p>这种方式是不可以的，不可以用auto-encoder。因为auto-encoder不能学到informative的信息，即用auto-encoder表示的向量不能表达word的语义。</p><h2 id="Exploit-the-Context"><a href="#Exploit-the-Context" class="headerlink" title="Exploit the Context"></a>Exploit the Context</h2><p>A word can be understood by its context.</p><p>所以Word Embedding可以利用上下文来学习word的语义。</p><p>如何利用单词的上下文来学习呢？</p><ul><li><p>Count based</p><p>如果两个单词 $w_i$ 和 $w_j$ 在文章中经常同时出现，那么 $V(w_i)$ ( $w_i$ 的向量表示)和 $V(w_j)$ 的向量表示会很close.</p><p>E.g. Glove Vector: <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></p><p>GloVe的表示法有两个亮点：</p><ol><li><p>Nearest neighbors：vectors之间的欧几里得距离（或者余弦相似度）能较好表示words之间的语义相似度。</p></li><li><p>Linear substructures：用GloVe方法表示的vectors有有趣的线性子结构。</p><img src="https://s1.ax1x.com/2020/10/11/0gLfxg.png" alt="0gLfxg.png" style="zoom:%;" />  </li></ol></li><li><p>Prediction based</p><p>使用预测的方式来表示。</p></li></ul><h2 id="Prediction-based"><a href="#Prediction-based" class="headerlink" title="Prediction based"></a>Prediction based</h2><h3 id="How-to-predict？"><a href="#How-to-predict？" class="headerlink" title="How to predict？"></a>How to predict？</h3><p>prediction based的方法是用前一个单词来预测当前单词。</p><img src="https://s1.ax1x.com/2020/10/11/0gLHI0.png" alt="0gLHI0.png" style="zoom:25%;" /> <img src="https://s1.ax1x.com/2020/10/11/0gLORU.png" alt="0gLORU.png" style="zoom:33%;" /> <p>训练时： $w_{i-1}$ 的1-of-N encoding编码作为输入，$w_i$ 的1-of-N encoding的编码作为输出。</p><p>NN如上图，$w_{i-1}$ 的1-of-N encoding编码作为输入，输出的vector表示下一个单词是 $w_i$ 的概率。</p><p>word2vec : $w_{i-1}$ 的1-of-N encoding编码作为NN的输入，$w_i$ 的向量表示为第一个hidden layer的neurons的输入 $z$ 。</p><h3 id="Why-it-works"><a href="#Why-it-works" class="headerlink" title="Why it works?"></a>Why it works?</h3><p>直觉的解释他为什么能work。</p><img src="https://s1.ax1x.com/2020/10/11/0gL7aq.png" alt="0gdL7aq.png" style="zoom:33%;" /> <p>如上图，蔡英文 宣誓就职 和 马英九 宣誓就职，虽然 $w_{i-1}$ 不同，但NN的输出中，“宣誓就职”的概率应该最大。</p><p>即hidden layers必须把不同的 $w_{i-1}$ project到相同的space，要求hidden layer的input是相近的，NN的输出才是相近的。</p><h3 id="Prediction-based-：Various-Architecture"><a href="#Prediction-based-：Various-Architecture" class="headerlink" title="Prediction-based ：Various Architecture"></a>Prediction-based ：Various Architecture</h3><p>因为一个单词的下一个单词范围非常大，所以使用前一个单词预测当前单词的方法，performance是较差的。</p><p>因此常常会使用多个单词来预测下一个单词，NN的输入是多个单词连接在一起组成的向量，一般NN的输入至少为10个单词，word embedding的performance较好。</p><p>除了使用多个单词的方法，prediction-based的方法还用两种变体结构。</p><ul><li><p>Continuous bag of word (CBOW) model: predicting the word given its context.</p><p>使用单词的前后文（前一个单词和后一个单词）来预测当前单词。</p><img src="https://s1.ax1x.com/2020/10/11/0gL5rj.png" alt="0gL5drj.png" style="zoom:25%;" /> </li><li><p>Skip-gram: predicting the context given a word.</p><p>使用中间单词来预测单词的前一个单词和后一个单词。</p><img src="https://s1.ax1x.com/2020/10/11/0gLIqs.png" alt="0gLdIqs.png" style="zoom:25%;" />  </li></ul><h3 id="Sharing-Parameters"><a href="#Sharing-Parameters" class="headerlink" title="Sharing Parameters"></a>Sharing Parameters</h3><p>使用多个单词作为NN的输入，提高了word embedding的performance，但也大幅增加了模型训练的参数数量。</p><p>使用sharing parameters（共享参数）能大量减少模型的参数数量。</p><img src="https://s1.ax1x.com/2020/10/11/0gOfFx.png" alt="0gOfFix.png" style="zoom:33%;" /> <p>如上图，输入单词连接到neurons的权重应该是相同的。</p><p>除了能减少参数，sharing parameters也是必要的。否则，如果NN的输入的单词顺序交换，那么得到的单词向量是不同的。</p><p><strong>How to train sharing parameters?</strong> </p><p>假设两个单词相同维度连接到neuron的weight是 $w_i,w_j$ ，在训练中，如何让 $w_i=w_j$ ?</p><ol><li><p>Given the same initialization.(相同的初始化)</p></li><li><p>原来的参数更新：<br>$$<br>w_i \longleftarrow w_i - \frac{\partial C}{\partial w_i} \<br>w_j \longleftarrow w_j - \frac{\partial C}{\partial w_j}<br>$$<br>虽然有相同的初始化，但在Backpropagation求偏微分时，$\frac{\partial C}{\partial w_i}$ 和 $\frac{\partial C}{\partial w_j}$ 不一样，那么参数 $w_i$ 和 $w_j$ 更新一次后就不同了。</p><p>在训练sharing parameters的参数更新：<br>$$<br>w_i \longleftarrow w_i - \frac{\partial C}{\partial w_i} -\frac{\partial C}{\partial w_j}\<br>w_j \longleftarrow w_j - \frac{\partial C}{\partial w_j}-\frac{\partial C}{\partial w_i}<br>$$<br>这样更新后，$w_i$ 和 $w_j$ 仍保持一致。如果有多个单词，亦然。</p></li></ol><hr><p><strong>Word2Vec</strong> </p><p>在word2vec时，根据sharing parameters的性质，计算单词的向量表示时，可以简化运算。</p><img src="https://s1.ax1x.com/2020/10/11/0gLWRS.png" alt="0gLWRS.png" style="zoom:33%;" /> <p>如上图，用前文单词 $x_{i-1},x_{i-2}$  表示单词 $x_i$ 的向量表示 $z=W_1x_{i-2}+W_2x_{i-1}=W(x_{i-2}+x_{i-1})$ .</p><p>其中  $x_{i-1},x_{i-2}$ 的维度是|V|，$x_i$ 的向量表示 $z$ 的维度是 |Z|，$W_1=W_2=W$ 的维度为|Z|*|V|。</p><h1 id="Advantages-of-Word-Embedding"><a href="#Advantages-of-Word-Embedding" class="headerlink" title="Advantages of Word Embedding"></a>Advantages of Word Embedding</h1><p>Word Embedding能得到一些有趣的特性。</p><ul><li><p>向量之间有趣的线性子结构</p><img src="https://s1.ax1x.com/2020/10/11/0gL2Pf.png" alt="0gL2Pf.png" style="zoom:40%;" /> </li><li><p>相近的向量有相近的语义</p><img src="https://s1.ax1x.com/2020/10/11/0gLTZn.png" alt="0gLTZn.png" style="zoom:40%;" /> </li><li><p>向量之间表示的语义特性</p><img src="https://s1.ax1x.com/2020/10/11/0gLRG8.png" alt="0gLRG8.png" style="zoom:35%;" />  </li></ul><h2 id="其他应用"><a href="#其他应用" class="headerlink" title="其他应用"></a>其他应用</h2><h3 id="Multi-lingual-Embedding：实现翻译"><a href="#Multi-lingual-Embedding：实现翻译" class="headerlink" title="Multi-lingual Embedding：实现翻译"></a>Multi-lingual Embedding：实现翻译</h3><img src="https://s1.ax1x.com/2020/10/11/0gORT1.png" alt="0gORdT1.png" style="zoom:40%;" />  <p>不同语言之间分开训练，训练出的不同语言所对应词汇的向量表示肯定不同，再将对应词汇的向量project到同一点，即实现了翻译。</p><h3 id="Multi-domain-Embedding"><a href="#Multi-domain-Embedding" class="headerlink" title="Multi-domain Embedding"></a>Multi-domain Embedding</h3><p>还可以做影像嵌入。</p><img src="https://s1.ax1x.com/2020/10/11/0gLcIP.png" alt="0gLcIP.png" style="zoom:25%;" /> <h3 id="Document-Embedding：将文件表示为一个向量"><a href="#Document-Embedding：将文件表示为一个向量" class="headerlink" title="Document Embedding：将文件表示为一个向量"></a>Document Embedding：将文件表示为一个向量</h3><ul><li><p>Bag of Word:</p><p>用Bag-of-word的方式编码文件，再实现semantic embedding。得到的文件表示向量可以表示文件的语义主题。</p><img src="https://s1.ax1x.com/2020/10/11/0gLyVI.png" alt="0gLyVI.png" style="zoom:33%;" /> </li><li><p>Beyond Bag of Word:</p><p>句子中单词的顺序也很大程度影响句子的语义。</p><p>因此，下图的两句话有相同的bag-of-word，但表达的含义完全相反。</p><img src="https://s1.ax1x.com/2020/10/11/0gLrqA.png" alt="0gLrqA.png" style="zoom:25%;" /> <p>关于beyond bag of word的相关工作参考reference 2.</p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a> </p></li><li><p>beyond bag of word:</p><img src="https://s1.ax1x.com/2020/10/11/0gL4MQ.png" alt="0gL4MQ.png" style="zoom:33%;"  />    </li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章主要是介绍一种无监督学习——Word Embedding（词嵌入）。&lt;/p&gt;
&lt;p&gt;文章开篇介绍了word编码的1-of-N encoding方式和word class方式，但这两种方式得到的单词向量表示都不能很好表达单词的语义和单词之间的语义联系。&lt;/p&gt;
&lt;p&gt;Word Embedding可以很好的解决这个问题。&lt;/p&gt;
&lt;p&gt;Word Embedding有count based和prediction based两种方法。文章主要介绍了prediction based的方法，包括如何predict the word vector? 为什么这样的模型works？介绍了prediction based的变体；详细阐述了该模型中sharing parameters的做法和其必要性。&lt;/p&gt;
&lt;p&gt;文章最后简单列举了word embedding的相关应用，包括multi-lingual embedding, multi-domain embedding, document embedding 等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="Unsupervised-learning" scheme="https://f7ed.com/tags/Unsupervised-learning/"/>
    
      <category term="Word Embedding" scheme="https://f7ed.com/tags/Word-Embedding/"/>
    
  </entry>
  
  <entry>
    <title>「LeetCode」：Math</title>
    <link href="https://f7ed.com/2020/10/10/Leetcode-math/"/>
    <id>https://f7ed.com/2020/10/10/Leetcode-math/</id>
    <published>2020-10-09T16:00:00.000Z</published>
    <updated>2020-10-10T10:12:05.425Z</updated>
    
    <content type="html"><![CDATA[<p>LeetCode Math 专题记录。</p><p>10月初。</p><p>Albert Einstein:</p><p> “I believe that not everything that can be counted counts, and not everything that counts can be counted”</p><p>「并非所有重要的东西都是可以被计算的，也并不是所有能被计算的东西都那么重要。」</p><a id="more"></a><h2 id="7-Reverse-Integer-E"><a href="#7-Reverse-Integer-E" class="headerlink" title="7. Reverse Integer[E]"></a>7. Reverse Integer[E]</h2><p><a href="https://leetcode.com/problems/reverse-integer/">7. Reverse Integer[E]</a> </p><p>Problem:</p><p>反转32bits的有符号数字，如果反转后会溢出则返回0.</p><p>Solution：</p><p>直观的解决它，先算出反转后的数字，用比较大小来看是否溢出。（最开始还想着转换为bit串来看，就复杂了）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverse</span><span class="params">(self, x: int)</span> -&gt; int:</span></span><br><span class="line">        n_min = -(<span class="number">2</span> ** <span class="number">31</span>)</span><br><span class="line">        n_max = <span class="number">2</span> ** <span class="number">31</span> - <span class="number">1</span></span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        flag = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">            x = -x</span><br><span class="line">        <span class="keyword">while</span> x != <span class="number">0</span>:</span><br><span class="line">            r = x % <span class="number">10</span></span><br><span class="line">            s = s * <span class="number">10</span> + r</span><br><span class="line">            x //=<span class="number">10</span></span><br><span class="line">        <span class="keyword">if</span> flag <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">            s = -s</span><br><span class="line">        <span class="keyword">if</span> s &lt; n_min <span class="keyword">or</span> s &gt; n_max:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><hr><ul><li><p>十进制转换为二进制、八进制、十六进制：</p><ul><li>二进制：<code>bin(a)</code>  ,也可以直接赋二进制的值<code>0b10101</code> </li><li>八进制：<code>oct(a)</code> ，赋值八进制的值<code>0o263361</code> </li><li>十六进制：<code>hex(a)</code> ,赋值十六进制<code>0x1839ac29</code> </li></ul></li></ul><h2 id="165-Compare-Version-Numbers-M"><a href="#165-Compare-Version-Numbers-M" class="headerlink" title="165. Compare Version Numbers[M]"></a>165. Compare Version Numbers[M]</h2><p><a href="https://leetcode.com/problems/compare-version-numbers/">165. Compare Version Numbers[M]</a> </p><p>Problem：</p><p>比较版本号。</p><p>Solution：</p><p>直观～Easy～</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compareVersion</span><span class="params">(self, version1: str, version2: str)</span> -&gt; int:</span></span><br><span class="line">        li1 = version1.split(<span class="string">'.'</span>)</span><br><span class="line">        li2 = version2.split(<span class="string">'.'</span>)</span><br><span class="line">        n_1 = len(li1)</span><br><span class="line">        n_2 = len(li2)</span><br><span class="line">        n = max(n_1, n_2)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            a = int(li1[i]) <span class="keyword">if</span> i &lt; n_1 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            b = int(li2[i]) <span class="keyword">if</span> i &lt; n_2 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> a &gt; b:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> a &lt; b:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><hr><ul><li>Python中的强制转换：<ol><li>字符串转换为int : <code>int_value = int(str_value)</code> </li><li>int转换为字符串：<code>str.value = str(int_value)</code> </li><li>int转换为unicode： <code>unicode(int_value)</code> </li><li>unicode转换为int：<code>int(unicode_value)</code> </li><li>字符串转换为unicode：<code>unicode(str_value)</code> </li><li>unicode转换为字符串：<code>str(unicode_value)</code> </li></ol></li><li>Java中的强制转换：<ol><li>字符串String转化为int：<code>int_value = String.parseInt(string_value)</code> 或者 <code>(int)string_value)</code> </li><li>int转化为字符串String：<code>string_value = (String)int_value</code> </li></ol></li></ul><h2 id="66-Plus-One-E"><a href="#66-Plus-One-E" class="headerlink" title="66. Plus One[E]"></a>66. Plus One[E]</h2><p><a href="https://leetcode.com/problems/plus-one/">66. Plus One[E]</a> </p><p>Problem:</p><p>用列表表示一个正数，返回正数+1的列表结果。</p><p>Solution：</p><p>记录一个最高位的进位情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plusOne</span><span class="params">(self, digits: List[int])</span> -&gt; List[int]:</span></span><br><span class="line">        c_bit = <span class="number">0</span></span><br><span class="line">        n = len(digits)</span><br><span class="line">        i = n - <span class="number">1</span></span><br><span class="line">        digits[i] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> i &gt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> digits[i] &lt; <span class="number">10</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            digits[i] %= <span class="number">10</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                c_bit = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                digits[i<span class="number">-1</span>] += <span class="number">1</span></span><br><span class="line">            i -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> c_bit == <span class="number">1</span>:</span><br><span class="line">            digits.insert(<span class="number">0</span>, c_bit)</span><br><span class="line">        <span class="keyword">return</span> digits</span><br></pre></td></tr></table></figure><hr><ul><li><p>Python中list添加元素的集中方法：（append(); extend(); insert(); +加号）</p><ol><li><p>append() ：在List尾部追加单个元素，只接受一个参数，参数可以是任意数据类型。</p></li><li><p>extend() ：在list尾部追加一个列表，将该参数列表中的每个元素连接到原列表。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; a = [1,2,3]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; b = [3,4,5]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; a.extend(b)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; a</span></span><br><span class="line">[1, 2, 3, 3, 4, 5]</span><br></pre></td></tr></table></figure></li><li><p>insert(index, object)：将一个元素插入到列表中，第一个参数是插入的索引点，第二个是插入的元素。</p></li><li><p>+加号：将两个list相加，返回一个新的list对象。</p></li></ol><p>区别：前三种方法(append, extend, insert)可以对列表增加元素，没有返回值，是直接修改原数据对象，而+加号是需要创建新的list对象，需要消耗额外的内存。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LeetCode Math 专题记录。&lt;/p&gt;
&lt;p&gt;10月初。&lt;/p&gt;
&lt;p&gt;Albert Einstein:&lt;/p&gt;
&lt;p&gt; “I believe that not everything that can be counted counts, and not everything that counts can be counted”&lt;/p&gt;
&lt;p&gt;「并非所有重要的东西都是可以被计算的，也并不是所有能被计算的东西都那么重要。」&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/tags/LeetCode/"/>
    
      <category term="Math" scheme="https://f7ed.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>「LeetCode」：String</title>
    <link href="https://f7ed.com/2020/09/29/LeetCode_String/"/>
    <id>https://f7ed.com/2020/09/29/LeetCode_String/</id>
    <published>2020-09-28T16:00:00.000Z</published>
    <updated>2020-09-29T06:56:50.890Z</updated>
    
    <content type="html"><![CDATA[<p>LeetCode String 专题记录。</p><p>9月毕。</p><p>「我祝福你有时有坏运气，你会意识到概率和运气在人生中扮演的角色，并理解你的成功并不完全是你应得的，其他人的失败也并不完全是他们应得的。」</p><p>「不想要刚好错过的悔恨，那就要有完全碾压的实力。」</p><a id="more"></a><h1 id="String"><a href="#String" class="headerlink" title="String"></a>String</h1><h2 id="28-Implement-strStr"><a href="#28-Implement-strStr" class="headerlink" title="28-Implement strStr()"></a>28-Implement strStr()</h2><p><a href="https://leetcode.com/problems/implement-strstr/solution/">28-Implement strStr()</a> </p><p>Problem:</p><p>返回第一个字串出现的下标</p><p>Solution：</p><p>Python就暴力匹配。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">strStr</span><span class="params">(self, haystack: str, needle: str)</span> -&gt; int:</span></span><br><span class="line">        n1 = len(haystack)</span><br><span class="line">        n2 = len(needle)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n1-n2+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> haystack[i:i+n2] == needle:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h2 id="14-Longest-Common-Prefix"><a href="#14-Longest-Common-Prefix" class="headerlink" title="14-Longest Common Prefix"></a>14-Longest Common Prefix</h2><p><a href="https://leetcode-cn.com/problems/longest-common-prefix/">14-Longest Common Prefix</a></p><p>Problem:</p><p>返回串的公共最长前缀。</p><p>Solution：</p><p>暴力匹配长度就好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        LCP = <span class="number">0</span></span><br><span class="line">        n = len(strs)</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n):</span><br><span class="line">                <span class="keyword">if</span> LCP &lt; len(strs[i]) <span class="keyword">and</span> strs[i][LCP] == strs[<span class="number">0</span>][LCP]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">return</span> strs[<span class="number">0</span>][<span class="number">0</span>: LCP]</span><br><span class="line">            LCP += <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="58-Length-of-Last-Word"><a href="#58-Length-of-Last-Word" class="headerlink" title="58-Length of Last Word"></a>58-Length of Last Word</h2><p><a href="https://leetcode.com/problems/length-of-last-word/">58-Length of Last Word</a> </p><p>Problem:</p><p>单词串由字母和空格组成，返回最后一个单词的长度。</p><p>Solution：</p><p>注意串最后的空格。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLastWord</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        end = n<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> end<span class="number">-1</span> &gt;= <span class="number">0</span> <span class="keyword">and</span> s[end] == <span class="string">' '</span>:</span><br><span class="line">            end -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(end, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> s[i] == <span class="string">' '</span>:</span><br><span class="line">                <span class="keyword">return</span> length</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                length += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> length</span><br></pre></td></tr></table></figure><h2 id="58-First-Unique-Character-in-a-String"><a href="#58-First-Unique-Character-in-a-String" class="headerlink" title="58-First Unique Character in a String"></a>58-First Unique Character in a String</h2><p>Problem:</p><p>找第一个没有重复出现的字符下标。</p><p>Solution：</p><p>暴力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">firstUniqChar</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        a_ascii = ord(<span class="string">'a'</span>)</span><br><span class="line">        cnt = [<span class="number">0</span>]*(<span class="number">26</span>+<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">            cnt[ord(i)-a_ascii] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> cnt[ord(i)-a_ascii] == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> s.index(i)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><hr><p>寻找子串开始索引：</p><ol><li><strong>str.find(substr, beg=0, end=len(string))</strong> <ul><li>substr: 字串</li><li>beg: 开始索引</li><li>end: 结束索引，默认字符串长度。</li><li>如果字符串不包含子串，则返回<code>-1</code> </li></ul></li><li><strong>str.index(str, beg=0, end=len(string))</strong> <ul><li>和find差不多，如果不包含子串会抛出异常。</li></ul></li></ol><h2 id="383-Ransom-Note"><a href="#383-Ransom-Note" class="headerlink" title="383-Ransom Note"></a>383-Ransom Note</h2><p><a href="https://leetcode.com/problems/ransom-note/">383-Ransom Note</a> </p><p>Problem:</p><p>给两个字符串，判断串1的字符能否由串2的字符组成。</p><p>Solution：</p><p>字典计数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canConstruct</span><span class="params">(self, ransomNote: str, magazine: str)</span> -&gt; bool:</span></span><br><span class="line">        ransomDir = &#123;&#125;</span><br><span class="line">        magazineDir = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> ransomNote:</span><br><span class="line">            ransomDir.setdefault(ch, <span class="number">0</span>)</span><br><span class="line">            ransomDir[ch] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> magazine:</span><br><span class="line">            magazineDir.setdefault(ch, <span class="number">0</span>)</span><br><span class="line">            magazineDir[ch] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> (k, v) <span class="keyword">in</span> ransomDir.items():</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> magazineDir:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> ransomDir[k] &gt; magazineDir[k]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><hr><ol><li>初始化字典的值：<code>dic.setdefault(ch, 0)</code> </li></ol><h2 id="344-Reverse-String"><a href="#344-Reverse-String" class="headerlink" title="344-Reverse String"></a>344-Reverse String</h2><p><a href="https://leetcode.com/problems/reverse-string/">344-Reverse String</a> </p><p>Problem: </p><p>in-place 反转字符串 with O(1) 的额外空间。</p><p>Solution：</p><p>前后两个指针交换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseString</span><span class="params">(self, s: List[str])</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Do not return anything, modify s in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        l = <span class="number">0</span></span><br><span class="line">        r = n<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            s[l], s[r] = s[r], s[l]</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">            r -= <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="151-Reverse-Words-in-a-String"><a href="#151-Reverse-Words-in-a-String" class="headerlink" title="151-Reverse Words in a String"></a>151-Reverse Words in a String</h2><p><a href="https://leetcode.com/problems/reverse-words-in-a-string/">151-Reverse Words in a String</a> </p><p>Problem:</p><p>反转字符串word by word.(结果中单词间只能有一个空格)</p><p>Solution：</p><p>把单词存入列表，再输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseWords</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        li = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; n:</span><br><span class="line">            <span class="keyword">while</span> i &lt; n <span class="keyword">and</span> s[i] == <span class="string">' '</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            l = i</span><br><span class="line">            <span class="keyword">while</span> i &lt; n <span class="keyword">and</span> s[i] != <span class="string">' '</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            r = i</span><br><span class="line">            <span class="keyword">if</span> r != l:</span><br><span class="line">                li.append(s[l:r])</span><br><span class="line"></span><br><span class="line">        ans = <span class="string">' '</span>.join(li[<span class="number">-1</span>::<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><p><a href="https://segmentfault.com/a/1190000015475309">Python连接字符串总结</a> </p><ol><li>加号连接：<code>&#39;a&#39; + &#39;b&#39;</code> </li><li>逗号连接，只能用于print打印: <code>print(a, b)</code> </li><li>直接连接: <code>print(&#39;a&#39; &#39;b&#39;)</code> </li><li>使用 <code>%</code> 格式化字符串：<code>&#39;%s %s&#39; % (&#39;hello&#39;, &#39;world&#39;)</code> </li><li><code>format</code> 格式化字符串：<code>&#39;{}{}&#39;.format(&#39;hello&#39;, &#39;world&#39;)</code> </li><li><code>join</code> 内置方法：用字符来连接一个序列，数组或列表等：<code>&#39;-&#39;.join([&#39;aa&#39;, &#39;bb&#39;, &#39;cc&#39;])</code> </li><li><code>f-string</code> 方法：<code>aa, bb = &#39;hello&#39;, &#39;world&#39;</code> , <code>f&#39;{aa} {bb}&#39;</code> </li><li><code>*</code> 操作符：字符串乘法。</li></ol><p>反转列表：<code>[-1: : -1]</code> </p><h2 id="186-Reverse-Words-in-a-String-II"><a href="#186-Reverse-Words-in-a-String-II" class="headerlink" title="186-Reverse Words in a String II"></a>186-Reverse Words in a String II</h2><p><a href="https://leetcode-cn.com/problems/reverse-words-in-a-string-ii/">186-Reverse Words in a String II</a> </p><p>Problem:</p><p>反转单词in-places.</p><p>Solution:</p><p>两次反转，第一次整体反转，第二次再单词反转。</p><p>（不额外开个数组来逐个赋值AC不了，不知道为啥q w q)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseWords</span><span class="params">(self, s: List[str])</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Do not return anything, modify s in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        temp = s[<span class="number">-1</span>::<span class="number">-1</span>]</span><br><span class="line">        n = len(temp)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; n:</span><br><span class="line">            l = i</span><br><span class="line">            <span class="keyword">while</span> i &lt; n <span class="keyword">and</span> temp[i] != <span class="string">' '</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            r = i</span><br><span class="line">            temp[l:r] = list(reversed(temp[l:r]))</span><br><span class="line"></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(n):</span><br><span class="line">            s[index] = temp[index]</span><br></pre></td></tr></table></figure><hr><ul><li><p>Python 反转列表的方法：</p><ol><li><code>list(reversed(a))</code> , reversed(a)返回的是迭代器，转换成list。</li><li><code>a[::-1]</code> </li></ol></li><li><p>Python 字符串(str)和列表(list)互相转换：</p><ol><li><p>str 转换为 list</p><ul><li><code>list()</code> 转换为单个字符列表</li><li><code>str.split()</code> 或者<code>str.split(&#39; &#39;)</code> 空格分割转换</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">str1 = <span class="string">"123"</span></span><br><span class="line">list1 = list(str1)</span><br><span class="line"><span class="keyword">print</span> list1</span><br><span class="line"><span class="comment"># ['1', '2', '3']</span></span><br><span class="line"></span><br><span class="line">str2 = <span class="string">"123 sjhid dhi"</span></span><br><span class="line">list2 = str2.split() <span class="comment">#or list2 = str2.split(" ")</span></span><br><span class="line"><span class="keyword">print</span> list2</span><br><span class="line"><span class="comment"># ['123', 'sjhid', 'dhi']</span></span><br><span class="line"></span><br><span class="line">str3 = <span class="string">"www.google.com"</span></span><br><span class="line">list3 = str3.split(<span class="string">"."</span>)</span><br><span class="line"><span class="keyword">print</span> list3</span><br><span class="line"><span class="comment"># ['www', 'google', 'com']</span></span><br></pre></td></tr></table></figure></li><li><p>list转换为str:</p><ul><li><code>&quot;&quot;.join(list)</code> 无空格连接</li><li><code>&quot;.&quot;.join(list)</code>  </li></ul></li></ol></li></ul><h2 id="345-Reverse-Vowels-of-a-String"><a href="#345-Reverse-Vowels-of-a-String" class="headerlink" title="345-Reverse Vowels of a String"></a>345-Reverse Vowels of a String</h2><p><a href="https://leetcode.com/problems/reverse-vowels-of-a-string/">345-Reverse Vowels of a String</a> </p><p>Problem:</p><p>反转字符串中的元音字母。</p><p>Solution：</p><p>元音字母，包括大写元音字母和小写元音字母。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseVowels</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        s = list(s)</span><br><span class="line">        dic = &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'e'</span>: <span class="number">1</span>, <span class="string">'i'</span>: <span class="number">1</span>, <span class="string">'o'</span>: <span class="number">1</span>, <span class="string">'u'</span>: <span class="number">1</span>&#125;</span><br><span class="line">        rev = [<span class="number">0</span>] * n</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> s[index] <span class="keyword">in</span> dic <span class="keyword">or</span> s[index].lower() <span class="keyword">in</span> dic:</span><br><span class="line">                rev[index] = <span class="number">1</span></span><br><span class="line">        l = <span class="number">0</span></span><br><span class="line">        r = n - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> l &lt; r:</span><br><span class="line">            <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> rev[l] == <span class="number">0</span>:</span><br><span class="line">                l += <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> l &lt; r <span class="keyword">and</span> rev[r] == <span class="number">0</span>:</span><br><span class="line">                r -= <span class="number">1</span></span><br><span class="line">            s[l], s[r] = s[r], s[l]</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">            r -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(s)</span><br></pre></td></tr></table></figure><hr><ul><li><p>Python大小写转换：</p><ul><li>所有字符转换为大写：<code>str.upper()</code> </li><li>所有字符转换为小写：<code>str.lower()</code> </li><li>第一个字母转换为大写字母，其余小写：<code>str.capitalize()</code> </li><li>把每个单词的第一个字母转换为大写，其余小写。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">"www.runoob.com"</span></span><br><span class="line">print(str.upper())          <span class="comment"># 把所有字符中的小写字母转换成大写字母</span></span><br><span class="line">print(str.lower())          <span class="comment"># 把所有字符中的大写字母转换成小写字母</span></span><br><span class="line">print(str.capitalize())     <span class="comment"># 把第一个字母转化为大写字母，其余小写</span></span><br><span class="line">print(str.title())          <span class="comment"># 把每个单词的第一个字母转化为大写，其余小写</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># WWW.RUNOOB.COM</span></span><br><span class="line"><span class="comment"># www.runoob.com</span></span><br><span class="line"><span class="comment"># Www.runoob.com</span></span><br><span class="line"><span class="comment"># Www.Runoob.Com</span></span><br></pre></td></tr></table></figure></li><li><p>Python中string是不可变对象，不能通过下标的方式（如<code>str[0]=&#39;a&#39;</code> )改变字符串。</p></li></ul><h2 id="205-Isomorphic-Strings"><a href="#205-Isomorphic-Strings" class="headerlink" title="205-Isomorphic Strings"></a>205-Isomorphic Strings</h2><p><a href="https://leetcode.com/problems/isomorphic-strings/">205-Isomorphic Strings</a> </p><p>Problem:</p><p>判断是否同构字符串。</p><p>Solution：</p><p>字符到字符的映射，必须是单射。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isIsomorphic</span><span class="params">(self, s: str, t: str)</span> -&gt; bool:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        dic = &#123;&#125;</span><br><span class="line">        vSet = set()  <span class="comment"># satisfy single map</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">            ch = s[idx]</span><br><span class="line">            <span class="comment"># single map</span></span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">not</span> <span class="keyword">in</span> dic <span class="keyword">and</span> t[idx] <span class="keyword">not</span> <span class="keyword">in</span> vSet:</span><br><span class="line">                dic[ch] = t[idx]</span><br><span class="line">                vSet.add(t[idx])</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">in</span> dic <span class="keyword">and</span> dic[ch] == t[idx]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><hr><ul><li><p>Python 集合的操作：</p><ol><li><p>创建空集合：<code>set()</code> </p></li><li><p>创建有初值的集合：<code>SET = {v0, v1, v2}</code> 或者<code>SET = set(v0)</code> </p></li><li><p>判断元素是否在集合中：<code>x in SET</code> </p></li><li><p>集合运算：</p><p><code>a-b</code> :属于a集合不属于b集合</p><p><code>a|b</code> :属于a集合或属于b集合</p><p><code>a&amp;b</code> :集合a和集合b都包含的元素</p><p><code>a^b</code> : 不同时包含于集合a和集合b的元素</p></li><li><p>集合中添加元素：<code>s.add(x)</code> </p></li><li><p>集合中添加元素，且参数可以是列表、元组、字典(是每个元素都添加进去）等：<code>s.update(x)</code> </p></li><li><p>移除元素：<code>s.remove(x)</code> ，如果元素不存在，则会发生错误。</p></li><li><p>移除元素：<code>s.discard(x)</code> ，如果元素不存在，不会发生错误。</p></li><li><p>随机删除集合中的一个元素：<code>s.pop()</code> （原理：对集合无序排序，然后删除无序排列集合的第一个）</p></li><li><p>计算集合元素的个数：<code>len(s)</code> </p></li><li><p>清空集合<code>s.clear()</code> </p></li></ol></li><li><p>List Comprehension &amp;&amp; Set Comprehension &amp;&amp; Dictionary Comprehension</p><p>这个相当于数学中的 $S={2\cdot x\mid x\in \left[0,9\right)}$ 的表达。</p><ol><li><p>List Comprehension</p><p>如果用数学中的这个表达来看下面的式子，就很显而易见了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arr = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br></pre></td></tr></table></figure><p>再看看加了其他限制的例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># filter the elements</span></span><br><span class="line">arr1 = [x <span class="keyword">for</span> x <span class="keyword">in</span> arr <span class="keyword">if</span> x % <span class="number">2</span>==<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># add more conditions</span></span><br><span class="line">arr2 = [x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> arr <span class="keyword">if</span> x &gt;= <span class="number">3</span> <span class="keyword">and</span> x % <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># use nested for loops</span></span><br><span class="line">arr3 = [(x, y) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>) <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br></pre></td></tr></table></figure><p>使用List Comprehension不仅优美，而且效率也会很高。</p></li><li><p>Set Comprehension</p><p>同样的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s = &#123;x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">100</span>) <span class="keyword">if</span> x%<span class="number">2</span> != <span class="number">0</span> <span class="keyword">and</span> x%<span class="number">3</span> != <span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></li><li><p>Dictionary Comprehension</p><p>Syntax：<code>{expression(variable): expression(variable) for variable, variable in input_set [predicate][, …]}</code>  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [(set_k), (set_v)]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>)]&#125;</span><br><span class="line">&#123;<span class="number">1</span>: <span class="number">2</span>, <span class="number">3</span>: <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;n: n <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>)&#125;</span><br><span class="line">&#123;<span class="number">0</span>: <span class="number">0</span>, <span class="number">1</span>: <span class="number">1</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;chr(n): n <span class="keyword">for</span> n <span class="keyword">in</span> (<span class="number">65</span>, <span class="number">66</span>, <span class="number">66</span>)&#125;</span><br><span class="line">&#123;<span class="string">'A'</span>: <span class="number">65</span>, <span class="string">'B'</span>: <span class="number">66</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ((k1, v1), (k2, v2))</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> ((<span class="string">'I'</span>, <span class="number">1</span>), (<span class="string">'II'</span>, <span class="number">2</span>))&#125;</span><br><span class="line">&#123;<span class="string">'I'</span>: <span class="number">1</span>, <span class="string">'II'</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> ((<span class="string">'a'</span>, <span class="number">0</span>), (<span class="string">'b'</span>, <span class="number">1</span>)) <span class="keyword">if</span> v == <span class="number">1</span>&#125;</span><br><span class="line">&#123;<span class="string">'b'</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure></li></ol></li></ul><h2 id="68-Text-Justification"><a href="#68-Text-Justification" class="headerlink" title="68-Text Justification"></a>68-Text Justification</h2><p><a href="https://leetcode.com/problems/text-justification/">68-Text Justification</a> </p><p>Problem:</p><p>文本对齐，总结下来有以下几点要求。</p><ul><li>如果不是最后一行，且该行不止一个单词，则要求左右对齐。<ul><li>左右对齐：尽可能让单词间的空格均匀分布，如果不能均匀分布，则单词左边的空格应该比右边的空格多。</li><li>贪心的思想：应该尽可能的多放单词。</li></ul></li><li>如果是最后一行，或者该行只有一个单词，则要求左对齐。</li></ul><p>Solution：</p><p>分两种情况处理，判断是左对齐，还是右对齐。</p><ol><li><p>左对齐：该行有x个单词</p><p>前x-1个单词的后面都应该只有一个空格。</p><p>最后一个单词后面就应该补齐所有空格。</p></li><li><p>左右对齐：该行有x个单词，有x-1个空格间隙。</p><p>计算得到该行的空格数w，则如果能均匀分配，则每个间隙应该有aver = w // (x-1) 个空格。</p><p>但也许不会均匀分配，因此，可能会多出m个空格（m &lt; x-1 )</p><p>即前m个单词，单词的后面应该有（aver+1)个空格，后面的(x-1) - m个单词应该有aver个空格。</p><p>最后一个单词的后面没有空格。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.words = <span class="literal">None</span></span><br><span class="line">        self.maxWidth = <span class="literal">None</span></span><br><span class="line">        self.sum = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">leftJustify</span><span class="params">(self, l: int, r: int)</span> -&gt; str:</span></span><br><span class="line">        wordsNum = r - l + <span class="number">1</span></span><br><span class="line">        lengthNum = self.sum[r] <span class="keyword">if</span> l == <span class="number">0</span> <span class="keyword">else</span> self.sum[r] - self.sum[l - <span class="number">1</span>]</span><br><span class="line">        spaceNum = self.maxWidth - lengthNum</span><br><span class="line">        temp = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l, r):</span><br><span class="line">            temp += self.words[i] + <span class="string">" "</span></span><br><span class="line">        temp += self.words[r] + <span class="string">" "</span>*(spaceNum - (wordsNum - <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">leftRightJustify</span><span class="params">(self, l: int, r: int)</span> -&gt; str:</span></span><br><span class="line">        wordsNum = r - l + <span class="number">1</span></span><br><span class="line">        lengthNum = self.sum[r] <span class="keyword">if</span> l == <span class="number">0</span> <span class="keyword">else</span> self.sum[r] - self.sum[l - <span class="number">1</span>]</span><br><span class="line">        spaceNum = self.maxWidth - lengthNum</span><br><span class="line">        temp = <span class="string">""</span></span><br><span class="line">        averSpace = spaceNum // (wordsNum - <span class="number">1</span>)</span><br><span class="line">        moreSpace = spaceNum - averSpace*(wordsNum - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(moreSpace):</span><br><span class="line">            temp += self.words[l+i] + <span class="string">" "</span> * (averSpace + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l+moreSpace, r):</span><br><span class="line">            temp += self.words[i] + <span class="string">" "</span> * averSpace</span><br><span class="line">        temp += self.words[r]</span><br><span class="line">        <span class="keyword">return</span> temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fullJustify</span><span class="params">(self, words: List[str], maxWidth: int)</span> -&gt; List[str]:</span></span><br><span class="line">        self.words = words</span><br><span class="line">        self.maxWidth = maxWidth</span><br><span class="line">        n = len(words)</span><br><span class="line">        sum = [<span class="number">0</span>]*n</span><br><span class="line">        sum[<span class="number">0</span>] = len(words[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># sum prefix length of words</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            sum[i] = sum[i - <span class="number">1</span>] + len(words[i])</span><br><span class="line">        self.sum = sum</span><br><span class="line">        l = <span class="number">0</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">while</span> l &lt; n:</span><br><span class="line">            r = l</span><br><span class="line">            lengthNum = len(self.words[l])</span><br><span class="line">            <span class="keyword">while</span> r+<span class="number">1</span> &lt; n <span class="keyword">and</span> lengthNum + len(self.words[r+<span class="number">1</span>]) + <span class="number">1</span> &lt;= maxWidth:</span><br><span class="line">                lengthNum += len(self.words[r+<span class="number">1</span>]) + <span class="number">1</span>  <span class="comment"># space</span></span><br><span class="line">                r += <span class="number">1</span></span><br><span class="line">            <span class="comment"># only one word or the last line</span></span><br><span class="line">            <span class="keyword">if</span> r - l + <span class="number">1</span> == <span class="number">1</span> <span class="keyword">or</span> r == n - <span class="number">1</span>:</span><br><span class="line">                ans.append(self.leftJustify(l, r))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ans.append(self.leftRightJustify(l, r))</span><br><span class="line">            l = r + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><ul><li><p>Python的三元运算符：</p><p>#如果条件为真，返回真 否则返回假<br>condition_is_true if condition else condition_is_false</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">is_fat = <span class="literal">True</span></span><br><span class="line">state = <span class="string">"fat"</span> <span class="keyword">if</span> is_fat <span class="keyword">else</span> <span class="string">"not fat"</span></span><br></pre></td></tr></table></figure></li><li><p>Python的整除是：<code>\\</code> ，实数除是：<code>\</code> </p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Python中字符串的连接方法总结： <a href="https://segmentfault.com/a/1190000015475309">https://segmentfault.com/a/1190000015475309</a></li><li></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LeetCode String 专题记录。&lt;/p&gt;
&lt;p&gt;9月毕。&lt;/p&gt;
&lt;p&gt;「我祝福你有时有坏运气，你会意识到概率和运气在人生中扮演的角色，并理解你的成功并不完全是你应得的，其他人的失败也并不完全是他们应得的。」&lt;/p&gt;
&lt;p&gt;「不想要刚好错过的悔恨，那就要有完全碾压的实力。」&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/tags/LeetCode/"/>
    
      <category term="Algorithms" scheme="https://f7ed.com/tags/Algorithms/"/>
    
      <category term="String" scheme="https://f7ed.com/tags/String/"/>
    
      <category term="Data-Structure" scheme="https://f7ed.com/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>「LeetCode」：Array</title>
    <link href="https://f7ed.com/2020/09/15/LeetCode_array/"/>
    <id>https://f7ed.com/2020/09/15/LeetCode_array/</id>
    <published>2020-09-14T16:00:00.000Z</published>
    <updated>2020-10-09T13:24:32.989Z</updated>
    
    <content type="html"><![CDATA[<p>8月某司实训+准备开学期末考，我可太咕了q w q…dbq，（希望）高产博主我.我..又回来了。</p><p>LeetCode Array专题，持久更新。（<a href="https://github.com/f1ed/LeetCode">GitHub</a>)</p><a id="more"></a><h1 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h1><h2 id="27-Remove-Elements"><a href="#27-Remove-Elements" class="headerlink" title="27-Remove Elements"></a>27-Remove Elements</h2><p><a href="https://leetcode.com/problems/remove-element/">27-Remove Elements</a> </p><p>Solution</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeElement</span><span class="params">(self, nums: List[int], val: int)</span> -&gt; int:</span></span><br><span class="line"></span><br><span class="line">        tot = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> element <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> element == val:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                nums[tot] = element</span><br><span class="line">                tot = tot + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> tot</span><br></pre></td></tr></table></figure><ol><li><p>Python的参数传递和函数返回值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeElement</span><span class="params">(self, nums:List[int], val:int)</span> -&gt; int:</span></span><br></pre></td></tr></table></figure></li><li><p>题目要求：</p><p>“remove all instances of that value <a href="https://en.wikipedia.org/wiki/In-place_algorithm"><strong>in-place</strong></a> and return the new length.”</p><p>“Do not allocate extra space for another array, you must do this by <strong>modifying the input array <a href="https://en.wikipedia.org/wiki/In-place_algorithm">in-place</a></strong> with O(1) extra memory.”</p><p>“Confused why the returned value is an integer but your answer is an array?</p><p>Note that the input array is passed in by <strong>reference</strong>, which means modification to the input array will be known to the caller as well.”</p><p>题目要求是在原数组上删除数值，不能额外开新的空间存储数组。</p><p>意思就是说，虽然函数返回的是一个数值，但实际返回答案是一个数组。</p><p>因为数组的传递是指针传递，返回的是数组长度，则相当于返回了这个in-place的新数组。</p></li></ol><h2 id="26-Remove-Duplicates-from-Sorted-Array"><a href="#26-Remove-Duplicates-from-Sorted-Array" class="headerlink" title="26-Remove Duplicates from Sorted Array"></a>26-Remove Duplicates from Sorted Array</h2><p><a href="https://leetcode.com/problems/remove-duplicates-from-sorted-array/">26-Remove Duplicates from Sorted Array</a> </p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeDuplicates</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line"></span><br><span class="line">        tot = <span class="number">0</span></span><br><span class="line">        before = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[index] != before:</span><br><span class="line">                nums[tot] = nums[index]</span><br><span class="line">                before = nums[index]</span><br><span class="line">                tot = tot + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tot</span><br></pre></td></tr></table></figure><hr><ol><li><p>第一次提交的时候<code>before = nums[0] - 1</code> 报错了，原因是传入数组长度为0，下标越界。</p><p>注意空数组的下标越界问题。</p></li></ol><h2 id="80-Remove-Duplicates-from-Sorted-Array-II"><a href="#80-Remove-Duplicates-from-Sorted-Array-II" class="headerlink" title="80-Remove Duplicates from Sorted Array II"></a>80-Remove Duplicates from Sorted Array II</h2><p><a href="https://leetcode.com/problems/remove-duplicates-from-sorted-array-ii/">80-Remove Duplicates from Sorted Array II</a> </p><p>Solution:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeDuplicates</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        before = <span class="literal">None</span></span><br><span class="line">        before_cnt = <span class="number">0</span></span><br><span class="line">        length = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[index] != before:</span><br><span class="line">                nums[length] = nums[index]</span><br><span class="line">                before = nums[index]</span><br><span class="line">                length += <span class="number">1</span></span><br><span class="line">                before_cnt = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                before_cnt += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> before_cnt &lt;= <span class="number">2</span>:</span><br><span class="line">                    nums[length] = nums[index]</span><br><span class="line">                    length += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> length</span><br></pre></td></tr></table></figure><h2 id="189-Rotate-Array（S123）"><a href="#189-Rotate-Array（S123）" class="headerlink" title="189-Rotate Array（S123）"></a>189-Rotate Array（S123）</h2><p><a href="https://leetcode.com/problems/rotate-array/">189-Rotate Array</a> </p><p>Problem:</p><p>简述题目大意，给一个列表nums，一个 $k$ 值，要求原址让列表循环右移 $k$ 位。</p><p>Solution:</p><p>其实以下三种做法时间空间复杂度差别不大，主要看个思路吧。</p><table><thead><tr><th>S</th><th>Runtime</th><th>Memory</th><th align="center">Language</th></tr></thead><tbody><tr><td>S1</td><td>64ms</td><td>15.2MB</td><td align="center">pyhon3</td></tr><tr><td>S2</td><td>64ms</td><td>15.1MB</td><td align="center">python3</td></tr><tr><td>S3</td><td>116ms</td><td>15.1MB</td><td align="center">python3</td></tr></tbody></table><h3 id="S1-简单做法：空间换时间"><a href="#S1-简单做法：空间换时间" class="headerlink" title="S1-简单做法：空间换时间"></a>S1-简单做法：空间换时间</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ ，循环右移时多开了一个数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"><span class="keyword">import</span> queue</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Do not return anything, modify nums in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        length = len(nums)</span><br><span class="line">        a = [<span class="number">0</span>] * length</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(length):</span><br><span class="line">            a[(index + k)%length] = nums[index]</span><br><span class="line">        nums[:] = a</span><br></pre></td></tr></table></figure><h3 id="S2-利用数学同余关系"><a href="#S2-利用数学同余关系" class="headerlink" title="S2-利用数学同余关系"></a>S2-利用数学同余关系</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ ，循环右移时只多开了一个变量。</p><p>原理：</p><blockquote><p>定理[1]：</p><p>设 $m$ 是正整数，整数 $a$ 满足 $(a,m)=1$ ，$b$ 是任意整数。若 $x$ 遍历模 $m$ 的一个完全剩余系，则 $ax+b$ 也遍历模 $m$ 的一个完全剩余系。</p></blockquote><p>由以上定理可以得知，设 $n$ 为列表长度， $x$ 是列表的下标，遍历 $n$ 的一个完全剩余系。</p><ul><li><p>如果 $(k,n)=1$ ， $kx$ 也遍历 $n$ 的一个完全剩余系。这种情况，列表下标通过 $k$ 的倍数的顺序连成一个环。</p><p>：只需要额外一个变量 $temp$ 存储移动占用的值。</p></li><li><p>如果 $(k,n)\neq 1$ ，那么 $kx$ 不会遍历一个 $n$ 的完全剩余系，会出现下图的情况（如绿色的线的元素的 $idx = kx+0$ ，红色线的元素都是 $idx=kx+1$ ），会在 $k$ 的某个剩余类一直循环。</p><p>：遍历每个 $k$ 的剩余类。 在每次循环移位时，需要记录该次循环的起始位，防止重复。</p><img src="https://s1.ax1x.com/2020/09/14/wrcSVs.png" alt="wrcSVs.png" style="zoom:50%;" /></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Do not return anything, modify nums in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        length = len(nums)</span><br><span class="line">        k %= length</span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> cnt &lt; length:</span><br><span class="line">            current, prev = start, nums[start]</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                current = (current + k) % length</span><br><span class="line">                prev, nums[current] = nums[current], prev</span><br><span class="line">                cnt += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> current == start:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            start += <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="S3-利用反转列表的思路"><a href="#S3-利用反转列表的思路" class="headerlink" title="S3-利用反转列表的思路"></a>S3-利用反转列表的思路</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>原理：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Original List                   : 1 2 3 4 5 6 7</span><br><span class="line">After reversing all numbers     : 7 6 5 4 3 2 1</span><br><span class="line">After reversing first k numbers : 5 6 7 4 3 2 1</span><br><span class="line">After revering last n-k numbers : 5 6 7 1 2 3 4 --&gt; Result</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Solution 3: Reverse</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverse</span><span class="params">(self, nums: list, begin: int, end: int)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">while</span> begin &lt; end:</span><br><span class="line">            nums[begin], nums[end] = nums[end], nums[begin]</span><br><span class="line">            begin += <span class="number">1</span></span><br><span class="line">            end -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        k %= n</span><br><span class="line">        self.reverse(nums, <span class="number">0</span>, n<span class="number">-1</span>)</span><br><span class="line">        self.reverse(nums, <span class="number">0</span>, k<span class="number">-1</span>)</span><br><span class="line">        self.reverse(nums, k, n<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><hr><ol><li><p>Python用引用管理对象。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a1 = <span class="number">1</span>, *p = &amp;a1;</span><br><span class="line"><span class="keyword">int</span> a2 = <span class="number">2</span>, &amp;b = a2;</span><br></pre></td></tr></table></figure><ul><li>指针：指针变量是一个新变量，这个变量存储的是（变量a1的）地址，该地址指向一个存储单元。（该存储单元存放的是a1的值）。</li><li>引用：引用的实质是变量的别名，所以a2和b实际是一个东西，在内存中占有同一个存储单元。</li></ul><p>所以python中交换对象可以直接<code>a,b = b,a</code> </p></li><li><p>Python 列表的操作：切片。</p></li></ol><h2 id="41-First-Missing-Positive"><a href="#41-First-Missing-Positive" class="headerlink" title="41-First Missing Positive"></a>41-First Missing Positive</h2><p><a href="https://leetcode.com/problems/first-missing-positive/">41-First Missing Positive</a>  </p><p>Solution:</p><p>排一下序，维护一个expect变量就行了。</p><p>时间复杂度：$\mathcal{O}(n\log{n})$ ，题目没有卡常。</p><p>Runtime: 36 ms, faster than 70.96% of Python3 online submissions for First Missing Positive.</p><p>空间复杂度：$\mathcal{O}(n)$</p><p>Memory Usage: 13.8 MB, less than 69.19% of Python3 online submissions for First Missing Positive. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">firstMissingPositive</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        expect = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> element <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> element == expect:</span><br><span class="line">                expect += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> element &gt; expect:</span><br><span class="line">                <span class="keyword">return</span> expect</span><br><span class="line">        <span class="keyword">return</span> expect</span><br></pre></td></tr></table></figure><h2 id="299-Bulls-and-Cows"><a href="#299-Bulls-and-Cows" class="headerlink" title="299-Bulls and Cows"></a>299-Bulls and Cows</h2><p><a href="https://leetcode.com/problems/bulls-and-cows/">299-Bulls and Cows</a> </p><p>Problem:</p><p>题目大意是：给定两个相同长度的字符串，计算这两个字符串有多少个对应位数字相同，和多少个位置不对应但数字相同的个数。</p><p>Solution:</p><p>应用字符0-9本身数字的性质。</p><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getHint</span><span class="params">(self, secret: str, guess: str)</span> -&gt; str:</span></span><br><span class="line">        <span class="comment"># 0-9 cnt for guess (expect the same digit)</span></span><br><span class="line">        cnt = [<span class="number">0</span>] * <span class="number">10</span></span><br><span class="line">        bulls = cows = <span class="number">0</span></span><br><span class="line">        g = <span class="keyword">lambda</span> a: ord(a) - ord(<span class="string">'0'</span>)</span><br><span class="line">        <span class="keyword">for</span> si, gi <span class="keyword">in</span> zip(secret, guess):</span><br><span class="line">            <span class="keyword">if</span> si == gi:</span><br><span class="line">                bulls += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cnt[g(gi)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> si, gi <span class="keyword">in</span> zip(secret, guess):</span><br><span class="line">            <span class="keyword">if</span> si == gi:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">elif</span> cnt[g(si)] &gt; <span class="number">0</span>:</span><br><span class="line">                cows += <span class="number">1</span></span><br><span class="line">                cnt[g(si)] -= <span class="number">1</span></span><br><span class="line">        output = <span class="string">"&#123;&#125;A&#123;&#125;B"</span>.format(bulls, cows)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><hr><ol><li><p>ord()函数和chr()函数</p><p>ord()返回字符的ASCII码，chr函数返回ASCII码对应的字符。</p></li><li><p>浅析lambda表达式，匿名函数，类似于C语言的宏。</p><p>格式：<code>lambda [arg1[, arg2,...]] : expression</code> </p></li><li><p>双变量同时遍历使用zip()函数</p></li></ol><h2 id="134-Gas-Station（S12）"><a href="#134-Gas-Station（S12）" class="headerlink" title="134-Gas Station（S12）"></a>134-Gas Station（S12）</h2><p><a href="https://leetcode.com/problems/gas-station/">134-Gas Station</a> </p><p>Problem：</p><p>题目大意是：有N个环形加油站，每个加油站能加油gas[i]，一个汽车起始油量为0，且从i个站开到第i+1个站需要花费cost[i]的油量。找出这个车能顺时针跑完一圈的起始点（如果有，则唯一），如果不能返回-1。</p><p>Solution：</p><table><thead><tr><th>Solution</th><th>Runtime</th><th>Memory</th><th>Language</th></tr></thead><tbody><tr><td>S1-简单做法</td><td>3244ms</td><td>14.9MB</td><td>python3</td></tr><tr><td>S2-原理优化</td><td>104ms</td><td>14.8MB</td><td>python3</td></tr></tbody></table><h3 id="S1-简单解法"><a href="#S1-简单解法" class="headerlink" title="S1-简单解法"></a>S1-简单解法</h3><p>时间复杂度：$\mathcal{O}(n^2)$ ，实际远达不到 $n^2$ ，算有一点贪心叭。</p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>该汽车从起点i能跑完的必要条件：</p><ul><li><p>起始点 <code>gas[i] - cost[i] &gt;= 0</code> 。并且维护一个数组存放 <code>gas[i] - cost[i]</code> ，即这个站自给自足的油量余量。</p></li><li><p>如果从满足条件1的起点开始跑一圈， 要求路程中的油量必须大于等于0。维护一个汽车当前总油量 $S$ （用前缀和维护），每跑过一段路程，都要求 $S&gt;=0$ 。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canCompleteCircuit</span><span class="params">(self, gas: List[int], cost: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(gas)</span><br><span class="line">        remain = []</span><br><span class="line">        <span class="keyword">for</span> g, c <span class="keyword">in</span> zip(gas, cost):</span><br><span class="line">            remain.append(g - c)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> remain[i] &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                S = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                    S += remain[(i + j) % n]</span><br><span class="line">                    <span class="keyword">if</span> S &lt; <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> S &gt;= <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h3 id="S2-对问题分析进行再简化"><a href="#S2-对问题分析进行再简化" class="headerlink" title="S2-对问题分析进行再简化"></a>S2-对问题分析进行再简化</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  i: <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line">g[i]: <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span></span><br><span class="line">c[i]: <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">1</span> <span class="number">2</span></span><br><span class="line">g-c :<span class="number">-2</span><span class="number">-2</span><span class="number">-2</span> <span class="number">3</span> <span class="number">3</span></span><br><span class="line">sum :<span class="number">-2</span><span class="number">-4</span><span class="number">-6</span><span class="number">-3</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><ul><li><p>如果 $S=\sum_{i=0}^{n-1} g[i]-c[i],S&lt;0$  那么一定无解， $S$ 称为总积累油量。</p></li><li><p>如果 $S&gt;=0$，如果有找出最优解的方法，则一定有解。</p><ol><li><p>起点满足 $g[i]-c[i]&gt;=0$ ，把这些点称为正余量点。</p></li><li><p>用 $\mathcal{O}(1)$ 算出从第 $i$ 个点出发到第 $n$ （n就是第0个点） 个点所积累的油量： $res[i] =S-sum[i-1]$ .即用总积累油量减去前 $i-1$ 段路程能积累的油量（一般积累为负）。(sum数组就是 g-c的前缀和)</p></li><li><p>对于满足起点要求 $g[i]-c[i]&gt;=0$ 的所有点，计算从第 $i$ 个点出发到第 $n$ 个点到油量积累。那么有最大油量积累的点即为最优起始点。（题目规定如果存在，则点唯一）</p></li><li><p>因此，因为 $S$ 固定，只需要找到 $sum$ 数组中的最小值的下标，下标+1即是结果。</p></li></ol><ul><li>证明其正确性：<ol><li>如果满足 $g[i]-c[i]&gt;=0$ 的上述点 $i,j（i&lt;j)$  ，如果 $res[i]&gt;=res[j]$ ，说明从 $i$ 到 $j$ 是正油量积累，贪心的思想，那肯定积累的油量越多越好， $i$ 比 $j$ 优。</li><li>如果 $res[i]&lt;=res[j]$ ，说明从 $i$ 到 $j$ 是负油量积累，如果从 $i$ 点出发，到 $j$ 点就负油量了；如果从 $j$ 点出发，该车最后再跑 $i$ 到$j$  段，因为保证了总积累油量是正，所以最后一定有足够的油量能跑完 $i$ 到 $j$ 段。</li><li>再证只要 $S&gt;=0$ 则一定有解。是动态尝试起始点，( $i,j$ 都满足 $g[i]-c[i]&gt;=0$ ）从点 $i$ 开始，跑到点 $j$ 时，如果该途中途有出现油量不够，那就把 $i$ 到 $j$ 的这段路程放到路途的后面来跑，等油量积累够了再跑这段。</li></ol></li></ul></li></ul><p>Code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">canCompleteCircuit</span><span class="params">(self, gas: List[int], cost: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(gas)</span><br><span class="line">        sum = <span class="number">0</span></span><br><span class="line">        remain = []</span><br><span class="line">        <span class="keyword">for</span> g, c <span class="keyword">in</span> zip(gas, cost):</span><br><span class="line">            remain.append(g - c)</span><br><span class="line">            sum += g - c</span><br><span class="line">        <span class="keyword">if</span> sum &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            remain[i] += remain[i - <span class="number">1</span>]</span><br><span class="line">        min_idx = remain.index(min(remain))</span><br><span class="line">        <span class="keyword">return</span> (min_idx + <span class="number">1</span>) % n</span><br></pre></td></tr></table></figure><hr><ol><li>前缀和</li><li><code>list.index(value)</code> 找出list中值为<code>value</code> 的第一个下标。</li><li><code>min(list)</code> 返回list中的最小值。</li></ol><h2 id="118-Pascal’s-Triangle"><a href="#118-Pascal’s-Triangle" class="headerlink" title="118-Pascal’s Triangle"></a>118-Pascal’s Triangle</h2><p><a href="https://leetcode.com/problems/pascals-triangle/submissions/">118-Pascal’s Triangle</a> </p><p>Problem:</p><p>给定一个数字，输出如下规则的值。</p><img src="https://s1.ax1x.com/2020/09/15/wyWI9s.gif" alt="wyWI9s.gif" style="zoom:50%;" />   <p>Solution：</p><p>注意边界吧。（不太喜欢这种题qwq</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(self, numRows: int)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, numRows):</span><br><span class="line">            temp = [<span class="number">1</span>] * (i+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, i):</span><br><span class="line">                temp[j] = ans[i<span class="number">-1</span>][j<span class="number">-1</span>] + ans[i<span class="number">-1</span>][j]</span><br><span class="line">            ans.append(temp)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><ol><li><p>Python 中的append会出现值被覆盖的情况：变量在循环外定义，但在循环中对该变量做出一定改变，然后append到列表，最后发现列表中的值都是一样的。</p><p>因为Python中很多时候都是以对象的形式管理对象，因此append给列表的是一个地址。</p></li></ol><h2 id="119-Pascal’s-Triangle-II"><a href="#119-Pascal’s-Triangle-II" class="headerlink" title="119-Pascal’s Triangle II"></a>119-Pascal’s Triangle II</h2><p><a href="https://leetcode.com/problems/pascals-triangle-ii/">119-Pascal’s Triangle II</a> </p><p>Problem：</p><p>给定一个数字，输出某一行。</p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getRow</span><span class="params">(self, rowIndex: int)</span> -&gt; List[int]:</span></span><br><span class="line">        temp = [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, rowIndex):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">                temp[j] += temp[j<span class="number">-1</span>]</span><br><span class="line">            temp.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> temp</span><br></pre></td></tr></table></figure><h2 id="169-Majority-Element"><a href="#169-Majority-Element" class="headerlink" title="169-Majority Element"></a>169-Majority Element</h2><p><a href="https://leetcode.com/problems/majority-element/">169-Majority Element</a> </p><p>Problem:</p><p>给一串数字，找到出现次数大于 <code>n/2</code> 的数字。</p><p>Solution：</p><p>用字典计数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">majorityElement</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        cnt = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> ele <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> ele <span class="keyword">in</span> cnt:</span><br><span class="line">                cnt[ele] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cnt[ele] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> max(cnt, key=cnt.get)</span><br></pre></td></tr></table></figure><hr><ol><li>返回值最大/最小的键/索引。<ul><li>列表：<ul><li>最大值的索引：list.index(max(list))</li><li>最小值的索引：list.index(min(list))</li></ul></li><li>字典：<ul><li>最大值的键：max(dict, key=dict.get)</li><li>最小值的键：min(dict, key=dict.get)</li></ul></li></ul></li></ol><h2 id="229-Majority-Element-II"><a href="#229-Majority-Element-II" class="headerlink" title="229-Majority Element II"></a>229-Majority Element II</h2><p><a href="https://leetcode.com/problems/majority-element-ii/">229-Majority Element II</a> </p><p>Problem: </p><p>给一串数字，返回出现次数大于 <code>n/3</code> 的数字。</p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">majorityElement</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        cnt = &#123;&#125;</span><br><span class="line">        ans = []</span><br><span class="line">        <span class="keyword">for</span> ele <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> ele <span class="keyword">in</span> cnt:</span><br><span class="line">                cnt[ele] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cnt[ele] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> (k, v) <span class="keyword">in</span> cnt.items():</span><br><span class="line">            <span class="keyword">if</span> v &gt; n/<span class="number">3</span>:</span><br><span class="line">                ans.append(k)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><ol><li><p>字典的实用方法：</p><table><thead><tr><th>操作</th><th>实现方法</th></tr></thead><tbody><tr><td>删除字典元素</td><td><code>del dict[&#39;Name&#39;]</code></td></tr><tr><td>清空字典所有条目</td><td><code>dict.clear()</code></td></tr><tr><td>删除字典</td><td><code>del dict</code></td></tr><tr><td>返回指定键的值，如果值不存在返回default的值</td><td><code>dict.get(key, default)</code></td></tr><tr><td>如果键不存在字典中，添加键并将值设为default,于get类似</td><td><code>dict.setdefault(key, default=None)</code></td></tr><tr><td>判读键是否存在</td><td>1. <code>if k in dict</code> 2. <code>dict.has_key(key)</code> 存在返回true</td></tr><tr><td>以列表返回可遍历的（键，值）元祖数组</td><td><code>dict.items()</code></td></tr><tr><td>以列表返回一个字典的所有键</td><td><code>dict.keys()</code></td></tr><tr><td>以列表返回字典中的所有值</td><td><code>dict.values()</code></td></tr><tr><td>返回最大值的键值</td><td><code>max(dict, key=dict.get)</code></td></tr><tr><td>返回最小值的键值</td><td><code>min(dict, key=dict.get)</code></td></tr></tbody></table></li><li><p>遍历字典的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">dict=&#123;<span class="string">"a"</span>:<span class="string">"Alice"</span>,<span class="string">"b"</span>:<span class="string">"Bruce"</span>,<span class="string">"J"</span>:<span class="string">"Jack"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例一：键循环</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dict:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"dict[%s]="</span> % i,dict[i]</span><br><span class="line"><span class="comment"># 结果:</span></span><br><span class="line"><span class="comment"># dict[a]= Alice</span></span><br><span class="line"><span class="comment"># dict[J]= Jack</span></span><br><span class="line"><span class="comment"># dict[b]= Bruce</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例二：键值元组循环</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span>  dict.items():</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="comment"># 结果:</span></span><br><span class="line"><span class="comment"># ('a', 'Alice')</span></span><br><span class="line"><span class="comment"># ('J', 'Jack')</span></span><br><span class="line"><span class="comment"># ('b', 'Bruce')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例三：键值元组循环</span></span><br><span class="line"><span class="keyword">for</span> (k,v) <span class="keyword">in</span>  dict.items():</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"dict[%s]="</span> % k,v</span><br><span class="line"><span class="comment"># 结果:</span></span><br><span class="line"><span class="comment"># dict[a]= Alice</span></span><br><span class="line"><span class="comment"># dict[J]= Jack</span></span><br><span class="line"><span class="comment"># dict[b]= Bruce</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="274-H-Index"><a href="#274-H-Index" class="headerlink" title="274-H-Index"></a>274-H-Index</h2><p><a href="https://leetcode.com/problems/h-index/">274-H-Index</a> </p><p>Problem:</p><p>给出研究人员论文的论文引用次数，计算它的H指数（有h篇论文的引用次数至少为h，剩下N-h篇论文的引用次数不超过h）。</p><p>Solution：</p><p>时间复杂度：$\mathcal{O}(n\log{n})$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>排序后，再二分。（感觉自己的二分写的有点丑qwq</p><p>还有一种思路是，排序完，从最大的h开始递减遍历，满足条件就返回。反正排序也要$\mathcal{O}(n\log{n})$ 的复杂度…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hIndex</span><span class="params">(self, citations: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(citations)</span><br><span class="line">        citations.sort()</span><br><span class="line">        begin = <span class="number">0</span></span><br><span class="line">        end = n - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> begin &lt;= end:</span><br><span class="line">            mid = (begin + end) &gt;&gt; <span class="number">1</span></span><br><span class="line">            h = n - mid</span><br><span class="line">            <span class="keyword">if</span> citations[mid] &gt;= h:</span><br><span class="line">                end = mid</span><br><span class="line">                <span class="keyword">if</span> begin == end:</span><br><span class="line">                    <span class="keyword">return</span> h</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                begin += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> n-begin</span><br></pre></td></tr></table></figure><hr><ol><li>非递归写二分：<code>while begin &lt;= end</code> </li></ol><h2 id="275-H-Index-II"><a href="#275-H-Index-II" class="headerlink" title="275-H-Index II"></a>275-H-Index II</h2><p><a href="https://leetcode.com/problems/h-index-ii/">275-H-Index II</a> </p><p>Problem:</p><p>和274一样，给了递增的论文引用数，希望能用指数时间返回H指数。</p><p>Solution：</p><p>啊，就二分鸭。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hIndex</span><span class="params">(self, citations: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(citations)</span><br><span class="line">        begin = <span class="number">0</span></span><br><span class="line">        end = n - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> begin &lt;= end:</span><br><span class="line">            mid = (begin + end) &gt;&gt; <span class="number">1</span></span><br><span class="line">            h = n - mid</span><br><span class="line">            <span class="keyword">if</span> citations[mid] &gt;= h:</span><br><span class="line">                end = mid</span><br><span class="line">                <span class="keyword">if</span> begin == end:</span><br><span class="line">                    <span class="keyword">return</span> h</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                begin += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> n - begin</span><br></pre></td></tr></table></figure><h2 id="243-Shortest-Word-Distance"><a href="#243-Shortest-Word-Distance" class="headerlink" title="243-Shortest Word Distance"></a>243-Shortest Word Distance</h2><p>qwq 这道题还收费来着，于是于是就开了个中国区的会员（中国区的会员便宜好多啊！！）</p><p><a href="https://leetcode-cn.com/problems/shortest-word-distance/">243-Shortest Word Distance</a> </p><p>Problem：</p><p>给定一串单词，单词1和单词2，计算单词1单词2在单词列表中的距离。</p><p>Solution：</p><table><thead><tr><th>Solution</th><th>Runtime</th><th>Memory</th><th>Language</th></tr></thead><tbody><tr><td>S1-二分查找</td><td>44ms</td><td>15.7MB</td><td>python3</td></tr><tr><td>S2-线性维护</td><td>40ms</td><td>15.6MB</td><td>python3</td></tr></tbody></table><h3 id="S1-二分查找"><a href="#S1-二分查找" class="headerlink" title="S1-二分查找"></a>S1-二分查找</h3><p>时间复杂度：$\mathcal{O}(n\log{n})$ </p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>（最开始还很疑惑啥是单词距离…单词1和单词2可能在单词列表中重复出现）</p><p>计算出单词1和单词2在单词列表中出现的索引值列表，是递增有序的。</p><p>对于单词1索引列表中的每个值，在单词2索引列表中查找该值的lower_bound，计算距离。</p><p>同理，对于单词2索引列表中的每个值，也同样计算距离。</p><p>找出最小距离。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower_bound</span><span class="params">(a: list, x: int)</span> -&gt; int:</span></span><br><span class="line">    n = len(a)</span><br><span class="line">    begin = <span class="number">0</span></span><br><span class="line">    end = n - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> begin &lt;= end:</span><br><span class="line">        mid = (begin + end) &gt;&gt; <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> a[mid] &gt;= x:</span><br><span class="line">            end = mid</span><br><span class="line">            <span class="keyword">if</span> begin == end:</span><br><span class="line">                <span class="keyword">return</span> begin</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            begin += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    ans = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findShortest</span><span class="params">(self, li1: list, li2: list)</span> -&gt; <span class="keyword">None</span>:</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> li1:</span><br><span class="line">            min_dis_idx = lower_bound(li2, idx)</span><br><span class="line">            <span class="keyword">if</span> min_dis_idx == <span class="number">-1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.ans = min(self.ans, li2[min_dis_idx] - idx)</span><br><span class="line">                <span class="keyword">if</span> self.ans == <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortestDistance</span><span class="params">(self, words: List[str], word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        n = len(words)</span><br><span class="line">        li1 = []</span><br><span class="line">        li2 = []</span><br><span class="line">        self.ans = n</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># li1 and li2 are ordered</span></span><br><span class="line">            <span class="keyword">if</span> words[idx] == word1:</span><br><span class="line">                li1.append(idx)</span><br><span class="line">            <span class="keyword">if</span> words[idx] == word2:</span><br><span class="line">                li2.append(idx)</span><br><span class="line">        self.findShortest(li1, li2)</span><br><span class="line">        <span class="keyword">if</span> self.ans &gt; <span class="number">1</span>:</span><br><span class="line">            self.findShortest(li2, li1)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.ans</span><br></pre></td></tr></table></figure><h3 id="S2-线性维护："><a href="#S2-线性维护：" class="headerlink" title="S2-线性维护："></a>S2-线性维护：</h3><p>时间复杂度：$\mathcal{O}(n)$  </p><p>空间复杂度：$\mathcal{O}(n)$ </p><p>问题还能再简化，线性扫描单词列表，维护两个变量，单词1出现的最近索引，单词2出现的最近索引。扫描时计算距离，每当单词1或单词2出现时，就用另一个单词的最近索引计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortestDistance</span><span class="params">(self, words: List[str], word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        n = len(words)</span><br><span class="line">        lst1 = <span class="literal">None</span></span><br><span class="line">        lst2 = <span class="literal">None</span></span><br><span class="line">        ans = n</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> words[idx] == word1:</span><br><span class="line">                lst1 = idx</span><br><span class="line">                <span class="keyword">if</span> lst2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    ans = min(ans, lst1-lst2)</span><br><span class="line">            <span class="keyword">if</span> words[idx] == word2:</span><br><span class="line">                lst2 = idx</span><br><span class="line">                <span class="keyword">if</span> lst1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    ans = min(ans, lst2-lst1)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><hr><ol><li><p>C++中：</p><p>lower_bound(begin, end, num)：返回num的下界，即大于等于num的第一个索引位置。</p><p>upper_bound(begin, end, num)：返回num的上界，即大于num的第一个索引位置。</p><p>Python中用二分实现这两个函数。 </p></li></ol><h2 id="244-Shortest-Word-Distance-II"><a href="#244-Shortest-Word-Distance-II" class="headerlink" title="244-Shortest Word Distance II"></a>244-Shortest Word Distance II</h2><p><a href="https://leetcode-cn.com/problems/shortest-word-distance-ii/">244-Shortest Word Distance II</a></p><p>Problem:</p><p>和上题题干类似，计算单词距离，但是此问是每一个单词列表，可能有多个询问。</p><p>Solution：</p><p>对每一个单词列表，都可能有多个询问。</p><p>因此，之前243的解法每次询问都会遍历一遍单词列表。如果对每个单词列表询问数为 $M$ ，那么时间复杂度为 $\mathcal{O}(NM)$ ，会超时，所以希望能将单词列表的有关信息存下来，再用常数时间处理每一个询问。</p><p>这里的解法是用一个字典把每个单词出现的index列表存下来，键是单词，值是index列表。这个列表相对于单词列表的数目应该是远远小于的，因此用二重循环应该也能过吧（没有尝试二重循环解法）</p><p>这里有两种思路，一种是自己想的归并思路，还有一种是官方解答的思路，官方思路比归并的思路更优雅一些，问题抽象的更好。（代码差距不大，时间差距也不太大）</p><table><thead><tr><th>Solution</th><th>Runtime</th><th>Memory</th><th>Language</th></tr></thead><tbody><tr><td>S1-归并思路查询</td><td>96ms</td><td>20.8MB</td><td>python3</td></tr><tr><td>S2-交叉跳跃查询</td><td>80ms</td><td>20.4MB</td><td>python3</td></tr></tbody></table><p>双指针：$i$ 指向列表1，$j$指向列表2.</p><h3 id="S1-归并思路"><a href="#S1-归并思路" class="headerlink" title="S1-归并思路"></a>S1-归并思路</h3><p>归并思路：列表的值都是有序的，再把两个列表的值按归并的思想再排序，可以想成把点一个一个有序放在数轴上。</p><p>$i$指针前进的情况：（排序时，取列表1的下一个数字）</p><ol><li>$i+1&lt;len1$  and $li1[i+1] &lt; li[j]$</li><li>$i+1&lt;len1$  and $li1[i+1] &lt; li[j+1]$ </li><li>$i+1 &lt; len1$ and $j+1 == len2$  ($j$ 已无法移动)</li></ol><p>其余情况：$j$ 移动。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordDistance</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, words: List[str])</span>:</span></span><br><span class="line">        self.words = words</span><br><span class="line">        self.len = len(words)</span><br><span class="line">        self.dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(self.len):</span><br><span class="line">            word = words[index]</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> self.dict:</span><br><span class="line">                temp = self.dict[word]</span><br><span class="line">                temp.append(index)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                temp = [index]</span><br><span class="line">                self.dict[word] = temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortest</span><span class="params">(self, word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        ans = self.len</span><br><span class="line">        li1 = self.dict[word1]</span><br><span class="line">        li2 = self.dict[word2]</span><br><span class="line">        len1 = len(li1)</span><br><span class="line">        len2 = len(li2)</span><br><span class="line">        i = j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len1 <span class="keyword">and</span> j &lt; len2:</span><br><span class="line">            ans = min(ans, abs(li1[i] - li2[j]))</span><br><span class="line">            <span class="keyword">if</span> ans == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> ans</span><br><span class="line">            <span class="comment"># i goes ahead</span></span><br><span class="line">            <span class="keyword">if</span> i + <span class="number">1</span> &lt; len1 <span class="keyword">and</span> ((li1[i + <span class="number">1</span>] &lt; li2[j]) <span class="keyword">or</span> (j+<span class="number">1</span> == len2) <span class="keyword">or</span> (j + <span class="number">1</span> &lt; len2 <span class="keyword">and</span> li1[i + <span class="number">1</span>] &lt; li2[j + <span class="number">1</span>])):</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h3 id="S2-交叉比较"><a href="#S2-交叉比较" class="headerlink" title="S2-交叉比较"></a>S2-交叉比较</h3><p>对于当前指向列表1和列表2的两个元素 $li1[i]$ 和 $li2[j]$ ，对 $li1[i]$来说，只需要和旁边的属于列表2的元素比较，对 $li2[j]$ 同理。</p><p>因此，当  $li1[i]&gt;li2[j]$ 时，下一次的比较应该让 $j$ 指针前移一位，继续计算指针 $i$ 所指元素和其旁边的列表2的元素。同理，当 $li1[i]&lt;li2[j]$ 时，下一次的比较应该让 $i$ 指针前移一位，继续计算指针 $j$ 和其旁边的列表1的元素。</p><p>具体移动如下图。</p><img src="https://s1.ax1x.com/2020/09/19/wIuW4K.jpg" alt="wIuW4K.jpg" style="zoom:50%;" /><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordDistance</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, words: List[str])</span>:</span></span><br><span class="line">        self.words = words</span><br><span class="line">        self.len = len(words)</span><br><span class="line">        self.dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(self.len):</span><br><span class="line">            word = words[index]</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> self.dict:</span><br><span class="line">                temp = self.dict[word]</span><br><span class="line">                temp.append(index)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                temp = [index]</span><br><span class="line">                self.dict[word] = temp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortest</span><span class="params">(self, word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        ans = self.len</span><br><span class="line">        li1 = self.dict[word1]</span><br><span class="line">        li2 = self.dict[word2]</span><br><span class="line">        len1 = len(li1)</span><br><span class="line">        len2 = len(li2)</span><br><span class="line">        i = j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len1 <span class="keyword">and</span> j &lt; len2:</span><br><span class="line">            ans = min(ans, abs(li1[i] - li2[j]))</span><br><span class="line">            <span class="keyword">if</span> ans == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> ans</span><br><span class="line">            <span class="comment"># i goes ahead</span></span><br><span class="line">            <span class="keyword">if</span> li1[i] &lt; li2[j]:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="277-Find-the-Celebrity"><a href="#277-Find-the-Celebrity" class="headerlink" title="277-Find the Celebrity"></a>277-Find the Celebrity</h2><p><a href="https://leetcode-cn.com/problems/find-the-celebrity/">277-Find the Celebrity</a> </p><p>Problem: </p><p>已有know(i, j) API，判断i是否知道j，i是名人的充要条件是其他所有人知道i，而i不知道其他所有人。</p><p>Solution：</p><p>两种思路，第一种较为直观，使用二重循环，但剪枝多，远达不到 $\mathcal{O}(n^2)$ ，第二种稍做优化。因此两种解法差距不太大。</p><table><thead><tr><th align="left">提交时间</th><th align="left">运行时间</th><th align="left">内存消耗</th><th>语言</th></tr></thead><tbody><tr><td align="left">S几秒前</td><td align="left">1896ms</td><td align="left">13.6MB</td><td>python3</td></tr><tr><td align="left">5 分钟前</td><td align="left">1772ms</td><td align="left">13.6MB</td><td>python3</td></tr></tbody></table><h3 id="S1-直观思路"><a href="#S1-直观思路" class="headerlink" title="S1-直观思路"></a>S1-直观思路</h3><p>时间复杂度：远不到 $\mathcal{O}(n^2)$ </p><p>用了二重循环，但剪枝很多，所以远达不到 $\mathcal{O}(n^2)$ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findCelebrity</span><span class="params">(self, n: int)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            fg = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># i knows j ?</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> i == j:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> knows(i, j):</span><br><span class="line">                    fg = <span class="literal">False</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> fg:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># j knows i ?</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> i == j:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> knows(j, i):</span><br><span class="line">                    fg = <span class="literal">False</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> fg:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h3 id="S2-排除法"><a href="#S2-排除法" class="headerlink" title="S2-排除法"></a>S2-排除法</h3><p>时间复杂度：$\mathcal{O}(n)$ </p><p>排除i：根据<code>know(i, j)=True</code> 可以认为i不是名人，j可能是名人。</p><p>对于n-1个关系，最后从n个人中选出一个可能的人，再根据名人的充要条件去判断他是否是名人。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findCelebrity</span><span class="params">(self, n: int)</span> -&gt; int:</span></span><br><span class="line">        celebrity = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> knows(celebrity, i):</span><br><span class="line">                celebrity = i</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> celebrity == i:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">not</span> knows(celebrity, i)) <span class="keyword">and</span> knows(i, celebrity):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">return</span> celebrity</span><br></pre></td></tr></table></figure><h2 id="245-Shortest-Word-Distance-III"><a href="#245-Shortest-Word-Distance-III" class="headerlink" title="245-Shortest Word Distance III"></a>245-Shortest Word Distance III</h2><p><a href="https://leetcode-cn.com/problems/shortest-word-distance-iii/">245-Shortest Word Distance III</a> </p><p>Problem:</p><p>题意增加了两个单词可能相同，分两种情况就好了。</p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shortestWordDistance</span><span class="params">(self, words: List[str], word1: str, word2: str)</span> -&gt; int:</span></span><br><span class="line">        n = len(words)</span><br><span class="line">        ptr1 = <span class="literal">None</span></span><br><span class="line">        ptr2 = <span class="literal">None</span></span><br><span class="line">        ans = n</span><br><span class="line">        <span class="keyword">if</span> word1 == word2:</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> words[idx] == word1:</span><br><span class="line">                    ptr1, ptr2 = idx, ptr1</span><br><span class="line">                <span class="keyword">if</span> (ptr1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (ptr2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">                    ans = min(ans, ptr1-ptr2)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(n):</span><br><span class="line">                <span class="keyword">if</span> words[idx] == word1:</span><br><span class="line">                    ptr1 = idx</span><br><span class="line">                <span class="keyword">if</span> words[idx] == word2:</span><br><span class="line">                    ptr2 = idx</span><br><span class="line">                <span class="keyword">if</span> (ptr1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (ptr2 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">                    ans = min(ans, abs(ptr1-ptr2))</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><h2 id="217-Contains-Duplicate-E"><a href="#217-Contains-Duplicate-E" class="headerlink" title="217-Contains Duplicate[E]"></a>217-Contains Duplicate[E]</h2><p><a href="https://leetcode.com/problems/contains-duplicate/">217-Contains Duplicate</a> </p><p>Problem:</p><p>判断数组中有无重复元素出现。</p><p>Solution：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsDuplicate</span><span class="params">(self, nums: List[int])</span> -&gt; bool:</span></span><br><span class="line">        S = set()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> S:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            S.add(i)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h2 id="219-Contains-Duplicate-II-E"><a href="#219-Contains-Duplicate-II-E" class="headerlink" title="219-Contains Duplicate II[E]"></a>219-Contains Duplicate II[E]</h2><p><a href="https://leetcode.com/problems/contains-duplicate-ii/">219-Contains Duplicate II</a> </p><p>Problem:</p><p>判断数组中是否有两个相同的值，他们的索引之差小于等于k。</p><p>Solution：</p><p>存放出现该值的最近的索引，扫一遍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">containsNearbyDuplicate</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; bool:</span></span><br><span class="line">        dict = &#123;&#125;</span><br><span class="line">        n = len(nums)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> nums[i] <span class="keyword">not</span> <span class="keyword">in</span> dict:</span><br><span class="line">                dict.setdefault(nums[i], i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> i - dict[nums[i]] &lt;= k:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                dict[nums[i]] = i</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h2 id="4-Median-of-Two-Sorted-Arrays-H-（2S）"><a href="#4-Median-of-Two-Sorted-Arrays-H-（2S）" class="headerlink" title="4-Median of Two Sorted Arrays[H] （2S）"></a>4-Median of Two Sorted Arrays[H] （2S）</h2><p><a href="https://leetcode.com/problems/median-of-two-sorted-arrays/">4-Median of Two Sorted Arrays[H]</a> </p><p>Problem:</p><p>给两个排好序的数组，返回一个中位数</p><p>Solution：</p><table><thead><tr><th>S</th><th>运行时间</th><th>内存消耗</th></tr></thead><tbody><tr><td>S1</td><td>$\mathcal{O}(m+n)$ ：92ms</td><td>14.3MB</td></tr><tr><td>S2</td><td>$\mathcal{O}(\log(m+n))$ ：52ms</td><td>13.3MB</td></tr></tbody></table><h3 id="S1"><a href="#S1" class="headerlink" title="S1:"></a>S1:</h3><p>归并排序的做法。</p><p>时间复杂度：$\mathcal{O}(m+n)$ </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMedianSortedArrays</span><span class="params">(self, nums1: List[int], nums2: List[int])</span> -&gt; float:</span></span><br><span class="line">        n1 = len(nums1)</span><br><span class="line">        n2 = len(nums2)</span><br><span class="line">        median1 = median2 = <span class="literal">None</span></span><br><span class="line">        i = j = <span class="number">0</span></span><br><span class="line">        tot = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; n1 <span class="keyword">or</span> j &lt; n2:</span><br><span class="line">            <span class="keyword">if</span> (i &lt; n1 <span class="keyword">and</span> j &lt; n2 <span class="keyword">and</span> nums1[i] &lt;= nums2[j]) <span class="keyword">or</span> (i &lt; n1 <span class="keyword">and</span> j &gt;= n2):</span><br><span class="line">                tot += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> tot == (n1 + n2)//<span class="number">2</span>:</span><br><span class="line">                    median1 = nums1[i]</span><br><span class="line">                <span class="keyword">if</span> tot == (n1 + n2)//<span class="number">2</span> + <span class="number">1</span>:</span><br><span class="line">                    median2 = nums1[i]</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tot += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> tot == (n1 + n2) // <span class="number">2</span>:</span><br><span class="line">                    median1 = nums2[j]</span><br><span class="line">                <span class="keyword">if</span> tot == (n1 + n2) // <span class="number">2</span> + <span class="number">1</span>:</span><br><span class="line">                    median2 = nums2[j]</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (n1 + n2) % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> median2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (median1 + median2)/<span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="S2"><a href="#S2" class="headerlink" title="S2:"></a>S2:</h3><p>二分的思路</p><p>时间复杂度：$\mathcal{O}(\log(m+n))$ </p><p><strong>一、</strong>首先讨论一个数组的中位数，数组有n个元素，如果n为奇数，则第(n+1)/2个是中位数；如果n为偶数，则第(n+1)/2和第(n+2)/2的平均值为中位数。</p><p>回到本题，因为是两个数组，如果根据奇偶性分类讨论就过于麻烦了，所以将两种情况统一以简化解题思路。</p><p>即无论n是奇数还是偶数，数组的中位数都是第(n+1)/2和第(n+2)/2的平均数。</p><p>回到本题，设数组1有n个元素，数组2有m个元素，则中位数为两个数组的有序序列的第(n+m+1)/2个和第(n+m+2)/2个的平均数。</p><p><strong>二、</strong>因此，题目需要求解的问题改为求这两个有序数组的有序序列的第k个数。</p><p>二分思想：两个数组分别找第k/2个数，（假设都存在），比较，<strong>如果第一个数组的这个数小于第二个数组，说明第k个数肯定不在第一个数组的前k/2个数中，因此就可以直接去掉数组1的前k/2个元素</strong>，查找有序序列的第k-k/2个数；同理，如果大于，则说明第k个数肯定不在第二个数组的前k/2个数中，去掉数组2的前k/2个元素。</p><p>使用一个数组起始指针l1和l2来实现数组的“去掉”前k/2个元素。</p><p>设数组1的元素个数为n，数组2的元素个数为m。</p><p><strong>递归函数Find(l1, l2, k)：查找起始指针为l1, l2的两个有序数组的第k个数。</strong></p><ul><li><p>讨论边界情况，有数组为空的情况。即 <code>l1 == n</code>  或者 <code>l2 == m</code> .</p><p>如果第一个数组已为空，则直接返回第二个数组的第k个数；</p><p>同理，如果第二个数组为空，则直接返回第一个数组的第k个数。</p></li><li><p>两个数组都不为空的情况。即 <code>l1 &lt; n</code>  或者 <code>l2 &lt; m</code> .</p><ul><li><p>递归边界： <code>k == 1</code>  ,即返回 <code>nums1[l1]</code> 和 <code>nums2[l2]</code> 中较小的那一个。</p></li><li><p>数组长度边界：即有数组的剩余元素个数小于 <code>k/2</code> ，那么拿出来比较的就应该是数组的最后一个元素。</p><p>维护两个值 <code>k1</code> 和 <code>k2</code> 来分别表示用两个数组的第 k1和k2个来比较。</p><p>(k1 k2都小于等于k/2)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># to avoid the rest length of nums1/nums2 is shorter than k//2</span></span><br><span class="line">k1 = k//<span class="number">2</span> <span class="keyword">if</span> l1+k//<span class="number">2</span> &lt;= n <span class="keyword">else</span> n-l1</span><br><span class="line">k2 = k//<span class="number">2</span> <span class="keyword">if</span> l2+k//<span class="number">2</span> &lt;= m <span class="keyword">else</span> m-l2</span><br></pre></td></tr></table></figure></li><li><p>比较<code>nums1[l1+k1-1]</code> 和 <code>nums2[l2+k2-1]</code> 的大小，递归：</p><ol><li><p>相等：</p><p>如果 <code>k-k1-k2 == 0</code> 说明nums1的前k1个和nums2的前k2个就是有序序列的前k个，返回 <code>nums1[l1+k1-1]</code> 。</p><p>否则，（即某一个数组的剩余长度小于k/2），分别去掉两个数组的前k1和k2个数，递归调用 <strong>Find(l1+k1, l2+k2, k-k1-k2)</strong> 。</p></li><li><p><code>nums1[l1+k1-1] &gt; nums2[l2+k2-1]</code>  </p><p>说明可以去掉数组2的前k2个数，递归调用 <strong>Find(l1, l2+k2, k-k2)</strong> </p></li><li><p><code>nums1[l1+k1-1] &gt; nums2[l2+k2-1]</code>  </p><p>说明可以去掉数组1的前k1个数，递归调用 <strong>Find(l1+k1, l2, k-k1)</strong> </p></li></ol></li></ul></li></ul><p>Code：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.nums1 = <span class="literal">None</span></span><br><span class="line">        self.nums2 = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findKthOfTwo</span><span class="params">(self, l1: int, l2: int, k: int)</span> -&gt; int:</span></span><br><span class="line">        nums1 = self.nums1</span><br><span class="line">        nums2 = self.nums2</span><br><span class="line">        n = len(nums1)</span><br><span class="line">        m = len(nums2)</span><br><span class="line">        <span class="comment"># nums1 is empty</span></span><br><span class="line">        <span class="keyword">if</span> l1 == n:</span><br><span class="line">            <span class="keyword">return</span> nums2[l2+k<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># nums2 is empty</span></span><br><span class="line">        <span class="keyword">if</span> l2 == m:</span><br><span class="line">            <span class="keyword">return</span> nums1[l1+k<span class="number">-1</span>]</span><br><span class="line">        <span class="comment"># both not empty</span></span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> nums1[l1] <span class="keyword">if</span> nums1[l1] &lt;= nums2[l2] <span class="keyword">else</span> nums2[l2]</span><br><span class="line">        <span class="comment"># to avoid the rest length of nums1/nums2 is shorter than k//2</span></span><br><span class="line">        k1 = k//<span class="number">2</span> <span class="keyword">if</span> l1+k//<span class="number">2</span> &lt;= n <span class="keyword">else</span> n-l1</span><br><span class="line">        k2 = k//<span class="number">2</span> <span class="keyword">if</span> l2+k//<span class="number">2</span> &lt;= m <span class="keyword">else</span> m-l2</span><br><span class="line">        <span class="keyword">if</span> nums1[l1+k1<span class="number">-1</span>] == nums2[l2+k2<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">return</span> nums1[l1+k1<span class="number">-1</span>] <span class="keyword">if</span> k-k1-k2 == <span class="number">0</span> <span class="keyword">else</span> self.findKthOfTwo(l1+k1, l2+k2, k-k1-k2)</span><br><span class="line">        <span class="keyword">elif</span> nums1[l1+k1<span class="number">-1</span>] &gt; nums2[l2+k2<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">return</span> self.findKthOfTwo(l1, l2+k2, k-k2)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.findKthOfTwo(l1+k1, l2, k-k1)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findMedianSortedArrays</span><span class="params">(self, nums1: List[int], nums2: List[int])</span> -&gt; float:</span></span><br><span class="line">        n = len(nums1)</span><br><span class="line">        m = len(nums2)</span><br><span class="line">        self.nums1 = nums1</span><br><span class="line">        self.nums2 = nums2</span><br><span class="line">        <span class="comment"># median: the average of (n+m+1)//2 th  and  (n+m+2)//2 th</span></span><br><span class="line">        <span class="keyword">return</span> (self.findKthOfTwo(<span class="number">0</span>, <span class="number">0</span>, (n+m+<span class="number">1</span>)//<span class="number">2</span>) + self.findKthOfTwo(<span class="number">0</span>, <span class="number">0</span>, (n+m+<span class="number">2</span>)//<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ol><li>《信息安全数学基础》 2.2同余类和剩余系。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;8月某司实训+准备开学期末考，我可太咕了q w q…dbq，（希望）高产博主我.我..又回来了。&lt;/p&gt;
&lt;p&gt;LeetCode Array专题，持久更新。（&lt;a href=&quot;https://github.com/f1ed/LeetCode&quot;&gt;GitHub&lt;/a&gt;)&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://f7ed.com/tags/LeetCode/"/>
    
      <category term="Algorithms" scheme="https://f7ed.com/tags/Algorithms/"/>
    
      <category term="Data-Structure" scheme="https://f7ed.com/tags/Data-Structure/"/>
    
      <category term="Array" scheme="https://f7ed.com/tags/Array/"/>
    
  </entry>
  
  <entry>
    <title>「Web」:HTML and CSS</title>
    <link href="https://f7ed.com/2020/07/22/html-css/"/>
    <id>https://f7ed.com/2020/07/22/html-css/</id>
    <published>2020-07-21T16:00:00.000Z</published>
    <updated>2020-07-25T03:44:57.007Z</updated>
    
    <content type="html"><![CDATA[<p>温故知新：对Web基础知识——HTML和CSS的持续更新。</p><a id="more"></a><h1 id="说在前面"><a href="#说在前面" class="headerlink" title="说在前面"></a>说在前面</h1><h3 id="B-S-软件结构"><a href="#B-S-软件结构" class="headerlink" title="B/S 软件结构"></a>B/S 软件结构</h3><p>C/S： Client Server（JavaSE）</p><p>B/S：Browser Server（JavaEE）</p><h3 id="前端开发流程"><a href="#前端开发流程" class="headerlink" title="前端开发流程"></a>前端开发流程</h3><ol><li>美术实现：网页设计</li><li>前端工程师：设计为静态网页</li><li>Java程序员：后端工程师修改为动态页面</li></ol><h3 id="网页端组成部分"><a href="#网页端组成部分" class="headerlink" title="网页端组成部分"></a>网页端组成部分</h3><p>内容：页面中可以看到的数据。一般使用html技术。</p><p>表现：内容在页面上的展示形式。一般使用CSS。</p><p>行为：页面中的元素与输入设备交互。一般使用javascript技术。</p><h1 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h1><h2 id="创建HTML文件"><a href="#创建HTML文件" class="headerlink" title="创建HTML文件"></a>创建HTML文件</h2><ol><li>创建一个Web静态工程</li><li>在工程下创建html页面</li></ol><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span><span class="comment">&lt;!--声明--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"zh_CN"</span>&gt;</span><span class="comment">&lt;!--html中包含两部分：head和body--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="comment">&lt;!--head中包含：title标签、CSS样式、js代码--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">Hello World!</span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="HTML标签"><a href="#HTML标签" class="headerlink" title="HTML标签"></a>HTML标签</h2><ul><li>标签名大小写不敏感</li><li>标签有自己的属性<ul><li>基本属性：修改简单样式</li><li>事件属性：设置事件响应后的代码</li></ul></li><li>标签分为单标签&lt;标签/&gt;和双标签&lt;标签&gt;&lt;/标签&gt;</li><li>标签的属性必须要有值，属性值加双引号。</li><li>显示特殊标签：&lt; &gt; 空格等等，建议查阅文档。</li></ul><h3 id="字体标签"><a href="#字体标签" class="headerlink" title="字体标签"></a>字体标签</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">"red"</span> <span class="attr">size</span>=<span class="string">"7"</span>&gt;</span></span><br><span class="line">        哒哒哒。</span><br><span class="line">    <span class="tag">&lt;/<span class="name">font</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="标题标签：h1-到-h6"><a href="#标题标签：h1-到-h6" class="headerlink" title="标题标签：h1 到 h6"></a>标题标签：h1 到 h6</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">h1</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span>标题1<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h2</span> <span class="attr">align</span>=<span class="string">"left"</span>&gt;</span>标题2<span class="tag">&lt;/<span class="name">h2</span>&gt;</span><span class="comment">&lt;!--align：显示位置,默认左--&gt;</span></span><br></pre></td></tr></table></figure><h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://baidu.com"</span> <span class="attr">target</span>=<span class="string">"_self"</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="comment">&lt;!--_self属性：当前窗口跳转--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://baidu.com"</span> <span class="attr">target</span>=<span class="string">"_blank"</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="comment">&lt;!--_blank属性：打开新窗口跳转--&gt;</span></span><br></pre></td></tr></table></figure><h3 id="列表标签"><a href="#列表标签" class="headerlink" title="列表标签"></a>列表标签</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">ul</span> <span class="attr">type</span>=<span class="string">"none"</span>&gt;</span><span class="comment">&lt;!--无序列表--&gt;</span><span class="comment">&lt;!--type属性可以更改列表前的符号--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>百度<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ol</span>&gt;</span><span class="comment">&lt;!--有序表格--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>1<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>2<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>3<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>4<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ol</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="img标签"><a href="#img标签" class="headerlink" title="img标签"></a>img标签</h3><ul><li><p>属性src:图片等路径位置</p></li><li><p>JavaSE中路径</p><ul><li>相对路径：从工程名字开始算</li><li>绝对路径：硬盘中的路径</li></ul></li><li><p>Web中的路径</p><ul><li><p>相对路径：</p><p><code>.</code>    ：表示当前文件所在的目录</p><p><code>..</code>   ：表示当前文件所在的上级目录</p><p><code>文件名</code>：表示当前文件所在目录的文件，相当于<code>./文件名</code></p></li><li><p>绝对路径：<code>http://ip:port/工程名/资源路径</code></p></li></ul></li><li><p>属性：weight; height; </p><ul><li>border：设置图片边框大小。</li><li>alt：当指定路径找不到图片时，用来代替显示的文本内容。</li></ul></li></ul><h3 id="表格标签：实现跨行跨列"><a href="#表格标签：实现跨行跨列" class="headerlink" title="表格标签：实现跨行跨列"></a>表格标签：实现跨行跨列</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span> <span class="attr">border</span>=<span class="string">"1"</span> <span class="attr">width</span>=<span class="string">"300"</span>&gt;</span><span class="comment">&lt;!--表格标签--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--border：设置边框、width：设置宽度、height：设置高度--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--align：设置表格对齐方式--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--cellspacing:单元格间距--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span><span class="comment">&lt;!--行标签--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">th</span>&gt;</span>h1<span class="tag">&lt;/<span class="name">th</span>&gt;</span><span class="comment">&lt;!--表头标签--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">th</span>&gt;</span>h2<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">th</span>&gt;</span>h3<span class="tag">&lt;/<span class="name">th</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>1.1<span class="tag">&lt;/<span class="name">td</span>&gt;</span><span class="comment">&lt;!--单元格标签--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span>1.2<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--align：设置单元格文本对齐方式--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span>1.3<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span> <span class="attr">colspan</span>=<span class="string">"2"</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">td</span>&gt;</span><span class="comment">&lt;!--colspan:列的宽度,实现单元格跨列--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span> <span class="attr">rowspan</span>=<span class="string">"2"</span>&gt;</span>2.2<span class="tag">&lt;/<span class="name">td</span>&gt;</span><span class="comment">&lt;!--rowspan:行的宽度，实现单元格跨行--&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span> <span class="attr">colspan</span>=<span class="string">"2"</span>&gt;</span>3.1<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="iframe框架标签"><a href="#iframe框架标签" class="headerlink" title="iframe框架标签"></a>iframe框架标签</h3><p>可以在html页面上开辟一个小区域加载单独的页面，实现内嵌窗口。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">iframe</span> <span class="attr">src</span>=<span class="string">"hello.html"</span> <span class="attr">width</span>=<span class="string">"400"</span> <span class="attr">height</span>=<span class="string">"600"</span> <span class="attr">name</span>=<span class="string">"abc"</span>&gt;</span><span class="tag">&lt;/<span class="name">iframe</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--name：表示该区域的名字--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"welcome.html"</span> <span class="attr">target</span>=<span class="string">"abc"</span>&gt;</span>欢迎<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--target：打开窗口显示的位置--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--a标签的target属性设置为iframe的name属性，就在开辟的区域打开链接窗口--&gt;</span></span><br></pre></td></tr></table></figure><h3 id="表单标签"><a href="#表单标签" class="headerlink" title="表单标签"></a>表单标签</h3><p>表单：html中用来收集用户信息的元素集合，将这些信息发送给服务器处理。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span>&gt;</span><span class="comment">&lt;!--表单标签--&gt;</span></span><br><span class="line">    用户名称：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">value</span>=<span class="string">"User"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span><span class="comment">&lt;!--input输入框标签--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--type：输入类型 value：默认值--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--text：文本类型--&gt;</span></span><br><span class="line">    用户密码：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> /&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--password：密码类型--&gt;</span></span><br><span class="line">    确认密码：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    性别：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>男</span><br><span class="line">     <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span>/&gt;</span>女<span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--radio：单选框; name属性：可对其分组; checked：默认选项--&gt;</span></span><br><span class="line">    兴趣爱好：<span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>Java</span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span>/&gt;</span>JavaScript<span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--checkbox：复选框; checked:默认选项--&gt;</span></span><br><span class="line">    国籍：</span><br><span class="line">        <span class="tag">&lt;<span class="name">select</span>&gt;</span><span class="comment">&lt;!--下拉列表框标签--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">option</span>&gt;</span>--请选择国籍--<span class="tag">&lt;/<span class="name">option</span>&gt;</span><span class="comment">&lt;!--选项标签--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">option</span> <span class="attr">selected</span>=<span class="string">"selected"</span>&gt;</span>中国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--selected：默认选择--&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">option</span>&gt;</span>美国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">option</span>&gt;</span>日本<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">select</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    自我评价：<span class="tag">&lt;<span class="name">textarea</span> <span class="attr">rows</span>=<span class="string">"10"</span> <span class="attr">cols</span>=<span class="string">"30"</span>&gt;</span>默认值<span class="tag">&lt;/<span class="name">textarea</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--textarea标签：多行文本输入框；属性 rows:行数; 属性 cols：列数--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--textarea起始标签和结束标签中的内容是默认值--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"reset"</span> <span class="attr">value</span>=<span class="string">"重新输入"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--reset：重置按钮; value属性：更改按钮文本--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"submit"</span>&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--submit：提交按钮--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"button"</span> <span class="attr">value</span>=<span class="string">"按钮"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--button:按钮--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"file"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--file:文件上传--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"hidden"</span>/&gt;</span><span class="tag">&lt;<span class="name">br</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--hidden:隐藏域，需要发送一些不需要用户参与的信息至服务器，可使用隐藏域--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="表单格式化"><a href="#表单格式化" class="headerlink" title="表单格式化"></a>表单格式化</h3><p>把表单放入表格，使表单排列整齐。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">table</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户名称：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">value</span>=<span class="string">"User"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> /&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>确认密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>性别：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>男</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span>/&gt;</span>女<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="表单提交的细节"><a href="#表单提交的细节" class="headerlink" title="表单提交的细节"></a>表单提交的细节</h3><p>以下格式化的表单：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"https://localhost:8080"</span> <span class="attr">method</span>=<span class="string">"get"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--form标签属性--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--action：设置提交的服务器地址--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--method：设置提交的方式，默认GET（也可以是POST）--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"hidden"</span> <span class="attr">name</span>=<span class="string">"action"</span> <span class="attr">value</span>=<span class="string">"login"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">table</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户名称：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">value</span>=<span class="string">"User"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> /&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>确认密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>性别：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>男</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span>/&gt;</span>女<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>兴趣爱好：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">checked</span>=<span class="string">"checked"</span>/&gt;</span>Java</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span>/&gt;</span>JavaScript</span><br><span class="line">            <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>国籍：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">select</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span>&gt;</span>--请选择国籍--<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">selected</span>=<span class="string">"selected"</span>&gt;</span>中国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span>&gt;</span>美国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span>&gt;</span>日本<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>自我评价:<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">textarea</span> <span class="attr">rows</span>=<span class="string">"10"</span> <span class="attr">cols</span>=<span class="string">"30"</span>&gt;</span>默认值<span class="tag">&lt;/<span class="name">textarea</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"reset"</span> <span class="attr">value</span>=<span class="string">"重新输入"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"submit"</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><p>格式化后的表单显示为：</p><img src="https://s1.ax1x.com/2020/07/25/UznJiT.png" alt="UznJiT.pndg" style="zoom:50%;" /> <p>表单提交后，url显示为：<code>https://localhost:8080/?action=login&amp;sex=on</code> </p><p>该url体现了三部分</p><ul><li>提交表单的服务器地址/action属性的值：localhost:8080/</li><li>分隔符：<code>?</code></li><li>请求参数/表单信息：action=login; sex=on</li></ul><hr><p><strong>表单提交的时候，数据没有发送给服务器的三种情况：</strong></p><ol><li>表单项input标签没有name属性值。</li><li>单选、复选输入标签以及下拉列表的option标签，还需要加value属性值，以便发送给服务器具体值，而不是on。</li><li>表单项不在提交的form标签中。</li></ol><p>修改后的表单代码如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span>=<span class="string">"https://localhost:8080"</span> <span class="attr">method</span>=<span class="string">"get"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--action：设置提交的服务器地址--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--method：设置提交的方式，默认GET--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"hidden"</span> <span class="attr">name</span>=<span class="string">"action"</span> <span class="attr">value</span>=<span class="string">"login"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">table</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户名称：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"user"</span> <span class="attr">value</span>=<span class="string">"User"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>用户密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> <span class="attr">name</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>确认密码：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> <span class="attr">name</span>=<span class="string">"password"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>性别：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">checked</span>=<span class="string">"checked"</span> <span class="attr">value</span>=<span class="string">"boy"</span>/&gt;</span>男</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">name</span>=<span class="string">"sex"</span> <span class="attr">value</span>=<span class="string">"girl"</span>/&gt;</span>女<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>兴趣爱好：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">checked</span>=<span class="string">"checked"</span> <span class="attr">name</span>=<span class="string">"hobby"</span> <span class="attr">value</span>=<span class="string">"Java"</span>/&gt;</span>Java</span><br><span class="line">                <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">name</span>=<span class="string">"hobby"</span> <span class="attr">value</span>=<span class="string">"js"</span>/&gt;</span>JavaScript</span><br><span class="line">            <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>国籍：<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">select</span> <span class="attr">name</span>=<span class="string">"country"</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">"none"</span>&gt;</span>--请选择国籍--<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">selected</span>=<span class="string">"selected"</span> <span class="attr">value</span>=<span class="string">"中国"</span>&gt;</span>中国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">"美国"</span>&gt;</span>美国<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">"日本"</span>&gt;</span>日本<span class="tag">&lt;/<span class="name">option</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span>自我评价:<span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">textarea</span> <span class="attr">rows</span>=<span class="string">"10"</span> <span class="attr">cols</span>=<span class="string">"30"</span>&gt;</span>默认值<span class="tag">&lt;/<span class="name">textarea</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"reset"</span> <span class="attr">value</span>=<span class="string">"重新输入"</span>/&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"submit"</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><p>表单提交后的url: <code>https://localhost:8080/?action=login&amp;user=fred&amp;password=123&amp;password=123&amp;sex=girl&amp;hobby=Java&amp;hobby=js&amp;country=中国</code> </p><hr><p><strong>表单标签method属性参数的区别</strong></p><ul><li><p>GET：</p><ol><li><p>浏览器的地址栏为：action属性值 + ? + 请求参数</p><p>请求参数格式为：<code>name=value&amp;name=value</code></p></li><li><p>不安全</p></li><li><p>有数据长度限制</p></li></ol></li><li><p>POST请求的特点：</p><ol><li>浏览器上的地址栏为：action属性值（没有请求参数）</li><li>相当于GET请求更安全</li><li>理论上没有数据长度限制</li></ol></li></ul><h3 id="div和span"><a href="#div和span" class="headerlink" title="div和span"></a>div和span</h3><ul><li>div 标签：默认独占一行</li><li>span 标签：长度是封装数据长度</li><li>p 标签：默认在段落的上方或下方各空出一行（如果已有空行则不空）</li></ul><h3 id="label标签"><a href="#label标签" class="headerlink" title="label标签"></a>label标签</h3><p>label标签为input元素定义标注。</p><p>该标签不会为用户呈现特殊的效果，但为鼠标用户改进了可用性，即在label元素内点击文本，就会触发该控件。即当用户选择该标签时，浏览器会自动将焦点转到和label标签绑定的表单项上。</p><p>常见的应用情况是：单选框/复选框，点击文本即可勾选，而不需要去点那个框。</p><ul><li>for : 表示该label是为表单中哪个控件服务，for属性点值设置为该元素的id属性值</li></ul><h1 id="CSS"><a href="#CSS" class="headerlink" title="CSS"></a>CSS</h1><h2 id="CSS简介"><a href="#CSS简介" class="headerlink" title="CSS简介"></a>CSS简介</h2><p>CSS：层叠样式表单，用于增强/控制网页样式，且允许将样式信息和网页内容分离的一种标记性语言。</p><p>语法规则：</p><img src="https://s1.ax1x.com/2020/07/25/UznYJU.png" alt="UznYJU.png" style="zoom:50%;" /> <ul><li><p>选择器：浏览器根据选择器决定受CSS样式影响到HTML元素/标签。</p></li><li><p>属性：<code>属性:值;</code> 形成一个完成的declaration。</p></li></ul><p>CSS中的注释：/**/</p><h2 id="CSS与HTML结合方式"><a href="#CSS与HTML结合方式" class="headerlink" title="CSS与HTML结合方式"></a>CSS与HTML结合方式</h2><h3 id="标签中的style"><a href="#标签中的style" class="headerlink" title="标签中的style"></a>标签中的style</h3><p>在标签的style属性设置<code>style=&quot;key: value1 value2;&quot;</code> </p><p>这种方式可读性差，且没有复用性。</p><h2 id="head标签中使用style标签"><a href="#head标签中使用style标签" class="headerlink" title="head标签中使用style标签"></a>head标签中使用style标签</h2><p>在head标签中，用style标签定义需要的css样式。</p><p>style标签中的语句是CSS语法。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span>&gt;</span></span><br><span class="line">        div&#123;</span><br><span class="line">            border: 1px solid red;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br></pre></td></tr></table></figure><p>可以在同一页面复用代码，不能在多个页面复用CSS代码，且维护不方便，需要修改每个页面。</p><h3 id="CSS文件"><a href="#CSS文件" class="headerlink" title="CSS文件"></a>CSS文件</h3><p>把CSS样式写成CSS文件，在html文件的head标签中通过link标签引用。</p><p>style.css</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">div</span>&#123;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">1px</span> red solid;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-tag">span</span>&#123;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">1px</span> red solid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>div.html</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--link标签专门在head中用来引入CSS样式代码--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"style.css"</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--rel:文档间的关系--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--type:目标URL的类型--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--href:URL--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br></pre></td></tr></table></figure><p>可以在多个页面中复用CSS样式，且维护方便。</p><h2 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h2><h3 id="标签名选择器"><a href="#标签名选择器" class="headerlink" title="标签名选择器"></a>标签名选择器</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">标签名&#123;</span><br><span class="line">    属性:值;</span><br><span class="line">    属性:值;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>标签名选择器决定哪些标签被动的使用这个样式。</p><h3 id="id选择器"><a href="#id选择器" class="headerlink" title="id选择器"></a>id选择器</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#id</span>选择器&#123;</span><br><span class="line">    属性:值;</span><br><span class="line">    属性:值;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>id选择器通过id属性选择性的使用这个样式。</p><p>html文件</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"id001"</span>&gt;</span>div1<span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="comment">&lt;!--标签的id属性--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"id002"</span>&gt;</span>div2<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>CSS文件：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="selector-tag">style</span>&gt;</span><br><span class="line">        <span class="selector-id">#id001</span>&#123;</span><br><span class="line">            <span class="attribute">border</span>: yellow <span class="number">1px</span> solid;</span><br><span class="line">            <span class="attribute">font-size</span>: <span class="number">30px</span>;</span><br><span class="line">            <span class="attribute">color</span>: blue;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="selector-id">#id001</span>&#123;</span><br><span class="line">            <span class="attribute">border</span>: <span class="number">5px</span> blue dotted;</span><br><span class="line">            <span class="attribute">font-size</span>: <span class="number">20px</span>;</span><br><span class="line">            <span class="attribute">color</span>: red;</span><br><span class="line">        &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure><h3 id="class-选择器"><a href="#class-选择器" class="headerlink" title="class 选择器"></a>class 选择器</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.class</span>属性值&#123;</span><br><span class="line">    属性:值;</span><br><span class="line">    属性:值;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>class属性多用来分组定义CSS样式。</p><p>class选择器通过class属性值选择性使用这个样式。</p><p>html文件</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"class0"</span>&gt;</span>div1<span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="comment">&lt;!--标签的class属性--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"class0"</span>&gt;</span>div2<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>CSS文件：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="selector-tag">style</span>&gt;</span><br><span class="line">    <span class="selector-class">.div</span>&#123;</span><br><span class="line">        <span class="attribute">color</span>: blue;</span><br><span class="line">        <span class="attribute">font-size</span>: <span class="number">30px</span>;</span><br><span class="line">        <span class="attribute">border</span>: <span class="number">1px</span> yellow solid;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure><h3 id="组合选择器"><a href="#组合选择器" class="headerlink" title="组合选择器"></a>组合选择器</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.class0</span>, <span class="selector-id">#id001</span>&#123;</span><br><span class="line">    <span class="attribute">color</span>: blue;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">30px</span>;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">1px</span> yellow solid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>组合选择器可以让多个选择器共用同样的CSS样式。</p><h2 id="常用样式"><a href="#常用样式" class="headerlink" title="常用样式"></a>常用样式</h2><p>具体可查阅</p><ul><li><p>字体颜色</p><p><code>color : red;</code> </p><p><code>color : rgb(33,33,13);</code>  </p><p> <code>color : #00F666;</code>  </p></li><li><p>宽度</p><p><code>width : 19px;</code>  </p><p><code>width : 20%;</code> </p></li><li><p>高度</p><p><code>height : 19px;</code>  </p><p><code>height : 20%;</code> </p></li><li><p>背景颜色</p><p><code>background-color : #0F2222;</code> </p></li><li><p>字体大小</p><p><code>font-size : 20px;</code> </p></li><li><p>边框</p><p><code>border : 1px solid red;</code> </p></li><li><p>DIV居中（相当于页面的居中）</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">margin-left</span> : <span class="selector-tag">auto</span>;</span><br><span class="line"><span class="selector-tag">margin</span> <span class="selector-tag">-right</span> : <span class="selector-tag">auto</span>;</span><br></pre></td></tr></table></figure></li><li><p>文本居中</p><p><code>text-align : center;</code> </p></li><li><p>超链接去下划线</p><p><code>text-decoration : none;</code></p></li><li><p>表格细线</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">table</span>&#123;</span><br><span class="line">    <span class="attribute">border </span>: <span class="number">1px</span> solid black;</span><br><span class="line">    <span class="attribute">border-collapse </span>: collapse;<span class="comment">/*合并表格边框*/</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-tag">td</span>,<span class="selector-tag">th</span>&#123;</span><br><span class="line">    <span class="attribute">border </span>: <span class="number">1px</span>, solid black;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>列表去修饰符</p><p><code>list-style : none</code> </p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;温故知新：对Web基础知识——HTML和CSS的持续更新。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tools" scheme="https://f7ed.com/categories/Tools/"/>
    
    
      <category term="Web" scheme="https://f7ed.com/tags/Web/"/>
    
      <category term="HTML" scheme="https://f7ed.com/tags/HTML/"/>
    
      <category term="CSS" scheme="https://f7ed.com/tags/CSS/"/>
    
  </entry>
  
  <entry>
    <title>「Tools」：Docker</title>
    <link href="https://f7ed.com/2020/07/21/Docker/"/>
    <id>https://f7ed.com/2020/07/21/Docker/</id>
    <published>2020-07-20T16:00:00.000Z</published>
    <updated>2020-07-25T03:34:59.360Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章主要分四个部分，首先介绍了Docker是什么：为什么会有Docker技术的出现；虚拟化技术和容器虚拟化技术的区别；Docker的基本组成；Docker的运行为什么会比虚拟机快。</p><p>第二个部分主要介绍了Docker的常用命令，包括镜像命令和容器命令，文中还从底层的角度分析Docker镜像。</p><p>第三个部分介绍了Docker中的容器数据卷，和如何挂载数据卷。</p><p>最后一个部分，简单介绍了Dockerfile文件。</p><a id="more"></a><h1 id="Docker简介"><a href="#Docker简介" class="headerlink" title="Docker简介"></a>Docker简介</h1><h2 id="Docker-是什么"><a href="#Docker-是什么" class="headerlink" title="Docker 是什么"></a>Docker 是什么</h2><p>开发和运维之间的环境和配置问题：在我的机器上可以正常工作。</p><p>把代码/配置/系统/数据等全部打包成镜像，运维工程师带环境安装软件。</p><p>Docker基于Go语言实现的云开源项目，Docker的主要目标是“Build，Ship and Run Any App,Anywhere”，做到一次封装，处处运行。</p><p>Linux 容器技术的出现就解决了这样一个问题，而 Docker 就是在它的基础上发展过来的。将应用运行在 Docker 容器上面，而 Docker 容器在任何操作系统上都是一致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作。</p><p>Docker解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体分布的容器虚拟化技术。</p><h2 id="能干嘛？"><a href="#能干嘛？" class="headerlink" title="能干嘛？"></a>能干嘛？</h2><h3 id="之前的虚拟化技术"><a href="#之前的虚拟化技术" class="headerlink" title="之前的虚拟化技术"></a>之前的虚拟化技术</h3><p>虚拟机是带环境安装的解决方案，可以在一种操作系统中运行另一种操作系统。</p><p>虚拟机用软件实现了硬件、内核、操作系统及应用程序，对底层来说，虚拟机就是一个普通文件。</p><p>虚拟机的缺点缺点：</p><ol><li>资源占用多</li><li>冗余步骤</li><li>启动慢</li></ol><h3 id="容器虚拟化技术"><a href="#容器虚拟化技术" class="headerlink" title="容器虚拟化技术"></a>容器虚拟化技术</h3><p>Linux容器（Linux Containers,LXC)，对进程隔离，将软件运行所需的资源打包到一个隔离的痛其中。</p><img src="https://s1.ax1x.com/2020/07/25/UzVUbV.png" alt="UzVUbV.png" style="zoom:50%;" /><p>Linux容器不是模拟一个完整的操作系统，而是将软件工作所需的库资源和设置等资源打包到一个隔离的容器中，因此Linux容器变得高效且轻量，并且能保证部署在任何环境中的软件都能始终如一地运行。在</p><p>宿主机上，Linux容器就是一个运行的进程，所以Linux容器是对进程进行隔离。</p><p>再看Docker的图标，上面的集装箱就是一个一个容器，鲸鱼就是宿主机的硬件、内核。</p><p>比较：</p><ol><li>传统虚拟机技术虚拟一套硬件，在其上运行一个完整的操作系统，再运行所需的应用进程。</li><li>容器内的应用直接运行于宿主的内核，容器内没有硬件虚拟，容器更轻便。</li><li>容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响。</li></ol><p>所以，可以认为容器是一个轻量的Linux。</p><h3 id="开发-运维（DevOps"><a href="#开发-运维（DevOps" class="headerlink" title="开发/运维（DevOps)"></a>开发/运维（DevOps)</h3><p>DevOps, Develop and Operations, 可以利用Docker实现开发自运维。</p><ol><li>更快速的应用交付和部署。</li><li>更便捷的升级和扩缩容器。</li><li>更简单的系统运维。</li><li>更高效的计算资源利用。</li></ol><h2 id="Docker的基本组成"><a href="#Docker的基本组成" class="headerlink" title="Docker的基本组成"></a>Docker的基本组成</h2><p><strong>Docker的三要素：</strong></p><ol><li>镜像(image)：只读的模版，类比Java中的类。镜像可以用来创造Docker容器。</li><li>容器(container)：镜像的实例，独立运行的一个或一组实例。可以把容器看作一个简易版的Linux环境。</li><li>仓库(repository)：保存镜像的场所。</li></ol><p>Docker本身是一个容器运行载体或管理引擎。</p><p>把应用程序和配置打包成为一个可交付的运行环境，打包好的运行环境就是一个image镜像文件，只有通过这个镜像文件才能生成Docker容器。image文件可以看作是容器的模版。Docker根据image文件生成容器的实例。</p><h2 id="Docker运行原理"><a href="#Docker运行原理" class="headerlink" title="Docker运行原理"></a>Docker运行原理</h2><p>Docker是一个C/S结构的系统。</p><p>Docker守护进程运行在宿主机上，客户通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。</p><h3 id="为什么比虚拟机快"><a href="#为什么比虚拟机快" class="headerlink" title="为什么比虚拟机快"></a>为什么比虚拟机快</h3><ol><li>Docker有比虚拟机更少的抽象层，不需要实现硬件资源虚拟化，运行在docker容器中的程序直接使用的都是实际物理机的硬件资源。</li><li>Docker使用宿主机上的内核，新建容器时，不需要和虚拟机一样重新加载一个操作系统内核。因此新建一个dock er容器只需要几秒钟。</li></ol><h2 id="Docker镜像加速"><a href="#Docker镜像加速" class="headerlink" title="Docker镜像加速"></a>Docker镜像加速</h2><p>可以登陆阿里云获得专属镜像加速器链接，配置本机Docker拉取镜像仓库的链接，将拉取镜像的链接从DockerHub换成阿里云的仓库，下载更快捷。</p><p>具体按照系统自行Google。</p><h1 id="Docker常用命令"><a href="#Docker常用命令" class="headerlink" title="Docker常用命令"></a>Docker常用命令</h1><p>docker version</p><p>docker info</p><p>docker –help 帮助命令</p><h2 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h2><ul><li><p>列出本地images</p><p><code>docker images</code></p><p>repo</p><ul><li>参数<ul><li>-a :包括中间映像层</li><li>-q : 只显示镜像id</li><li>–digests :显示摘要信息</li><li>–no-trunc :显示完整信息</li></ul></li></ul></li><li><p>从Docker Hub查询镜像名</p><p><code>docker search [OPTIONS] image_name</code> </p><ul><li>–no-trunc </li><li>-s n：收藏数不小于n的镜像</li><li>–automated</li></ul></li><li><p>下载/拉取镜像</p><p><code>docker pull 镜像名[:TAG]</code> </p><p>默认:latest</p></li><li><p>删除镜像</p><p><code>docker rmi 镜像唯一名字/镜像ID</code></p><p>-f :强制删除运行中的镜像文件</p><ul><li><p>删除单个：</p><p> <code>docker rmi -f 镜像ID</code></p></li><li><p>删除多个</p><p><code>docker rmi -f 镜像名1:TAG 镜像名2:TAG</code></p></li><li><p>删除全部：</p><p><code>docker rmi -f $(docker images -qa)</code> </p></li></ul></li></ul><h2 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h2><p>容器是一个建议的Linux。</p><ul><li><p>启动容器：</p><p><code>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</code></p><ul><li><code>--name 容器名</code> :为容器指定一个名字</li><li><code>-d</code> ：后台运行容器，返回</li><li><code>-i</code> : 以<strong>交互模式</strong>运行容器，通常与<code>-t</code> 一同使用</li><li><code>-t</code> :为容器重新分配一个伪输入终端，通常与<code>-i</code> 一同使用。</li><li><code>-p</code>  :主机端口和容器端口<ul><li><code>-p ip:hostPort:containerPort</code> </li><li><code>-p ip::containerPort</code> </li><li><code>-p hostPort:containerPort</code> </li><li><code>-p containerPort</code> </li></ul></li><li><code>-P</code> :随机分配端口</li></ul></li><li><p>列出当前运行所有容器：</p><p><code>docker ps</code> </p><ul><li><code>-a</code> : 列出当前所有正在运行的容器和历史上运行过的容器</li><li><code>-l</code> :显示最近创建的容器</li><li><code>-n</code> :显示最近创建的num个容器<ul><li><code>docker ps -n 3</code> </li></ul></li><li><code>-q</code> :静默模式，只显示容器编号</li><li><code>--no-trunc</code> : 不截断输出</li></ul></li><li><p>退出/停止容器</p><ul><li><p>容器停止退出</p><p><code>exit</code> </p></li><li><p>容器不停止退出</p><p>Ctrl + P + Q</p></li></ul></li><li><p>启动容器</p><p><code>docker start 容器名/容器ID</code> </p></li><li><p>重启容器</p><p><code>docker restart 容器名/容器ID</code> </p><p>重启成功后返回容器名/容器ID</p></li><li><p>停止容器</p><p><code>docker stop 容器名/容器ID</code></p></li><li><p>强制停止容器</p><p><code>docker kill 容器名/容器ID</code> </p></li><li><p>删除已停止的容器</p><p><code>docker rm 镜像ID</code> </p><ul><li><p>一次删除多个容器</p><p><code>docker rm -f $(docker ps -a -q)</code> </p><p><code>docker ps -a -q | xargs docker rm</code>  （管道传递参数）</p></li></ul></li></ul><hr><ul><li><p>启动守护式容器</p><p><code>docker run -d 镜像名/镜像ID</code> </p><p><code>docker run -d -p 主机端口:容器内端口 容器ID</code> </p><ul><li><p>如果使用 <code>docker ps -a</code> 查看，会发现容器已经退出</p></li><li><p><strong>Docker容器后台运行，就必须要有一个前台进程与之交互</strong> </p><p>如果容器后台运行，如果不是一直挂起的命令，他就会自动退出。</p></li><li><p>所以最佳的解决方式是将运行的进程以前台进程运行。</p></li></ul></li><li><p>查看容器日志</p><p><code>docker logs -f -t --tail 容器ID</code> </p><ul><li><code>-t</code>：显示加入时间戳</li><li><code>-f</code> ：持续显示最新的日志</li><li><code>--tail</code> ：显示最后多少条</li></ul></li><li><p>显示容器内运行的进程</p><p><code>docker top 容器ID</code> </p></li><li><p>查看容器内部的细节</p><p><code>docker inspect 容器ID</code> </p></li><li><p>进入正在运行的容器并以命令行与之交互</p><ul><li><p>直接进入容器启动命令的终端</p><p><code>docker attach 容器ID</code> </p></li><li><p>在容器中打开新的终端，并且可以启动新的进程。</p><p><code>docker exec -it 容器ID bashShell</code></p><p><code>docker exec -it 容器ID /bin/bash</code>  和<code>docker attach 容器ID</code> 相同。</p></li></ul></li><li><p>把容器内文件拷贝文件到主机上</p><p><code>docker cp 容器ID:容器内的路径 目录主机路径</code> </p><p><code>docker cp 130b1f6708dd:/x.txt /Users</code> </p></li></ul><h1 id="Docker镜像"><a href="#Docker镜像" class="headerlink" title="Docker镜像"></a>Docker镜像</h1><p>image：</p><p>镜像是轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，包含运行某个软件所需的所有内容，包括代码、库、环境变量、配置文件等。</p><h2 id="UnionFS"><a href="#UnionFS" class="headerlink" title="UnionFS"></a>UnionFS</h2><p>UnionFS（联合文件系统）是一种分层、轻量高性能的文件系统，支持对文件系统的修改作为一次提交来一层层的叠加，同时将不同目录挂载到同一个虚拟文件系统下。</p><p>Union文件系统时Docker镜像的基础。</p><p>镜像通过分层来进行继承，基于基础镜像可以制作各种具体的应用镜像。</p><p>特点：一次加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，最终的文件系统包含所有底层的文件和目录。</p><h2 id="Docker镜像的加载"><a href="#Docker镜像的加载" class="headerlink" title="Docker镜像的加载"></a>Docker镜像的加载</h2><p>Docker镜像实际是由一层一层的文件系统组成。</p><p>bootfs(boot file system)包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统。</p><p>Docker镜像的最底层就是bootfs，这一层和典型的Linux/Unix系统是一样的，包含bootloader和kernel。</p><p>当boot加载完成后，整个kernel就在内存中了，此时内存的使用权已由bootfs转交给kernel，此时系统也会卸载bootfs。</p><p>rootfs（root file system)，在bootfs之上，包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Linux，Centos等。</p><p>平常安装等虚拟机的CentOS都是几个G，为什么docker版的centos只有几百兆？</p><p>对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库，因为底层直接使用宿主机的kernel，自己只需要提供rootfs就行了。</p><p>因此，对于不同的Linux发行版，bootfs基本一致，rootfs会有差别，因此不同的发行版可以共用bootfs。</p><h2 id="分层的镜像"><a href="#分层的镜像" class="headerlink" title="分层的镜像"></a>分层的镜像</h2><p>在docker image下载、删除时，可以发现是一层一层的。</p><p>分层的镜像的一个最大的好处是共享资源。</p><p>如果有多个镜像都是从相同的base镜像build而来，那宿主机中只需在磁盘上保存一份base镜像，同时内存中也只需要加载一份base镜像，就可以为所有的容器服务了。</p><h2 id="镜像commit操作"><a href="#镜像commit操作" class="headerlink" title="镜像commit操作"></a>镜像commit操作</h2><p>Docker镜像都是只读的，但当镜像实例化，启动容器时，一个新的可写层被加载到镜像的顶部，这一层通常被称作“容器层”，“容器层”之下的都叫“镜像层”。</p><p>docker commit提交容器层副本使之成为一个新的镜像。</p><p><code>docker commit -m &quot;message&quot;  -a &quot;author&quot; 容器ID 命名空间/新建镜像名[:TAGS]</code> </p><h1 id="容器数据卷"><a href="#容器数据卷" class="headerlink" title="容器数据卷"></a>容器数据卷</h1><p>Docker理念：</p><p>将代码和运行的环境打包形成容器，运行伴随着容器，但希望运行中的数据是持久化的，希望容器之间是共享数据的。</p><p>如果不通过docker commit生成新的镜像，使得数据作为镜像的一部分保存下来，那么容器删除后，数据也没有了，为了保存数据，使用容器数据卷。</p><p>如果不使用commit 生成新的镜像，Docker容器产生的数据将随着容器的删除而一起删除，为了保存数据，我们使用卷。</p><h2 id="卷"><a href="#卷" class="headerlink" title="卷"></a>卷</h2><p>卷就是目录或者文件，存在于一个或多个容器中，由docker挂载到容器，但不属于UnionFS（联合文件系统），因此能绕过UnionFS，提供一些用于持续存储或共享数据的特性。</p><p>卷的设计目的就是为了数据持久化，完全独立于容器的生存周期，因此Docker不会在容器删除的时候删除其挂载的数据卷。</p><p>数据卷的特点：</p><ol><li>数据卷可以在容器之间共享或重用数据。</li><li>卷中的更改直接在所有共享该卷容器中生效。</li><li>数据卷中的更改不会包含在镜像的更新中。</li><li>数据卷的生命周期一直持续到没有容器使用它为止。</li></ol><h2 id="数据卷挂载"><a href="#数据卷挂载" class="headerlink" title="数据卷挂载"></a>数据卷挂载</h2><h3 id="直接命令添加"><a href="#直接命令添加" class="headerlink" title="直接命令添加"></a>直接命令添加</h3><ol><li><p>数据挂载(<code>-v</code> value)</p><p><code>docker run -it -v /宿主机目录:/容器内目录 镜像名</code> </p></li><li><p>查看挂载是否成功</p><p><code>docker inspect 镜像名</code> </p></li><li><p>宿主机和容器之间实现数据共享，在容器停止退出后，修改宿主机数据，数据完全同步。</p></li></ol><ul><li><p>带权限的数据挂载，加<code>:ro</code> (readonly) </p><p><code>docker run -it -v /宿主机绝对路径目录:/容器内目录:ro 镜像名</code></p><p>此时容器中对数据卷只读。</p></li></ul><hr><p>当挂载主机目录事，Docker访问出现<code>cannot open directory .: Permission denied</code> </p><p>解决办法：在挂砸目录后加参数 <code>--privileged=true</code> </p><h3 id="DockerFile添加"><a href="#DockerFile添加" class="headerlink" title="DockerFile添加"></a>DockerFile添加</h3><p>在DockerFile中可以使用<code>VOLUME</code> 指令给镜像添加一个或多个数据卷。</p><p>注意：</p><p>Docker出于可移植性和分享的考虑，指令中只有容器内的地址，因为宿主主机目录依赖于特定的主机。</p><ol><li><p>Dockerfile文件构建</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"><span class="keyword">VOLUME</span><span class="bash"> [<span class="string">"/dataVolumeContainer1"</span>, <span class="string">"/dataVolumeContainer2"</span>, <span class="string">"/dataVolumeContainer3"</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"finished,-----success"</span></span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> /bin/bash</span></span><br></pre></td></tr></table></figure><p>以上docker文件类似于一下命令挂载</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -v /host1:/dataVolumeContainer1 -v/host1:/dataVolumeContainer2 -v /host3:/dataVolumeContainer3 centos /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>build构建镜像（<code>-f</code> file)</p><p><code>docker build -f DockerFile文件路径 -t 命名空间/镜像名 镜像生成路径</code></p><p><code>docker build -f ./Dockerfile  -t fred/centos .</code></p></li></ol><h2 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h2><p>数据容器卷：</p><p>命名的容器挂载数据卷，其他容器通过挂载这个父容器实现数据共享，挂载数据卷的容器称为数据卷容器。</p><p><strong>容器之间可以传递配置信息，数据卷的生命周期一直持续到没有容器使用它为止。</strong></p><ol><li><p>挂载数据卷到父容器（命名为<code>dc01</code> ）上：命令添加/Dockerfile添加</p></li><li><p>容器继承父容器的数据卷(<code>--volumes-from</code> )</p><p><code>docker run -it --name 子容器名 --volumes-from 父容器名 生成子容器的镜像名</code> </p><p>e.g: <code>docker run -it --name dc02 --volumes-from dc01 fred/centos</code> </p></li></ol><p>dc01已经挂载数据卷，此时dc02继承它，那么dc01挂载的数据卷，dc02也实现了共享。</p><h1 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h1><p>Dockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。</p><p>构建容器卷的步骤：</p><ol><li>编写Dockerfile文件</li><li>docker build构建</li><li>docker run启动容器</li></ol><p>Centos的Dockerfile文件</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> scratch</span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> centos-7.8.2003-x86_64-docker.tar.xz /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> \</span></span><br><span class="line"><span class="bash">    org.label-schema.schema-version=<span class="string">"1.0"</span> \</span></span><br><span class="line"><span class="bash">    org.label-schema.name=<span class="string">"CentOS Base Image"</span> \</span></span><br><span class="line"><span class="bash">    org.label-schema.vendor=<span class="string">"CentOS"</span> \</span></span><br><span class="line"><span class="bash">    org.label-schema.license=<span class="string">"GPLv2"</span> \</span></span><br><span class="line"><span class="bash">    org.label-schema.build-date=<span class="string">"20200504"</span> \</span></span><br><span class="line"><span class="bash">    org.opencontainers.image.title=<span class="string">"CentOS Base Image"</span> \</span></span><br><span class="line"><span class="bash">    org.opencontainers.image.vendor=<span class="string">"CentOS"</span> \</span></span><br><span class="line"><span class="bash">    org.opencontainers.image.licenses=<span class="string">"GPL-2.0-only"</span> \</span></span><br><span class="line"><span class="bash">    org.opencontainers.image.created=<span class="string">"2020-05-04 00:00:00+01:00"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure><h2 id="Dockerfile构建过程"><a href="#Dockerfile构建过程" class="headerlink" title="Dockerfile构建过程"></a>Dockerfile构建过程</h2><p><strong>基础规则：</strong></p><ol><li>保留字指令必须大写，且后面必须至少一个参数。</li><li>指令顺序执行。</li><li>注释符号：<code>#</code> </li><li>每条指令都会创建一个新的镜像层，并对该镜像进行提交。</li></ol><p><strong>执行流程：</strong></p><ol><li>从基础镜像运行一个容器</li><li>执行一条指令后并对容器进行修改</li><li>执行类似docker commit操作提交一个新的镜像层</li><li>docker再基于刚提交的镜像运行一个容器</li><li>直到文件所有指令执行完成</li></ol><hr><p>辨析Dockerfile，Docker镜像，Docker容器：</p><p>Dockerfile、Docker镜像与Docker容器从软件应用的角度分别代表软件的三个不同阶段：</p><ol><li><p>Dockerfile是软件的原材料，是面向开发的。</p><p>Dockerfile定义了进程需要的一切东西。Dockerfile设计的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程等等。</p></li><li><p>Docker镜像是软件的交付品，是交付标准。</p><p>在用Dockerfile定义一个文件之后，docker build会产生一个Docker镜像，运行 Docker镜像时，才真正开始提供服务。</p></li><li><p>Docker容器则可以认为是软件的运行态，涉及部署和运维。</p><p>Docker容器是直接提供服务的。</p></li></ol><h2 id="Dockfile体系结构"><a href="#Dockfile体系结构" class="headerlink" title="Dockfile体系结构"></a>Dockfile体系结构</h2><img src="https://s1.ax1x.com/2020/07/25/UzVdET.png" alt="UzVdET.png" style="zoom:50%;" /> <ul><li><p>FROM</p><p>基础镜像</p></li><li><p>MAINTAINER</p><p>镜像维护者的姓名和邮箱地址</p></li><li><p>RUN</p><p>容器构建时需要运行的命令</p></li><li><p>EXPOSE</p><p>当前容器对外暴露的端口号</p></li><li><p>WORKDIR</p><p>指定在创建容器后，终端默认登陆进来的工作目录</p></li><li><p>ENV</p><p>构建容器中的设置环境变量</p></li><li><p>ADD</p><p>将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包</p></li><li><p>COPY</p><p>拷贝文件和目录到镜像中</p><p><code>COPY src dest</code></p><p><code>COPY [&quot;src&quot;, &quot;dest&quot;]</code> </p></li><li><p>VOLUME</p><p>容器数据卷 用于数据保存和持久化工作</p></li><li><p>CMD</p><p>指定一个容器启动时运行的命令</p><ul><li><p>shell 格式：CMD &lt;命令&gt;</p></li><li><p>exec格式：CMD[“可执行文件”, “arg1”, “arg2”,…]</p></li><li><p>参数列表格式：CMD [“arg1”, “arg2”,…] 在指定来ENTRYPOINT指令后，用⌘指定具体的参数。</p></li></ul><p>只有最后一个CMD生效，CMD会被docker run之后的参数替换</p></li><li><p>ENTRYPOINT</p><p>指定一个容器启动时运行的命令</p><p>会在docker run后面追加参数</p></li><li><p>ONBUILD</p><p>当构建一个被继承的Dockerfile时，父镜像在被子镜像继承后父镜像的ONBUILD被触发</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章主要分四个部分，首先介绍了Docker是什么：为什么会有Docker技术的出现；虚拟化技术和容器虚拟化技术的区别；Docker的基本组成；Docker的运行为什么会比虚拟机快。&lt;/p&gt;
&lt;p&gt;第二个部分主要介绍了Docker的常用命令，包括镜像命令和容器命令，文中还从底层的角度分析Docker镜像。&lt;/p&gt;
&lt;p&gt;第三个部分介绍了Docker中的容器数据卷，和如何挂载数据卷。&lt;/p&gt;
&lt;p&gt;最后一个部分，简单介绍了Dockerfile文件。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tools" scheme="https://f7ed.com/categories/Tools/"/>
    
    
      <category term="Tools" scheme="https://f7ed.com/tags/Tools/"/>
    
      <category term="Docker" scheme="https://f7ed.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>「Tools」:Git and GitHub</title>
    <link href="https://f7ed.com/2020/07/18/Git-and-GitHub/"/>
    <id>https://f7ed.com/2020/07/18/Git-and-GitHub/</id>
    <published>2020-07-17T16:00:00.000Z</published>
    <updated>2020-07-25T03:40:33.971Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章具体讲述了Git工具的基本本地库操作和与远程库交互的基本操作，包括使用GitHub进行团队外的协作开发。</p><a id="more"></a><h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><h2 id="Git简介"><a href="#Git简介" class="headerlink" title="Git简介"></a>Git简介</h2><h3 id="Git历史："><a href="#Git历史：" class="headerlink" title="Git历史："></a>Git历史：</h3><p>1991 Linus本人手动合并代码</p><p>2002 商业软件，授予Linux社区免费使用版本控制</p><p>2005 Linus自己用C语言开发了一个分布式版本控制系统：Git</p><p>Talk is cheap, show me the code.</p><p>2008 Github上线</p><hr><p>Git的优势：</p><ul><li><p>大部分操作在本地完成，不需要联网</p></li><li><p>完整性保证：每次提交进行哈希</p></li><li><p>尽可能添加数据而不是删除/修改数据，版本都在</p></li><li><p>分支操作快捷流畅，以快照的形式</p></li><li><p>与Linux命令全面兼容</p></li></ul><h3 id="Git的结构"><a href="#Git的结构" class="headerlink" title="Git的结构"></a>Git的结构</h3><img src="https://s1.ax1x.com/2020/07/18/UcvvkQ.png" alt="Git的结构" style="zoom:50%;" /><h3 id="Git和代码托管中心"><a href="#Git和代码托管中心" class="headerlink" title="Git和代码托管中心"></a>Git和代码托管中心</h3><p>代码托管中心的任务：维护远程库</p><p>局域网环境：搭建GitLab作为代码托管中心</p><p>外网环境：可以用GitHub和码云作为代码托管中心</p><h3 id="本地库和远程库的交互"><a href="#本地库和远程库的交互" class="headerlink" title="本地库和远程库的交互"></a>本地库和远程库的交互</h3><p>团队内：</p><img src="https://s1.ax1x.com/2020/07/18/UcvxYj.png" alt="团队内交互" style="zoom:50%;" /><p>团队外：</p><img src="https://s1.ax1x.com/2020/07/18/Ucvzfs.png" alt="团队外交互" style="zoom:50%;" /><p>fork：复制一份属于自己的远程库</p><p>开发新的内容后向库的拥有者 pull request拉取请求，原拥有者可以审核，审核通过后执行merge操作合并到自己的远程库的分支上。</p><h2 id="Git命令行基本操作"><a href="#Git命令行基本操作" class="headerlink" title="Git命令行基本操作"></a>Git命令行基本操作</h2><h3 id="本地库初始化"><a href="#本地库初始化" class="headerlink" title="本地库初始化"></a>本地库初始化</h3><ul><li><p>初始化本地库</p><p><code>git init</code> </p></li></ul><p>.git文件存放的是本地库相关的子目录和文件，不要删除和随意修改。</p><h3 id="本地库设置签名"><a href="#本地库设置签名" class="headerlink" title="本地库设置签名"></a>本地库设置签名</h3><ul><li><p>形式：</p><p>用户名：</p><p>Email：</p></li><li><p>作用：区分不同开发人员的身份</p></li></ul><p>注：这里设置的签名与远程代码托管中心没有关系。</p><ul><li><p>命令：</p><ul><li><p>项目级别：设置签名仅在本地库起效（如果既有项目级别和用户级别的签名，按照项目级别为准）</p><p>设置用户名命令：<code>git config user.name ***</code></p><p>设置用户邮箱： <code>git config user.email *****@outlook.com</code></p><p>该信息保存在.git/config文件中。</p></li><li><p>用户级别：设置签名在当前操作系统的用户范围</p><p>设置用户名命令：<code>git config --global user.name ***</code></p><p>设置用户邮箱命令：<code>git config --global user.email ****</code></p><p>该消息保存在系统文件~/.gitconfig文件</p></li></ul></li></ul><h3 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h3><ul><li><p>查看工作区、暂存区状态。</p><p> <code>git status</code> </p></li></ul><h3 id="暂存区操作：添加-修改-提交-删除"><a href="#暂存区操作：添加-修改-提交-删除" class="headerlink" title="暂存区操作：添加/修改/提交/删除"></a>暂存区操作：添加/修改/提交/删除</h3><ul><li><p>添加/修改：将工作区的文件添加到暂存区（/或更新暂存区的文件）。</p><p> <code>git add [filename]</code> </p></li><li><p>删除：将文件从暂存区删除</p><p><code>git rm --cached [filename]</code>  </p></li><li><p>提交：将暂存区的文件提交到本地库。（输入提交信息）</p><p><code>git commit -m &quot;commit message&quot; [filename]</code></p></li><li><p>修改后的提交：提交修改后的文件至本地库（已在暂存区有旧版本），同时更新暂存区和本地库。</p><p><code>git commit -m &quot;message&quot; [filename]</code> </p></li></ul><h3 id="本地库版本信息查看"><a href="#本地库版本信息查看" class="headerlink" title="本地库版本信息查看"></a>本地库版本信息查看</h3><p>HEAD: 指针，表示当前版本的位置。</p><p>显示版本：</p><ul><li><p>完整的版本信息记录（包括完整版本哈希值、作者、提交时间）</p><p><code>git log</code> （空格向下翻页；b 向上翻页； q退出显示）</p></li><li><p>一行只显示一个版本，简洁版。</p><p><code>git log --pretty=oneline</code> </p></li><li><p>一行也只显示一个版本，终极简洁版，哈希值也只显示前面的一部分（当作该版本的局部索引）。</p><p><code>git log --oneline</code> </p></li><li><p>HEAD@{i}：i表示HEAD指针移动到该版本需要后退的步数。</p><p><code>git reflog</code> </p></li></ul><h3 id="版本前进-后退"><a href="#版本前进-后退" class="headerlink" title="版本前进/后退"></a>版本前进/后退</h3><p>本质是HEAD指针的移动。</p><ul><li><p>基于索引操作：版本可以后退和前进。(索引就是reflog形式下的局部哈希值)</p><p><code>git reset --hard [局部索引值]</code>  </p></li><li><p>使用<code>^</code> ： 版本只能往后退 。（基于reflog形式下的步数）</p><p><code>git reset --hard HEAD^^</code> (后退两步)</p></li><li><p>使用<code>~n</code> ：版本往后退n步。</p><p><code>git reset --hard HEAD~3</code> </p></li></ul><p><strong>版本前进/后退reset命令的参数对比：</strong></p><ul><li><p><code>--soft</code></p><ul><li><p>仅仅在本地库移动HEAD指针。</p><p>如下图，显得暂存区和工作区版本比本地库前进了一步。</p><img src="https://s1.ax1x.com/2020/07/18/UcvXTg.png" alt="--soft命令" style="zoom:50%;" /></li></ul></li><li><p><code>--mixed</code></p><ul><li><p>在本地库移动HEAD指针</p></li><li><p>并重置暂存区，暂存区和本地库一致。</p><p>如下图，显得工作区版本比本地库和暂存区版本前进了一步。</p><img src="https://s1.ax1x.com/2020/07/18/UcvO0S.png" alt="--mixed命令" style="zoom:50%;" /></li></ul></li><li><p><code>--hard</code></p><ul><li>在本地库移动HEAD指针</li><li>重置暂存区</li><li>重置工作区</li></ul></li></ul><h3 id="删除文件后找回"><a href="#删除文件后找回" class="headerlink" title="删除文件后找回"></a>删除文件后找回</h3><p>前提：删除前，文件存在的状态提交到了本地库。</p><p>操作：</p><ul><li><p>删除的操作已经提交到本地库</p><ul><li><p>删除操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rm a.txt</span><br><span class="line">git add a.txt</span><br><span class="line">git commit -m "delete a.txt" a.txt</span><br></pre></td></tr></table></figure></li><li><p><code>git reset --hard [历史版本指针位置]</code> </p></li></ul></li><li><p>删除操作未提交到本地库</p><ul><li><p>删除操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//工作区删除</span><br><span class="line">rm a.txt</span><br><span class="line">//缓存区也删除</span><br><span class="line">rm a.txt</span><br><span class="line">git add a.txt</span><br></pre></td></tr></table></figure></li><li><p><code>git reset --hard HEAD</code> </p></li></ul></li></ul><h3 id="比较文件差异"><a href="#比较文件差异" class="headerlink" title="比较文件差异"></a>比较文件差异</h3><ul><li><p>工作区文件和暂存区文件比较</p><p><code>git diff [filename]</code> </p></li><li><p>工作区文件和本地库文件比较，指针可以使用<code>HEAD^</code> </p><p><code>git diff [指针] [filename]</code> </p></li><li><p>可以不加文件名，即比较全部文件。</p></li></ul><h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><h3 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h3><p>分支：版本控制过程中，使用多条线同时推进多个任务。 </p><img src="https://s1.ax1x.com/2020/07/18/Ucz94H.png" alt="Ucz94H.png" style="zoom:50%;" /><p>master: 主版本分支/部署到服务器运行的分支。</p><p>feature_ ：开发其他功能的分支。</p><p>hot_fix: 热修复，bug修复分支。</p><p>分支的好处：</p><ul><li>并行：同时并行推进多个功能开发。</li><li>独立：各个分支在开发过程中，如果有个分支开发失败，不会影响其他分支。</li></ul><h3 id="分支操作：创建-查看-切换-合并"><a href="#分支操作：创建-查看-切换-合并" class="headerlink" title="分支操作：创建/查看/切换/合并"></a>分支操作：创建/查看/切换/合并</h3><ul><li><p>创建分支</p><p><code>git branch [branch name]</code> </p></li><li><p>查看分支</p><p><code>git branch -v</code> </p></li><li><p>切换分支</p><p><code>git checkout [branch name]</code> </p></li><li><p>合并分支</p><ol><li><p>切换到接受修改的分支（如master）</p><p><code>git checkout [合并到的主分支]</code> </p></li><li><p>执行merge合并操作</p><p><code>git merge [有修改的分支]</code></p></li></ol></li></ul><h3 id="解决合并分支后产生的冲突"><a href="#解决合并分支后产生的冲突" class="headerlink" title="解决合并分支后产生的冲突"></a>解决合并分支后产生的冲突</h3><p>冲突的表现，显示到有冲突的文件：</p><img src="https://s1.ax1x.com/2020/07/18/UczPCd.png" alt="冲突文件内容" style="zoom:50%;" /><p>冲突解决：</p><ol><li><p>删除文件中的特殊符号</p></li><li><p>协商再编辑文件</p></li><li><p>添加新文件</p><p><code>git add [filename]</code></p></li><li><p>提交（注意：此时的提交不能带文件名）</p><p><code>git commit -m &quot;message&quot;</code> </p></li></ol><h2 id="Git-基本原理"><a href="#Git-基本原理" class="headerlink" title="Git 基本原理"></a>Git 基本原理</h2><h3 id="哈希算法"><a href="#哈希算法" class="headerlink" title="哈希算法"></a>哈希算法</h3><p>特点：</p><ul><li>得到的加密密文长度相同。</li><li>算法确定，输入确定后，输出一定确定。</li><li>输入数据发生一点点变化，输出的变化会很大。</li><li>Git底层采用SHA-1算法。</li></ul><p>哈希算法保证了Git的数据完整性。</p><h3 id="Git保存版本的机制"><a href="#Git保存版本的机制" class="headerlink" title="Git保存版本的机制"></a>Git保存版本的机制</h3><p>集中式版本控制工具（如SVN）：保存的信息是每个基本文件和每个文件随时间逐步累积的差异。</p><p>Git是分布式的版本控制工具。</p><p>Git把数据看作是文件系统的快照（可以理解为当前内存版本的文件的索引），每次提交更新时Git对当前内存的全部文件制作一个快照并保存这个快照的索引。如果文件没有修改，Git不会重新存储该文件，只是保留一个连接指向之前存储的文件。</p><p>Git的提交对象：</p><img src="https://s1.ax1x.com/2020/07/18/UcvLm8.png" alt="UcvLm8.png" style="zoom:50%;" /><p>上图中，每个文件都有一个哈希值/索引，提交时新建一个树结点，其中包含指向每个文件的指针/索引，提交的对象包括该树结点的指针/哈希值。</p><p>Git版本对象链条：</p><img src="https://s1.ax1x.com/2020/07/18/UcvbOf.png" alt="Git版本对象链条" style="zoom:50%;" /><p>所以：</p><p>Git 分支的创建：等于新建一个指向版本的指针。</p><img src="https://s1.ax1x.com/2020/07/18/UcvH6P.png" alt="分支创建" style="zoom:50%;" /><p>Git分支的切换：改变HEAD指针所指的指针。</p><img src="https://s1.ax1x.com/2020/07/18/Ucv7lt.png" alt="分支切换" style="zoom:50%;" /><p>Git分支版本的移动：分支指针的移动。</p><img src="https://s1.ax1x.com/2020/07/18/UcvTSI.png" alt="分支版本移动" style="zoom:50%;" /><h1 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h1><h2 id="基本交互"><a href="#基本交互" class="headerlink" title="基本交互"></a>基本交互</h2><h3 id="创建-查看远程库地址别名"><a href="#创建-查看远程库地址别名" class="headerlink" title="创建/查看远程库地址别名"></a>创建/查看远程库地址别名</h3><p>在GitHub创建远程库后</p><ul><li><p>在本地添加远程库地址别名</p><p><code>git remote add [别名] [https/ssh 地址]</code></p><p><code>git remote add orgin https://...</code>  </p></li><li><p>查看当前所有远程库地址别名</p><p><code>git remote -v</code> </p></li></ul><h3 id="本地库内容推送到远程库"><a href="#本地库内容推送到远程库" class="headerlink" title="本地库内容推送到远程库"></a>本地库内容推送到远程库</h3><p>前提：本地库已添加远程库地址别名。</p><ul><li><p>在本地将本地库推送到远程分支</p><p><code>git push [别名] [分支名]</code> </p><p><code>git push origin master</code> </p></li></ul><h3 id="将远程库克隆到本地库"><a href="#将远程库克隆到本地库" class="headerlink" title="将远程库克隆到本地库"></a>将远程库克隆到本地库</h3><ul><li><code>git clone https/ssh_address</code> </li><li>效果：完整把远程库下载到本地；添加origin作为远程库地址别名；初始化本地库（含有.git文件）</li></ul><h2 id="团队内协作"><a href="#团队内协作" class="headerlink" title="团队内协作"></a>团队内协作</h2><h3 id="团队成员邀请"><a href="#团队成员邀请" class="headerlink" title="团队成员邀请"></a>团队成员邀请</h3><p>项目创建者在项目”Setting”-“Callaborators”里邀请成员。</p><h3 id="拉取：同步本地库"><a href="#拉取：同步本地库" class="headerlink" title="拉取：同步本地库"></a>拉取：同步本地库</h3><ul><li><p>在本地pull操作同步本地库与远程库相同。</p></li><li><p>fetch：查看远程库分支，可以切换至远程库分支，查看远程库分支的文件具体内容，决定是否合并。</p><p><code>git fetch [远程库地址别名] [远程分支名]</code> </p><ul><li><p>切换至远程库分支</p><p><code>git checkout orgin/master</code></p></li></ul></li><li><p>merge：（切换至本地库master分支），合并远程库分支。</p><p><code>git merge [远程库地址别名]/[远程分支名]</code> </p></li><li><p>pull = fetch + merge</p><p><code>git pull [远程库地址别名] [远程分支名]</code> </p></li></ul><p>注：如果是简单的修改，可以直接pull拉取，如果不确定远程库修改内容，可以先fetch后再合并分支。</p><h3 id="本地拉取与远程库冲突"><a href="#本地拉取与远程库冲突" class="headerlink" title="本地拉取与远程库冲突"></a>本地拉取与远程库冲突</h3><ul><li>冲突发生原因：不是基于GitHub远程库的最新版进行修改，就不能push，在修改之前必须pull。</li><li>pull拉取下来后如果进入冲突状态，就按照“分支冲突解决办法”</li></ul><h2 id="跨团队协作"><a href="#跨团队协作" class="headerlink" title="跨团队协作"></a>跨团队协作</h2><ol><li><p>fork操作：复制一份远程库。</p><p>团队外的人，在项目节目点fork，即可fork一份远程库，该远程库的来源是创建该库的开发者，而fork出的远程库的所有者是执行fork操作的人。</p></li><li><p>clone操作：下载到本地库。</p></li><li><p>push操作：本地修改，推送至远程库。</p></li><li><p>pull request 请求：在远程库（代码托管中心GitHub）执行pull request请求，请求合并该修改到原远程库。</p></li><li><p>（原远程库所有者）审核操作：确认是否合并。</p></li></ol><h2 id="SSH登陆"><a href="#SSH登陆" class="headerlink" title="SSH登陆"></a>SSH登陆</h2><ol><li><p>在当前用户的根目录，生产.ssh密钥目录</p><p><code>ssh-keygen -t rsa -C email@address</code> </p></li><li><p>将<code>.ssh/id_rsa.pub</code> 文件的内容复制到GitHub新建ssh密钥的窗口下。</p></li><li><p>创建ssh远程地址别名</p><p><code>git remote add origin ssh_address</code> </p></li></ol><ul><li><p>Git仓库和SSH-key关联</p><p><code>ssh-add &quot;id_rsa address</code> </p></li></ul><h1 id="Git工作流"><a href="#Git工作流" class="headerlink" title="Git工作流"></a>Git工作流</h1><p>待补充[1]</p><h1 id="Gitlab服务器搭建"><a href="#Gitlab服务器搭建" class="headerlink" title="Gitlab服务器搭建"></a>Gitlab服务器搭建</h1><p>待补充[2]</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Git工作流待补充</li><li>Gitlab服务器搭建</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章具体讲述了Git工具的基本本地库操作和与远程库交互的基本操作，包括使用GitHub进行团队外的协作开发。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tools" scheme="https://f7ed.com/categories/Tools/"/>
    
    
      <category term="Tools" scheme="https://f7ed.com/tags/Tools/"/>
    
      <category term="Git" scheme="https://f7ed.com/tags/Git/"/>
    
      <category term="GitHub" scheme="https://f7ed.com/tags/GitHub/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」:Semi-supervised Learning</title>
    <link href="https://f7ed.com/2020/07/03/semi-supervised/"/>
    <id>https://f7ed.com/2020/07/03/semi-supervised/</id>
    <published>2020-07-02T16:00:00.000Z</published>
    <updated>2020-07-03T11:32:00.705Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？</p><p>再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。</p><p>对于Generative Model，文章重点讲述了如何用EM算法来训练模型。</p><p>对于Low-density，文章重点讲述了如何让模型进行Self-training，并且在训练中引入Entropy-based Regularization term来尽可能low-density的假设。</p><p>对于Smoothness Assumption，文章重点讲述了Graph-based Approach（基于图的方法），并且在训练中引入Smoothness Regularization term来尽可能满足Smoothness Assumption的假设。</p><p>对于Better Representation，本篇文章只是简单阐述了其思想，具体介绍见这篇博客。</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>什么是Semi-supervised learning(半监督学习)？和Supervised learning（监督式学习）的区别在哪？</p><p><strong>Supervised learning（监督式学习）</strong>：</p><p>用来训练的数据集 $R$  中的数据labeled data，即 ${(x^r,\hat{y}^r)}_{r=1}^R$ .</p><p>比如在图像分类数据集中： $x^r$ 是image，对应的target output $y^r$ 是分类的label。</p><p>而<strong>Semi-supervised learning（半监督式学习）</strong>：</p><p>用来的训练的数据集由两部分组成 $\{(x^r,\hat{y}^r)\}_{r=1}^R$   ,    $\{x^u\}_{u=R}^{R+U}$   ，即labeled data和unlabeled data，而且通常情况下，unlabeled data的数量远远高于labeled data是数量，即 $U&gt;&gt;R$ .</p><p>Semi-supervised learning 又分为两种，Transductive learning （转导/推论推导）和 Inductive learning（归纳推理）</p><ul><li>Transductive learing: unlabeled data is the testing data. 即测试数据在训练中用过。</li><li>Inductive learning: unlabeled data is not the testing data.测试数据是训练中没有用过的数据。</li><li>这里的使用testing data是指用testing data的feature，而不是使用testing data的label。</li></ul><hr><p>为什么会有semi-supervised learning？</p><ul><li><p>Collecting data is easy, but collecting “labelled” data is expensive.</p><p>【收集数据很简单，但收集有label的数据很难】</p></li><li><p>We do semi-supervised learning in our lives</p><p>【在生活中，更多的也是半监督式学习，我们能明白少量看到的事物，但看到了更多我们不懂的，即unlabeled data】</p></li></ul><h2 id="Why-Semi-supervised-learning-helps"><a href="#Why-Semi-supervised-learning-helps" class="headerlink" title="Why Semi-supervised learning helps"></a>Why Semi-supervised learning helps</h2><p>为什么半监督学习能帮助解决一些问题？</p><p>如上图所示，如果只有labeled data，分类所画的boundary可能是一条竖线。</p><img src="https://s1.ax1x.com/2020/07/03/NXRNz6.md.png" alt="NXRNz6.md.png" style="zoom:75%;" /><p>但如果有一些unlabeled data（如灰色的点），分类所画的boundary可能是一条斜线。</p><p>The distribution of the unlabeled data tell us something.</p><p>半监督式学习之所以有用，是因为这些unlabeled data的分布能告诉我们一些东西。</p><p>通常这也伴随着一些假设，所以半监督式学习是否有用往往取决于这些假设是否合理。</p><h1 id="Semi-supervised-Learning-for-Generative-Model"><a href="#Semi-supervised-Learning-for-Generative-Model" class="headerlink" title="Semi-supervised Learning for Generative Model"></a>Semi-supervised Learning for Generative Model</h1><h2 id="Supervised-Generative-Model"><a href="#Supervised-Generative-Model" class="headerlink" title="Supervised Generative Model"></a>Supervised Generative Model</h2><p>在<a href="/2020/03/21/Classification1/" title="这篇">这篇</a>文章中，有详细讲述分类问题中的generative model。</p><p>给定一个labelled training data $x^r\in C_1,C_2$ 训练集。</p><p>prior probability（先验概率）有 $P(C_i)$ 和 $P(x|C_i)$ ，假设是Gaussian模型，则 $P(x|C_i)$ 由Gaussian模型中的 $\mu^i,\Sigma$ 参数决定。</p><p>根据已有的labeled data，计算出假设的Gaussian模型的参数（如下图），从而得出prior probability。</p><img src="https://s1.ax1x.com/2020/07/03/NXonAS.md.png" alt="NXonAS.md.png" style="zoom:75%;" /><p>即可算出posterior probability  $P\left(C_{1} \mid x\right)=\frac{P\left(x \mid C_{1}\right) P\left(C_{1}\right)}{P\left(x \mid C_{1}\right) P\left(C_{1}\right)+P\left(x \mid C_{2}\right) P\left(C_{2}\right)}$ </p><h2 id="Semi-supervised-Generative-Model"><a href="#Semi-supervised-Generative-Model" class="headerlink" title="Semi-supervised Generative Model"></a>Semi-supervised Generative Model</h2><p>在只有labeled data的图中，算出来的 $\mu,\Sigma$ 参数如下图所示：</p><img src="https://s1.ax1x.com/2020/07/03/NXonAS.md.png" alt="NXonAS.md.png" style="zoom:75%;" /><p>但如果有unlabeled data（绿色点），会发现分布的模型参数更可能是是下图：</p><img src="https://s1.ax1x.com/2020/07/03/NXRtRx.md.png" alt="NXRtRx.md.png" style="zoom:75%;" /><p>The unlabeled data $x^u$ help re-estimate $P(C_1),P(C_2),\mu^1,\mu^2,\Sigma$ .</p><p>因此，unlabeled data会影响分布，从而影响prior probability，posterior probability，最终影响 boundary。</p><h3 id="EM"><a href="#EM" class="headerlink" title="EM"></a>EM</h3><p>所以有unlabeled data, 这个Semi-supervised 的算法怎么做呢？</p><p>其实就是<strong>EM</strong>（Expected-maximization algorithm，期望最大化算法。）</p><ol><li><p>Initialization : $\theta={P(C_1),P(C_2),\mu^1,\mu^2,\Sigma}$ .</p><p>初始化Gaussian模型参数，可以随机初始，也可以通过labeled data得出。</p><p>虽然这个算法最终会收敛，但是初始化的参数影响收敛结果，就像gradient descent一样。</p></li><li><p>E：Step 1: compute the posterior probability of unlabeled data $P_\theta(C_1|x^u)$ (depending on model $\theta$ )</p><p>根据当前model的参数，计算出unlabeled data的posterior probability $P(C_1|x^u)$ .(以$P(C_1|x^u)$ 为例) </p></li><li><p>M：Step 2: update model. Back to step1 until the algorithm converges enventually.</p><p>用E步得到unlabeled data的posterior probability来最大化极大似然函数，更新得到新的模型参数，公式很直觉。(以 $C_1$ 为例)</p><p>（$N$ ：data 的总数，包括unlabeled data; $N_1$ :label= $C_1$ 的data数）</p><ul><li><p>$P(C_1)=\frac{N_1+\Sigma_{x^u}P(C_1|x^u)}{N}$  </p><p>对比没有unlabeled data之前的式子， $P(C_1)=\frac{N_1}{N}$ ，除了已有label= $C_1$ ，还多了一部分，即unlabeled data中属于 $C_1$ 的概率和。</p></li><li>$\mu^{1}=\frac{1}{N_{1}} \sum_{x^{r} \in C_{1}} x^{r}+\frac{1}{\sum_{x^{u}} P\left(C_{1} \mid x^{u}\right)} \sum_{x^{u}} P\left(C_{1} \mid x^{u}\right) x^{u}$  <p>对比没有unlabeled data的式子 ，$\mu^{1}=\frac{1}{N_{1}} \sum_{x^{r} \in C_{1}} x^{r}$ ，除了已有的label= $C_1$ ，还多了一部分，即unlabeled data的 $x^u$ 的加权平均（权重为 $P(C_1\mid x^u)$ ，即属于 $C_1$ 的概率）。</p></li><li><p>$\Sigma$ 公式也包括了unlabeled data.</p></li></ul></li></ol><p>所以这个算法的Step 1就是EM算法的Expected期望部分，根据已有的labeled data得出极大似然函数的估计值；</p><p>Step 2就是EM算法的Maximum部分，利用unlabeled data（通过已有模型的参数）最大化E步的极大似然函数，更新模型参数。</p><p>最后反复迭代Step 1和Step 2，直至收敛。</p><h3 id="Why-EM"><a href="#Why-EM" class="headerlink" title="Why EM"></a>Why EM</h3><p>[1]挖坑EM详解。</p><p>为什么可以用EM算法来解决Semi-supervised?</p><ul><li><p>只有labeled data</p><p>极大似然函数 $\log{L(\theta)}=\sum_{x^r}\log{P_\theta(x^r,\hat{y}^r)}$ , 其中 $P_\theta(x^r,\hat{y}^r)=P_\theta(x^r\mid \hat{y}^r)P(\hat{y}^r)$ .</p><p>对上式子求导是有closed-form solution的。</p></li><li><p>有labeled data和unlabeled data</p><p>极大似然函数增加了一部分  $\log L(\theta)=\sum_{x^{r}} \log P_{\theta}\left(x^{r}, \hat{y}^{r}\right)+\sum_{x^{u}} \log P_{\theta}\left(x^{u}\right)$ .</p><p>将后部分用全概率展开， $P_{\theta}\left(x^{u}\right)=P_{\theta}\left(x^{u} \mid C_{1}\right) P\left(C_{1}\right)+P_{\theta}\left(x^{u} \mid C_{2}\right) P\left(C_{2}\right)$  .</p><p>如果要求后部分，因为是unlabeled data, 所以模型 $\theta$ 需要得知unlabeled data的label，即 $P(C_1\mid x^u)$ ,而求这个式子，也需要得到 prior probability $P(x^u\mid C_1)$ ,但这个式子需要事先得知模型 $\theta$ ，因此陷入了死循环。</p><p>因此这个极大似然函数不是convex（凸），不能直接求解，因此用迭代的EM算法逐步maximum极大似然函数。</p></li></ul><h1 id="Low-density-Separation-Assumption"><a href="#Low-density-Separation-Assumption" class="headerlink" title="Low-density Separation Assumption"></a>Low-density Separation Assumption</h1><p>另一种假设是Low-density Separation的假设，即这个世界是非黑即白的”Black-or-white”。</p><p>两种类别之间是low-density，交界处有明显的鸿沟，因此要么是类别1，要么是类别2，没有第三种情况。</p><h2 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h2><p>对于Low-density Separation Assumption的假设，使用Self-training的方法。</p><p>Given：labeled data set $={(x^r,\hat{y}^r}<em>{r=1}^R$ ,unlabeled data set $={x^u}</em>{u=l}^R+U$ .</p><p><strong>Repeat：</strong> </p><ol><li><p>Train model $f^<em>$ from labeled data set. ($f^</em>$ is independent to the model)</p><p>从labeled data set中训练出一个模型</p></li><li><p>Apply $f^*$ to the unlabeled data set. Obtain pseudo-label ${(x^u,y^u}_{u=l}^{R+U}$ .</p><p>用这个模型 $f^*$ 来预测unlabeled data set， 获得伪label</p></li><li><p>Remove a set of data from unlabeled data set, and add them into the labeled data set.</p><p>拿出一些unlabeled data(pseudo-label)，放到labeled data set中，回到步骤1，再训练。</p><ul><li><p>how to choose the data set remains open</p><p>如何选择unlabeled data 是自设计的</p></li><li><p>you can also provide a weight to each data.</p><p>训练中可以对unlabeled data(pseudo-label)和labeled data 赋予不同的权重.</p></li></ul></li></ol><p><strong>注意：</strong> Regression模型是不能self-training的，因为unlabeled data和其pseudo-label放在模型中的loss为0，无法再minimize。</p><h2 id="Hard-Label"><a href="#Hard-Label" class="headerlink" title="Hard Label"></a>Hard Label</h2><p><strong>V.S.  semi-supervised learning for generative model</strong> </p><p>Semi-supervised learning for generative model和Low-density Separation的区别其实是soft label 和soft label的区别。</p><p>generative model是利用来unlabeled data的 $P(C_1|x^u)$ posterior probability来计算新的prior probability，迭代更新模型。</p><p>而low-density是计算出unlabeled data的pseudo-label，选择性扩大labeled data set(即加入部分由pseudo-label的unlabeled data)来迭代训练模型。</p><p>因此，如果考虑Neural Network：</p><p>($\theta^*$ 是labeled data计算所得的network parameters)</p><p>如下图，unlabeled data $x^u$ 放入模型中预测，得到 $\begin{bmatrix} 0.7 \ 0.3\end{bmatrix}$ .</p><img src="https://s1.ax1x.com/2020/07/03/NXRYJ1.md.png" alt="NXRYJ1.md.png" style="zoom:75%;" /><p>如果是使用hard label，则 $x^u$ 的target是 $\begin{bmatrix} 1 \ 0\end{bmatrix}$ .</p><p>如果是使用soft label，则 $x^u$ 的target是 $\begin{bmatrix} 0.7 \ 0.3\end{bmatrix}$ .</p><p>如果是使用soft label，则self-training不会有效，因为新的data加进去，不会增大模型的loss，也就无法再minimize.</p><p><strong>所以基于Low-density Separation的假设，是非黑即白的，需要使用hard label来self-training。</strong> </p><h2 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h2><p>在训练模型中，我们需要尽量保证unlabeled data在模型中的分布是low-density separation。</p><p>即下图中，unlabeled data得到的pseudo-label的分布应该尽量集中，而不应该太分散。</p><img src="https://s1.ax1x.com/2020/07/03/NXRJiR.md.png" alt="NXRJiR.md.png" style="zoom:50%;" /> <p>所以，在训练中，<strong>如何评估 $y^u$ 的分布的集中度？</strong></p><p>根据信息学，使用 $y^u$ 的entropy，即  $E\left(y^{u}\right)=-\sum_{m=1}^{5} y_{m}^{u} \ln \left(y_{m}^{u}\right)$ </p><p>(注：这里的 $y^u_m$ 应该是  $y^u=m$ 的概率)</p><p>当 $E(y^u)$ 越小，说明 $y^u$ 分布越集中，如下图。</p><img src="https://s1.ax1x.com/2020/07/03/NXR3dJ.md.png" alt="NXR3dJ.md.png" style="zoom:50%;" /><hr><p>因此，在self-training中：</p><p>$L=\sum_{y^r} C(x^r,\hat{y}^r)+\lambda\sum_{x^u}E(y^u)$ </p><p>Loss function的前一项（cross entropy）minimize保证分类的正确性，后一项（entropy of  $y^u$ ) minimize保证 unlabeled data分布尽量集中，最大可能满足low-density separation的假设。</p><p>training：gradient decent.</p><p>因为这样的形式很像之前提到过的regularization(具体见<a href="/2020/04/21/tips-for-DL/" title="这篇文章的3.2">这篇文章的3.2</a>)，所以又叫entropy-based regularization.</p><h2 id="Outlook-Semi-supervised-SVM"><a href="#Outlook-Semi-supervised-SVM" class="headerlink" title="Outlook: Semi-supervised SVM"></a>Outlook: Semi-supervised SVM</h2><p>SVM也是解决semi-supervised learning的方法.</p><img src="https://s1.ax1x.com/2020/07/03/NXRaQK.md.png" alt="NXRaQK.md.png" style="zoom:50%;" /><p>上图中，在有unlabeled data的情况下，希望boundary 分的越开越好（largest margin）和有更小的error.</p><p>因此枚举unlabeled data所有可能的情况，但枚举在计算量上是巨大的，因此SVM（Support Vector Machines）可以实现枚举的目标，但不需要这么大的枚举量。</p><h1 id="Smoothness-Assumption"><a href="#Smoothness-Assumption" class="headerlink" title="Smoothness Assumption"></a>Smoothness Assumption</h1><p>Smoothness Assumption的思想可以用以下话归纳：</p><p>“You are known by the company you keep”</p><p>近朱者赤，近墨者黑。</p><p>蓬生麻中，不扶而直。白沙在涅，与之俱黑。</p><p>Assumption：“similar” $x$ has the same $\hat{y}$ .</p><p>【意思就是说：相近的 $x$ 有相同的label $\hat{y}$ .】</p><p><strong>More precise assumption：</strong></p><ul><li>x is not uniform</li><li>if $x^1$ and $x^2$ are close in a hign density region, $\hat{y}^1$ and $\hat{y}^2$ are the same.</li></ul><p>Smoothness Assumption假设更准确的表述是：</p><p> x不是均匀分布，如果 $x^1$ 和 $x^2$ 通过一个high density region的区域连在一起，且离得很近，则 $\hat{y}^1$ 和 $\hat{y}^2$ 相同。</p><p>如下图， $x^1$ 和 $x^2$ 通过high density region连接在一起，有相同的label，而 $x^2$ 和 $x^3$ 有不同的label.</p><img src="https://s1.ax1x.com/2020/07/03/NXR1Z4.md.png" alt="NXR1Z4.md.png" style="zoom:50%;" /><hr><p>Smoothness Assumption通过观察大量unlabeled data，可以得到一些信息。</p><p>比如下图中的两张人的左脸和右脸图片，都是unlabeled，但如果给大量的过渡形态（左脸转向右脸）unlabeled data，可以得出这两张图片是相似的结论.</p><p><a href="https://imgchr.com/i/NXoQpj"><img src="https://s1.ax1x.com/2020/07/03/NXoQpj.md.png" alt="NXoQpj.md.png"></a> </p><p>Smoothness Assumption还可以用在文章分类中，比如分类天文学和旅游学的文章。</p><p>如下图， 文章 d1和d3有overlap word（重叠单词），所以d1和d3是同一类，同理 d4和d2是一类。</p><img src="https://s1.ax1x.com/2020/07/03/NXoutg.md.png" alt="NXoutg.md.png" style="zoom:50%;" /><p>如果，下图中，d1和d3没有overlap word，就无法说明d1和d3是同一类。</p><img src="https://s1.ax1x.com/2020/07/03/NXoKhQ.md.png" alt="NXoKhQ.md.png" style="zoom:50%;" /><p>但是，如果我们收集到足够多但unlabeled data，如下图，通过high density region的连接和传递，也可以得出d1和d3一类，d2和d4一类。</p><img src="https://s1.ax1x.com/2020/07/03/NXol1s.md.png" alt="NXol1s.md.png" style="zoom:80%;" /><h2 id="Cluster-and-then-Label"><a href="#Cluster-and-then-Label" class="headerlink" title="Cluster and then Label"></a>Cluster and then Label</h2><p>在Smoothness Assumption假设下，直观的可以用cluster and then label，先用所有的data训练一个classifier。</p><p>直接聚类标记(比较难训练）。</p><h2 id="Graph-based-Approach"><a href="#Graph-based-Approach" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h2><p>另一种方法是利用图的结构（Graph structure）来得知 $x^1$ and $x^2$ are close in a high density region (connected by a high density path).</p><p>Represent the data points as a graph.</p><p>【把这些数据点看作一个图】</p><img src="https://s1.ax1x.com/2020/07/03/NXRKMT.md.png" alt="NXRKMT.md.png" style="zoom:50%;" /><p>建图有些时候是很直观的，比如网页中的超链接，论文中的引用。</p><p>但有的时候也需要自己建图。</p><p>注意：</p><p>如果是影像类，base on pixel，performance就不太好，一般会base on autoencoder，将feature抽象出来，效果更好。</p><h3 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h3><p>建图过程如下：</p><ol><li><p>Define the similarity $s(x^i, x^j)$ between $x^i$ and $x^j$ .</p><p>【定义data $x^i$ 和 $x^j$ 的相似度】</p></li><li><p>Add edge【定义数据点中加边（连通）的条件】</p><ul><li><p>K Nearest Neighbor【和该点最近的k个点相连接】</p><img src="https://s1.ax1x.com/2020/07/03/NXReGq.png" alt="NXReGq.png" style="zoom:50%;" /></li><li><p>e-Neighborhood【与离该点距离小于等于e的点相连接】</p><img src="https://s1.ax1x.com/2020/07/03/NXRZin.png" alt="NXRZin.png" style="zoom:50%;" /></li></ul></li><li><p>Edge weight is proportional to $s(x^i, x^j)$ 【边点权重就是步骤1定义的连接两点的相似度】</p><p>Gaussian Radial Basis Function： $s\left(x^{i}, x^{j}\right)=\exp \left(-\gamma\left\|x^{i}-x^{j}\right\|^{2}\right)$ </p><p>一般采用如上公式（经验上取得较好的performance）。</p><p>因为利用指数化后（指数内是两点的Euclidean distance），函数下降的很快，只有当两点离的很近时，该相似度 $s(x^i,x^j)$  才大，其他时候都趋近于0.</p></li></ol><h3 id="Graph-based-Approach-1"><a href="#Graph-based-Approach-1" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h3><p>图建好后：</p><p>The labeled data influence their neighbors.</p><p>Propagate through the graph.</p><p>【label data 不仅会影响他们的邻居，还会一直传播下去】</p><img src="https://s1.ax1x.com/2020/07/03/NXRmR0.md.png" alt="NXRmR0.md.png" style="zoom:50%;" /><p>如果data points够多，图建的好，就会像下图这样：</p><img src="https://s1.ax1x.com/2020/07/03/NXREIs.png" alt="NXREIs.png" style="zoom:50%;" /><p>但是，如果data较少，就可能出现下图这种label传不到unlabeled data的情况：</p><img src="https://s1.ax1x.com/2020/07/03/NXRPsS.png" alt="NXRPsS.png" style="zoom:50%;" /><h3 id="Smoothness-Definition"><a href="#Smoothness-Definition" class="headerlink" title="Smoothness Definition"></a>Smoothness Definition</h3><p>因为是基于Smoothness Assumption，所以最后训练出的模型应让得到的图尽可能满足smoothness的假设。</p><p><strong>注意：</strong> 这里的因果关系是，unlabeled data作为NN的输入，得到label $y$ ，该label $y$ 和labeled data的 label $\hat{y}$  一起得到的图是尽最大可能满足Smoothness Assumption的。</p><p>（<strong>而不是</strong>建好图，然后unlabeled data的label $y$ 是labeled data原有的 $\hat{y}$ 直接传播过来的，不然训练NN干嘛）</p><p>把unlabeled data作为NN的输入，得到label ，对labeled data和”unlabeled data” 建图。</p><p>为了在训练中使得最后的图尽可能满足假设，定义<strong>smoothness of the labels on the graph</strong>.</p>$S=\frac{1}{2} \sum_{i,j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}$  <p>（对于所有的labeled data 和 “unlabeled data”（作为NN输入后，有label））</p><p>按照上式计算，得到的Smoothness如下图所示：</p><p><a href="https://imgchr.com/i/NXRiqg"><img src="https://s1.ax1x.com/2020/07/03/NXRiqg.md.png" alt="NXRiqg.md.png"></a> </p><p><strong>Smaller means smoother.</strong> </p><p>【Smoothness $S$ 越小，表示图越满足这个假设】</p><hr><p>计算smoothness $S$ 有一种简便的方法：</p>$S=\frac{1}{2} \sum_{i, j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}=y^{T} L y$  (这里的1/2只是为了计算方便)<ul><li><p>$y$ : (R+U)-dim vector，是所有label data和”unlabeled data” 的label，所以是R+U维。</p><p>$y=\begin{bmatrix}…y^i…y^j…\end{bmatrix}^T$ </p></li><li><p>$L$ :(R+U) $\times$ (R+U) matrix，也叫Graph Laplacian（调和矩阵，拉普拉斯矩阵）</p><p>$L$ 的计算方法：$L=D-W$ </p><p>其中 $W$ 矩阵算是图的邻接矩阵（区别是无直接可达边的值是0）</p><p>$D$ 矩阵是一个对角矩阵，对角元素的值等于 $W$ 矩阵对应行的元素和</p><p>矩阵表示如下图所示：</p><img src="https://s1.ax1x.com/2020/07/03/NXRkZQ.md.png" alt="NXRkZQ.md.png" style="zoom:50%;" /></li></ul><p>（证明据说很枯燥，暂时略[2])</p><h3 id="Smoothness-Regularization"><a href="#Smoothness-Regularization" class="headerlink" title="Smoothness Regularization"></a>Smoothness Regularization</h3>$S=\frac{1}{2} \sum_{i, j} w_{i, j}\left(y^{i}-y^{j}\right)^{2}=y^{T} L y$  <p> $S$ 中的 $y$ 其实是和network parameters有关的（unlabeled data的label），所以把 $S$ 也放进损失函数中minimize，以求尽可能满足smoothness assumption.</p><p>以满足smoothness assumption的损失函数： $L=\sum_{x^r} C\left(y^{r}, \hat{y}^{r}\right)+\lambda S$ </p><p>损失函数的前部分使labeled data的输出更贴近其label，后部分 $\lambda S$ 作为regularization term，使得labeled data和unlabeled data尽可能满足smoothness assumption.</p><p>除了让NN的output满足smoothness的假设，还可以让NN的任何一层的输出满足smoothness assumption，或者让某层外接一层embedding layer，使其满足smoothness assumption，如下图：</p><img src="https://s1.ax1x.com/2020/07/03/NXRAaj.png" alt="NXRAaj.png" style="zoom:50%;" /><h1 id="Better-Representation"><a href="#Better-Representation" class="headerlink" title="Better Representation"></a>Better Representation</h1><p>Better Presentation的思想就是：去芜存菁，化繁为简。</p><ul><li>Find the latent(潜在的) factors behind the observation.</li><li>The latent factors (usually simpler) are better representation.</li></ul><p>【找到所观察事物的潜在特征，即该事物的better representation】</p><p>该部分后续见这篇博客。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>挖坑：EM算法详解</p></li><li><p>挖坑：Graph Laplacian in smoothness.</p></li><li><p>Olivier Chapelle：Semi-Supervised Learning </p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章开篇讲述了什么是Semi-supervised Learning（半监督学习）？&lt;/p&gt;
&lt;p&gt;再次，文章具体阐述了四种Semi-supervised Learning，包括Generative Model，Low-density，Smoothness Assumption和Better Representation。&lt;/p&gt;
&lt;p&gt;对于Generative Model，文章重点讲述了如何用EM算法来训练模型。&lt;/p&gt;
&lt;p&gt;对于Low-density，文章重点讲述了如何让模型进行Self-training，并且在训练中引入Entropy-based Regularization term来尽可能low-density的假设。&lt;/p&gt;
&lt;p&gt;对于Smoothness Assumption，文章重点讲述了Graph-based Approach（基于图的方法），并且在训练中引入Smoothness Regularization term来尽可能满足Smoothness Assumption的假设。&lt;/p&gt;
&lt;p&gt;对于Better Representation，本篇文章只是简单阐述了其思想，具体介绍见这篇博客。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="Semi-supervised" scheme="https://f7ed.com/tags/Semi-supervised/"/>
    
  </entry>
  
  <entry>
    <title>「算法导论」:排序-总结</title>
    <link href="https://f7ed.com/2020/06/29/sort-preview/"/>
    <id>https://f7ed.com/2020/06/29/sort-preview/</id>
    <published>2020-06-28T16:00:00.000Z</published>
    <updated>2020-07-03T08:50:09.474Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章review了算法中的排序算法，包括冒泡排序、插入排序、归并排序、堆排序（以及用堆实现优先队列）、快速排序和计数排序。</p><p>分别从算法思路、算法伪代码实现、算法流程、算法时间复杂度四个方面阐述每个算法。</p><a id="more"></a><h1 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h1><p><strong>排序问题：</strong></p><p>输入：一个 $n$个数的序列 $&lt;a_1,a_1,…,a_n&gt;$ </p><p>输出：输入序列的一个重拍 $&lt;a_1’,a_2’,…,a_n’&gt;$ ，使得 $a_1’\leq a_2’ \leq…\leq a_n’$ .</p><p>在实际中，待排序的数很少是单独的数值，它们通常是一组数据，称为记录(record)。每个记录中包含一个关键字(key)，这就是需要排序的值。记录剩下的数据部分称为卫星数据(satellite data)，通常和关键字一同存取。</p><p><strong>原址排序：</strong>输入数组中仅有常数个元素需要在排序过程中存储在数组之外。</p><p>典型的原址排序有：插入排序、堆排序、快速排序。</p><hr><p><strong>符号说明：</strong> </p><ul><li><p>$\Theta$ 记号：</p><p>$\Theta$ 记号渐进给出一个函数的上界和下界。</p>  $\Theta(g(n))=\left\{f(n): \text { there exist positive constants } c_{1}, c_{2}, \text { and } n_{0}\text { such that } \right.  \left.0 \leq c_{1} g(n) \leq f(n) \leq c_{2} g(n) \text { for all } n \geq n_{0}\right\}$<p>$g(n)$ 称为 $f(n)$ 的一个渐进紧确界(asymptotically tight bound)</p></li><li><p>$O$ 记号</p><p>$O$ 记号只给出了函数的渐进上界。</p>  $O(g(n))=\left\{f(n): \text { there exist positive constants } c \text { and } n_{0}\text { such that } \right.  \left.0 \leq f(n) \leq c g(n) \text { for all } n \geq n_{0}\right\}$ </li><li><p>$\Omega$ 记号</p><p>$\Omega$ 记号给出了函数的渐进下界。</p>  $\Omega(g(n))=\left\{f(n): \text { there exist positive constants } c \text { and } n_{0}\text { such that } \right. \left.0 \leq c g(n) \leq f(n) \text { for all } n \geq n_{0}\right\}$ <p>符号比较如下图：</p></li></ul><p><a href="https://imgchr.com/i/Nh2if1"><img src="https://s1.ax1x.com/2020/06/29/Nh2if1.md.png" alt="Nh2if1.md.png"></a> </p><p><strong>排序算法运行时间一览</strong> ：</p><table><thead><tr><th>算法</th><th>最坏情况运行时间</th><th>平均情况/期望运行时间</th></tr></thead><tbody><tr><td>插入排序</td><td>$\Theta (n^2)$</td><td>$\Theta(n^2)$</td></tr><tr><td>归并排序</td><td>$\Theta(n\lg{n})$</td><td>$\Theta(n\lg{n})$</td></tr><tr><td>堆排序</td><td>$O(n\lg{n})$</td><td>-</td></tr><tr><td>快速排序</td><td>$\Theta(n^2)$</td><td>$\Theta(n\lg{n})$ (expected)</td></tr><tr><td>计数排序</td><td>$\Theta(k+n)$</td><td>$\Theta(k+n)$</td></tr><tr><td>基数排序</td><td>$\Theta(d(n+k))$</td><td>$\Theta(d(n+k))$</td></tr><tr><td>桶排序</td><td>$\Theta(n^2)$</td><td>$\Theta(n)$ (average-case)</td></tr></tbody></table><h1 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h1><p>反复交互相邻未按次序排列的元素。</p><p><strong>BUBBLESORT(A)</strong></p><ul><li>参数：A待排序数组</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to A.lengh<span class="literal">-1</span></span><br><span class="line"><span class="keyword">for</span> j = A.length downto i+<span class="number">1</span> //每次迭代找出A[<span class="type">i..j</span>]中最小的元素放在A[<span class="type">i</span>]位置</span><br><span class="line"><span class="keyword">if</span> A[<span class="type">j</span>] &lt; A[<span class="type">j</span>-<span class="number">1</span>]</span><br><span class="line">exchange A[<span class="type">j</span>] with A[<span class="type">j</span>-<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>冒泡排序是原址排序，流行但低效。</p><h1 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h1><p>如下图所示，插入排序就像打牌时排序一手扑克牌。</p><p><a href="https://imgchr.com/i/Nh2m0e"><img src="https://s1.ax1x.com/2020/06/29/Nh2m0e.png" alt="Nh2m0e.png"></a> </p><ol><li>开始时，我们的左手为空，桌子上的牌面向下。</li><li>然后，我们每次从桌子上拿走一张牌，想把它放在左手中的正确位置。</li><li>为了找到这张牌的正确位置，我们从右到左将这张牌和左手里的牌依次比较，放入正确的位置。</li><li>左手都是已排序好的牌。</li></ol><p><strong>INSERTION-SORT(A)</strong> </p><ul><li>A：待排序数组</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j = <span class="number">2</span> to A.length</span><br><span class="line">key = A[<span class="type">j</span>]</span><br><span class="line">//将key插入到已排序好的A[<span class="number">1</span><span class="type">..j</span>-<span class="number">1</span>]</span><br><span class="line">i = j - <span class="number">1</span> //pointer <span class="keyword">for</span> sorted sequence A[<span class="number">1</span><span class="type">..j</span>-<span class="number">1</span>]</span><br><span class="line"><span class="keyword">while</span> i &gt; <span class="number">0</span> and A[<span class="type">i</span>] &gt; key</span><br><span class="line">A[<span class="type">i</span>+<span class="number">1</span>] = A[<span class="type">i</span>]</span><br><span class="line">i--</span><br><span class="line">A[<span class="type">i</span>+<span class="number">1</span>] = key</span><br></pre></td></tr></table></figure><p>插入排序是原址排序，对于少量元素是一个有效的算法。</p><p>最坏情况的运行时间： $\Theta(n^2)$ .</p><h1 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h1><h2 id="分治"><a href="#分治" class="headerlink" title="分治"></a>分治</h2><p><strong>分治</strong>： 将原问题分解为几个规模较小但类似于原问题的子问题，递归地求解这些子问题，然后再合并这些子问题的解来建立原问题的解。</p><p><strong>分治的步骤</strong>：</p><ol><li>分解：分解原问题为若干规模较小的子问题。</li><li>解决：递归地求解各子问题，规模较小，可直接求解。</li><li>合并：合并这些子问题的解成原问题的解。</li></ol><h2 id="算法核心"><a href="#算法核心" class="headerlink" title="算法核心"></a>算法核心</h2><p><strong>归并排序中的分治</strong> ：</p><ol><li>分解：分解待排序的n个元素序列成各n/2个元素序列的两个子序列。</li><li>解决：使用归并排序递归地排序两个子序列，当序列长度为1时，递归到达尽头。</li><li>合并：合并两个已经排序好的子序列以产生排序好的原序列。</li></ol><p><strong>核心</strong> ：合并两个已经排序好的子序列——MERGE(A, p, q, r)</p><ul><li>A: 待排序原数组。</li><li>p, q, r: 数组下标，满足 $p\leq q&lt;r$ 。</li><li>假设子数组 A[p..q] 和A[q+1..r]都已经排好序，合并这两个数组代替原来的A[p..r]子数组。</li></ul><hr><p><strong>MERGE算法理解：</strong> </p><ol><li>牌桌上有两堆牌面朝上，每堆都已排好序，最小的牌在顶上。希望将两堆牌合并成排序好的输出牌堆，且牌面朝下。</li><li>比较两堆牌顶顶牌，选取较小的那张，牌面朝下的放在输出牌堆。</li><li>重复步骤2直至某一牌堆为空。</li><li>将剩下的另一堆牌面朝下放在输出堆。</li></ol><p>MERGE合并的过程如下图所示：</p><p><a href="https://imgchr.com/i/Nh2E6K"><img src="https://s1.ax1x.com/2020/06/29/Nh2E6K.md.png" alt="Nh2E6K.md.png"></a> </p><p><strong>MERGE算法分析：</strong> </p><p>在上述过程中，n个元素，我们最多执行n个步骤，所以MERGE合并需要 $\Theta(n)$ 的时间。</p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><p><strong>MERGE(A, p, q, r)</strong> </p><ul><li>功能：合并已排序好的子数组A[p..q]和A[q+1..r]</li><li>参数：A为待排序数组，p, q, r为数组下标，且满足 $p\leq q&lt;r$ </li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Let S[<span class="type">p..r</span>] be new arrays</span><br><span class="line">k = p //pointer <span class="keyword">for</span> S[]</span><br><span class="line">i = p, j = q+<span class="number">1</span> //pointer <span class="keyword">for</span> subarray</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> k &lt;= r </span><br><span class="line"><span class="keyword">while</span> ( i &lt;= q and j &lt;= r and A[<span class="type">i</span>] &lt;= A[<span class="type">j</span>] ) or j &gt; r  // 取A[<span class="type">p..q</span>]牌堆</span><br><span class="line">S[<span class="type">k</span>++] = A[<span class="type">i</span>++]</span><br><span class="line"><span class="keyword">while</span> ( i &lt;= q and j &lt;= r and A[<span class="type">i</span>] &gt;= A[<span class="type">j</span>] ) or i &gt; q //取A[<span class="type">q</span>+<span class="number">1</span><span class="type">..r</span>]牌堆</span><br><span class="line"></span><br><span class="line">A[<span class="type">p..r</span>] = S[<span class="type">p..r</span>]</span><br></pre></td></tr></table></figure><hr><p><strong>MERGE-SORT(A, p, r)</strong> </p><ul><li>功能：排序子数组A[p..r]</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> p &lt; r</span><br><span class="line">q = (p+r)/<span class="number">2</span></span><br><span class="line"><span class="built_in">MERGE-SORT</span>(A, p, q)</span><br><span class="line"><span class="built_in">MERGE-SORT</span>(A, q+<span class="number">1</span>, r)</span><br><span class="line">MERGE(A, p, q, r)</span><br></pre></td></tr></table></figure><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><h3 id="分治算法运行时间分析"><a href="#分治算法运行时间分析" class="headerlink" title="分治算法运行时间分析"></a>分治算法运行时间分析</h3><p>分治算法运行时间递归式来自三个部分。</p><p>假设 $T(n)$ 是规模为 $n$ 的一个问题的运行时间。若规模问题足够小，则直接求解需要常量时间，将其写作 $\Theta(1)$ 。</p><p>假设把原问题分解成 $a$ 个子问题，每个子问题的规模是原问题的 $1/b$ (在归并排序中， $a$ 和 $b$ 都为2，但很多分治算法中 $a\neq b$ )。为了求解一个规模为 $n/b$ 规模的子问题，需要 $T(n/b)$ 的时间，所以需要 $aT(n/b)$ 的时间求解 $a$ 个子问题。</p><p>如果分解子问题需要 $D(n)$ 时间，合并子问题需要 $C(n)$ 时间。</p><p>递归式：</p>$$T(n)=\left\{\begin{array}{ll}\Theta(1) & \text { if } n \leq c \\ a T(n / b)+D(n)+C(n) & \text { otherwise }\end{array}\right.$$<h3 id="归并排序分析"><a href="#归并排序分析" class="headerlink" title="归并排序分析"></a>归并排序分析</h3><p>前文分析了MERGE(A, p, q, r) 合并两个子数组的时间复杂度是 $\Theta(n)$ ，即 $C(n)=\Theta(n)$ ，且 $D(n)=\Theta(n)$ .</p><p>归并排序的最坏情况运行时间 $T(n)$ :</p>$$T(n)=\left\{\begin{array}{ll}\Theta(1) & \text { if } n=1 \\ 2 T(n / 2)+\Theta(n) & \text { if } n>1\end{array}\right.$$<p>用递归树的思想求解递归式：</p><p><a href="https://imgchr.com/i/Nh2Al6"><img src="https://s1.ax1x.com/2020/06/29/Nh2Al6.md.png" alt="Nh2Al6.md.png"></a> </p><p>即递归树每层的代价为 $\Theta(n)=cn$ ，共有 $\lg{n}+1$ 层，所以归并排序的运行时间结果是 $\Theta(n\lg{n})$  .</p><h1 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h1><h2 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h2><p><strong>自由树</strong> ：连通的、无环的无向图。</p><p><strong>有根数</strong> ：是一棵自由树，其顶点存在一个与其他顶点不同的顶点，称为树的根。</p><p><strong>度</strong> ：有根树中一个结点 $x$ 孩子的数目称为 $x$ 的度。</p><p><strong>深度</strong> :从根 $r$ 到结点 $x$ 的简单路径。</p><p><strong>二叉树</strong> ：不包括任何结点，或者包括三个不相交的结点集合：一个根结点，一棵称为左子树的二叉树和一棵称为右子树的二叉树。</p><p><strong>完全k叉树</strong> ：所有叶结点深度相同，且所有内部结点度为k的k叉树。</p><hr><p><strong>（二叉）堆</strong> ：是一个数组，它可以被看成一个近似的完全二叉树，树上的每个结点对应数组中的一个元素。除了最底层外，该树被完全填满，并且是从左到右填充。如下图所示。</p><p><a href="https://imgchr.com/i/Nh2Ck9"><img src="https://s1.ax1x.com/2020/06/29/Nh2Ck9.md.png" alt="Nh2Ck9.md.png"></a> </p><p>堆的数组$A$ 有两个属性：</p><ul><li><p>$A.length$ ：数组元素的个数，A[1..A.length]中都存有值。</p></li><li><p>$A.heap-size$ ：有多少个堆元素在数组，A[1..heap-size]中存放的是堆的有效元素。</p><p>（$0\leq A.heap-size\leq A.lengh$ )</p></li></ul><p><strong>堆的性质：</strong> </p><ul><li><p>$A[1]$ :存放的是树的根结点。</p></li><li><p>对于给定的一个结点 $i$ ，很容易计算他的父结点、左孩子和右孩子的下标。</p><ul><li><p><strong>PARENT(i)</strong> </p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> i/<span class="number">2</span> //i&gt;&gt;&gt;<span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p><strong>LEFT(i)</strong> </p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> <span class="number">2</span>*i //i&lt;&lt;&lt;<span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p><strong>RIGHT(i)</strong> </p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> <span class="number">2</span>*i+<span class="number">1</span> //i&lt;&lt;&lt;<span class="number">1</span> | <span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>包含$n$ 个元素的堆的高度为 $\Theta(\lg{n})$ </p></li><li><p>堆结构上的基本操作的运行时间至多和堆的高度成正比，即时间复杂度为 $O(\lg{n})$ .</p></li><li><p>叶子结点：n/2+1 , n/2+2 , … , n</p></li></ul><p><strong>堆的分类：</strong> </p><ul><li><p>最大堆：</p><p>除了根以外的结点 $i$ 都满足 $A[\text{PARENT}(i)]\geq A[i]$ .</p><p>某个结点最多和其父结点一样大。</p><p>堆的最大元素存放在根结点中。</p></li><li><p>最小堆：</p><p>除了根以外的结点 $i$ 都满足 $A[\text{PARENT}(i)]\leq A[i]$ .</p><p>堆的最小元素存放在根结点中。</p></li></ul><p><strong>堆的基本过程</strong> :</p><ul><li>MAX-HEAPIFY：维护最大堆的过程，时间复杂度为 $O(\lg{n})$ </li><li>BUILD-MAX-HEAP：将无序的输入数据数组构造一个最大堆，具有线性时间复杂度 $O(n\lg{n})$ 。</li><li>HEAPSORT：对一个数组进行原址排序，时间复杂度为 $O(n\lg{n})$ </li><li>MAX-HEAP-INSERT、HEAP-EXTRACT-MAX、HEAP-INCREASE-KEY和HEAP-MAXIMUM：利用堆实现一个优先队列，时间复杂度为 $O(\lg{n})$ .</li></ul><h2 id="维护：MAX-HEAPIFY"><a href="#维护：MAX-HEAPIFY" class="headerlink" title="维护：MAX-HEAPIFY"></a>维护：MAX-HEAPIFY</h2><p>调用MAX-HEAPIFY的时候，假定根结点LEFT(i)和RIGHT(i)的二叉树都是最大堆，但A[i]可能小于其左右孩子，因此违背了堆的性质。</p><p>MAX-HEAPIFY通过让 A[i]“逐级下降”，从而使下标为i的根结点的子树满足最大堆的性质。</p><p><strong>MAX-HEAPIFY(A, i)</strong> </p><ul><li>功能：维护下标为i的根结点的子树，使其满足最大堆的性质。</li><li>参数：i 是该子树的根结点，其左子树右子树均满足最大堆的性质。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">l = LEFT(i)</span><br><span class="line">r = RIGHT(i)</span><br><span class="line"><span class="keyword">if</span> l &lt;= A.heap<span class="literal">-size</span> and A[<span class="type">l</span>] &gt; A[<span class="type">i</span>]</span><br><span class="line">largest = l</span><br><span class="line"><span class="keyword">else</span> largest = i</span><br><span class="line"><span class="keyword">if</span> r &lt;= A.heap<span class="literal">-size</span> and A[<span class="type">r</span>] &gt; A[<span class="type">i</span>]</span><br><span class="line">largest = r</span><br><span class="line"><span class="keyword">if</span> largest != i</span><br><span class="line">exchange A[<span class="type">i</span>] with A[<span class="type">largest</span>]</span><br><span class="line">MAX<span class="literal">-HEAPIFY</span>(A, largest)</span><br></pre></td></tr></table></figure><p>下图是执行 MAX-HEAPIFY(A, 2)的执行过程。A.heap-size=10, 图(a)(b)(c)依次体现了值为4的结点依次下降的过程。</p><p><a href="https://imgchr.com/i/Nh2PYR"><img src="https://s1.ax1x.com/2020/06/29/Nh2PYR.md.png" alt="Nh2PYR.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：</p><p>MAX-HEAPIFY的时间复杂度为 $O(lg{n})$.</p><h2 id="建堆：BUILD-MAX-HEAP"><a href="#建堆：BUILD-MAX-HEAP" class="headerlink" title="建堆：BUILD-MAX-HEAP"></a>建堆：BUILD-MAX-HEAP</h2><p>堆的性质：</p><p>子数组A[n/2+1..n]中的元素都是树的叶子结点。因为下标最大的父结点是n/2，所以n/2以后的结点都没有孩子。</p><p><strong>建堆</strong> ：每个叶结点都可以看成只包含一个元素的堆，利用自底向上的方法，对树中其他结点都调用一次MAX-HEAPIFY，把一个大小为n = A.length的数组A[1..n]转换为最大堆。</p><p><strong>BUILD-MAX-HEAP(A)</strong> </p><ul><li>功能：把A[1..n]数组转换为最大堆</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A.heap<span class="literal">-size</span> = A.length</span><br><span class="line"><span class="keyword">for</span> i = A.length/<span class="number">2</span> downto <span class="number">1</span></span><br><span class="line">MAX<span class="literal">-HEAPIFY</span>(A, i)</span><br></pre></td></tr></table></figure><p>下图是把A数组构造成最大堆的过程：</p><p><a href="https://imgchr.com/i/Nh2kSx"><img src="https://s1.ax1x.com/2020/06/29/Nh2kSx.md.png" alt="Nh2kSx.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：</p><p>BUILD-MAX-HEAP需要 $O(n)$ 次调用MAX-HEAPIFY，因此构造最大堆的时间复杂度是 $O(n\lg{n})$ .</p><h2 id="排序：HEAPSORT"><a href="#排序：HEAPSORT" class="headerlink" title="排序：HEAPSORT"></a>排序：HEAPSORT</h2><p><strong>算法思路</strong>： </p><ol><li>初始化时，调用BUILD-MAX-HEAP将输入数组A[1..n]建成最大堆，其中 n = A.length。</li><li>调用后，最大的元素在A[1]，将A[1]和A[n]互换，可以把元素放在正确的位置。</li><li>将n结点从堆中去掉(通过减少A.heap-size实现)，剩余结点中，原来根的孩子仍是最大堆，但根结点可能会违背堆的性质，调用MAX-HEAPIFY(A, 1)，从而构造一个新的最大堆。</li><li>重复步骤3，直到堆的大小从n-1降为2.</li></ol><p><strong>HEAPSORT(A)</strong> </p><ul><li>功能：利用堆对数组排序</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BUILD<span class="literal">-MAX</span><span class="literal">-HEAP</span>(A)</span><br><span class="line"><span class="keyword">for</span> i = A.length downto <span class="number">2</span></span><br><span class="line">exchange A[<span class="number">1</span>] with A[<span class="type">i</span>]</span><br><span class="line">A.heap<span class="literal">-size</span> = A.heap<span class="literal">-size</span> - <span class="number">1</span></span><br><span class="line">MAX<span class="literal">-HEAPIFY</span>(A, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下图为调用HEAPSORT的过程图：</p><p><a href="https://imgchr.com/i/NhgjyT"><img src="https://s1.ax1x.com/2020/06/29/NhgjyT.md.png" alt="NhgjyT.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：</p><p>建堆BUILD-MAX-HEAP的时间复杂度为 $O(n\lg{n})$ ，n-1次调用MAX-HEAPIFY的时间复杂度为 $O(n\lg{n})$ ，所以堆排序的时间复杂度为 $O(n\lg{n})$ .</p><h2 id="堆的应用：优先队列"><a href="#堆的应用：优先队列" class="headerlink" title="堆的应用：优先队列"></a>堆的应用：优先队列</h2><p>这里关注如何用最大堆实现最大优先队列。</p><p><strong>优先队列(priority queue)：</strong> </p><p>一种用来维护由一组元素构成的集合S的数据结构，其中每一个元素都有一个相关的值，称为关键字(key)。</p><p><strong>（最大）优先队列支持的操作</strong> ：</p><ul><li>INSERT(S, x)：把元素 $x$ 插入集合S中，时间复杂度为 $O(\lg{n})$ 。</li><li>MAXIMUM(S)：返回S中具有最大关键字的元素，时间复杂度为 $O(1)$ 。</li><li>EXTRACT-MAX(S)：去掉并返回S中的具有最大关键字的元素，时间复杂度为 $O(\lg{n})$ 。</li><li>INCREASE-KEY(S, x, k)：将元素 $x$ 的关键字值增加到k，这里假设k的大小不小于元素 $x$ 的原关键字值，时间复杂度为 $O(\lg{n})$ 。</li></ul><h3 id="MAXIMUM"><a href="#MAXIMUM" class="headerlink" title="MAXIMUM"></a>MAXIMUM</h3><p>将集合S已建立最大堆的前提下，调用HEAP-MAXIMUM在 $\Theta(1)$ 实现MAXIMUM的操作。</p><p><strong>HEAP-MAXIMUM(A)</strong> </p><ul><li>功能：实现最大优先队列MAXIMUM的操作，即返回集合中最大关键字的元素。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> A[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><strong>时间复杂度分析</strong> ：$\Theta(1)$ </p><h3 id="EXTRACT-MAX"><a href="#EXTRACT-MAX" class="headerlink" title="EXTRACT-MAX"></a>EXTRACT-MAX</h3><p>类似于HEAPSORT的过程。</p><ul><li><p>A[1]为最大的元素，A[1]的孩子都是最大堆。</p></li><li><p>将A[1]和A[heap-size]交换，减少堆的大小(heap-size)。</p></li><li><p>此时根结点的孩子满足最大堆，而根不一定满足最大堆性质，维护一下当前堆。</p></li></ul><p><strong>HEAP-EXTRACT-MAX(A)</strong> </p><ul><li>功能：实现最大优先队列EXTRACT-MAX的操作，即去掉并返回集合中最大关键字的元素。</li></ul><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> A.heap-<span class="keyword">size</span> &lt; <span class="number">1</span></span><br><span class="line"><span class="keyword">error</span> <span class="string">"heap underflow"</span></span><br><span class="line"><span class="keyword">max</span> = A[<span class="number">1</span>]</span><br><span class="line">A[<span class="number">1</span>] = A[A.heap-<span class="keyword">size</span>]</span><br><span class="line">A.heap-<span class="keyword">size</span> = A.heap-<span class="keyword">size</span> - <span class="number">1</span></span><br><span class="line">MAX-HEAPIFY(A, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">max</span></span><br></pre></td></tr></table></figure><p><strong>时间复杂度分析</strong> ：$O(\lg{n})$ .</p><h3 id="INCREASE-KEY"><a href="#INCREASE-KEY" class="headerlink" title="INCREASE-KEY"></a>INCREASE-KEY</h3><p>如果增加A[i]的关键词，可能会违反最大堆的性质，所以实现HEAP-INCREASE-KEY的过程类似插入排序：从当前i结点到根结点的路径上为新增的关键词寻找恰当的插入位置。</p><ol><li><p>当前元素不断和其父结点比较，如果当前元素的关键字更大，则和父结点进行交换。</p></li><li><p>步骤1不断重复，直至当前元素的关键字比父结点小。</p></li></ol><p><strong>HEAP-INCREASE-KEY(A, i, key)</strong> </p><ul><li>功能：实现最大优先队列INCREASE-KEY的功能，即将A[i]的关键字值增加为key.</li><li>参数：i为待增加元素的下标，key为新关键字值。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> key &lt; A[<span class="type">i</span>]</span><br><span class="line">error <span class="string">"new key is smaller than current key"</span></span><br><span class="line">A[<span class="type">i</span>] = key</span><br><span class="line"><span class="keyword">while</span> i &gt; <span class="number">1</span> and A[<span class="type">PARENT</span>(<span class="type">i</span>)] &lt; A[<span class="type">i</span>]</span><br><span class="line">exchange A[<span class="type">i</span>] with A[<span class="type">PARENT</span>(<span class="type">i</span>)]</span><br><span class="line">i = PARENT(i)</span><br></pre></td></tr></table></figure><p>下图展示了HEAP-INCREASE-KEY的过程：</p><p><a href="https://imgchr.com/i/Nh2Sw4"><img src="https://s1.ax1x.com/2020/06/29/Nh2Sw4.md.png" alt="Nh2Sw4.md.png"></a> </p><p><strong>时间复杂度分析</strong> ：$O(\lg{n})$ </p><h3 id="INSERT"><a href="#INSERT" class="headerlink" title="INSERT"></a>INSERT</h3><p>如何插入一个元素扩展最大堆？</p><ol><li>先通过增加一个关键字值为 $-\infin$ 的叶子结点扩展最大堆。</li><li>再调用HEAP-INCREASE-KEY过程为新的结点设置对应的关键字值。</li></ol><p><strong>MAX-HEAP-INSERT(A, key)</strong></p><ul><li>功能：实现最大优先队列的INSERT功能，即将关键字值为key的新元素插入到最大堆中。</li><li>参数：key是待插入元素的关键字值。</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A.heap<span class="literal">-size</span> = A.heap<span class="literal">-size</span> + <span class="number">1</span></span><br><span class="line">A[<span class="type">A.heap</span>-<span class="type">size</span>] = -∞</span><br><span class="line">HEAP<span class="literal">-INCREASE</span><span class="literal">-KEY</span>(A, A.heap<span class="literal">-size</span>, key)</span><br></pre></td></tr></table></figure><p><strong>时间复杂度分析</strong> ：$O(\lg{n})$ .</p><h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><p>对于包含 $n$个数的输入数组来说，快速排序是一个最坏情况时间复杂度为 $\Theta(n^2)$ 的排序算法。</p><p>虽然最坏情况时间复杂度很差，但是快速排序通常是实际排序应用中最好的选择，因为他的平均性能非常好：他的期望时间复杂度为 $\Theta(n\lg{n})$ ，而且 $\Theta(n\lg{n})$ 中隐含的常数因子非常小。</p><p>另外，它还能进行原址排序。</p><h3 id="分治-1"><a href="#分治-1" class="headerlink" title="分治"></a>分治</h3><p>对A[p..r]子数组进行快速排序的分治过程：</p><ul><li><p>分解：</p><p>数组A[p..r]被划分为两个（可能为空）的子数组A[p..q-1]和A[q+1..r]。</p><p>使得A[p..q-1]中的每个元素都小于等于A[q]，A[q+1..r]中的每个元素都大于等于A[q]。</p><p>其中计算下标q也是分解过程的一部分。</p></li><li><p>解决：通过递归调用快速排序，对子数组A[p..q-1]和A[q+1..r]进行排序。</p></li><li><p>合并：因为子数组都是原址排序的，所以不需要合并操作，A[p..r]已经排好序。</p></li></ul><h3 id="快速排序：QUICKSORT"><a href="#快速排序：QUICKSORT" class="headerlink" title="快速排序：QUICKSORT"></a>快速排序：QUICKSORT</h3><p>按照分治的过程。</p><p><strong>QUICKSORT(A, p, r)</strong> </p><ul><li>功能：快速排序子数组A[p..r]</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> p &lt; r</span><br><span class="line">q = PARTITION(A, p, r)</span><br><span class="line">QUICKSORT(A, p, q<span class="literal">-1</span>)</span><br><span class="line">QUICKSORT(A, q+<span class="number">1</span>, r)</span><br></pre></td></tr></table></figure><h3 id="数组的划分：PARTITION"><a href="#数组的划分：PARTITION" class="headerlink" title="数组的划分：PARTITION"></a>数组的划分：PARTITION</h3><p>快速排序的关键部分就在于如何对数组A[p..r]进行划分，即找到位置q。</p><p><strong>PARTITION(A, p, r)</strong> </p><ul><li>功能：对子数组A[p..r] 划分为两个子数组A[p..q-1]和子数组A[q+1..r]，其中A[p..q-1] 小于等于A[q]小于等于A[q+1..r]</li><li>返回：数组的划分下标q</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = A[<span class="type">r</span>]</span><br><span class="line">i = p - <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> j = p to r - <span class="number">1</span> // j is pointer <span class="keyword">for</span> comparation</span><br><span class="line"><span class="keyword">if</span> A[<span class="type">j</span>] &lt;= x</span><br><span class="line">i = i+<span class="number">1</span></span><br><span class="line">exchange A[<span class="type">i</span>] with A[<span class="type">j</span>]</span><br><span class="line">exchange A[<span class="type">i</span>+<span class="number">1</span>] with A[<span class="type">r</span>]</span><br><span class="line"><span class="keyword">return</span> i+<span class="number">1</span></span><br></pre></td></tr></table></figure><p>PARTITION总是选择一个 $x=A[r]$ 作为主元(pivot element)，并围绕它来划分子数组A[p..r]。</p><p>在循环中，数组被划分为下图四个（可能为空的）区域：</p><p><a href="https://imgchr.com/i/NhgvOU"><img src="https://s1.ax1x.com/2020/06/29/NhgvOU.md.png" alt="NhgvOU.md.png"></a> </p><ol><li>$p\leq k\leq i$ ，则 $A[k]\leq x$ .</li><li>$i+1\leq k \leq j-1$ ，则 $A[k]&gt;x$.</li><li>$k = r$ ，则 $A[k]=x$ .</li><li>$j\leq k\leq r-1$ ，则 $A[k]$ 与 $x$ 无关。</li></ol><p>下图是将样例数组PARTITION的过程：</p><p><a href="https://imgchr.com/i/Nh2pTJ"><img src="https://s1.ax1x.com/2020/06/29/Nh2pTJ.md.png" alt="Nh2pTJ.md.png"></a> </p><h3 id="快速排序的性能"><a href="#快速排序的性能" class="headerlink" title="快速排序的性能"></a>快速排序的性能</h3><p>[*]待补充</p><h3 id="快速排序的随机化版本"><a href="#快速排序的随机化版本" class="headerlink" title="快速排序的随机化版本"></a>快速排序的随机化版本</h3><p>与始终采用 $A[r]$ 作为主元的方法不同，随机抽样是从子数组A[p..r]随机选择一个元素作为主元。</p><p>加入随机抽样，在平均情况下，对子数组A[p..r]的划分是比较均匀的。</p><p><strong>RANDOMIZED-PEARTITION(A, p, r)</strong> </p><ul><li>功能：数组划分PARTITION的随机化主元版本</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i = RANDOM(p, r)</span><br><span class="line">exchange A[<span class="type">r</span>] with A[<span class="type">i</span>]</span><br><span class="line"><span class="keyword">return</span> PARTITION(A, p, r)</span><br></pre></td></tr></table></figure><p><strong>RANDOMIZED-QUICKSORT(A, p, r)</strong> </p><ul><li>功能：使用随机化主元的快速排序</li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> p &lt; r</span><br><span class="line">q = RANDOMIZED<span class="literal">-PARTITION</span>(A, p, r)</span><br><span class="line">RANDOMIZED<span class="literal">-QUICKSORT</span>(A, p, q<span class="literal">-1</span>)</span><br><span class="line">RANDOMIZED<span class="literal">-QUICKSORT</span>(A, q+<span class="number">1</span>, r)</span><br></pre></td></tr></table></figure><h1 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h1><p><strong>计数排序</strong> ：</p><p>假设 $n$ 个输入元素中的每一个都是在 0到 $k$ 区间内到一个整数，其中 $k$ 为某个整数。当 $k = O(n)$ 时，排序的运行时间为 $\Theta(n)$ .</p><p><strong>计数排序的思想</strong> ：</p><p>对每一个输入元素 $x$，确定小于 $x$ 的元素个数。利用这个信息，就可以直接把 $x$ 放在输出数组正确的位置了。</p><p><strong>COUNTING-SORT(A, B, k)</strong> </p><ul><li>功能：计数排序</li><li>参数：<ul><li>A[1..n]输入的待排序数组，A.length = n</li><li>B[1..n] 存放排序后的输出数组；</li><li>临时存储空间 C[0..k] ，A[1..n]中的元素大小不大于k.</li></ul></li></ul><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">let C[<span class="number">0</span><span class="type">..k</span>] be a new array</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">0</span> to k</span><br><span class="line">C[<span class="type">i</span>] = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j = <span class="number">1</span> to A.length</span><br><span class="line">C[<span class="type">A</span>[<span class="type">j</span>]] = C[<span class="type">A</span>[<span class="type">j</span>]] + <span class="number">1</span></span><br><span class="line">//C[<span class="type">i</span>] now contains the number of elements equal to i.</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to k</span><br><span class="line">C[<span class="type">i</span>] = C[<span class="type">i</span>] + C[<span class="type">i</span>-<span class="number">1</span>]</span><br><span class="line">//C[<span class="type">i</span>] now contains the number of elements less than or equal to i.</span><br><span class="line"><span class="keyword">for</span> j = A.length downto <span class="number">1</span></span><br><span class="line">B[<span class="type">C</span>[<span class="type">A</span>[<span class="type">j</span>]]] = A[<span class="type">j</span>]</span><br><span class="line">C[<span class="type">A</span>[<span class="type">j</span>]] = C[<span class="type">A</span>[<span class="type">j</span>]] - <span class="number">1</span></span><br></pre></td></tr></table></figure><p>下图是计数排序的过程：</p><p><a href="https://imgchr.com/i/NhgzmF"><img src="https://s1.ax1x.com/2020/06/29/NhgzmF.md.png" alt="NhgzmF.md.png"></a> </p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Introduction to Algorithms.</li><li>算法导论</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章review了算法中的排序算法，包括冒泡排序、插入排序、归并排序、堆排序（以及用堆实现优先队列）、快速排序和计数排序。&lt;/p&gt;
&lt;p&gt;分别从算法思路、算法伪代码实现、算法流程、算法时间复杂度四个方面阐述每个算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法导论" scheme="https://f7ed.com/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/"/>
    
    
      <category term="Algorithms" scheme="https://f7ed.com/tags/Algorithms/"/>
    
      <category term="Intro-to-Algorithms" scheme="https://f7ed.com/tags/Intro-to-Algorithms/"/>
    
      <category term="Sort" scheme="https://f7ed.com/tags/Sort/"/>
    
  </entry>
  
  <entry>
    <title>「Cryptography-Dan」:Stream Cipher 3</title>
    <link href="https://f7ed.com/2020/06/26/StreamCipher3/"/>
    <id>https://f7ed.com/2020/06/26/StreamCipher3/</id>
    <published>2020-06-25T16:00:00.000Z</published>
    <updated>2020-07-03T08:45:16.978Z</updated>
    
    <content type="html"><![CDATA[<p>Stream Cipher的第三篇文章。</p><p>文章主要分为两部分，前部分逐步定义Secure PRG的定义，通过引入statistical test（统计测试）和Advantage（优势）得出当且仅当PRG is unpredictable,PRG is secure的结论。</p><p>后部分介绍了密码学中的一个重要概念Semantic Security的定义，通过引入 computationally indistinguishable(计算上不可区分)的概念给出定义，并证明了OTP的语意安全和在安全PRG条件下的流密码的语意安全，得出如果流密码中使用的PRG is secure,那么流密码就具备semantic security。</p><p>文章开头，也简单介绍了密码学中negligible和non-negligible的含义。</p><a id="more"></a><h1 id="Negligible-and-non-negligible"><a href="#Negligible-and-non-negligible" class="headerlink" title="Negligible and non-negligible"></a>Negligible and non-negligible</h1><p><strong>In practice:</strong> $\epsilon$ is a scalar and </p><ul><li>$\epsilon$ non-neg: $\epsilon \geq 1/2^{30}$ (likely to happen over 1GB of data)</li><li>$\epsilon$ negligible: $\epsilon \leq 1/2^{80}$ (won’t happen over life of key)</li></ul><p>在实践中，$\epsilon$ 是一个数值，如果是non-neg不可忽略的话，大约在1GB的数据下就会发生，如果是可忽略的值，在密钥的生存周期内基本不会发生。</p><p><strong>In theory:</strong> $\epsilon$ is a function  $\varepsilon: Z^{\geq 0} \rightarrow R^{\geq 0}$  and</p><ul><li>$\epsilon$ non-neg:  $\exists d: \epsilon(\lambda)\geq 1/\lambda^d$ ($\epsilon \geq 1/\text{poly} $, for many $\lambda$ )</li><li>$\epsilon$ negligible:  $\forall d, \lambda \geq \lambda_{d}:  \varepsilon(\lambda) \leq 1 / \lambda^{d}$  ( $\epsilon \leq 1/\text{poly}$, for large $\lambda$ )</li></ul><p>[0]（理论中，这个还不太理解，待补充。）</p><h1 id="PRG-Security-Defs"><a href="#PRG-Security-Defs" class="headerlink" title="PRG Security Defs"></a>PRG Security Defs</h1><p>Let  $G:K\longrightarrow \{0,1\}^n$  be a PRG.</p><p><u>Goal:</u> define what it means that </p><p>[  $k\stackrel{R}{\longleftarrow} \mathcal{K}$   , output G(k) ] is “indistinguishable” from [  $r\stackrel{R}{\longleftarrow} \{0,1\}^n$   , output r] .</p><p>【使得PGR的输出和真随机是不可区分的】（ $\stackrel{R}{\longleftarrow}$ 的意思是在均匀分布中随机取）</p><p>下图中，红色的圈是全部的 ${0,1}^n$ 串，按照定义是均匀分布。而粉色G()是PRG的输出，由于seed很小，相对于全部的 ${0,1}^n$ ，所以G()的输出范围也很小。</p><p><a href="https://imgchr.com/i/NsuU4s"><img src="https://s1.ax1x.com/2020/06/26/NsuU4s.png" alt="NsuU4s.png"></a> </p><p>因此，attacker观测G(k)的输出，是不能和uniform distribution（均匀分布）的输出区分开。</p><p>这也就是我们所想构造的安全PGR的目标。</p><h2 id="Statistical-Tests"><a href="#Statistical-Tests" class="headerlink" title="Statistical Tests"></a>Statistical Tests</h2><p>Statistical test on ${0,1}^n$ ：有一个算法A，$x\in{0,1}^n$ ,A(x) 根据算法定义输出”0”或”1”.</p><p>统计测试是自定义的。</p><p>Example：</p><ol><li><p>A(x)=1 if  $| \#0(x)-\#1(x)|\leq 10\cdot\sqrt{n}$   </p><p>【期望串中0、1的数目差不多，这样look random】</p></li><li><p>A(x)=1 if  $|\#00(x)-\frac{n}{4}\leq10\cdot\sqrt{n}$   </p><p>【期望Pr(00)=1/4 ,串中00出现的概率为1/4，认为是look random】</p></li><li><p>A(x)=1if max-run-of-0(x) &lt; 10·log(n)</p><p>【期望串中0的最大游程不要超过规定值】</p></li></ol><p>上面的第三个例子，如果输出为全1，满足上述的统计条件输出1，但全1串看起来并不random。</p><p>统计测试也由于是自定义的，所以通过统计测试的也不一定是random，其PRG也不一定是安全的。</p><p>所以，如何评估一个统计测试的好坏？</p><p>下面引入一个重要的定义advantage，优势。</p><h2 id="Advantage"><a href="#Advantage" class="headerlink" title="Advantage"></a>Advantage</h2><p>引入Advantage（优势）来评估一个统计测试的好坏。</p><p>Let G: $k \rightarrow \{0,1\}^n$  be a PRG and A a stat. test on ${0,1}^n$ </p><p>【G是一个PRG，A是一个对01串的统计测试】</p><p>Define: the advantage of statisticaltest A relative to PRG G Adv$_\text{PRG}[A,G]$ </p>  $\text{Adv}_\text{PRG}[A,G]=|Pr[A(G(k))=1]-Pr[A(r)=1]|\in[0,1]$   ,  $k\stackrel{R}{\longleftarrow} \mathcal{K}, r\stackrel{R}{\longleftarrow}\{0,1\}^n$  <p>【定义：Adv$_\text{PRG}[A,G]$ 为统计测试A对于PRG G的优势为统计测试以PRG作为输入输出为1的概率 减去 统计测试以truly random string 作为输入输出为1的概率】</p><ul><li>Adv close to 1 : 统计测试能区分PRG的输出和truly random string，即adversary可以利用区分PRG的输出和random的这一点从而破解系统。</li><li>Adv close to 0 : 统计测试不能区分PRG的输出和truly random string, 即adversary认为PRG的输出和random别无二致。</li></ul><blockquote><p><a href="https://en.wikipedia.org/wiki/Advantage_(cryptography)">Advantage</a> 优势[1]</p><p>In cryptography, an adversary’s advantage is a measure of how successfully it can attack a cryptographic algorithm, by distinguishing it from an idealized version of that type of algorithm.Note that in this context, the “adversary” is itself an algorithm and not a person. A cryptographic algorithm is considered secure if no adversary has a non-negligible advantage, subject to specified bounds on the adversary’s computational resources (see concrete security). </p><p>在密码学中，adversary的优势是评估它通过某种理想算法破解一个加密算法的成功尺度。</p><p>这里的adversary是一种破解算法而不是指攻击者这个人。</p><p>当没有 adversary对该加密算法有不可忽略的优势时，该加密算法被认为是安全的，因为adversary只有有限的计算资源。</p></blockquote><p>e.g.1 : A(x) = 0，统计测试A对PRG的任何输出都输出0，则Adv[A,G] = 0.</p><p>e.g.2 : </p><p>G: $k \rightarrow {0,1}^n$ satisfies msb(G(k))=1 for 2/3 of keys in K.</p><p>Define statistical test A(x) as : if[ msb(x)=1 ] output “1” else output “0”</p><p>【PRG G(k)的2/3的输出的最高有效位是1，定义统计测试A(x),输入的最高有效位为1输出1，否则输出0】</p><blockquote><p>msb: most significant bit,最高有效位。</p></blockquote><p>则 Adv[A,G] = | Pr[A(G(k))] - Pr[A(r)] | = 2/3 - 1/2 = 1/6</p><p>即 A can break the generator G with advantage 1/6, PRG G is not good. </p><h2 id="Secure-PRGs-crypto-definition"><a href="#Secure-PRGs-crypto-definition" class="headerlink" title="Secure PRGs: crypto definition"></a>Secure PRGs: crypto definition</h2><p><u>Def:</u> We say that G: $k \rightarrow {0,1}^n$ is <strong>secure PRG</strong></p><p> if $\forall$ “eff” stat. tests A : Adv$_\text{PRG}$ [A,G]  is “negligible”.</p><p>这里的”eff”,指多项式时间内。</p><p>这个定义，“所有的统计测试”，这一点难以达到，因此没有provably secure PRGs。</p><p>但我们有heuristic candidates.（有限的stat. test 能满足）</p><h3 id="Easy-fact-a-secure-PRG-is-unpredictable"><a href="#Easy-fact-a-secure-PRG-is-unpredictable" class="headerlink" title="Easy fact: a secure PRG is unpredictable"></a>Easy fact: a secure PRG is unpredictable</h3><p>证明命题： a secure PRG is unpredictable.</p><p>即证明其逆否命题： PRG is predictable is insecure。</p><p>suppose A is an eddicient algorithm s.t. $\text{Pr}_{k\stackrel{R}{\longleftarrow}\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\frac{1}{2} + \epsilon$    for non-negligible $\epsilon$ .</p><p>【 算法A是一个有效的预测算法， $\text{Pr}_{k\stackrel{R}{\longleftarrow}\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]>\frac{1}{2} + \epsilon$   , $\epsilon$ 是不可忽略的值，即A能以大于1/2的概率推测下一位。】</p><p>Adversary能利用算法A来区分这个PRG和random依次来破解系统。</p><p>Define statistical test B as: B(x)=1 if $A(x|<em>{1,…,i})=x</em>{i+1}$ , else B(x)=0.</p><p>【定义一个统计测试B，如果预测算法A预测下一位正确输出1，否则输出0】</p><hr><p>计算Adv$_\text{PRG}$ :</p><ul><li> $r\stackrel{R}{\longleftarrow}\{0,1\}^n$ : Pr[B(r) = 1] = 1/2</li><li> $r\stackrel{R}{\longleftarrow}\{0,1\}^n$  : Pr[B(G(k)) = 1] = 1/2+ $\epsilon$ </li><li>Adv$_\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | &gt; $\epsilon$ </li></ul><p>Adversary 能以 $\epsilon$ 的优势区分PRG和random，因此PRG 不安全。</p><p>所以，如果A是一个好的预测算法，那么B就是一个好的统计算法，Adversary就能通过B以 $\epsilon$ 的优势破解系统。</p><p>在此，证明了 if A can predict PRG, PRG is insecure $\Rightarrow$ <strong>A secure PRG is unpredictable.</strong> </p><h3 id="Thm-Yao’82-an-unpredictable-PRG-is-secure"><a href="#Thm-Yao’82-an-unpredictable-PRG-is-secure" class="headerlink" title="Thm(Yao’82): an unpredictable PRG is secure"></a>Thm(Yao’82): an unpredictable PRG is secure</h3><p>上节证明了A secure PRG is unpredictable.</p><p>在1982 Yao 的论文[2]中证明了其逆命题： an unpredictable PRG is secure.</p><p>G: $k \rightarrow {0,1}^n$ be PRG</p><p><strong>“Thm</strong>“ : <strong>if $\forall i \in$ {0,…, n-1} PRG G is unpredictable at pos. i then G is a secure PRG.</strong></p><p>【定理：如果在任何位置PRG都是不可预测的，那么PRG就是安全的】</p><p><strong>Proof：</strong> </p><ul><li>A: 预测算法： $\forall i \quad\text{Pr}_{k\stackrel{R}{\longleftarrow}\mathcal{K}}[A(G(k))|_{1,...,i}=G(k)|_{i+1}]=\frac{1}{2} $  </li><li>B:统计测试： B(x)=1 if $A(x|<em>{1,…,i})=x</em>{i+1}$ , else B(x)=0.</li><li>Adv$_\text{PRG}$[B,G] = |Pr[B(r)=1] - Pr[B(G(k))=1] | = 0 </li></ul><p>If next-bit predictors cannot distinguish G from random then no statistial test can!</p><p>【next-bit predictor指用预测算法的统计测试】</p><p>e.g.</p><p> Let G: $k \rightarrow {0,1}^n$ be PRG such that from the last n/2 bits of G(k) it is easy to compute the first n/2 bits.</p><p>Is G predictable for som i $\in$ {0, …, n-1} ?</p><p>: Yes.当n=2时，可以预测出第一位。 </p><p><strong>在此，可以得出结论：</strong></p><p><strong>Adversary不可区分PRG的输出和truly random string时被认为是安全的。</strong></p><p><strong>当且仅当PRG在任意位置不可预测时，PRG是安全的。</strong> </p><h2 id="More-generally-computationally-indistinguishable"><a href="#More-generally-computationally-indistinguishable" class="headerlink" title="More generally: computationally indistinguishable"></a>More generally: computationally indistinguishable</h2><p>Let P1 and P2 be two distributions over ${0,1}^n$ </p><p><u>Def</u> : We say that P1 and P2 are <strong>computationally indistinguishable</strong> (denoted $P_1\approx_p P_2$) </p><p>If $\forall$ “eff” stat. tests A  $\left|\text{Pr}_{x\leftarrow_{P_1}}[A(x)=1]-\text{Pr}_{x\leftarrow_{P_2}}[A(x)=1]\right|<\text{negligible}$   .</p><p>【P1,P2都是01串上的两个分布，且对任意的有效统计测试满足上式，则认为P1和P2两个分布在计算上不可区分，记做$P_1\approx_p P_2$】</p><p>所以，安全的PRG等价于G(k)的分布和均匀分布计算上不可区分，即 $\{\mathrm{k} \stackrel{\mathrm{R}}{\longleftarrow}{\mathrm{K}}: \mathrm{G}(\mathrm{k})\} \approx_{\mathrm{p}} \text { uniform }\left(\{0,1\}^{\mathrm{n}}\right)$  </p><h1 id="Semantic-Security"><a href="#Semantic-Security" class="headerlink" title="Semantic Security"></a>Semantic Security</h1><p>上文详细了定义secure PRGs 的过程，本节的目标是定义”secure” stream cipher,安全流密码。</p><p><strong>What is a secure cipher?</strong></p><p>attacker abilities: obtains one ciphertext(唯密文攻击)</p><p>下面这些可能的安全需求是否能定义是安全的加密解密算法？</p><p>Possible security requirements：</p><ol><li><p>attacker cannot recover secret key.</p><p>attacker不能恢复密钥。</p><p>E(k,m)=m直接输出明文，这个算法满足条件，但显然不安全。</p></li><li><p>attacker cannot recover all of plaintext.</p><p>attacker不能恢复所有的明文。</p><p>E(k, m0||m1) = m0||k $\oplus$ m1,该算法泄漏了一半的明文，不安全。</p></li><li><p>Shannon’s idea: CT should reveal no “info” about PT.</p></li></ol><h2 id="Recall-Shannon‘s-perfect-secrecy"><a href="#Recall-Shannon‘s-perfect-secrecy" class="headerlink" title="Recall Shannon‘s perfect secrecy"></a>Recall Shannon‘s perfect secrecy</h2><p>在<a href="/2020/03/15/StreamCipher1/" title="这篇文章1.2.2">这篇文章1.2.2</a> 定义了perfect secrecy。</p><p>A cipher $(E,D)$ over  $\mathcal{(K,M,C)}$ has <strong>perfect security</strong> if  $\forall m_0,m_1 \in \mathcal{M}\ (|m_0|=|m_1|) \quad \text{and} \quad \forall c\in \mathcal{C} $ </p>$$Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \qquad \text{where} \ k\overset{R}{\longleftarrow}\mathcal{K}$$<p>下面根据perfect security的定义逐步放宽条件定义：</p><p>Let (E,D) be a cipher over (K,M,C)</p><ul><li><p><a href="https://imgchr.com/i/NsutEQ"><img src="https://s1.ax1x.com/2020/06/26/NsutEQ.md.png" alt="NsutEQ.md.png"></a> </p><p>上图中的定义就是perfect security的定义，密钥加密m0和m1的分布完全相同。</p><p>引入上文中的computationally indistinguishable（计算不可区分）的概率放宽条件，得到下面的定义。</p></li><li><p><a href="https://imgchr.com/i/NsuGDS"><img src="https://s1.ax1x.com/2020/06/26/NsuGDS.md.png" alt="NsuGDS.md.png"></a></p><p>密钥加密m0和m1的分布计算上不可区分。</p></li><li><p>但枚举所有的m0和m1的定义也太强了，因此希望能让adversary列举计算上可列举的m0和m1.</p></li></ul><h2 id="Semantic-Security-for-OTP"><a href="#Semantic-Security-for-OTP" class="headerlink" title="Semantic Security for OTP"></a>Semantic Security for OTP</h2><h3 id="attack-game"><a href="#attack-game" class="headerlink" title="attack game"></a>attack game</h3><p>在定义OTP的semantic security （语意安全）之前，定义一个attack game。</p><p><strong>Attack Game</strong>[3]: For a given cipher $\mathcal{E}$ = (E, D), deﬁned over (K, M , C), and for a given adversary $\mathcal{A}$ , we deﬁne two experiments, Experiment 0 and Experiment 1. For b = 0, 1, we deﬁne</p><p>【定义一个adversary $\mathcal{A}$，两个实验也对应两个challenger，实验定义如下】</p><p><strong>Experiment b:</strong></p><ul><li><p>The adversary computes m0 , m1 ∈ M , of the same length, and sends them to the challenger.</p><p>【adversary向challenger发m0和m1，他们长度相同】</p></li><li><p>The challenger computes $k\stackrel{R}{\leftarrow}\mathcal{K}$, $c\stackrel{R}{\leftarrow}E(k,m_b)$, and sends c to the adversary.</p><p>【实验b中challenger随机选取一个密钥k,用密钥加密mb,再将结果c返回给adversary】</p></li><li><p>The adversary outputs a bit $\hat{b}\in$  { 0, 1 } .</p><p>【adversary收到c，猜测c是加密m0还是m1的，输出 $\hat{b}$ 】</p></li></ul><p>过程如下图所示，对于adversary $\mathcal{A}$ 来说，就像只有一个challenger，没有EXP(0)和EXP(1)的区别，因为它本来就是猜测challenger发送的c是加密m0所的还是m1所的。而对于challenger来说，才有EXP(0)和EXP(1)，在EXP(0)中，加密算法随机选择密钥加密m0返回，在EXP(1)中，加密算法随机选择密钥加密m1返回。</p><p><a href="https://imgchr.com/i/Nsuu4A"><img src="https://s1.ax1x.com/2020/06/26/Nsuu4A.md.png" alt="Nsuu4A.md.png"></a> </p><p>for b=0,1 $W_b$ = [event that $\mathcal{A}$  output 1 in experiment b].</p><p>【定义事件 $W_b$ 为在实验EXP(b)中$\mathcal{A}$ 输出1】</p><p><u>Def</u>： $\mathcal{A}$‘s <strong>semantic security advantage</strong> with respect to $\mathcal{E}$ as </p>$$\operatorname{Adv_{SS}}[\mathcal{A}, \mathcal{E}]:=\left|\operatorname{Pr}\left[W_{0}\right]-\operatorname{Pr}\left[W_{1}\right]\right|$$<p>【定义adversary $\mathcal{A}$ 对于cipher $\mathcal{E}$ 的语意安全优势为上式】</p><p>Note that in the above game, the events W0 and W1 are deﬁned with respect to the probability space determined by the random choice of k, the random choices made (if any) by the encryption algorithm, and the random choices made (if any) by the adversary. The value Advss[ A , E] is a number between 0 and 1.</p><p>【注意：上面的游戏中，W0和W1的概率空间分布是与密钥的随机选择，加密算法的随机选择（<strong>该加密算法选择的是challenger0加密的m0还是challenger1加密的m1，adversary并不知道</strong>）和adversary的”随机”输出（按照adversary的判断方式的)有关的】</p><p>当challenger用随机密钥加密m0/m1时，观测adversary的行为是否相同，如果   $\left|\operatorname{Pr}\left[W_{0}\right]-\operatorname{Pr}\left[W_{1}\right]\right|$   等于0，说明adversary无法区分这两个实验，如果  $\left|\operatorname{Pr}\left[W_{0}\right]-\operatorname{Pr}\left[W_{1}\right]\right|$   等于1，则adversary可以区分这两个实验。</p><h3 id="semantic-security"><a href="#semantic-security" class="headerlink" title="semantic security"></a>semantic security</h3><p><u>Def</u>：<strong>semantic security</strong>：A cipher $\mathcal{E}$ is semantially secure if for all efficient $\mathcal{A}$ , the value  $\operatorname{Adv_{SS}}[\mathcal{A}, \mathcal{E}]$  is negligible.</p><p>当所有的有效的攻击算法 $\mathcal{A}$ 对于加密算法 $\mathcal{E}$ 的语意安全优势是可忽略的，则认为加密算法 $\mathcal{E}$ 是语意安全的。</p><p>即没有有效的攻击算法能区分对m0和m1对加密结果。</p><p>即任意密钥加密m0 m1的分布是计算上不可区分的，for all explicit m0, m1 $\in M:$   $\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{0}\right)\right\} \approx_{\mathrm{p}}\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{1}\right)\right\}$ .</p><h3 id="example-1"><a href="#example-1" class="headerlink" title="example 1"></a>example 1</h3><p>Suppose efficient A can always deduce LSB of PT from CT.</p><p>【假设算法A总能从CT中推断出PT的最低有效位】</p><p>在下图cipher系统中，Adversary B算法向challenger发送的m0和m1，其发送的m0 m1特点是LSB(m0)=0,LSB(m1)=1。</p><p><a href="https://imgchr.com/i/NsuJHg"><img src="https://s1.ax1x.com/2020/06/26/NsuJHg.md.png" alt="NsuJHg.md.png"></a> </p><p>当adversary B收到密文c时，利用有效的算法A推断出对应明文的最低有效位LSB，如果是0，则输出0，即认为是实验0中challenger加密的m0，反之输出1。</p><p>adversary B对于cipher $\mathcal{E}$ 的优势是：  $\operatorname{Adv_{SS}}[\mathcal{B}, \mathcal{E}]:=\left|\operatorname{Pr}\left[\text{EXP}(0)=1\right]-\operatorname{Pr}\left[\text{EXP}(1)=1\right]\right|=|0-1|=1$ </p><p> 所以该cipher $\mathcal{E}$ 不具有语意安全。</p><p>不仅是LSB，<strong>如果adversary能从密文中学到明文的任何一位，则该cipher都不具有semantic security。</strong></p><h3 id="example-2-OTP"><a href="#example-2-OTP" class="headerlink" title="example 2: OTP"></a>example 2: OTP</h3><p>OTP虽然不实用，但是具备perfect security，下面来证明OTP也具有semantic security。</p><p><a href="https://imgchr.com/i/NsunNd"><img src="https://s1.ax1x.com/2020/06/26/NsunNd.md.png" alt="NsunNd.md.png"></a> </p><p>OTP的perfect security的性质是  $Pr[E(k,m_0)=c]=Pr[E(k,m_1)=c] \qquad \text{where} \ k\overset{R}{\longleftarrow}\mathcal{K}$  ，k和任意m异或都是均匀分布，因此上图中的  $\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{0}\right)\right\} =\left\{\mathrm{E}\left(\mathrm{k}, \mathrm{m}_{1}\right)\right\}$  是同分布。</p><p>For all A :  $\operatorname{Adv}_{\operatorname{ss}}[A, \text { OTP }]=\mid \operatorname{Pr}\left[A\left(\mathbf{k} \oplus m_{0}\right)=1\right]-\operatorname{Pr}\left[A\left(\mathbf{k} \oplus m_{1}\right)=1\right]|=0$   </p><h2 id="Stream-Ciphers-are-semantically-secure"><a href="#Stream-Ciphers-are-semantically-secure" class="headerlink" title="Stream Ciphers are semantically  secure"></a>Stream Ciphers are semantically  secure</h2><p>前面介绍了什么是安全的PRG，什么是语意安全，而流密码是否具有语意安全呢？</p><p><strong>Thm:  Let G: $k \rightarrow {0,1}^n$ is a secure PRG $\Rightarrow$ stream cipher E derived from G is semantic secure.</strong></p><p> 要证明以上定理：即要证明 $\forall$ sem. sec. adversary A $\operatorname{Adv}_{\operatorname{SS}}[A,E]$  is negligible.</p><p>如果 $\forall$ sem. sec. adversary A, $\exist$ a PRG adversay B s.t.  $A d v_{s s}[A, E] \leq 2 \cdot A d v_{P R G}[B, G]$ 不等式成立，则定理得证。</p><p>因为G is a secure PRG, 根据定义，Adv$<em>\text{PRG}$ [A,G]  is negligible，所以左边$\operatorname{Adv}</em>{\operatorname{SS}}[A,E]$  is negligible.</p><h3 id="Proof-intuition"><a href="#Proof-intuition" class="headerlink" title="Proof: intuition"></a>Proof: intuition</h3><p>如果直观上证明Adv$_\text{PRG}$ [A,G]  is negligible，如下图：</p><p><a href="https://imgchr.com/i/Nsu1jf"><img src="https://s1.ax1x.com/2020/06/26/Nsu1jf.md.png" alt="Nsu1jf.md.png"></a> </p><p>（上左上右下左下右：图1234，G(k):PRG 的输出， r: truly random string）</p><p>图1和图2中，由于G is a secure PRG，因此A计算上无法区分用G(k)和 random对m0加密的结果， 即$\ E(m_0,\mathrm{G}(\mathrm{k}))\} \approx_{\mathrm{p}} \{E(m_0,r)\}$  。同理图三图四中，A计算上也无法区分用G(k)和random 对m1加密的结果，即  $\ E(m_1,\mathrm{G}(\mathrm{k}))\} \approx_{\mathrm{p}} \{E(m_1,r)\}$  。</p><p>而图2图4中，是OPT的，即r和任意m异或都是均匀分布，满足 $\left\{\mathrm{E}\left(\mathrm{r}, \mathrm{m}_{0}\right)\right\} =\left\{\mathrm{E}\left(\mathrm{r}, \mathrm{m}_{1}\right)\right\}$  是同分布.(图中应该可以写严格的等号)</p><p>所以可以直观得到  $\ E(m_0,\mathrm{G}(\mathrm{k}))\} \approx_{\mathrm{p}} E(m_1,\mathrm{G}(\mathrm{k}))\}$ ，即$\forall$ sem. sec. adversary A $\operatorname{Adv}_{\operatorname{SS}}[A,E]$  is negligible.</p><h3 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h3><p>Let A be a sem. sec. adversary.</p><p>使用PRG，即$E(m,k) = m\oplus G(k)$ 时，如下图：</p><p><a href="https://imgchr.com/i/NsuEnO"><img src="https://s1.ax1x.com/2020/06/26/NsuEnO.md.png" alt="NsuEnO.md.png"></a> </p><p> for b = 0,1 Wb = [event that b’=1]</p><p>$\operatorname{Adv}_{\operatorname{SS}}[A,E]=|\text{Pr}[W_0]-\text{Pr}[W_1]|$  </p><hr><p>使用OTP时，即 $E(m,r)=m\oplus r$ 时，如下图：</p><p><a href="https://imgchr.com/i/NsuVBD"><img src="https://s1.ax1x.com/2020/06/26/NsuVBD.md.png" alt="NsuVBD.md.png"></a> </p><p>for b = 0,1 Rb = [event that b’=1]</p><p>$\operatorname{Adv}_{\operatorname{SS}}[A,OTP]=|\text{Pr}[R_0]-\text{Pr}[R_1]|=0$ </p><hr><p>如果把Pr[]的关系画在数轴上，如下图：</p><p><a href="https://imgchr.com/i/NsuZHe"><img src="https://s1.ax1x.com/2020/06/26/NsuZHe.md.png" alt="NsuZHe.md.png"></a> </p><ol><li><p>根据OTP的semantic security ，已经得到 $|\text{Pr}[R_0]-\text{Pr}[R_1]|=\operatorname{Adv}_{\operatorname{SS}}[A,OTP]=0$ ，所以Pr[R0]=Pr[R1]。</p></li><li><p>因为G是secure PRG，所以 $\exists$ adversary B满足 $|\text{Pr}[W_b]-\text{Pr}[R_b]|=\operatorname{Adv}_{\operatorname{PRG}}[B,G]$ ,如下图：</p><p><a href="https://imgchr.com/i/Nsu8u8"><img src="https://s1.ax1x.com/2020/06/26/Nsu8u8.md.png" alt="Nsu8u8.md.png"></a> </p></li></ol><p>所以根据数轴的关系，Pr[W0]和Pr[W1]的距离最大为 $2 \cdot \mathrm{Adv}_{\mathrm{PRG}}[\mathrm{B}, \mathrm{G}]$ .</p><p>即  $\Rightarrow\mathrm{Adv}<em>{\mathrm{SS}}[\mathrm{A}, \mathrm{E}]=\left|\operatorname{Pr}\left[\mathrm{W}</em>{0}\right]-\operatorname{Pr}\left[\mathrm{W}<em>{1}\right]\right| \leq 2 \cdot \mathrm{Adv}</em>{\mathrm{PRG}}[\mathrm{B}, \mathrm{G}]$  </p><p>证毕。</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>secure PRG 就是计算上无法区分G(k)的输出和truly random string。</p><p>semantic secure 就是计算上无法区分m0和m1的加密结果。</p><p>而使用secure PRG的流密码是具有semantic security的。</p><h1 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h1><ol start="0"><li><p>Negligible, super-poly, and poly-bounded functions.(Book p28)</p></li><li><p>Advantage Wiki定义：<a href="https://en.wikipedia.org/wiki/Advantage_(cryptography)">https://en.wikipedia.org/wiki/Advantage_(cryptography)</a></p></li><li><p>Yao’82: Theory and application of trapdoor functions:<a href="https://ieeexplore.ieee.org/document/4568378">https://ieeexplore.ieee.org/document/4568378</a></p></li><li><p>Dan Boneh and Victor Shoup: A Graduate Course in Applied Cryptography</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Stream Cipher的第三篇文章。&lt;/p&gt;
&lt;p&gt;文章主要分为两部分，前部分逐步定义Secure PRG的定义，通过引入statistical test（统计测试）和Advantage（优势）得出当且仅当PRG is unpredictable,PRG is secure的结论。&lt;/p&gt;
&lt;p&gt;后部分介绍了密码学中的一个重要概念Semantic Security的定义，通过引入 computationally indistinguishable(计算上不可区分)的概念给出定义，并证明了OTP的语意安全和在安全PRG条件下的流密码的语意安全，得出如果流密码中使用的PRG is secure,那么流密码就具备semantic security。&lt;/p&gt;
&lt;p&gt;文章开头，也简单介绍了密码学中negligible和non-negligible的含义。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cryptography-Dan" scheme="https://f7ed.com/categories/Cryptography-Dan/"/>
    
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="Cryptography" scheme="https://f7ed.com/tags/Cryptography/"/>
    
      <category term="StreamCipher" scheme="https://f7ed.com/tags/StreamCipher/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Recurrent Neural Network（RNN）</title>
    <link href="https://f7ed.com/2020/06/11/rnn/"/>
    <id>https://f7ed.com/2020/06/11/rnn/</id>
    <published>2020-06-10T16:00:00.000Z</published>
    <updated>2020-07-03T08:43:33.524Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中首先从RNN能解决什么问题入手，分析了RNN与一般NN的区别。<br>然后着重讲解了基于RNN的LSTM模型，包括LSTM的细节、和一般NN的区别，以及如何训练LSTM模型。<br>具体阐述了在模型（RNN类模型）训练过程中为什么会遇到剧烈抖动问题和如何解决抖动的工程解决方法。</p><a id="more"></a><h1 id="Example-application"><a href="#Example-application" class="headerlink" title="Example application"></a>Example application</h1><h2 id="Solt-filling"><a href="#Solt-filling" class="headerlink" title="Solt filling"></a>Solt filling</h2><p>先从RNN的应用说起，RNN能做什么？</p><p>RNN可以做智慧系统：</p><p>如下图中，用户告诉订票系统：”I would like to arrive Taipei on November 2nd”.</p><p>订票系统能从这句话中得到Destination: Taipei，time of arrival: November 2nd.</p><p><a href="https://imgchr.com/i/tqZaQJ"><img src="https://s1.ax1x.com/2020/06/11/tqZaQJ.md.png" alt="tqZaQJ.md.png"></a> </p><p>这个过程也就是<strong>Solt Filling</strong> （槽位填充）。</p><p>如果用Feedforward network来解决solt filling问题，输入就是单词，输出是每个槽位（slot）的单词，如下图。</p><p><a href="https://imgchr.com/i/tqZ8oV"><img src="https://s1.ax1x.com/2020/06/11/tqZ8oV.md.png" alt="tqZ8oV.md.png"></a></p><p>上图中，如何将word表示为一个vector？</p><h2 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h2><p>How to represent each word as a vector?</p><h3 id="1-of-N-encoding"><a href="#1-of-N-encoding" class="headerlink" title="1-of-N encoding"></a>1-of-N encoding</h3><p>最简单的方式是1-of-N encoding方式（独热方式）。</p><p>向量维度大小是整个词汇表的大小，每一个维度代表词汇表中的一个单词，如果该维度置1，表示这个维度代表的单词。</p><h3 id="Beyond-1-of-N-encoding"><a href="#Beyond-1-of-N-encoding" class="headerlink" title="Beyond 1-of-N encoding"></a>Beyond 1-of-N encoding</h3><p>对1-of-N encoding方式改进。</p><p>第一种：<u>Dimension for “Other”</u> </p><p><a href="https://imgchr.com/i/tqZNz4"><img src="https://s1.ax1x.com/2020/06/11/tqZNz4.md.png" alt="tqZNz4.md.png"></a> </p><p>在1-of-N的基础上增加一维度——‘other’维度，即当单词不在系统词汇表中，将other维度置1代表该单词。</p><p>第二种：<u>Word hashing</u> </p><p><a href="https://imgchr.com/i/tqZtWF"><img src="https://s1.ax1x.com/2020/06/11/tqZtWF.md.png" alt="tqZtWF.md.png"></a> </p><p>即便是增加了”other”维度，编码vector的维度也很大，用word hashing的方式将大幅减少维度。</p><p>以apple为例，拆成app, ppl, ple三个部分，如上图所示，vector中表示这三个部分的维度置1。</p><p>用这样的word hashing方式，vector的维度只有 $26\times 26\times26$ ，大幅减少词向量的维度。</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>通过encoding的方式，单词用vector来表示，用前馈神经网络来解决solt filling问题。</p><p>如下图.</p><p>input:一个单词（encoding为vector）</p><p>output: input单词中属于该槽位(solts)的概率分布(vector)。</p><p><a href="https://imgchr.com/i/tqZ8oV"><img src="https://s1.ax1x.com/2020/06/11/tqZ8oV.md.png" alt="tqZ8oV.md.png"></a> </p><hr><p>但用普通的前馈神经网络处理solt filling问题会出现下图问题：</p><p><a href="https://imgchr.com/i/tqZJiT"><img src="https://s1.ax1x.com/2020/06/11/tqZJiT.md.png" alt="tqZJiT.md.png"></a> </p><p>上图中，arrive Taipei on November 2nd 和 leave Taipei on November 2nd，将这两句话的每个单词（vector）放入前馈神经网络，得出的dest槽位都应该是Taipei。</p><p>但，通过之前的语意，arrive Taipei的Taipei应该是终点，而leave Taipei的Taipei是起点。</p><p>因此，在处理这种问题时，我们的神经网络应该需要memory，对该输入的上下文有一定的记忆存储。</p><h1 id="Recurrent-Neural-Network-RNN"><a href="#Recurrent-Neural-Network-RNN" class="headerlink" title="Recurrent Neural Network(RNN)"></a>Recurrent Neural Network(RNN)</h1><h2 id="Basic-structure"><a href="#Basic-structure" class="headerlink" title="Basic structure"></a>Basic structure</h2><p>因此，我们对一般的前馈神经网络加入记忆元件a, a 存储hidden layer的输出，同时a也作为下一次计算的输入部分,下图就是最基础的RNN模型。</p><p><a href="https://imgchr.com/i/tqZYJU"><img src="https://s1.ax1x.com/2020/06/11/tqZYJU.md.png" alt="tqZYJU.md.png"></a> </p><p>举一个例子来说明该过程：</p><p>Input sequence: $\begin{bmatrix}1 \ 1 \end{bmatrix}$  $\begin{bmatrix}1 \ 1 \end{bmatrix}$  $\begin{bmatrix}2 \ 2 \end{bmatrix}$ …</p><p>RNN模型如下图所示：所有的weight都是1，没有bias; 所有的神经元的activation function 都是线性的。</p><ol><li><p>input : $\begin{bmatrix}1 \ 1 \end{bmatrix}$, 记忆元件初值 a1=0 a2=0.</p><p><a href="https://imgchr.com/i/tqZ3d0"><img src="https://s1.ax1x.com/2020/06/11/tqZ3d0.md.png" alt="tqZ3d0.md.png"></a> </p><p>记忆元件也作为输入的一部分，hidden layer的输出为 2 2, 更新记忆元件的值.</p><p>output: $\begin{bmatrix}4 \ 4 \end{bmatrix}$ , 记忆元件存储值 a1=2 a2=2.</p></li><li><p>input : $\begin{bmatrix}1 \ 1 \end{bmatrix}$  , 记忆元件存储值 a1=2 a2=2.</p><p><a href="https://sbimg.cn/image/0000D"><img src="https://wx2.sbimg.cn/2020/06/11/2020-06-11-8.42.06.md.png" alt="2020-06-11-8.42.06.md.png"></a> </p></li></ol><p>   记忆元件也作为输入的一部分，hidden layer 的输出为6 6,更新记忆元件的值。</p><p>   output: $\begin{bmatrix}12 \ 12 \end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6.</p><p>   这里可以发现，第一次和第二次的输入相同，但是由于有记忆元件的缘故，两次输出不同。</p><ol start="3"><li><p>input : $\begin{bmatrix}2 \ 2 \end{bmatrix}$ , 记忆元件存储值 a1=6 a2=6.</p><p><a href="https://imgchr.com/i/tqZQLn"><img src="https://s1.ax1x.com/2020/06/11/tqZQLn.md.png" alt="tqZQLn.md.png"></a> </p><p>记忆元件也作为输入的一部分，hidden layer 的输出为16 16,更新记忆元件的值。</p><p>output: $\begin{bmatrix}32 \ 32 \end{bmatrix}$ , 记忆元件存储值 a1=16 a2=16.</p></li></ol><p>RNN中，由于有memory，会和一般前馈模型有两个不同的地方：一是输入相同的vector，输出可能是不同的；二是将一个sequence连续放进RNN模型中，如果sequence中改变顺序，输出也大多不同。</p><hr><p>用这个RNN模型来解决之前的solt filling问题，就可以解决上下文语意不同影响solt的问题。</p><p>将arrive Taipei on November 2nd的每个单词都放入同样的模型中。</p><p><a href="https://imgchr.com/i/tqZAdP"><img src="https://s1.ax1x.com/2020/06/11/tqZAdP.md.png" alt="tqZAdP.md.png"></a> </p><p>因此将RNN展开，如上图，像不同时间点的模型，但其实是不同时间点循环使用同一个模型。</p><p><a href="https://imgchr.com/i/tqZKMj"><img src="https://s1.ax1x.com/2020/06/11/tqZKMj.md.png" alt="tqZKMj.md.png"></a> </p><p>由于左边的前文是arrive，右边的前文是leave，所以存储在memory中的值不同，Taipei作为input的输出（槽位的概率分布）也不同。</p><h2 id="Elman-Network-amp-Jordan-Network"><a href="#Elman-Network-amp-Jordan-Network" class="headerlink" title="Elman Network &amp; Jordan Network"></a>Elman Network &amp; Jordan Network</h2><p>上文中只是RNN模型中的一种，即Elman Network，记忆元件存储的是上一个时间点hidden layer的输出。</p><p>而Jordan Network模型中,他的记忆元件存储的是上一时间点的output。</p><p>（据说，记忆元件中存储output的值会有较好的performance，因为output是有target vector的，因此能具象的体现放进memory的是什么）</p><p><a href="https://imgchr.com/i/tqZEIf"><img src="https://s1.ax1x.com/2020/06/11/tqZEIf.md.png" alt="tqZEIf.md.png"></a> </p><h2 id="Bidirectional-RNN"><a href="#Bidirectional-RNN" class="headerlink" title="Bidirectional RNN"></a>Bidirectional RNN</h2><p>上文中的RNN模型，记忆元件中存储的都是上文的信息，如果要同时考虑上下文信息，即是bidirectional RNN(双向RNN)。</p><p>模型如下图。</p><p><a href="https://sbimg.cn/image/002sN"><img src="https://wx1.sbimg.cn/2020/06/11/2020-06-11-8.45.55.md.png" alt="2020-06-11-8.45.55.md.png"></a> </p><p>双向RNN的好处是看的范围比较广，当计算输出 $y^t$ 时，上下文的内容都有考虑到。</p><h2 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory(LSTM)"></a>Long Short-term Memory(LSTM)</h2><p>现在最常用的RNN模型是LSTM，Long Short-term Memory，这里的long是相当于上文中的RNN模型，因为上文提到的RNN模型都是short-term,即每一个时间点，都会把memory中的值洗掉，LSTM的long，就是会把memory的值保留的相对于久一些。</p><p>LSTM如下图，与一般NN不同的地方是，他有4个inputs,一个outputs。</p><p><a href="https://imgchr.com/i/tqZmRg"><img src="https://s1.ax1x.com/2020/06/11/tqZmRg.md.png" alt="tqZmRg.md.png"></a> </p><p>LSTM主要有四部分组成：</p><ul><li>Input Gate：输入门，下方箭头是输入，左方箭头是输入信号控制输入门的打开程度，完全打开LSTM才能将输入值完全读入，打开的程度也是NN自己学。</li><li>Output Gate：输出门，上方箭头是输出，左方箭头是输入信号控制输出门的打开程度，同理，打开程度也是NN自己学习。</li><li>Memory Cell：记忆元件。</li><li>Forget Gate：遗忘门，右边的箭头是输入信号控制遗忘门的打开程度，控制将memory cell洗掉的程度。</li></ul><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>更详细的阐述LSTM的内部机制：</p><p>注意：</p><ul><li>$z_o,z_i,z_f$ 是门的signal control,其实就等同于一般NN中neuron的输入z，是scalar。</li><li>gate其实就是一个neuron，通常gate neuron 的activation function f取 sigmod,因为值域在0到1之间，即对应门的打开程度。</li><li>input/forget/output gate的neuron的activation function是f(sigmod function), input neuron的activation function是g。</li></ul><p><a href="https://imgchr.com/i/tqZZi8"><img src="https://s1.ax1x.com/2020/06/11/tqZZi8.md.png" alt="tqZZi8.md.png"></a> </p><ol><li>input gate控制输入:$g(z)f(z_i)$ <ul><li>input:  z $\rightarrow$  $g(z)$ </li><li>input gate signal control:  $z_i \rightarrow f(z_i)$ </li><li>multiply：$g(z)f(z_i)$ </li></ul></li><li>forget gate 控制memory：$cf(z_f)$ <ul><li>forget gate signal control: $z_f\rightarrow f(z_f)$ </li><li>如果 $f(z_f)=1$ ,说明memory里的值保留；如果 $f(z_f)=0$ ,说明memory里的值洗掉。</li></ul></li><li>更新当前时间点的memory(输入+旧的memory值) ：$c’=g(z)f(z_i)+cf(z_f)$ </li><li>output gate 控制输出：$h(c’)f(z_o)$ <ul><li>output: $c’ \rightarrow h(c’)$ </li><li>output gare signal control:  $z_o \rightarrow f(z_o)$ </li><li>multiply: $h(c’)f(z_o)$ </li></ul></li></ol><hr><p>LSTM模型（trained）如下图：</p><p>输入序列为 $\begin{bmatrix}3 \ 1 \ 0 \end{bmatrix}$$\begin{bmatrix}4 \ 1 \ 0 \end{bmatrix}$ $\begin{bmatrix}2 \ 0 \ 0 \end{bmatrix}$ $\begin{bmatrix}1 \ 0 \ 1 \end{bmatrix}$ $\begin{bmatrix}3 \ -1 \ 0 \end{bmatrix}$ </p><p><a href="https://imgchr.com/i/tqZkZt"><img src="https://s1.ax1x.com/2020/06/11/tqZkZt.md.png" alt="tqZkZt.md.png"></a> </p><p>该LSTM activation function: g、h都为linear function（即输出等于输入），f为sigmod.</p><p>通过该LSTM的输出序列为： 0 0 0 7 0 0</p><p>（建议手算一遍）</p><h2 id="Compared-with-Original-Network"><a href="#Compared-with-Original-Network" class="headerlink" title="Compared with Original Network"></a>Compared with Original Network</h2><p>original network如下图：</p><p><a href="https://imgchr.com/i/tqVXa6"><img src="https://s1.ax1x.com/2020/06/11/tqVXa6.md.png" alt="tqVXa6.md.png"></a> </p><p>LSTM 的NN即用LSTM替换原来的neuron，这个neuron有四个inputs，相对于original network也有4倍的参数，如下图：</p><p><a href="https://imgchr.com/i/tqZSRe"><img src="https://s1.ax1x.com/2020/06/11/tqZSRe.md.png" alt="tqZSRe.md.png"></a> </p><hr><p>所以原来RNN的neuron换为LSTM，就是下图：</p><p><a href="https://imgchr.com/i/tqVjIK"><img src="https://s1.ax1x.com/2020/06/11/tqVjIK.md.png" alt="tqVjIK.md.png"></a> </p><p>上图中：</p><p>这里的 $z^f,z^u,z,z^o$ 都是 $x^t \begin{bmatrix} \quad\end{bmatrix}$ 矩阵运算得到的vector, 因为上图中有多个LSTM，因此 $z^i$ 的第k个元素，就是控制第k个LSTM的input signal control scalar。所以，$z^f,z^u,z,z^o$ 的维度等于下一层neuron/LSTM的个数。</p><p>所以这里memory（cell）$c^t$ 也是一个vector，第k个元素是第k个LSTM中cell存储的值。</p><p>向量运算和scalar一样，LSTM细节如下图：</p><p><a href="https://imgchr.com/i/tqVxPO"><img src="https://s1.ax1x.com/2020/06/11/tqVxPO.md.png" alt="tqVxPO.md.png"></a> </p><h2 id="Extension：“peephole”"><a href="#Extension：“peephole”" class="headerlink" title="Extension：“peephole”"></a>Extension：“peephole”</h2><p>上小节的LSTM是simplified，将LSTM hidden layer的输出 $h^t$ 和cell中存储的值 $c^t$  和下一时间点的输入 $x^{t+1}$ 一同作为下一时间点的输入，就是LSTM的扩展版”peephole”。</p><p>如下图：</p><p><a href="https://imgchr.com/i/tqVqq1"><img src="https://s1.ax1x.com/2020/06/11/tqVqq1.md.png" alt="tqVqq1.md.png"></a> </p><h2 id="Multi-layer-LSTM"><a href="#Multi-layer-LSTM" class="headerlink" title="Multi-layer LSTM"></a>Multi-layer LSTM</h2><p>多层的peephole LSTM如下图：</p><p><img src="https://i.loli.net/2020/06/11/sD46OVQpxokjiw8.png" alt="1截屏2020-04-19 下午4.41.50.png"> </p><p>（：wtf 我到底看到了什么</p><p>不要怕：Keras PyTorch等套件都有 “LSTM”，“GUR，”SimpleRNN“ 已实现好的layers.</p><h2 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h2><p>训练RNN时，输入与target如下所示：</p><p><img src="https://i.loli.net/2020/06/11/AK1kSPltUDaqVmR.png" alt="截屏2020-06-11 下午8.52.09.png"> </p><p>估测模型的好坏，计算RNN的Loss时，需要看作一个整体，计算每个时间点RNN输出与target的crossentropy的和。</p><p>训练也可同样用Backpropagation，但考虑到时间点，有一个进阶版的”Backpropogation through time(BPTT)”[1]。</p><p>RNN一般就用BPTT训练。</p><h2 id="How-to-train-well"><a href="#How-to-train-well" class="headerlink" title="How to train well"></a>How to train well</h2><h3 id="not-easy-to-train"><a href="#not-easy-to-train" class="headerlink" title="not easy to train"></a>not easy to train</h3><p>RNN-based network is not always easy to learn.</p><p>但基于RNN的模型往往不太好训练，总是会出现下图中的绿色线情况（即抖动）。</p><p><a href="https://imgchr.com/i/tqVbrR"><img src="https://s1.ax1x.com/2020/06/11/tqVbrR.md.png" alt="tqVbrR.md.png"></a> </p><h3 id="error-surface-is-rough"><a href="#error-surface-is-rough" class="headerlink" title="error surface is rough"></a>error surface is rough</h3><p>error surface，即total loss在参数变化时的函数图。</p><p>会发现基于RNN的模型的error surface会长下图这个样子：有时很平坦(flat)有时很陡峭(steep)</p><p><a href="https://imgchr.com/i/tqZiqI"><img src="https://s1.ax1x.com/2020/06/11/tqZiqI.md.png" alt="tqZiqI.md.png"></a> </p><ul><li><p>橙色点出发：</p><ul><li>起初处在flat的位置。</li><li>随着一次次更新，gradient在变小，learning rate即会变大。</li><li>可能稍微不幸，就会出现跨过悬崖，即出现了剧烈震荡的问题。</li><li>如果刚好当前处在悬崖低，这时的gradient很大，learning rate也很大，step就会很大，飞出去，极可能出现segment fault(NaN).</li></ul></li><li><p>Thomas Mikolv 用工程师的角度来解决这个问题，即当此时的gradient大于某个阈值(threshold)时，就不要让当前的gradient超过这个阈值（通常取15）。</p></li><li><p>这样处在悬崖低的橙色点，（Clipping路线），更新就会到绿色的，继续更新。</p></li></ul><h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><p>为什么RNN模型会出现抖动的情况呢？</p><p>用下图这个简单例子说明（一般activation function用sigmod,而ReLu的performance一般较差）：</p><p><a href="https://imgchr.com/i/tqVh5T"><img src="https://s1.ax1x.com/2020/06/11/tqVh5T.md.png" alt="tqVh5T.md.png"></a> </p><p>上图中，输入序列是1 0 0 0 …，memory连接下一个时间点的权重是w，可以轻易得到最后一个时间点的输出 $y^{1000}=w^{999}$ 。</p><p>上图中，循环输出1000次，如果w变化 $\Delta w$  ，看输出 $y^{1000}$ 的变化，来直观体现gradient 的变化：</p><p><a href="https://imgchr.com/i/tqV5PU"><img src="https://s1.ax1x.com/2020/06/11/tqV5PU.md.png" alt="tqV5PU.md.png"></a> </p><p>上图中，可以看出：</p><ul><li>绿色部分：当w从1变化为1.01时， $y^{1000}$ 的输出变化即大，既有较大的gradient，理应有小的learning rate。</li><li>黄色部分：当w从0.99变化为0.01时， $y^{1000}$ 的输出几乎不变化，即有较小的gradient，理应有大大learning rate.</li><li>在很小的地方（0.01 到 1.01），他的gradient就变化即大，即抖动的出现。</li></ul><p><strong>Reason</strong>：RNN，虽然可以看作不同时间点的展开计算，但始终是同一个NN的权重计算（cell连接到下一个时间点的权重），在不同时间中，反复叠乘，因此会出现这种情况。</p><h3 id="Helpful-Techniques"><a href="#Helpful-Techniques" class="headerlink" title="Helpful Techniques"></a>Helpful Techniques</h3><ol><li>LSTM几乎已经算RNN的一个标准了，为什么LSTM的performance比较好呢。</li></ol><ul><li><p>为什么用LSTM替换为RNN？</p><p>:Can deal with gradient vanishing(not gradient explode).</p><p>可以解决gradient vanish的问题（gradient vanish problem 具体见 <a href="/2020/04/21/tips-for-DL/" title="这篇文章2.1.1">这篇文章2.1.1</a>）</p></li><li><p>为什么LSTM可以解决gradient vanish问题</p><p>：memory and input are added.（LSTM的的输出与输入和memory有关）</p><p>: The influence never disappears unless forget gate is closed.（memory的影响可以很持久）</p></li></ul><ol start="2"><li><p>GRU[2]（Gated Recurrent Unit）：是只有两个Gate，比LSTM简单，参数更少，不容易overfitting</p></li><li><p>玄学了叭</p><p><a href="https://imgchr.com/i/tqVHM9"><img src="https://s1.ax1x.com/2020/06/11/tqVHM9.md.png" alt="tqVHM9.md.png"></a> </p></li></ol><h1 id="More-Applications"><a href="#More-Applications" class="headerlink" title="More Applications"></a>More Applications</h1><p>【待更新】</p><h2 id="Many-to-One"><a href="#Many-to-One" class="headerlink" title="Many to One"></a>Many to One</h2><h2 id="Many-to-Many"><a href="#Many-to-Many" class="headerlink" title="Many to Many"></a>Many to Many</h2><h2 id="Beyond-Sequence"><a href="#Beyond-Sequence" class="headerlink" title="Beyond Sequence"></a>Beyond Sequence</h2><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><h3 id="Auto-encoder-Text"><a href="#Auto-encoder-Text" class="headerlink" title="Auto-encoder-Text"></a>Auto-encoder-Text</h3><h3 id="Auto-encoder-Speech"><a href="#Auto-encoder-Speech" class="headerlink" title="Auto-encoder-Speech"></a>Auto-encoder-Speech</h3><h2 id="Chat-bot"><a href="#Chat-bot" class="headerlink" title="Chat-bot"></a>Chat-bot</h2><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>BPTT</li><li>GRU</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中首先从RNN能解决什么问题入手，分析了RNN与一般NN的区别。&lt;br&gt;然后着重讲解了基于RNN的LSTM模型，包括LSTM的细节、和一般NN的区别，以及如何训练LSTM模型。&lt;br&gt;具体阐述了在模型（RNN类模型）训练过程中为什么会遇到剧烈抖动问题和如何解决抖动的工程解决方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
      <category term="RNN" scheme="https://f7ed.com/tags/RNN/"/>
    
      <category term="LSTM" scheme="https://f7ed.com/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Convolution Neural Network（CNN）</title>
    <link href="https://f7ed.com/2020/04/25/CNN/"/>
    <id>https://f7ed.com/2020/04/25/CNN/</id>
    <published>2020-04-24T16:00:00.000Z</published>
    <updated>2020-10-17T12:16:20.231Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中首先介绍了为什么要用CNN做图像识别，或者说图像识别问题的特点是什么？<br>文章中也详细介绍了CNN的具体架构，主要包括Convolution、Max Pooling、Flatten。<br>文章最后简要介绍了CNN在诸多领域的应用。</p><a id="more"></a><h1 id="Why-CNN-for-Image"><a href="#Why-CNN-for-Image" class="headerlink" title="Why CNN for Image?"></a>Why CNN for Image?</h1><p>图片本质都是pixels。</p><p>在做图像识别时，本质是对图片中的某些特征像素（properities)识别。</p><p><strong>So Why CNN for image?</strong></p><ol><li><p>Some patterns are much smaller than the whole image.</p><p>A neuron does <strong>not have to see the whole image</strong> to discover the pattern.</p><p>Connecting to small region with <strong>less parameters.</strong></p><p>【很多特征图案的大小远小于整张图片的大小，因此一个neuron不需要为了识别某个pattern而看完整张图片。并且，如果只识别某个小的region，会减少大量参数的数目。】</p><p>如下图，用一个neuron识别红框中的beak，即能大概率认为图片中有bird。</p><img src="https://s1.ax1x.com/2020/04/25/JyitKJ.md.png" alt="JyitKJ.md.png" style="zoom:75%;" /></li><li><p>The same patterns appear in different regions. They can <strong>use the same set of parameters.</strong></p><p>【同样的pattern可能出现在图片的不同位置。pattern几乎相同，因此可以用同一组参数。】</p><p>如下图，两个neuron识别两个不同位置的beak。被识别的beak几乎无差别，因此neuron的参数可以是相同的。</p><img src="https://s1.ax1x.com/2020/04/25/JyiNr9.md.png" alt="JyiNr9.md.png" style="zoom:50%;" /></li><li><p><strong>Subsampling</strong> the pixels will not change the object.</p><p>【一张图片是由许多pixel组成的，如下图，如果去掉图片的所有奇数行偶数列的pixel，图片内容几乎无差别。并且，Subsample pixels，即减少了输入的size，也可以减少NN的参数数量。】</p><img src="https://s1.ax1x.com/2020/04/25/JyiJv4.md.png" alt="JyiJv4.md.png" style="zoom:50%;" /></li></ol><h1 id="The-whole-CNN"><a href="#The-whole-CNN" class="headerlink" title="The whole CNN"></a>The whole CNN</h1><p>CNN的架构如下图。</p><img src="https://s1.ax1x.com/2020/04/25/JyiG2F.md.png" alt="JyiG2F.md.png" style="zoom:67%;" /><p>一张图片经过多次Convolution、Max Pooling得到新的image，再将新的image Flatten（拉直）得到一组提取好的features，将这组features放入前馈神经网络。</p><p>Convolution满足图片识别的：</p><ul><li>Property 1 : Some patterns are much smaller than the whole image.</li><li>Property 2 : The same patterns appear in different regions.</li></ul><p>Max Pooling满足图片识别的：</p><ul><li>Property 3 : Subsamplingthe pixels will not change the object.</li></ul><h2 id="CNN-Convolution"><a href="#CNN-Convolution" class="headerlink" title="CNN-Convolution"></a>CNN-Convolution</h2><p>一张简单的黑白图片如下图，0为白色，1为黑色。</p><img src="https://s1.ax1x.com/2020/04/25/Jyi88U.md.png" alt="Jyi88U.md.png" style="zoom:33%;" /> <p>如果图片是彩色的，即用RGB三原色来表示，用三个matrix分别表示R、G、B的值，如下图：</p><img src="https://s1.ax1x.com/2020/04/25/JyiMEq.md.png" alt="JyiMEq.md.png" style="zoom:50%;" /><p>下文中，以黑白图举例。</p><h3 id="Property-1"><a href="#Property-1" class="headerlink" title="Property 1"></a>Property 1</h3><p>设计Filer matrix满足Property 1，如下图：</p><img src="https://s1.ax1x.com/2020/04/25/Jyi3CT.png" alt="Jyi3CT.png" style="zoom:33%;" /> <p>上图中，filter的大小是3*3，可以检测到小区域的某个pattern。</p><p>每个filter的参数都是NN中的参数，需要learned。</p><p>如果是彩色图片，filter应该是3张3*3matrix组成的，分别代表R、G、B的filter。</p><h3 id="Property-2"><a href="#Property-2" class="headerlink" title="Property 2"></a>Property 2</h3><p>为了满足Property 2，filter可以在图片中移动。设置stride，即每次filter移动的步长。</p><p>filter与覆盖图片的位置做内积，需要走完整张图片，最后得到一张feature map。</p><p>下图为stride=1的convolution结果：</p><img src="https://s1.ax1x.com/2020/04/25/Jyil5V.md.png" alt="Jyil5V.md.png" style="zoom:50%;" /><p>Convolution layer（卷积层）有几个filter，就会得到几张feature maps。</p><h3 id="Convolution-v-s-Fully-Connected"><a href="#Convolution-v-s-Fully-Connected" class="headerlink" title="Convolution v.s. Fully Connected"></a>Convolution v.s. Fully Connected</h3><p><strong>Fully Connected:</strong> </p><p>如果用全连接的方式做图片识别，图片的每一个pixel都要和第一层的所有neurons连接，需要大量参数。</p><p>如下图：</p><img src="https://s1.ax1x.com/2020/04/25/JyinDs.md.png" alt="JyinDs.md.png" style="zoom:70%;" /><hr><p><strong>Convolution:</strong> </p><p>而在Convolution中，把feature map中的每一个值作为neuron的输出，因此图片中只有部分pixels会和第一层的第一个neuron连接，而不是全部pixels。</p><p>对于一个3*3的filter，一个neuron的连接如下：</p><img src="https://s1.ax1x.com/2020/04/25/Jyiubn.md.png" alt="Jyiubn.md.png" style="zoom:70%;" /><p>filter中的值是连接参数，则每一个neuron只需要与3*3个input连接，与全连接相比减少了大量参数。</p><p><strong>shared weights</strong> </p><p>filter在图中移动时，filter的参数不变，即第二个neuron的连接参数和第一个neuron的连接参数是相同的，连接图如下：</p><img src="https://s1.ax1x.com/2020/04/25/Jyimuj.md.png" alt="Jyimudj.md.png" style="zoom:70%;" /><p>通过filter实现了shared weights（参数共享），更大幅度减少了参数数量。</p><h2 id="CNN-Max-Pooling"><a href="#CNN-Max-Pooling" class="headerlink" title="CNN-Max Pooling"></a>CNN-Max Pooling</h2><p>Max Pooling：将convolution layer的neuron作为输入，neuron的activation function其实就是Maxout（Maxout介绍见  的介绍）。</p><p>将convolution layer得到的feature map做Max pooling（池化），即取下图中每个框中的最大值。</p><img src="https://s1.ax1x.com/2020/04/25/JyiZvQ.md.png" alt="JyifZvddQ.md.png" style="zoom:50%;" /><p>如下图，6*6的image经过Convolution layer 和 Max Pooling layer后，得到了new but smaller image，新的image的由两层channel组成，每层channel都是2 * 2的image。</p><img src="https://s1.ax1x.com/2020/04/25/JyiAC8.md.png" alt="JyiAC8.md.png" style="zoom:67%;" /><p>一个image每经过一次Convolution layer 和 Max Pooling layer，都会得到a new image。</p><p>This new image is smaller than the origin image. And the number of channel (of the new image) is the number of filters.</p><p>举个例子：</p><p>Convolution layer有25个filters，再经过Max Pooling，得到的新的image有25 个channel。</p><p>再重复一次Convolution 和Max Pooling，新的Convolution layer也有25个filters，再经过Max Pooling，得到的新的image有多少个channel呢？</p><p>答案是25个channel。</p><p><strong>注意</strong> ：在第二次Convolution中，image有depth，depth=25。因此在convolution中，filter其实是一个cubic，也有depth，depth=image-depth=25，再做内积。</p><p>因此，新的image的channel数是等于filter数的。</p><h2 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h2><p>Flatten很好理解，将最后得到的新的image 拉直（Flatten）为一个vector。</p><img src="https://s1.ax1x.com/2020/04/25/JyiE8S.md.png" alt="Jy8iE8S.md.png" style="zoom:50%;" /><p>拉直后的vector是一组提取好的features，作为 前馈神经网络的输入。</p><h2 id="zero-padding"><a href="#zero-padding" class="headerlink" title="zero padding"></a>zero padding</h2><p>如何让卷积后的图像不变小？</p><p>答案就是zero padding，在原图的padding填0，再做卷积。</p><p>zero-padding后如下图：</p><img src="https://s1.ax1x.com/2020/10/17/0LvkR0.png" alt="0LvdkR0.png" style="zoom:40%;" /><p>卷积后，图像大小不变：</p><img src="https://s1.ax1x.com/2020/10/17/0LvZsU.png" alt="0LvZdsU.png" style="zoom:40%;" /><h1 id="What-dose-CNN-learn"><a href="#What-dose-CNN-learn" class="headerlink" title="What dose CNN learn"></a>What dose CNN learn</h1><p>为什么CNN能够学习pattern，最终达到识别图像的目的？</p><h2 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h2><p>在下图CNN过程中，我们先分析能从Convolution layer的filter能够学到什么？</p><img src="https://s1.ax1x.com/2020/04/25/JyiF4f.md.png" alt="JyiF4f.md.png" style="zoom:33%;" /><p>每个filter本质上是一组shared weights 的neuron。</p><p>因此，定义这组filter的激活程度，即：</p><p> Degree of the activation of the k-th filter: $a^k=\sum_{i=1}^{11}\sum_{j=1}^{11}a_{ij}^{k}$ .</p><p>目标是找到使k-th filter激活程度最大的输入image，即</p><p>$x^{*}=\arg \max _{x} a^{k}$ ，(method :gradient descent).</p><p>部分结果如下图：</p><img src="https://s1.ax1x.com/2020/04/25/JyiVgg.md.png" alt="JyiVgg.md.png" style="zoom:33%;" /> <p>(每一张图都代表一个让filter激活程度最大的 $x$)</p><p>上图中，找到使filter激活程度最大的image，即上图中每个filter可以检测一定的条纹，只有当图像中有该条纹，filter（一组neuron）的激活程度（即输出）才能达到最大。</p><h2 id="Neuron（Hidden-layer）"><a href="#Neuron（Hidden-layer）" class="headerlink" title="Neuron（Hidden layer）"></a>Neuron（Hidden layer）</h2><p>这里的neuron指前馈神经网络中的neuron，如下图的 $a_j$ :</p><img src="https://s1.ax1x.com/2020/04/25/JyiiUP.png" alt="JyiiUP.png" style="zoom:50%;" /><p>目标：找到使neuron的输出最大的输入image，即：</p><p>$x^{*}=\arg \max _{x} a^{j}$ .</p><p>部分结果如下：</p><img src="https://s1.ax1x.com/2020/04/25/JykouQ.md.png" alt="JykouQ.md.png" style="zoom:33%;" /> <p>（每一张图代表一个neuron)</p><p>在上图中，感觉输入像一个什么东西吧emmmm。</p><p>但和filter学到的相比，neuron学到的不仅是图中的小小的pattern（比如条纹、鸟喙等），neuron学的是看整张图像什么。</p><h2 id="Output（Output-layer）"><a href="#Output（Output-layer）" class="headerlink" title="Output（Output layer）"></a>Output（Output layer）</h2><p>再用同样的方法，看看输出层的neuron学到了什么，如下图的 $y_i$  ：</p><img src="https://s1.ax1x.com/2020/04/25/Jyk5jg.png" alt="Jyk5jg.png" style="zoom:33%;" /><p>在手写数字辨识中 $y_i$ 是数字为 $i$ 的概率，因此目标是：找到一个使输出是数字 $i$ 概率最大的输入image，即：</p><p>$x^{*}=\arg \max _{x} y^{i}$ .</p><p>结果如下图：</p><img src="https://s1.ax1x.com/2020/04/25/JyiSud.md.png" alt="JyiSud.md.png" style="zoom:33%;" /> <p>结果和我们期望相差甚远，根本不能辨别以上图片是某个数字。</p><p>这其实也是DNN的一个特点: Deep Neural Networks are Easily Fooled [1]，即NN学到的东西往往和人类学到的东西是不一样的。</p><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>所以CNN到底学到了什么？</p><p>上文中，output 学到的都是一团密密麻麻杂乱的像素点，根本不像数字。</p><p>但是，再考虑手写数字image的特点：图片中应该有少量模式，大片空白部分。</p><p>因此目标改进为：  $x^{*}=\arg \max _{x}\left(y^{i}+\sum_{i, j}\left|x_{i j}\right|\right)$  </p><p>$\sum_{i, j}\left|x_{i j}\right|$ 就像是regularization的限制。</p><p>结果如下：</p><img src="https://s1.ax1x.com/2020/04/25/Jyi9HI.md.png" alt="Jyi9HI.md.png" style="zoom:33%;" /> <p>（注：图中白色为墨水，黑色为空白）</p><h1 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h1><h2 id="Deep-Dream"><a href="#Deep-Dream" class="headerlink" title="Deep Dream"></a>Deep Dream</h2><p>CNN exaggerates what it sees.</p><p>CNN可以夸大图片中他所看到的东西。</p><p>比如：</p><p>可以把下图</p><img src="https://s1.ax1x.com/2020/04/25/JyiPEt.md.png" alt="JyiPEt.md.png" style="zoom:50%;" />  <p>变成下图（emmmm看着有点难受）</p><img src="https://s1.ax1x.com/2020/04/25/JyPxjH.md.png" alt="JyPxjH.md.png" style="zoom:50%;" /> <p>附上生成deep dream image的网站<a href="http://deepdreamgenerator.com/">[2]</a> .</p><h2 id="Deep-Style-3"><a href="#Deep-Style-3" class="headerlink" title="Deep Style[3]"></a>Deep Style<a href="https://arxiv.org/abs/1508.06576">[3]</a></h2><p>Given a photo, make its style like famous paintings.</p><img src="https://s1.ax1x.com/2020/04/25/JyPX9O.md.png" alt="JyPX9O.md.png" style="zoom:50%;" /><p>上图中，用一个CNN学习图中的content，用另一个CNN学习风格图中的style。</p><p>再用一个CNN使得输入的图像content像原图，风格像另一张图。</p><h2 id="Playing-Go"><a href="#Playing-Go" class="headerlink" title="Playing Go"></a>Playing Go</h2><p>CNN 还可以用在下围棋中，如下图，输入是19 * 19的围棋局势（matrix/image），通过CNN，学出下一步应该走哪？</p><img src="https://s1.ax1x.com/2020/04/25/JyPL4K.md.png" alt="JyPL4K.md.png" style="zoom:50%;" /><h3 id="Why-CNN-playing-Go"><a href="#Why-CNN-playing-Go" class="headerlink" title="Why CNN playing Go?"></a>Why CNN playing Go?</h3><p>下围棋满足以下两个property：</p><ol><li><p>Some patterns are much smaller than the whole image.</p><img src="https://s1.ax1x.com/2020/04/25/JyPou9.png" alt="JyPou9.png" style="zoom:50%;" /> <p>（围棋新手，博主只下赢过几次hhh)</p><p>如果白棋棋手，看到上图的pattern，上图的白子只有一口气了，被堵住就会被吃掉，那白棋棋手大概率会救那个白子，下在白棋的下方。</p><p>Alpha Go uese 5 * 5 for first layer.</p></li><li><p>The same patterns appear in different regions.</p><img src="https://s1.ax1x.com/2020/04/25/JyP7H1.md.png" alt="JyP7H1.md.png" style="zoom:50%;" /> </li></ol><hr><p>但如何解释CNN的另一结构——Max Pooling？</p><p>因为围棋的棋谱matrix不像image的pixel，subsample后，围棋的棋谱就和原棋谱完全不像了。</p><p>Alpha Go的论文中：Alpha Go并没有用Max Pooling。</p><img src="https://s1.ax1x.com/2020/04/25/JyPbAx.md.png" alt="JyPbAx.md.png" style="zoom:75%;" /><p>所以，可以根据要训练的东西调整CNN模型。</p><h2 id="Speech"><a href="#Speech" class="headerlink" title="Speech"></a>Speech</h2><p>可以用CNN学习<a href="[https://zh.wikipedia.org/wiki/%E6%97%B6%E9%A2%91%E8%B0%B1](https://zh.wikipedia.org/wiki/时频谱)">Spectrogram</a> ，即识别出这一时段说的是什么话。</p><img src="https://s1.ax1x.com/2020/04/25/JyPqN6.md.png" alt="JyPqN6.md.png" style="zoom:75%;" /><h2 id="Text"><a href="#Text" class="headerlink" title="Text"></a>Text</h2><p>CNN还可以用在文本的情感分析中，对句子中每个word embedding后，通过CNN，学习sentence表达的是negative 还是positive还是neutral的情绪。</p><img src="https://s1.ax1x.com/2020/04/25/JyPTBR.md.png" alt="JyPTBR.md.png" style="zoom:75%;" /><h3 id="More"><a href="#More" class="headerlink" title="More"></a>More</h3><p>（挖坑…生命很漫长，学无止境QAQ）</p><ul><li><p>The methods of visualization in these slides：</p><p> <a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html">https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html</a></p></li><li><p>More about visualization：</p><p><a href="http://cs231n.github.io/understanding-cnn/">http://cs231n.github.io/understanding-cnn/</a></p></li><li><p>Very cool CNN visualization toolkit</p><p><a href="http://yosinski.com/deepvis">http://yosinski.com/deepvis</a></p><p><a href="http://scs.ryerson.ca/~aharley/vis/conv/">http://scs.ryerson.ca/~aharley/vis/conv/</a></p></li><li><p>The 9 Deep Learning Papers You Need To Know About</p><p><a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html</a></p></li><li><p>How to let machine draw an image</p><ul><li><p>PixelRNN</p><p><a href="https://arxiv.org/abs/1601.06759">https://arxiv.org/abs/1601.06759</a></p></li><li><p>Variation Autoencoder (VAE)</p><p><a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></p></li><li><p>Generative Adversarial Network (GAN)</p><p><a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a></p></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p>Deep Neural Networks are Easily Fooled： <a href="https://www.youtube.com/watch?v=M2IebCN9Ht4">https://www.youtube.com/watch?v=M2IebCN9Ht4</a></p></li><li><p>deep dream generator: <a href="http://deepdreamgenerator.com/">http://deepdreamgenerator.com/</a></p></li><li><p>A Neural Algorithm of Artistic Style: <a href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中首先介绍了为什么要用CNN做图像识别，或者说图像识别问题的特点是什么？&lt;br&gt;文章中也详细介绍了CNN的具体架构，主要包括Convolution、Max Pooling、Flatten。&lt;br&gt;文章最后简要介绍了CNN在诸多领域的应用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="机器学习" scheme="https://f7ed.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CNN" scheme="https://f7ed.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Tips for Deep Learning</title>
    <link href="https://f7ed.com/2020/04/21/tips-for-DL/"/>
    <id>https://f7ed.com/2020/04/21/tips-for-DL/</id>
    <published>2020-04-20T16:00:00.000Z</published>
    <updated>2020-11-23T12:43:04.900Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。<br>tips从Training和Testing两个方面展开。<br>在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有RMSprop、Momentum、Adam等）。<br>当在Training中得到好的performance，但在testing中perform bad时，即遇到了overfitting，又该怎么处理呢？文章后半部分详尽介绍了EarlyStopping、Regularization和Dropout三个solution。</p><a id="more"></a><h1 id="Recipe-of-Deep-Learning"><a href="#Recipe-of-Deep-Learning" class="headerlink" title="Recipe of Deep Learning"></a>Recipe of Deep Learning</h1><p>Deep Learning 的三个步骤：</p><img src="https://s1.ax1x.com/2020/04/21/JGW8js.md.png" alt="JGW8js.md.png" style="zoom:53%;" /><p>如果在Training Data中没有得到好的结果，需要重新训练Neural Network。</p><p>如果在Training Data中得到好的结果，在Testing Data（这里的Testing Data是指有Label的Data，比如Kaggle的Public Data或者是从Training Data中划分出的Development Data）没有得到的好的结果，说明Overfitting了，需要重新设计Neural Network的结构。</p><h2 id="Do-not-always-blame-Overfitting"><a href="#Do-not-always-blame-Overfitting" class="headerlink" title="Do not always blame Overfitting"></a>Do not always blame Overfitting</h2><img src="https://s1.ax1x.com/2020/04/21/JGW3cj.md.png" alt="JGW3cj.md.png" style="zoom:50%;" /><p>如果在Testing Data中，看到上图，20-layer的error小，56-layer的error大，56-layer一定overfitting了。</p><p>No!!!不要总把原因归咎于Overfitting。</p><img src="https://s1.ax1x.com/2020/04/21/JGW13Q.md.png" alt="JGW13Q.md.png" style="zoom:50%;" /><p>再看Testing Data error之前，先看看Training Data的error。上图中，56-layer的DNN在Training Data的error本来就比20-layer的大，说明56-layer的DNN根本没有train好。</p><p>所以56-layer的DNN在Testing Data上的error大，原因不是overfitting，而是模型根本没有train好。</p><p><strong>注：</strong> Overfitting是在Training Data上error小，但在Testing Data上的error大。</p><p>因此，对于在Training Data上得到不好的结果和在Training Data上得到好的结果但在Testing Data上得到不好的结果这两种情况，需要不同的解决方法。</p><h1 id="Bad-Results-on-Training-Data"><a href="#Bad-Results-on-Training-Data" class="headerlink" title="Bad Results on Training Data"></a>Bad Results on Training Data</h1><p>在不重新设计DNN结构时，如果在Training Data中得到Bad Results，一般有两种方法来改进结果：</p><ul><li>New activation function【neuron换新的激活函数】</li><li>Adaptive Learning Rate</li></ul><h2 id="New-activation-function"><a href="#New-activation-function" class="headerlink" title="New activation function"></a>New activation function</h2><h3 id="Vanishing-Gradient-Problem"><a href="#Vanishing-Gradient-Problem" class="headerlink" title="Vanishing Gradient Problem"></a>Vanishing Gradient Problem</h3><img src="https://s1.ax1x.com/2020/04/21/JGWl9g.md.png" alt="JGWl9g.md.png" style="zoom:50%;" /><p>上图表示，在手写数字辨识中，Deeper layers并不能有好的performance。</p><p>为什么会这样呢？</p><p>因为出现了Vanishing Gradient Problem，即gradient随着deeper layer逐渐消失的问题。</p><img src="https://s1.ax1x.com/2020/04/21/JGWM4S.md.png" alt="JGWM4S.md.png" style="zoom:70%;" /><p>上图中，假设neuron的activation function是sigmod函数。</p><p>靠近Input layer层的参数的变化对Loss的影响很小，所以对Loss function做微分，gradient很小，参数更新慢。</p><p>而靠近Output layer层的参数的编号对Loss的影响更大，所以对Loss function做微分，gradient很大，参数更新快。</p><p>因为靠近Output Layer层的参数更新快，所以很快converge（收敛、趋于稳定）；但靠近Input Layer层的参数更新慢，几乎还处在random（随机）的状态。</p><p>当靠近Output Layer层的参数趋于稳定时，由于靠近Output Layer层的参数对Loss影响大，所以观察到的Loss的值也趋于稳定，于是，你就把training停掉了。</p><p>但是，靠近Input层的参数几乎处在random状态，所以拿模型用在Testing Data上，发现结果几乎是随机的。</p><hr><p>怎么直观理解靠近Input Layer的参数的gradient小呢？</p><p>用微分的直观含义来表示gradient $\partial{l}/\partial{w}$ : </p><p><strong>当 $w$ 增加 $\Delta{w}$ 时，如果 $l$ 的变化 $\Delta{l}$ 变化大，说明 $\partial{l}/\partial{w}$ 大，否则 $\partial{l}/\partial{w}$ 小。</strong></p><img src="https://s1.ax1x.com/2020/04/21/JGWKN8.md.png" alt="JGWKN8.md.png" style="zoom:67%;" /><p>我们在DNN中使用的activation function是sigmod函数，sigmod函数会把值压到0和1之间。</p><p>因此，上图中，其他值不变，只有连接 $x_N$ 的参数 $w$ 增加 $\Delta w$ 时，输入通过neuron的sigmod函数，函数的输出增加的 $\Delta$ 会变小，随着Deeper Layer，neuron的输出的 $\Delta$ 会越变越小，趋至0。</p><p>最后DNN输出的变化对 loss的影响小，即 $\Delta{l}$ 趋至0，即参数的gradient  $\partial{l}/\partial{w}$ 趋至0。（即 Vanishing Gradient）</p><h3 id="ReLu-：Rectified-Linear-Unit"><a href="#ReLu-：Rectified-Linear-Unit" class="headerlink" title="ReLu ：Rectified Linear Unit"></a>ReLu ：Rectified Linear Unit</h3><p>为了防止发生Vanishing Gradient Problem，在DNN中选择使用新的activation function。</p><p>ReLu长下面这个样子：</p><img src="https://s1.ax1x.com/2020/04/21/JGWuAf.md.png" alt="JGWuAf.md.png" style="zoom:33%;" /><p>z: input</p><p>a: output</p><p>当 $z\leq0$ 时， $a=0$ ；当 $z &gt;0$ 时， $a=z$ 。</p><p><u>Reason :</u> </p><ol><li>Fast to compute</li><li>Biological reason【有生物上的原因】</li><li>Infinite sigmod with different biases. 【是无穷个 有不同bias的sigmod函数 的叠加】</li><li><strong>Vanishing gradient problem</strong> 【最重要的是没有vanishing gradient problem】</li></ol><hr><p>为什么ReLu没有vanishing gradient problem</p><img src="https://s1.ax1x.com/2020/04/21/JGWmHP.md.png" alt="JGWmHP.md.png" style="zoom:67%;" /><p>上图DNN中，ReLu在输入是负数时，输出是0。因此这些输出是0的neuron可以去掉。</p><p>就变成了下图这个A Thinner linear network。由于ReLu函数的性质，靠近Input Layer的参数不会有smaller gradient。</p><img src="https://s1.ax1x.com/2020/04/21/JGWeBt.md.png" alt="JGWeBt.md.png" style="zoom:75%;" /><p>这里有一个Q&amp;A: </p><p>Q1: function变成linear的，会不会DNN就变弱了？</p><p>： 当neuron的operation region不变的话，DNN的确是linear的，但是当neuron的operation region改变后，就是unlinear的。</p><p>：即，当input的变化小，operation region不变（即输入不会从大于0变成小于0，小于0变成大于0这种），model还是linear的；但当input的变化大时，很多neuron的operation region都变化了，model其实就是unlinear的。</p><p>Q2: ReLu 怎么微分？</p><p>：ReLu在0点不可微，那就随便指定为0这样（台湾腔QAQ）。</p><h3 id="ReLu-variant"><a href="#ReLu-variant" class="headerlink" title="ReLu - variant"></a>ReLu - variant</h3><p>当 $z\leq 0$ 时，输出为0，就不能更新参数了。于是就有下图变体：</p><img src="https://s1.ax1x.com/2020/04/21/JGWZnI.md.png" alt="JGWZnI.md.png" style="zoom:40%;" /><p>当 $z\leq0$ 时，gradient都为0.01，为什么不能是其他值。于是就有下图变体：其中 $\alpha$ 也是一个需要学习的参数</p><h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>Maxout，如下图，在设计neural network时，会给每一层的neuron分组，成为一个新的neuron。</p><img src="https://s1.ax1x.com/2020/04/21/JGWk1H.md.png" alt="JGWk1H.md.png" style="zoom:75%;" /><p>Maxout也是一个Learnable activation function。</p><p><strong>ReLu是Maxout学出来的一个特例。</strong></p><img src="https://s1.ax1x.com/2020/04/21/JGWAcd.md.png" alt="JGWAcd.md.png" style="zoom:75%;" /><p>上图中，左图是ReLu。 ReLu的输入 $z = wx+b$ ，输出 $a$ 如上图的绿色的线。</p><p>右图是Maxout。Maxout的输入 $z_1 =wx+b,z_2=0$ ，那么输出取max，输出 $a$ 如上图中绿色的线，和左图的ReLu相同。</p><hr><p><strong>Maxout is more than ReLu。</strong> </p><p>当参数更新时，Maxout的函数图像如下图：</p><img src="https://s1.ax1x.com/2020/04/21/JGWPhD.md.png" alt="JGWPhD.md.png" style="zoom:77%;" /><p>DNN中的参数是learnable的，所以Maxout也是一个learnable的activation function。</p><p><u>Reason ：</u></p><ul><li><p>Learnable activation function [Ian J. Goodfellow, ICML’13]</p><ul><li><p>Activation function in maxout network can be any piecewise linear convex function.</p><p>在maxout神经网络中的激活函数可以是任意的分段凸函数。</p></li><li><p>How many pieces depending on how many elements in a group.</p><p>分段函数分几段取决于一组中有多少个元素。</p><img src="https://s1.ax1x.com/2020/04/21/JGWCtO.md.png" alt="JGWCtO.md.png" style="zoom:70%;" /></li></ul></li></ul><h3 id="Maxout-how-to-train"><a href="#Maxout-how-to-train" class="headerlink" title="Maxout : how to train"></a>Maxout : how to train</h3><p>Given a training data x, we know which z would be the max.</p><p>【当给出每笔training data时，我们能知道Maxout neuron中哪一个最大】</p><img src="https://s1.ax1x.com/2020/04/21/JGW9AK.md.png" alt="JGW9AK.md.png" style="zoom:75%;" /><p>如上图，在这笔training data x中，我们只train this thin and linear network 的参数，即max z相连的参数。</p><p>每笔不同的training data x，会得到不同的thin and linear network，最后，会train到每一个参数。</p><h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><h3 id="Review-Adagrad"><a href="#Review-Adagrad" class="headerlink" title="Review Adagrad"></a>Review Adagrad</h3><p>在这篇文章： <a href="/2020/03/01/Gradient/" title="Gradient">Gradient</a> 第一小节讲到一种adaptive learning rate的gradient 算法：Adagrad 算法。在那篇文章中，我们得出的结论是 the best step $\propto$  |First dertivative| / Second derivative.</p><img src="https://s1.ax1x.com/2020/04/21/JGWF9e.md.png" alt="JGWF9e.md.png" style="zoom:67%;" /><p>在上图中，两个方向，因为蓝色方向的二阶微分更小，所以蓝色方向应该有更大的learning rate。</p><p>因此，在Adagrad中，我们用一阶微分来估量二阶微分的大小：</p>$$w^{t+1}  \leftarrow w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}\left(g^{i}\right)^{2}}} g^{t}$$<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>但是，在训练NN时，Error Surface（Total Loss对参数的变化）的图像可能会更复杂，如下图：</p><img src="https://s1.ax1x.com/2020/04/21/JGWS76.md.png" alt="JGWS76.md.png" style="zoom:67%;" /><p>因为函数图像过于复杂，可能在同一方向的不同位置，也需要有不同的learning rate。</p><p>RMSProp是Adagrad的进阶版。</p><p><strong>RMSProp过程：</strong> </p><ol><li> $w^{1} \leftarrow w^{0}-\frac{\eta}{\sigma^{0}} g^{0} \quad \sigma^{0}=g^{0}$ </li><li> $w^{2} \leftarrow w^{1}-\frac{\eta}{\sigma^{1}} g^{1} \quad \sigma^{1}=\sqrt{\alpha (\sigma^{0})^2+(1-\alpha)(g^1)^2}$ </li><li> $w^{3} \leftarrow w^{2}-\frac{\eta}{\sigma^{2}} g^{2} \quad \sigma^{2}=\sqrt{\alpha (\sigma^{1})^2+(1-\alpha)(g^2)^2}$ </li><li><p>…</p></li><li> $w^{t+1} \leftarrow w^{t}-\frac{\eta}{\sigma^{t}} g^{t} \quad \sigma^{t}=\sqrt{\alpha (\sigma^{t-1})^2+(1-\alpha)(g^t)^2}$ <p>$\sigma^t$ 也是在算gradients的 root mean squar。</p></li></ol><p>但是在RMSProp中，加入了参数 $\alpha$ (需要手动调节大小的参数)，可以给当前算出来的gradient $g^t$ 更大的权重，即更相信现在gradient的方向，不那么相信以前gradient的方向。</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Momentum，则是引用物理中的惯性。</p><img src="https://s1.ax1x.com/2020/04/21/JGRxn1.md.png" alt="JGRxn1.md.png" style="zoom:47%;" /><p>上图中，当小球到达local minima时，会因为惯性继续往前更新，则有可能到达minima的位置。</p><p>这里的Momentum，就代指上一次前进（参数更新）的方向。</p><p><strong>Vanilla Gradient Descent</strong> </p><p>如果将Gradient的步骤画出图来，就是下图这样：</p><img src="https://s1.ax1x.com/2020/04/21/JGRXc9.md.png" alt="JGRXc9.md.png" style="zoom:47%;" /><p>过程：</p><ol><li><p>Start at position $\theta^0$</p></li><li><p>Compute gradietn at $\theta^0$</p><p>Move to  $\theta^1=\theta^0-\eta\nabla{L(\theta^0)}$ </p></li><li><p>Compute gradietn at $\theta^1$ </p><p>Move to  $\theta^2=\theta^1-\eta\nabla{L(\theta^1)}$ </p></li><li><p>… …Stop until $\nabla{L(\theta^t)}\approx0$</p></li></ol><hr><p><strong>Momentum</strong> </p><p>在Momentum中，参数更新方向是当前Gradient方向和Momentum方向（上一次更新方向）的叠加。</p><img src="https://s1.ax1x.com/2020/04/21/JGRjXR.md.png" alt="JGRjXR.md.png" style="zoom:50%;" /><p>Movement方向：上一次更新方向 - 当前gradient方向。</p><p>过程：</p><ol><li><p>Start at position $\theta^0$</p><p>Movement: $v^0=0$ </p></li><li><p>Compute gradient at $\theta^0$ </p><p>Movement  $v^1=\lambda v^0-\eta\nabla{L(\theta^0)}$  </p><p>Move to  $\theta^1=\theta^0+v^1$ </p></li><li><p>Compute gradient at $\theta^1$  </p><p>Movement  $v^2=\lambda v^1-\eta\nabla{L(\theta^1)}$ </p><p>Move to $\theta^2=\theta^1+v^2$  </p></li><li><p>… …Stop until $\nabla{L(\theta^t)}\approx0$ </p></li></ol><p>和Vanilla Gradient Descent比较，$v^i$ 其实是过去gradient( $\nabla{L(\theta^0)}$ 、$\nabla{L(\theta^1)}$ 、… 、 $\nabla{L(\theta^{i-1})}$  )的加权和。</p><ul><li>迭代过程：<ul><li>$v^0=0$ </li><li> $v^1=-\eta\nabla{L(\theta^0)}$ </li><li> $v^2=-\lambda\eta\nabla{L(\theta^0)}-\eta\nabla{L(\theta^1)}$ </li><li>…</li></ul></li></ul><hr><p>再用那个小球的例子来直觉的解释Momentum：</p><img src="https://s1.ax1x.com/2020/04/21/JGRO1J.md.png" alt="JGRO1J.md.png" style="zoom:70%;" /><p>当小球在local minima时，gradient为0，但是Momentum（即上次移动方向）是继续往前，于是小球可以继续向前更新。</p><h3 id="Adam-RMSProp-Momentum"><a href="#Adam-RMSProp-Momentum" class="headerlink" title="Adam = RMSProp + Momentum"></a>Adam = RMSProp + Momentum</h3><img src="https://s1.ax1x.com/2020/04/21/JGRLp4.md.png" alt="JGRLp4.md.png" style="zoom:77%;" /><p>Algorithm：Adam, our proposed algorithm for stochastic optimization. </p><p>【Adam，是为了优化stochastic gradient】（至于什么是stochastic gradient，建议戳)</p><p>$g_t^2$ indicates the elementwise square $g_t\odot g_t$ .</p><p>【$g_t^2$ 是gradient $g_t$ 向量和 $g_t$ 的元素乘】</p><p>Good default settings for the tested machine learning problems are $\alpha=0.001$ , $\beta_1=0.9$ , $\beta_2=0.999$ and $\epsilon=10^{-8}$ . All operations on vectors are element-wise. With $\beta_1^t$ and $\beta_2^t$ we denote $\beta_1$ and $\beta_2$ to the power t.</p><p>【参数说明：算法默认的参数设置是 $\alpha=0.001$ , $\beta_1=0.9$ , $\beta_2=0.999$ ， $\epsilon=10^{-8}$ 。算法中所有vector之间的操作都是对元素操作。 $\beta_1^t$ 和 $\beta_2^t$ 是 $\beta_1$ 和 $\beta_2$ 的 $t$ 次幂】</p><p><strong>Adam Pseudo Code：</strong> </p><ol start="0"><li><p><strong>Require</strong>：$\alpha$ : Stepsize 【步长/learning rate $\eta$ 】</p><p><strong>Require</strong>：$\beta_1,\beta_2\in\left[0,1\right)$ : Exponential decay rates for the moment estimates.</p><p><strong>Require</strong>：$f(\theta)$ : Stochastic objective function with parameters $\theta$ .【参数 $\theta$ 的损失函数】</p><p><strong>Require</strong>: $\theta_0$ ：Initial parameter vector 【初值】</p></li><li><p>$m_0\longleftarrow 0$ (Initial 1st moment vector) 【 $m$ 是Momentum算法中的更新参数后的方向 $v$ 】</p><p>$v_0\longleftarrow 0$ (Initial 2nd moment vector) 【 $v$ 是RMSprop算法中gradient的root mean square $\sigma$ 】</p><p>$t\longleftarrow 0$ (Initial timestep) 【更新次数】</p></li><li><p><strong>while</strong> $\theta_t$ not concerged <strong>do</strong> 【当 $\theta$ 趋于稳定，即 $\nabla{f(\theta)}\approx0$ 时】</p><ol><li><p>$t\longleftarrow t+1$ </p></li><li> $g_t\longleftarrow \nabla{f_t(\theta_{t-1})}$  (Get gradients w.r.t. stochastic objective at timestep t)<p>【算第t次时 $\theta$ 的gradient】</p></li><li> $m_{t} \leftarrow \beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t}$   (Update biased first momen t estimate)<p>【用Momentum算更新方向】</p></li><li> $v_{t} \leftarrow \beta_{2} \cdot v_{t-1}+\left(1-\beta_{2}\right) \cdot g_{t}^{2}$  (Update biased second raw moment estimate)<p>【RMSprop估测最佳步长（ 和$v$ 负相关） 】</p></li><li> $\widehat{m}_{t} \leftarrow m_{t} /\left(1-\beta_{1}^{t}\right)$ （Comppute bbi. as-corrected first momen t estima te)<p>【算出来的值有bias，论文中有具体解释为什么有。当更新次数增加时， $1-\beta_1^t$ 也趋近于1】</p></li><li> $\widehat{v}_{t} \leftarrow v_{t} /\left(1-\beta_{2}^{t}\right)$  (Compute bias-corrected second raw momen t estimate)<p>【和上同理】</p></li><li> $\theta_{t} \leftarrow \theta_{t-1}-\alpha \cdot \widehat{m}_{t} /(\sqrt{\widehat{v}_{t}}+\epsilon)$ （Update parameters）<p>【 $\widehat{m}<em>t$ 相当于是更准确的gradient的方向，$\sqrt{\widehat{v}</em>{t}}+\epsilon$ 是为了估测最好的步长，调节learning rate】</p></li></ol></li></ol><h3 id="Gradient-Descent-Limitation？"><a href="#Gradient-Descent-Limitation？" class="headerlink" title="Gradient Descent Limitation？"></a>Gradient Descent Limitation？</h3><p>在<a href="/2020/03/01/Gradient/" title="Gradient">Gradient</a>这篇文章中，讲到过Gradient有一些问题不能处理：</p><ul><li>Stuck at local minima</li><li>Stuck at saddle point</li><li>Very slow at the plateau</li></ul><img src="https://s1.ax1x.com/2020/04/21/JG4l9O.md.png" alt="JG4l9O.md.png" style="zoom:77%;" /><p>（李老师说的，不是我说的QAQ）：但是Andrew（吴恩达）在2017年说过，不用太担心这个问题。为什么呢？</p><p>如果要stuck at local minima，前提是每一维度都是local minima。</p><p>如果在一个维度遇到local minima的概率是p，当NN很复杂时，有很多参数时，比如1000，那么遇到local minima的概率是 $p^{1000}$ ，趋近于0了，几乎不会发生。</p><p>：所以不用太担心Gradient Descent的局限性。</p><h1 id="Bad-Results-on-Testing-Data"><a href="#Bad-Results-on-Testing-Data" class="headerlink" title="Bad Results on Testing Data"></a>Bad Results on Testing Data</h1><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p>在更新参数时，可能会出现这样曲线图：</p><img src="https://s1.ax1x.com/2020/04/21/JGRbhF.md.png" alt="JGRbhF.md.png" style="zoom:75%;" /><p>图中，Total Loss在training set中逐渐减小，但在validation set中逐渐增大。</p><p>而我们真正关心的其实是validation set的Loss。</p><p>所以想让参数停在validation set中loss最低时。</p><p>Keras能够实现EarlyStopping功能[1]：click <a href="https://keras.io/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore">here</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> EarlyStopping</span><br><span class="line">early_stopping = EarlyStopping(monitor=<span class="string">'val_loss'</span>, patience=<span class="number">2</span>)</span><br><span class="line">model.fit(x, y, validation_split=<span class="number">0.2</span>, callbacks=[early_stopping])</span><br></pre></td></tr></table></figure><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Regularization：Find a set of weight not only minimizing original cost but also close to zero.</p><p>构造一个新的loss function，除了最小化原来的loss function，还能使得参数趋紧0，使得function更平滑。</p><p>function的曲线更平滑，当输入有轻微扰动，不会太影响输出的结果。</p><h3 id="L2-norm-regularization"><a href="#L2-norm-regularization" class="headerlink" title="L2 norm regularization"></a>L2 norm regularization</h3><p><strong>New loss function:</strong> </p>$$\begin{equation}\begin{aligned}\mathrm{L}^{\prime}(\theta)&=L(\theta)+\lambda \frac{1}{2}\|\theta\|_{2}\\ \theta &={w_1,w_2,...}\\ \|\theta\|_2&=(w1)^2+(w_2)^2+...\end{aligned}\end{equation}$$<p>其中用第二范式 $\lambda\frac{1}{2}|\theta|_2$ 作为regularization term。做regularization是为了使函数更平滑，所以一般不考虑bias)</p><p><strong>New gradient:</strong> </p>$$\frac{\partial \mathrm{L}^{\prime}}{\partial w}=\frac{\partial \mathrm{L}}{\partial w}+\lambda w$$<p><strong>New update:</strong> </p>$$\begin{equation}\begin{aligned}w^{t+1} &\longrightarrow w^{t}-\eta \frac{\partial \mathrm{L}^{\prime}}{\partial w}\\ &=w^{t}-\eta\left(\frac{\partial \mathrm{L}}{\partial w}+\lambda w^{t}\right)\\ &=(1-\eta \lambda) w^{t}-\eta \frac{\partial \mathrm{L}}{\partial w}\end{aligned}\end{equation}$$<p>在更新参数时，先乘一个 $(1-\eta\lambda)$ ，再更新。</p><p>weight decay（权值衰减）：由于 $\eta,\lambda$ 都是很小的值，所以 $w^t$ 每次都会先乘一个小于1的数，即逐渐趋于0，实现regularization。但是，因为更新中还有gradient部分，所以不会等于0。</p><h3 id="L1-norm-regularization"><a href="#L1-norm-regularization" class="headerlink" title="L1 norm regularization"></a>L1 norm regularization</h3><p>Regularization除了用第二范式，还可以用其他的，比如第一范式 $|\theta|_1=|w_1|+|w_2|+…$ </p><p><strong>New loss function:</strong> </p>$$\begin{equation}\begin{aligned}\mathrm{L}^{\prime}(\theta)&=L(\theta)+\lambda \frac{1}{2}\|\theta\|_1\\ \theta &={w_1,w_2,...}\\ \|\theta\|_1&=|w_1|+|w_2|+...\end{aligned}\end{equation}$$<p>用sgn()符号函数来表示绝对值的求导。</p><blockquote><p>符号函数：Sgn(number)</p><p>如果number 大于0，返回1；等于0，返回0；小于0，返回-1。</p></blockquote><p><strong>New gradient:</strong> </p>$$\frac{\partial \mathrm{L}^{\prime}}{\partial w}=\frac{\partial \mathrm{L}}{\partial w}+\lambda \text{sgn}(w)$$<p><strong>New update:</strong> </p>$$\begin{equation}\begin{aligned}w^{t+1} &\longrightarrow w^{t}-\eta \frac{\partial \mathrm{L}^{\prime}}{\partial w}\\ &=w^{t}-\eta\left(\frac{\partial \mathrm{L}}{\partial w}+\lambda \text{sgn}(w^t)\right)\\ &=w^{t}-\eta \frac{\partial \mathrm{L}}{\partial w}-\eta \lambda \operatorname{sgn}\left(w^{t}\right)\end{aligned}\end{equation}$$<p>在用第一范式做regularization时，每次 $w^t$ 都要减一个值 $\eta\lambda\text{sgn}(w^t)$ ，和用第二范式做regularization比较，后者每次都要乘一个小于1的值，即使是乘0.99，w下降也很快。</p><blockquote><p>Weight decay（权值衰减）的生物意义：</p><p>Our brain prunes（修剪） out the useless link between neurons.</p><img src="https://s1.ax1x.com/2020/04/21/JGRHtU.md.png" alt="JGRHtU.md.png" style="zoom:47%;" /></blockquote><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Wiki: <strong>Dropout</strong>是Google提出的一种正则化技术，用以在人工神经网络中对抗过拟合。Dropout有效的原因，是它能够避免在训练数据上产生复杂的相互适应。Dropout这个术语代指在神经网络中丢弃部分神经元（包括隐藏神经元和可见神经元）。在训练阶段，dropout使得每次只有部分网络结构得到更新，因而是一种高效的神经网络模型平均化的方法。[2]</p><p>这里讲Dropout怎么做。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><img src="https://s1.ax1x.com/2020/04/21/JG4M4K.md.png" alt="JG4M4K.md.png" style="zoom:55%;" /> <ul><li><p>Each time before updating the parameters:</p><ul><li><p>Each neuron has p% to dropout. Using the new thin network for training.</p><p>【如上图，每个neuron有p的概率被dropout。于是NN就变成了下图thinner的NN】</p><img src="https://s1.ax1x.com/2020/04/21/JGR5mq.md.png" alt="JGR5mq.md.png" style="zoom:50%;" /> </li><li><p>For each mini-batch, we resample the dropout neurons.</p><p>【每次mini-batch，都要重新dropout，更新NN的结构】</p></li></ul></li></ul><h3 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h3><p><strong>Testing中不做dropout</strong> </p><ul><li><p>If the dropout rate at training is p%, all the weights times 1-p%.</p><p>【如果在training中 dropout rate是 p%，在testing是，每个参数都乘 （1-p%)】</p><p>【比如dropout rate 是0.5。如果train出来的w是 1，那么testing中 w=0.5】</p></li></ul><h3 id="Why-dropout-in-training：Intuitive-Reason"><a href="#Why-dropout-in-training：Intuitive-Reason" class="headerlink" title="Why dropout in training：Intuitive Reason"></a>Why dropout in training：Intuitive Reason</h3><ol><li><p>这是一个比较有趣的比喻：</p><img src="https://s1.ax1x.com/2020/04/21/JGRI00.md.png" alt="JGRI00.md.png" style="zoom:50%;" /> </li><li><p>这也是一个有趣的比喻hhh:</p><img src="https://s1.ax1x.com/2020/04/21/JGRo7V.md.png" alt="JGRo7V.md.png" style="zoom:50%;" /> <p>即，团队合作的时候，如果每个人都认为队友在带我，那每个人都可能划水。</p><p>但是，（training中）如果你知道你的队友在划水，那你可能会做的更好。</p><p>但是，（testing中）发现每个人都有更好地做，都没有划水，那么结果就会很好。</p><p>（hhhh，李老师每次讲Intuitive Reason的时候，都觉得好有道理hhh，科学的直觉orz给我也整一个）</p></li></ol><h3 id="Why-multiply-1-p-in-testing-Intuitive-reason"><a href="#Why-multiply-1-p-in-testing-Intuitive-reason" class="headerlink" title="Why multiply (1-p%) in testing: Intuitive reason"></a>Why multiply (1-p%) in testing: Intuitive reason</h3><p>为什么在testing中 weights要乘（1-p%)?</p><p>用一个具体的例子来直观说明：</p><img src="https://s1.ax1x.com/2020/04/21/JGRf6s.md.png" alt="JGRf6s.md.png" style="zoom:67%;" /><p>上图中，如果dropout rate=0.5，假设只训练一次， $w_2,w_4$ 相连的neuron都被dropout。</p><p>在testing中，因为不对neurondropout，所以如果不改变weight，计算出的结果 $z’\approx 2z$ 。</p><p>因此将所有weight简单地和(1-p%) 相乘，能尽量保证计算出的结果 $z’\approx z$ 。</p><h3 id="Dropout-is-a-kind-of-ensemble"><a href="#Dropout-is-a-kind-of-ensemble" class="headerlink" title="Dropout is a kind of ensemble"></a>Dropout is a kind of ensemble</h3><p>Ensemble(合奏)，如下图，将testing data丢给train好的NN来估计，最后的估计值取所有NN输出的平均，如下图：</p><img src="https://s1.ax1x.com/2020/04/21/JGRhXn.md.png" alt="JGRhXn.md.png" style="zoom:60%;" /><p>为什么说dropout is a kind of ensemble?</p><img src="https://s1.ax1x.com/2020/04/21/JGRRpQ.md.png" alt="JGRRpQ.md.png" style="zoom:50%;" /><ul><li><p>Using one mini-batch to train one network</p><p>【dropout相当于每次用一个mini-batch来训练一个network】</p></li><li><p>Some parameters in the network are shared</p><p>【有些参数可能会在很多个mini-batch都被train到】</p></li></ul><p>由于每个神经元有 p%的概率被dropout，因此理论上，如果有M个neuron，可能会训练 $2^M$ 个network。</p><p>但是在Ensemble中，将每个network存下来，testing的时候输出取平均，这样的过程太复杂了，结果也不一定会很好。</p><p>所以在testing中，no dropout，对原始network中的每个参数乘 (1-p%)，用这样简单的操作来达到ensemble的目的。</p><img src="https://s1.ax1x.com/2020/04/21/JGRWlj.md.png" alt="JGRWlj.md.png" style="zoom:60%;" /><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Keras: <a href="https://keras.io/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore">how can i interrupt training when the validation loss isn’t decresing anymore.</a> </li><li>Dropout-wiki：<a href="https://zh.wikipedia.org/wiki/Dropout">https://zh.wikipedia.org/wiki/Dropout</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，详尽阐述了在训练Deep Neural Network时，改善performance的一些tips。&lt;br&gt;tips从Training和Testing两个方面展开。&lt;br&gt;在Training中结果不尽人意时，可以采取更换新的activation function（如ReLu,Maxout等）和采用Adaptive Learning Rate的GradientDescent算法（除了Adagrad,还有RMSprop、Momentum、Adam等）。&lt;br&gt;当在Training中得到好的performance，但在testing中perform bad时，即遇到了overfitting，又该怎么处理呢？文章后半部分详尽介绍了EarlyStopping、Regularization和Dropout三个solution。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="机器学习" scheme="https://f7ed.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DNN" scheme="https://f7ed.com/tags/DNN/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Backpropagation</title>
    <link href="https://f7ed.com/2020/04/18/Backpropagation/"/>
    <id>https://f7ed.com/2020/04/18/Backpropagation/</id>
    <published>2020-04-17T16:00:00.000Z</published>
    <updated>2020-07-03T08:39:21.739Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，讲解了Deep Learning中使用的一种高效Gradient Descent的算法：BackPropagation。<br>BackPropagation通过正向传播和反向传播两个阶段，最后能一起算出损失函数对每一个参数的gradient。</p><a id="more"></a><h1 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h1><p>在Neural Network中，参数的更新也是通过Gradient Descent。</p><p>但是当Neural Network层数很深，结构很复杂的时候，会有millions of parapmeters。</p><p>Backpropagation：To compute the gradient efficiently.</p><h2 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h2><p>BP中需要用到的数学知识：微积分中的链式法则。</p><p><a href="https://imgchr.com/i/Jmc7z4"><img src="https://s1.ax1x.com/2020/04/18/Jmc7z4.md.png" alt="Jmc7z4.md.png"></a> </p><h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><p><a href="https://imgchr.com/i/JmcTWF"><img src="https://s1.ax1x.com/2020/04/18/JmcTWF.md.png" alt="JmcTWF.md.png"></a> </p><p>在NN中，定义损失函数 $L(\theta)=\sum_{n=1}^{N} C^{n}(\theta)$ （$\theta$ 代指NN中所有的weight 和bias）</p><p>对某一参数的gradient为  $\frac{\partial L(\theta)}{\partial w}=\sum_{n=1}^{N} \frac{\partial C^{n}(\theta)}{\partial w}$ </p><p><a href="https://imgchr.com/i/JmcoJU"><img src="https://s1.ax1x.com/2020/04/18/JmcoJU.md.png" alt="JmcoJU.md.png"></a> </p><p>在上图NN中，我们先只研究红框部分，即是以下结构：</p><p><a href="https://imgchr.com/i/JmcIiT"><img src="https://s1.ax1x.com/2020/04/18/JmcIiT.md.png" alt="JmcIiT.md.png"></a> </p><p>z：每个activation function的输入。</p><p>根据链式法则， $\frac{\partial C}{\partial w}= \frac{\partial z}{\partial w} \frac{\partial C}{\partial z}$  .</p><p>要计算每个参数的  $\frac{\partial C}{\partial w}$  ，分为两部分。</p><ol><li><u>Forward pass:</u>  compute $\frac{\partial z}{\partial w} $ for all parameters.</li><li><u>Backward pass:</u>  compute $\frac{\partial C}{\partial z} $ for all activation function inputs z.</li></ol><h2 id="BP：Forward-pass"><a href="#BP：Forward-pass" class="headerlink" title="BP：Forward pass"></a>BP：Forward pass</h2><p><strong>Compute $\frac{\partial z}{\partial w} $ for all parameters.</strong></p><p><a href="https://imgchr.com/i/Jmchd0"><img src="https://s1.ax1x.com/2020/04/18/Jmchd0.md.png" alt="Jmchd0.md.png"></a> </p><p>还是只看上图这一部分，可以轻易得出： $\partial{z}/\partial{w_1}=x_1\qquad \partial{z}/\partial{w_2}=x_2$  </p><p>得到结论： $\frac{\partial z}{\partial w} $  等于 the value of the input connected by the weight. </p><p>【$\frac{\partial z}{\partial w} $ 等于 连接w的输入的值】</p><hr><p>那么，如何计算出NN中全部的 $\frac{\partial z}{\partial w} $ ？</p><p><a href="https://imgchr.com/i/Jmc4oV"><img src="https://s1.ax1x.com/2020/04/18/Jmc4oV.md.png" alt="Jmc4oV.md.png"></a> </p><p>：Forward pass.</p><p>用当前参数（w,b)</p><p>从hidden layer的第一层开始，计算出第一层的输出，即第二层的输入。</p><p>依次相前计算，计算出每一层的输出，即下一层的输入，即输入所连接权重的 $\frac{\partial z}{\partial w}$ 。</p><h2 id="BP：Backward-pass"><a href="#BP：Backward-pass" class="headerlink" title="BP：Backward pass"></a>BP：Backward pass</h2><p><strong>Compute $\frac{\partial C}{\partial z} $ for all activation function inputs z.</strong> </p><p><a href="https://imgchr.com/i/JmcfZq"><img src="https://s1.ax1x.com/2020/04/18/JmcfZq.md.png" alt="JmcfZq.md.png"></a> </p><p>z：activation function的 input</p><p>a：activation function的 output</p><p>这里的activation function 是 sigmod函数  $a=\sigma(z)=\frac{1}{1+e^{-z}}$ </p><p>要求  $\frac{\partial C}{\partial z}$  ， 再根据链式法则： $\frac{\partial C}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial C}{\partial a}$ </p><ol><li><p>求  $\frac{\partial{a}}{\partial{z}}$  :   $\frac{\partial{a}}{\partial{z}}=\sigma'(z)=\sigma(z)(1-\sigma(z))$  （是其他activation function 也能轻易求出）</p></li><li><p>求 $\frac{\partial C}{\partial a}$ ：根据链式法则： $\frac{\partial C}{\partial a}=\frac{\partial z^{\prime}}{\partial a} \frac{\partial C}{\partial z^{\prime}}+\frac{\partial z^{\prime \prime}}{\partial a} \frac{\partial C}{\partial z^{\prime \prime}}$  </p><p><a href="https://imgchr.com/i/JmcfZq"><img src="https://s1.ax1x.com/2020/04/18/JmcfZq.md.png" alt="JmcfZq.md.png"></a> </p><ul><li> $\frac{\partial z^{\prime}}{\partial a} =w_3$  ， $\frac{\partial z^{\prime\prime}}{\partial a} =w_4$ </li><li> $\frac{\partial C}{\partial z^{\prime}}$  和 $\frac{\partial C}{\partial z^{\prime\prime}}$ ？假设，已经通过某种方法算出这个值。</li></ul></li><li> $\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_{3} \frac{\partial C}{\partial z^{\prime}}+w_{4} \frac{\partial C}{\partial z^{\prime \prime}}\right]$  <p>这个式子，可以画成一个反向传播的NN，见下图。</p><p><a href="https://imgchr.com/i/JmcRLn"><img src="https://s1.ax1x.com/2020/04/18/JmcRLn.md.png" alt="JmcRLn.md.png"></a> </p> $\frac{\partial C}{\partial z^{\prime}},\frac{\partial C}{\partial z^{\prime\prime}}$  是这个neuron的输入， $w_3,w_4$ 仍然是 neuron的 weight（无bias）。<p>$\sigma’(z)$ 是一个常数，因为在forward pass中每一个activation的输入已经被算出来了。</p><p>和forward pass中的NN的区别是，forward 中是一个activation function，输入z作用于这个函数；</p><p>而在 backward pass中，这更像一个放缩器，将他的输入变小，即乘上一个 $\sigma’(z)$ 。</p></li></ol><hr><p>问题还是如何计算  $\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_{3} \frac{\partial C}{\partial z^{\prime}}+w_{4} \frac{\partial C}{\partial z^{\prime \prime}}\right]$  ？</p><p>分为两种情况讨论， $z’,z’’$ 是否为输出层的输入？</p><ul><li><p>Output Layer：</p><p><a href="https://imgchr.com/i/Jmc2ss"><img src="https://s1.ax1x.com/2020/04/18/Jmc2ss.md.png" alt="Jmc2ss.md.png"></a> </p><p>z’,z’’：activation function的输入。</p><p>y1,y2：actiavtion function（也是NN）的输出。</p><p>C：NN输出和target的cross entropy。</p><p>根据链式法则： $\frac{\partial C}{\partial z^{\prime}}=\frac{\partial y_{1}}{\partial z^{\prime}} \frac{\partial C}{\partial y_{1}} \quad \frac{\partial C}{\partial z^{\prime \prime}}=\frac{\partial y_{2}}{\partial z^{\prime \prime}} \frac{\partial C}{\partial y_{2}}$  </p><p>所以，已知activation function（simod或者其他），可以轻易求出  $\frac{\partial y_{1}}{\partial z^{\prime}}(=\sigma'(z'))$ 和  $\frac{\partial y_{2}}{\partial z^{\prime\prime}}(=\sigma''(z''))$      。</p><p>所以，已知损失函数，也可以轻易求出 $\frac{\partial C}{\partial y_1}$ 和  $\frac{\partial C}{\partial y_2}$  。（  $C\left(y, \hat{y}\right)=-\left[\hat{y} \ln y+\left(1-\hat{y}\right) \ln \left(1-y\right)\right]$ )</p><p>所以，可以直接求出  $\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_{3} \frac{\partial C}{\partial z^{\prime}}+w_{4} \frac{\partial C}{\partial z^{\prime \prime}}\right]$  。</p></li><li><p>Not Output Layer:</p><p><a href="https://imgchr.com/i/JmcsJS"><img src="https://s1.ax1x.com/2020/04/18/JmcsJS.md.png" alt="JmcsJS.md.png"></a> </p><p>上图中，如果我们要计算 $\frac{\partial C}{\partial z’}$ ，必须要已知下一层的 $\frac{\partial C}{\partial z_a}$ ，然后一直递归下去，直到到达最后的输出层，也就是上面一种情况，可以直接计算出，再递归回来，计算当前层的 $\frac{\partial C}{\partial z’}$ 。</p><p>但是，这样计算每个参数的 $\frac{\partial{C}}{\partial{z}}$ 都要一直递归到输出层，效率显然太低了。</p><p><a href="https://imgchr.com/i/JmcyRg"><img src="https://s1.ax1x.com/2020/04/18/JmcyRg.md.png" alt="JmcyRg.md.png"></a>  </p><p>计算方法如上图：</p><p>当我们已知输出层的  $\frac{\partial{C}}{\partial{z'}},\frac{\partial{C}}{\partial{z''}}$  时，再通过上面的步骤3（且的确算出了    $\frac{\partial{C}}{\partial{z'}},\frac{\partial{C}}{\partial{z''}}$ ），画成反向的NN，计算$\frac{\partial{C}}{\partial{z}}$. </p><p>再依次反向传播计算出每一个neuron的输出z （也是正向传播neuron的输入）的 $\frac{\partial{C}}{\partial{z}}$ .</p></li></ul><hr><p><strong>Backforward pass 的做法：</strong></p><p><a href="https://imgchr.com/i/Jmcri8"><img src="https://s1.ax1x.com/2020/04/18/Jmcri8.md.png" alt="Jmcri8.md.png"></a> </p><ol><li>先计算出输出层的 $\frac{\partial{C}}{\partial{z}}$ （也就是上图的  $\frac{\partial{C}}{\partial{z_5}}$ 和 $\frac{\partial{C}}{\partial{z_6}}$  ）</li><li>用反向传播的NN，向后依次计算出每一层每一个neuron的 $\frac{\partial{C}}{\partial{z}}$ 。</li></ol><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><a href="https://imgchr.com/i/JmcgMj"><img src="https://s1.ax1x.com/2020/04/18/JmcgMj.md.png" alt="JmcgMj.md.png"></a> </p><p>公式：  $\frac{\partial z}{\partial w} \frac{\partial C}{\partial z}=\frac{\partial C}{\partial w}$ </p><p>在正向传播NN中，z是neuron的activation function的输入。</p><p>在反向传播NN中，z是neuron的放缩器的输出。</p><p>通过Forward Pass计算出正向传播NN的每一个neuron的 $\frac{\partial z}{\partial w}$ ，等于该层neuron的输入。</p><p>通过Backward Pass计算出反向传播NN的每一个neuron的 $\frac{\partial C}{\partial z}$ 。</p><p>然后，通过相乘，计算出每个参数的 $\frac{\partial C}{\partial w}$。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，讲解了Deep Learning中使用的一种高效Gradient Descent的算法：BackPropagation。&lt;br&gt;BackPropagation通过正向传播和反向传播两个阶段，最后能一起算出损失函数对每一个参数的gradient。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="Backpropagation" scheme="https://f7ed.com/tags/Backpropagation/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：Deep Learning-Introduction</title>
    <link href="https://f7ed.com/2020/04/18/DL-introdunction/"/>
    <id>https://f7ed.com/2020/04/18/DL-introdunction/</id>
    <published>2020-04-17T16:00:00.000Z</published>
    <updated>2020-07-03T08:41:15.486Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，介绍了Deep Learning的一般步骤。</p><a id="more"></a><h1 id="Up-and-downs-of-Deep-Learning"><a href="#Up-and-downs-of-Deep-Learning" class="headerlink" title="Up and downs of Deep Learning"></a>Up and downs of Deep Learning</h1><ul><li><p>1958: Perceptron (linear model)</p></li><li><p>1969: Perceptron has limitation</p></li><li><p>1980s: Multi-layer perceptron </p><p>​            Do not have significant difference from DNN today</p></li><li><p>1986: Backpropagation</p><p>​            Usually more than 3 hidden layers is not helpful</p></li><li><p>1989: 1 hidden layer is “good enough”, why deep?</p></li><li><p>2006: RBM initialization (breakthrough) </p></li><li><p>2009: GPU</p></li><li><p>2011: Start to be popular in speech recognition【语音辨识】</p></li><li><p>2012: win ILSVRC image competition 【图像识别】</p></li></ul><h1 id="Step-1-Neural-Network"><a href="#Step-1-Neural-Network" class="headerlink" title="Step 1: Neural Network"></a>Step 1: Neural Network</h1><p>在将Regression 和 Classification时，Step 1 是确定一个function set。</p><p>在Deep Learning中，也是相同的，只是这里的function set就是一个neural network的结构。</p><p><a href="https://imgchr.com/i/JmFnne"><img src="https://s1.ax1x.com/2020/04/18/JmFnne.md.png" alt="JmFnne.md.png"></a> </p><p>上图中，一个Neuron就是如上图所示的一个unit，neuron之间不同的连接方式构成不同的Neural Network。</p><h2 id="Fully-Connect-Feedforward-Network"><a href="#Fully-Connect-Feedforward-Network" class="headerlink" title="Fully Connect Feedforward Network"></a>Fully Connect Feedforward Network</h2><p><a href="https://imgchr.com/i/JmFV1K"><img src="https://s1.ax1x.com/2020/04/18/JmFV1K.md.png" alt="JmFV1K.md.png"></a> </p><p>这是一个Fully Connect Feedforward Network【全连接反馈网络】，其中每个neuron的activation function都是一个sigmod函数。</p><p><a href="https://imgchr.com/i/JmFeXD"><img src="https://s1.ax1x.com/2020/04/18/JmFeXD.md.png" alt="JmFeXD.md.png"></a>  </p><p>为什么说neural network其实就是一个function呢？上面两张图中，输入是一个vector，输出也是一个vector，可以用下面函数来表示。</p>$$f\left(\left[\begin{array}{c}1 \\ -1\end{array}\right]\right)=\left[\begin{array}{c}0.62 \\ 0.83\end{array}\right] f\left(\left[\begin{array}{l}0 \\ 0\end{array}\right]\right)=\left[\begin{array}{l}0.51 \\ 0.85\end{array}\right]$$<p><a href="https://imgchr.com/i/JmFZ6O"><img src="https://s1.ax1x.com/2020/04/18/JmFZ6O.md.png" alt="JmFZ6O.md.png"></a> </p><p>上图为全连接网络的一般形式，第一层是Input Layer，最后一层是Output Layer，中间的其他层称为Hidden Layer。</p><p>而Deep Learning中的Deep的含义就是Many hidden layers的意思。</p><h2 id="Matrix-Operation"><a href="#Matrix-Operation" class="headerlink" title="Matrix Operation"></a>Matrix Operation</h2><p><a href="https://imgchr.com/i/JmFkfx"><img src="https://s1.ax1x.com/2020/04/18/JmFkfx.md.png" alt="JmFkfx.md.png"></a> </p><p>上图的全连接网络中，第一个hidden layer的输出可以写成矩阵和向量的形式：</p>$$\sigma\left(\left[\begin{array}{cc}1 & -2 \\ -1 & 1\end{array}\right]\left[\begin{array}{c}1 \\ -1\end{array}\right]+\left[\begin{array}{c}1 \\ 0\end{array}\right]\right)=\left[\begin{array}{c}0.98 \\ 0.12\end{array}\right]$$<p><a href="https://imgchr.com/i/JmFEp6"><img src="https://s1.ax1x.com/2020/04/18/JmFEp6.md.png" alt="JmFEp6.md.png"></a> </p><p>更为一般的公式，用W表示权重，b代表bias，a表示hidden layer的输出。输出vector y可以写成 $y = f(x)$ 的形式，即： $y= f(x)=$</p><p><a href="https://imgchr.com/i/JmFFt1"><img src="https://s1.ax1x.com/2020/04/18/JmFFt1.md.png" alt="JmFFt1.md.png"></a> </p><p>转换为矩阵运算的形式，就可以使用并行计算的硬件技术（GPU）来加速矩阵运算，这也是为什么用GPU来训练Neural Network 更快的原因。</p><h2 id="Output-Layer"><a href="#Output-Layer" class="headerlink" title="Output Layer"></a>Output Layer</h2><p>在 <a href="/2020/04/01/Classification2/" title="Logistic Regression">Logistic Regression</a>中第4节讲到Logistic Regression有局限，消除局限的一种方法是Feature Transformation。</p><p>但是Feature Transformation需要人工设计，不太“机器学习”。</p><p>在下图全连接图中，把Output Layer换成一个Multi-class Classifier（SoftMax），而其中Hidden Layers的作用就是Feature extractor，从feature x提取出新的feature，也就是 output layer的输入。</p><p><a href="https://imgchr.com/i/JmFpm4"><img src="https://s1.ax1x.com/2020/04/18/JmFpm4.md.png" alt="JmFpm4.md.png"></a> </p><p>这样就不需要人工设计Feature Transformation/Feature engineering，可以让机器自己学习：如何将原来的feature转换为更好分类的feature。</p><hr><p><strong>Handwriting Digit Recognition</strong> </p><p><a href="https://imgchr.com/i/Jmix6U"><img src="https://s1.ax1x.com/2020/04/18/Jmix6U.md.png" alt="Jmix6U.md.png"></a> </p><p>在手写数字辨别中，输出是一个16*16的image（256维的vector），输出是一个10维的vector，每一维表示是该image是某个数字的概率。</p><p>在手写数字辨别中，需要设计neural network的结构来提取输入的256维feature。</p><p><a href="https://imgchr.com/i/JmFC79"><img src="https://s1.ax1x.com/2020/04/18/JmFC79.md.png" alt="JmFC79.md.png"></a> </p><h1 id="Step-2-Goodness-of-function"><a href="#Step-2-Goodness-of-function" class="headerlink" title="Step 2: Goodness of function"></a>Step 2: Goodness of function</h1><p>之前我们已经使用过的最小二乘法和交叉熵作为损失函数。</p><p>一般在Neural Network中，使用output vector 和target vector的交叉熵作为Loss。</p><h1 id="Step-3-Pick-the-best-function"><a href="#Step-3-Pick-the-best-function" class="headerlink" title="Step 3: Pick the best function"></a>Step 3: Pick the best function</h1><p>在NN中，也使用Gradient Descent。</p><p><a href="https://imgchr.com/i/JmF90J"><img src="https://s1.ax1x.com/2020/04/18/JmF90J.md.png" alt="JmF90J.md.png"></a> </p><p>但是，Deep Neural Network中，参数太多了，计算结构也很复杂。</p><p>Backpropagation：an efficient way to compute $\partial{L}/\partial{w}$ in neural network.</p><p>Backpropagation本质也是Gradient Descent，只是一种更高效进行Gradient Descent的算法。</p><p>在很多 toolkit（TensorFlow，PyTorch ，Caffe等）中都实现了Backpropgation。</p><p>Backpropagation部分，见<a href="/2020/04/18/Backpropagation/" title="下一篇博客">下一篇博客</a>。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，介绍了Deep Learning的一般步骤。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="DeepLearning" scheme="https://f7ed.com/tags/DeepLearning/"/>
    
      <category term="open-classes" scheme="https://f7ed.com/tags/open-classes/"/>
    
  </entry>
  
  <entry>
    <title>「机器学习-李宏毅」：HW2-Binary Income Predicting</title>
    <link href="https://f7ed.com/2020/04/15/ml-lee-hw2/"/>
    <id>https://f7ed.com/2020/04/15/ml-lee-hw2/</id>
    <published>2020-04-14T16:00:00.000Z</published>
    <updated>2020-07-03T08:42:48.419Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。<br>包括对数据集的处理，训练模型，可视化，预测等。<br>有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的<a href="https://github.com/f1ed/ML-HW2">GitHub</a></p><a id="more"></a><h1 id="Task-introduction-and-Dataset"><a href="#Task-introduction-and-Dataset" class="headerlink" title="Task introduction and Dataset"></a>Task introduction and Dataset</h1><p> Kaggle competition: <a href="https://www.kaggle.com/c/ml2020spring-hw2">link</a> </p><p><strong>Task: Binary Classification</strong></p><p>Predict whether the income of an individual exceeds $50000 or not ?</p><p>*<em>Dataset: *</em> Census-Income (KDD) Dataset</p><p>(Remove unnecessary attributes and balance the ratio between positively and negatively labeled data)</p><h1 id="Feature-Format"><a href="#Feature-Format" class="headerlink" title="Feature Format"></a>Feature Format</h1><ul><li><p>train.csv, test_no_label.csv【都是没有处理过的数据，可作为数据参考和优化参考】</p><ul><li><p>text-based raw data</p></li><li><p>unnecessary attributes removed, positive/negative ratio balanced.</p></li></ul></li><li><p>X_train, Y_train, X_test【已经处理过的数据，可以直接使用】</p><ul><li><p>discrete features in train.csv =&gt; one-hot encoding in X_train (education, martial state…)</p></li><li><p>continuous features in train.csv =&gt; remain the same in X_train (age, capital losses…).</p></li><li><p>X_train, X_test : each row contains one 510-dim feature represents a sample.</p></li><li><p>Y_train: label = 0 means “&lt;= 50K” 、 label = 1 means “ &gt;50K ”</p></li></ul></li></ul><p>注：数据集超大，用notepad查看比较舒服；调试时，也可以先调试小一点的数据集。</p><h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><p>Logistic Regression 原理部分见<a href="/2020/04/01/Classification2/" title="这篇博客">这篇博客</a>。</p><h2 id="Prepare-data"><a href="#Prepare-data" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>本文直接使用X_train Y_train X_test 已经处理好的数据集。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_Train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./logistic_output/output_logistic.csv'</span></span><br><span class="line">fpath = <span class="string">'./logistic_output/logistic'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br></pre></td></tr></table></figure><p>统计一下数据集：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In logistic model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of Training set: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of development set: &#123;&#125;\n'</span>.format(dev_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n'</span>.format(data_dim))</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">In</span> logistic model:</span><br><span class="line">Size of Training set: <span class="number">48830</span></span><br><span class="line">Size of development set: <span class="number">5426</span></span><br><span class="line">Size of test set: <span class="number">27622</span></span><br><span class="line">Dimension of <span class="keyword">data</span>: <span class="number">510</span></span><br></pre></td></tr></table></figure><h3 id="normalize"><a href="#normalize" class="headerlink" title="normalize"></a>normalize</h3><p>normalize data.</p><p>对于train data，计算出每个feature的mean和std，保存下来用来normalize test data。</p><p>代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br></pre></td></tr></table></figure><h3 id="Development-set-split"><a href="#Development-set-split" class="headerlink" title="Development set split"></a>Development set split</h3><p>在logistic regression中使用的gradient，没有closed-form解，所以在train set中划出一部分作为development set 优化参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_dev_split</span><span class="params">(X, Y, dev_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br></pre></td></tr></table></figure><h2 id="Useful-function"><a href="#Useful-function" class="headerlink" title="Useful function"></a>Useful function</h2><h3 id="shuffle-X-Y"><a href="#shuffle-X-Y" class="headerlink" title="_shuffle(X, Y)"></a>_shuffle(X, Y)</h3><p>本文使用mini-batch gradient。</p><p>所以在每次epoch时，以相同顺序同时打乱X_train,Y_train数组，再mini-batch。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br></pre></td></tr></table></figure><h3 id="sigmod-z"><a href="#sigmod-z" class="headerlink" title="_sigmod(z)"></a>_sigmod(z)</h3><p>计算 $\frac{1}{1+e^{-z}}$ ，注意：防止溢出，给函数返回值规定上界和下界。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br></pre></td></tr></table></figure><h3 id="f-X-w-b"><a href="#f-X-w-b" class="headerlink" title="_f(X, w, b)"></a>_f(X, w, b)</h3><p>是sigmod函数的输入，linear part。</p><ul><li>输入：<ul><li>X：shape = [size, data_dimension]</li><li>w：weight vector, shape = [data_dimension, 1]</li><li>b: bias, scalar</li></ul></li><li>输出：<ul><li>属于Class 1的概率（Label=0，即收入小于$50k的概率）</li></ul></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br></pre></td></tr></table></figure><h3 id="predict-X-w-b"><a href="#predict-X-w-b" class="headerlink" title="_predict(X, w, b)"></a>_predict(X, w, b)</h3><p>预测Label=0？（0或者1，不是概率）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br></pre></td></tr></table></figure><h3 id="accuracy-Y-pred-Y-label"><a href="#accuracy-Y-pred-Y-label" class="headerlink" title="_accuracy(Y_pred, Y_label)"></a>_accuracy(Y_pred, Y_label)</h3><p>计算预测出的结果（0或者1）和真实结果的正确率。</p><p>这里使用 $1-\overline{error}$ 来表示正确率。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure><h3 id="cross-entropy-loss-y-pred-Y-label"><a href="#cross-entropy-loss-y-pred-Y-label" class="headerlink" title="_cross_entropy_loss(y_pred, Y_label)"></a>_cross_entropy_loss(y_pred, Y_label)</h3><p>计算预测出的概率（是sigmod的函数输出）和真实结果的交叉熵。</p><p>计算公式为： $\sum_n {C(y_{pred},Y_{label})}=-\sum[Y_{label}\ln{y_{pred}}+(1-Y_{label})\ln(1-{y_{pred}})]$ </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="gradient-X-Y-label-w-b"><a href="#gradient-X-Y-label-w-b" class="headerlink" title="_gradient(X, Y_label, w, b)"></a>_gradient(X, Y_label, w, b)</h3><p>和Regression的最小二乘一样。（严谨的说，最多一个系数不同）</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">def</span> <span class="string">_gradient(X, Y_label, w, b):</span></span><br><span class="line"><span class="comment">    # This function calculates the gradient of cross entropy</span></span><br><span class="line"><span class="comment">    # X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    <span class="attr">y_pred</span> = <span class="string">_f(X, w, b)</span></span><br><span class="line">    <span class="attr">pred_error</span> = <span class="string">Y_label - y_pred</span></span><br><span class="line">    <span class="attr">w_grad</span> = <span class="string">- np.dot(X.T, pred_error)</span></span><br><span class="line">    <span class="attr">b_grad</span> = <span class="string">- np.sum(pred_error)</span></span><br><span class="line">    <span class="attr">return</span> <span class="string">w_grad, float(b_grad)</span></span><br></pre></td></tr></table></figure><h2 id="Training-Adagrad"><a href="#Training-Adagrad" class="headerlink" title="Training (Adagrad)"></a>Training (Adagrad)</h2><p>初始化一些参数。</p><p><strong>这里特别注意</strong> :</p><p>由于adagrad的参数更新是 $w \longleftarrow w-\eta \frac{gradient}{ \sqrt{gradsum}}$ .</p><p><strong>防止除0</strong>，初始化gradsum的值为一个较小值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.float(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.float(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br></pre></td></tr></table></figure><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Aagrad具体原理见<a href="/2020/03/01/Gradient/" title="这篇博客">这篇博客</a>的1.2节。</p><p>迭代更新时，每次epoch计算一次loss和accuracy，以便可视化更新过程，调整参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> range(int(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br></pre></td></tr></table></figure><h3 id="Loss-amp-accuracy"><a href="#Loss-amp-accuracy" class="headerlink" title="Loss &amp; accuracy"></a>Loss &amp; accuracy</h3><p>输出最后一次迭代的loss和accuracy。</p><p>结果如下：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Training loss: <span class="number">0.2933570286596322</span></span><br><span class="line">Training accuracy: <span class="number">0.8839238173254147</span></span><br><span class="line">Development loss: <span class="number">0.31029505347634456</span></span><br><span class="line">Development accuracy: <span class="number">0.8336166253549906</span></span><br></pre></td></tr></table></figure><p>画出loss 和 accuracy的更新过程：</p><p>loss：</p><p><a href="https://imgchr.com/i/JPCjx0"><img src="https://s1.ax1x.com/2020/04/15/JPCjx0.png" alt="JPCjx0.png"></a> </p><p>accuracy：</p><p><a href="https://imgchr.com/i/JPCxMV"><img src="https://s1.ax1x.com/2020/04/15/JPCxMV.png" alt="JPCxMV.png"></a> </p><p>由于Feature数量较大，将权重影响最大的feature输出看看：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Other Rel &lt;<span class="number">18</span> spouse of subfamily RP: [<span class="number">7.11323764</span>]</span><br><span class="line"> Grandchild &lt;<span class="number">18</span> ever marr not <span class="keyword">in</span> subfamily: [<span class="number">6.8321061</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> ever marr RP of subfamily: [<span class="number">6.77322397</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> ever marr RP of subfamily: [<span class="number">6.76688406</span>]</span><br><span class="line"> Other Rel &lt;<span class="number">18</span> never married RP of subfamily: [<span class="number">6.37488958</span>]</span><br><span class="line"> Child &lt;<span class="number">18</span> spouse of subfamily RP: [<span class="number">5.97717831</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.53932651</span>]</span><br><span class="line"> Grandchild <span class="number">18</span>+ spouse of subfamily RP: [<span class="number">5.42948497</span>]</span><br><span class="line"> United<span class="literal">-States</span>: [<span class="number">5.41543809</span>]</span><br><span class="line"> Mexico: [<span class="number">4.79920763</span>]</span><br></pre></td></tr></table></figure><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>完整数据集、代码等，欢迎光临小透明<a href="https://github.com/f1ed/ML-HW2">GitHub</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#################</span></span><br><span class="line"><span class="comment"># Data:2020-04-05</span></span><br><span class="line"><span class="comment"># Author: Fred Lau</span></span><br><span class="line"><span class="comment"># ML-Lee: HW2 : Binary Classification</span></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################</span></span><br><span class="line"><span class="comment"># prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_Train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./logistic_output/output_logistic.csv'</span></span><br><span class="line">fpath = <span class="string">'./logistic_output/logistic'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X.</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_train_dev_split</span><span class="params">(X, Y, dev_ratio=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    <span class="comment"># This function splits data into training set and development set.</span></span><br><span class="line">    train_size = int(X.shape[<span class="number">0</span>] * (<span class="number">1</span> - dev_ratio))</span><br><span class="line">    <span class="keyword">return</span> X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train data and development data</span></span><br><span class="line">dev_ratio = <span class="number">0.1</span></span><br><span class="line">X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio=dev_ratio)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">dev_size = X_dev.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In logistic model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of Training set: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of development set: &#123;&#125;\n'</span>.format(dev_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################</span></span><br><span class="line"><span class="comment"># useful function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_shuffle</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="comment"># This function shuffles two two list/array, X and Y, together.</span></span><br><span class="line">    randomize = np.arange(len(X))</span><br><span class="line">    np.random.shuffle(randomize)</span><br><span class="line">    <span class="keyword">return</span> X[randomize], Y[randomize]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to calculate probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span> / (<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span> - (<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This is the logistic function, parameterized by w and b</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [batch_size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       predict probability of each row of X being positively labeled, shape = [batch_size, 1]</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This fucntion returns a truth value prediction for each row of X by logistic regression</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy_loss</span><span class="params">(y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the cross entropy of Y_pred and Y_label</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Argument:</span></span><br><span class="line">    <span class="comment">#          y_pred: predictions, float vector</span></span><br><span class="line">    <span class="comment">#          Y_label: truth labels, bool vector</span></span><br><span class="line">    cross_entropy = - np.dot(Y_label.T, np.log(y_pred)) - np.dot((<span class="number">1</span> - Y_label).T, np.log(<span class="number">1</span> - y_pred))</span><br><span class="line">    <span class="keyword">return</span> cross_entropy[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gradient</span><span class="params">(X, Y_label, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function calculates the gradient of cross entropy</span></span><br><span class="line">    <span class="comment"># X, Y_label, shape = [batch_size, ]</span></span><br><span class="line">    y_pred = _f(X, w, b)</span><br><span class="line">    pred_error = Y_label - y_pred</span><br><span class="line">    w_grad = - np.dot(X.T, pred_error)</span><br><span class="line">    b_grad = - np.sum(pred_error)</span><br><span class="line">    <span class="keyword">return</span> w_grad, float(b_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#######################################</span></span><br><span class="line"><span class="comment"># training by logistic model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial weights and bias</span></span><br><span class="line">w = np.zeros((data_dim, <span class="number">1</span>))</span><br><span class="line">b = np.float(<span class="number">0.</span>)</span><br><span class="line">w_grad_sum = np.full((data_dim, <span class="number">1</span>), <span class="number">1e-8</span>)  <span class="comment"># avoid divided by zeros</span></span><br><span class="line">b_grad_sum = np.float(<span class="number">1e-8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some parameters for training</span></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">batch_size = <span class="number">2</span>**<span class="number">3</span></span><br><span class="line">learning_rate = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Keep the loss and accuracy history at every epoch for plotting</span></span><br><span class="line">train_loss = []</span><br><span class="line">dev_loss = []</span><br><span class="line">train_acc = []</span><br><span class="line">dev_acc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterative training</span></span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="comment"># Random shuffle at every epoch</span></span><br><span class="line">    X_train, Y_train = _shuffle(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mini-batch training</span></span><br><span class="line">    <span class="keyword">for</span> id <span class="keyword">in</span> range(int(np.floor(train_size / batch_size))):</span><br><span class="line">        X = X_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line">        Y = Y_train[id*batch_size: (id+<span class="number">1</span>)*batch_size]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate gradient</span></span><br><span class="line">        w_grad, b_grad = _gradient(X, Y, w, b)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># adagrad gradient update</span></span><br><span class="line">        w_grad_sum = w_grad_sum + w_grad**<span class="number">2</span></span><br><span class="line">        b_grad_sum = b_grad_sum + b_grad**<span class="number">2</span></span><br><span class="line">        w_ada = np.sqrt(w_grad_sum)</span><br><span class="line">        b_ada = np.sqrt(b_grad_sum)</span><br><span class="line">        w = w - learning_rate * w_grad / np.sqrt(w_grad_sum)</span><br><span class="line">        b = b - learning_rate * b_grad / np.sqrt(b_grad_sum)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss and accuracy of training set and development set at every epoch</span></span><br><span class="line">    y_train_pred = _f(X_train, w, b)</span><br><span class="line">    Y_train_pred = np.around(y_train_pred)</span><br><span class="line">    train_loss.append(_cross_entropy_loss(y_train_pred, Y_train)/train_size)</span><br><span class="line">    train_acc.append(_accuracy(Y_train_pred, Y_train))</span><br><span class="line"></span><br><span class="line">    y_dev_pred = _f(X_dev, w, b)</span><br><span class="line">    Y_dev_pred = np.around(y_dev_pred)</span><br><span class="line">    dev_loss.append(_cross_entropy_loss(y_dev_pred, Y_dev)/dev_size)</span><br><span class="line">    dev_acc.append(_accuracy(y_dev_pred, Y_dev))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'Training loss: &#123;&#125;\n'</span>.format(train_loss[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Training accuracy: &#123;&#125;\n'</span>.format(train_acc[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Development loss: &#123;&#125;\n'</span>.format(dev_loss[<span class="number">-1</span>]))</span><br><span class="line">    f.write(<span class="string">'Development accuracy: &#123;&#125;\n'</span>.format(dev_acc[<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###################</span></span><br><span class="line"><span class="comment"># Plotting Loss and accuracy curve</span></span><br><span class="line"><span class="comment"># Loss curve</span></span><br><span class="line">plt.plot(train_loss, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(dev_loss, label=<span class="string">'dev'</span>)</span><br><span class="line">plt.title(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'./logistic_output/loss.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(train_acc, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(dev_acc, label=<span class="string">'dev'</span>)</span><br><span class="line">plt.title(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'./logistic_output/acc.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################</span></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id, label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> id, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;, &#123;&#125;\n'</span>.format(id, label[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################</span></span><br><span class="line"><span class="comment"># Output the weights and bias</span></span><br><span class="line">ind = (np.argsort(np.abs(w), axis=<span class="number">0</span>)[::<span class="number">-1</span>]).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>, <span class="number">0</span>: <span class="number">10</span>]:</span><br><span class="line">       f.write(<span class="string">'&#123;&#125;: &#123;&#125;\n'</span>.format(content[i], w[i]))</span><br></pre></td></tr></table></figure><h1 id="Generative-Model"><a href="#Generative-Model" class="headerlink" title="Generative Model"></a>Generative Model</h1><p>Generative Model 原理部分见 <a href="/2020/03/21/Classification1/" title="这篇博客">这篇博客</a></p><h2 id="Prepare-data-1"><a href="#Prepare-data-1" class="headerlink" title="Prepare data"></a>Prepare data</h2><p>这部分和Logistic regression一样。</p><p>只是，因为generative model有closed-form solution，不需要划分development set。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./generative_output/output_&#123;&#125;.csv'</span></span><br><span class="line">fpath = <span class="string">'./generative_output/generative'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In generative model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of training data: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n\n'</span>.format(data_dim))</span><br></pre></td></tr></table></figure><h2 id="Useful-functions"><a href="#Useful-functions" class="headerlink" title="Useful functions"></a>Useful functions</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="公式再推导"><a href="#公式再推导" class="headerlink" title="公式再推导"></a>公式再推导</h3><p>计算公式： </p>$$\begin{equation}\begin{aligned}P\left(C_{1} | x\right)&=\frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)+P\left(x | C_{2}\right) P\left(C_{2}\right)}\\&=\frac{1}{1+\frac{P\left(x | C_{2}\right) P\left(C_{2}\right)}{P\left(x | C_{1}\right) P\left(C_{1}\right)}}\\&=\frac{1}{1+\exp (-z)} =\sigma(z)\qquad(z=\ln \frac{P\left(x | C_{1}\right) P\left(C_{1}\right)}{P\left(x | C_{2}\right) P\left(C_{2}\right)}\end{aligned}\end{equation}$$<p>计算z的过程：</p><ol><li>首先计算Prior Probability。</li><li>假设模型是Gaussian的，算出 $\mu_1,\mu_2 ,\Sigma$  的closed-form solution 。</li><li>根据 $\mu_1,\mu_2,\Sigma$ 计算出 $w,b$ 。</li></ol><hr><ol><li><p><strong>计算Prior Probability。</strong> </p><p>程序中用list comprehension处理较简单。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br></pre></td></tr></table></figure></li><li><p>计算 $\mu_1,\mu_2 ,\Sigma$ （Gaussian）</p><p>$\mu_0=\frac{1}{C0} \sum_{n=1}^{C0} x^{n} $  (Label=0)</p><p>$\mu_1=\frac{1}{C1} \sum_{n=1}^{C1} x^{n} $  (Label=0)</p><p>$\Sigma_0=\frac{1}{C0} \sum_{n=1}^{C0}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$  (<strong>注意</strong> ：这里的 $x^n,\mu$ 都是行向量，注意转置的位置）</p><p>$\Sigma_1=\frac{1}{C1} \sum_{n=1}^{C1}\left(x^{n}-\mu^{<em>}\right)^{T}\left(x^{n}-\mu^{</em>}\right)$ </p><p>$\Sigma=(C0 \times\Sigma_0+C1\times\Sigma_1)/(C0+C1)$   (shared covariance) </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>计算 $w,b$ </p><p>在 <a href="/2020/03/21/Classification1/" title="这篇博客">这篇博客</a>中的第2小节中的公式推导中， $x^n,\mu$ 都是列向量，公式如下：</p>   $$   z=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} x-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}   $$     $w^T=\left(\mu^{1}-\mu^{2}\right)^{T} \Sigma^{-1} \qquad b=-\frac{1}{2}\left(\mu^{1}\right)^{T} \Sigma^{-1} \mu^{1}+\frac{1}{2}\left(\mu^{2}\right)^{T} \Sigma^{-1} \mu^{2}+\ln \frac{N_{1}}{N_{2}}$ <hr><p><strong>但是</strong> ，一般我们在处理的数据集，$x^n,\mu$ 都是行向量。推导过程相同，公式如下：</p><p><font color=#f00> <strong>（主要注意转置和矩阵乘积顺序）</strong> </font></p>   $$   z=x\cdot \Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  -\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}   $$     $w=\Sigma^{-1}\left(\mu^{1}-\mu^{2}\right)^{T}  \qquad b=-\frac{1}{2}  \mu^{1}\Sigma^{-1}\left(\mu^{1}\right)^{T}+\frac{1}{2}\mu^{2}\Sigma^{-1} \left(\mu^{2}\right)^{T} +\ln \frac{N_{1}}{N_{2}}$ </li></ol><hr><p><font color=#f00>但是，协方差矩阵的逆怎么求呢？ </font> </p><p>numpy中有直接求逆矩阵的方法(np.linalg.inv)，但当该矩阵是nearly singular，是奇异矩阵时，就会报错。</p><p>而我们的协方差矩阵（510*510）很大，很难保证他不是奇异矩阵。</p><p>于是，有一个 <del>牛逼</del> 强大的数学方法，叫SVD(singular value decomposition, 奇异值分解) 。</p><p>原理步骤我……还没有完全搞清楚QAQ（先挖个坑）[1]</p><p>利用SVD，可以将任何一个矩阵（即使是奇异矩阵），分界成 $A=u s v^T$ 的形式：其中u,v都是标准正交矩阵，s是对角矩阵。（numpy.linalg.svd方法实现了SVD）</p><p><font color=#f00>可以利用SVD求矩阵的伪逆 </font> </p><ul><li>$A=u s v^T$<ul><li>u,v是标准正交矩阵，其逆矩阵等于其转置矩阵</li><li>s是对角矩阵，其”逆矩阵“<strong>（注意s矩阵的对角也可能有0元素）</strong> 将非0元素取倒数即可。</li></ul></li><li>$A^{-1}=v s^{-1} u$</li></ul><p>计算 $w,b$ 的代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><p> accuracy结果：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Training accuracy: <span class="number">0.8756450899439694</span></span><br></pre></td></tr></table></figure><p>也将权重较大的feature输出看看：</p><figure class="highlight ps"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">age: [-<span class="number">0.51867291</span>]</span><br><span class="line"> Masters degree(MA MS MEng MEd MSW MBA): [-<span class="number">0.49912643</span>]</span><br><span class="line"> Spouse of householder: [<span class="number">0.49786805</span>]</span><br><span class="line">weeks worked <span class="keyword">in</span> year: [-<span class="number">0.44710924</span>]</span><br><span class="line"> Spouse of householder: [-<span class="number">0.43305697</span>]</span><br><span class="line">capital gains: [-<span class="number">0.42608727</span>]</span><br><span class="line">dividends from stocks: [-<span class="number">0.41994666</span>]</span><br><span class="line"> Doctorate degree(PhD EdD): [-<span class="number">0.39310961</span>]</span><br><span class="line">num persons worked <span class="keyword">for</span> employer: [-<span class="number">0.37345994</span>]</span><br><span class="line"> Prof school degree (MD DDS DVM LLB JD): [-<span class="number">0.35594107</span>]</span><br></pre></td></tr></table></figure><h2 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h2><p>具体数据集和代码，欢迎光临小透明<a href="https://github.com/f1ed/ML-HW2">GitHub</a> </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment">##############################################</span></span><br><span class="line"><span class="comment"># Prepare data</span></span><br><span class="line">X_train_fpath = <span class="string">'./data/X_train'</span></span><br><span class="line">Y_train_fpath = <span class="string">'./data/Y_train'</span></span><br><span class="line">X_test_fpath = <span class="string">'./data/X_test'</span></span><br><span class="line">output_fpath = <span class="string">'./generative_output/output_&#123;&#125;.csv'</span></span><br><span class="line">fpath = <span class="string">'./generative_output/generative'</span></span><br><span class="line"></span><br><span class="line">X_train = np.genfromtxt(X_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">Y_train = np.genfromtxt(Y_train_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line">X_test = np.genfromtxt(X_test_fpath, delimiter=<span class="string">','</span>)</span><br><span class="line"></span><br><span class="line">X_train = X_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">Y_train = Y_train[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line">X_test = X_test[<span class="number">1</span>:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_normalization</span><span class="params">(X, train=True, X_mean=None, X_std=None)</span>:</span></span><br><span class="line">    <span class="comment"># This function normalize columns of X</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#       X: normalized data</span></span><br><span class="line">    <span class="comment">#       X_mean, X_std</span></span><br><span class="line">    <span class="keyword">if</span> train:</span><br><span class="line">        X_mean = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">        X_std = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">       X[:, j] = (X[:, j] - X_mean[j]) / (X_std[j] + <span class="number">1e-8</span>)  <span class="comment"># avoid X_std==0</span></span><br><span class="line">    <span class="keyword">return</span> X, X_mean, X_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize train_data and test_data</span></span><br><span class="line">X_train, X_mean, X_std = _normalization(X_train, train=<span class="literal">True</span>)</span><br><span class="line">X_test, _, _ = _normalization(X_test, train=<span class="literal">False</span>, X_mean=X_mean, X_std=X_std)</span><br><span class="line"></span><br><span class="line">train_size = X_train.shape[<span class="number">0</span>]</span><br><span class="line">test_size = X_test.shape[<span class="number">0</span>]</span><br><span class="line">data_dim = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'In generative model:\n'</span>)</span><br><span class="line">    f.write(<span class="string">'Size of training data: &#123;&#125;\n'</span>.format(train_size))</span><br><span class="line">    f.write(<span class="string">'Size of test set: &#123;&#125;\n'</span>.format(test_size))</span><br><span class="line">    f.write(<span class="string">'Dimension of data: &#123;&#125;\n\n'</span>.format(data_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment">########################</span></span><br><span class="line"><span class="comment"># Useful functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmod</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="comment"># Sigmod function can be used to compute probability</span></span><br><span class="line">    <span class="comment"># To avoid overflow</span></span><br><span class="line">    <span class="keyword">return</span> np.clip(<span class="number">1</span>/(<span class="number">1.0</span> + np.exp(-z)), <span class="number">1e-8</span>, <span class="number">1</span>-(<span class="number">1e-8</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_f</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function is the linear part of sigmod function</span></span><br><span class="line">    <span class="comment"># Arguments:</span></span><br><span class="line">    <span class="comment">#   X: input data, shape = [size, data_dimension]</span></span><br><span class="line">    <span class="comment">#   w: weight vector, shape = [data_dimension, 1]</span></span><br><span class="line">    <span class="comment">#   b: bias, scalar</span></span><br><span class="line">    <span class="comment"># Output:</span></span><br><span class="line">    <span class="comment">#   predict probabilities</span></span><br><span class="line">    <span class="keyword">return</span> _sigmod(np.dot(X, w) + b)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(X, w, b)</span>:</span></span><br><span class="line">    <span class="comment"># This function returns a truth value prediction for each row of X belonging to class1(label=0)</span></span><br><span class="line">    <span class="keyword">return</span> np.around(_f(X, w, b)).astype(np.int)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_accuracy</span><span class="params">(Y_pred, Y_label)</span>:</span></span><br><span class="line">    <span class="comment"># This function computes prediction accuracy</span></span><br><span class="line">    <span class="comment"># Y_pred: 0 or 1</span></span><br><span class="line">    acc = <span class="number">1</span> - np.mean(np.abs(Y_pred - Y_label))</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"><span class="comment">#######################</span></span><br><span class="line"><span class="comment"># Generative Model: closed-form solution, can be computed directly</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class mean</span></span><br><span class="line">X_train_0 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">0</span>])</span><br><span class="line">X_train_1 = np.array([x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X_train, Y_train) <span class="keyword">if</span> y == <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">mean_0 = np.mean(X_train_0, axis=<span class="number">0</span>)</span><br><span class="line">mean_1 = np.mean(X_train_1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute in-class covariance</span></span><br><span class="line">cov_0 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line">cov_1 = np.zeros(shape=(data_dim, data_dim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_0:</span><br><span class="line">    <span class="comment"># (D,1)@(1,D) np.matmul(np.transpose([x]), x)</span></span><br><span class="line">    cov_0 += np.matmul(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> X_train_1:</span><br><span class="line">    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># shared covariance</span></span><br><span class="line">cov = (cov_0 * X_train_0.shape[<span class="number">0</span>] + cov_1 * X_train_1.shape[<span class="number">0</span>]) / (X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute weights and bias</span></span><br><span class="line"><span class="comment"># Since covariance matrix may be nearly singular, np.linalg.in() may give a large numerical error.</span></span><br><span class="line"><span class="comment"># Via SVD decomposition, one can get matrix inverse efficiently and  accurately.</span></span><br><span class="line"><span class="comment"># cov = u@s@vh</span></span><br><span class="line"><span class="comment"># cov_inv = dot(vh.T * 1 / s, u.T)</span></span><br><span class="line">u, s, vh = np.linalg.svd(cov, full_matrices=<span class="literal">False</span>)</span><br><span class="line">s_inv = s  <span class="comment"># s_inv avoid &lt;1e-8</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(s.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> s[i] &lt; (<span class="number">1e-8</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s_inv[i] = <span class="number">1.</span>/s[i]</span><br><span class="line">cov_inv = np.matmul(vh.T * s_inv, u.T)</span><br><span class="line"></span><br><span class="line">w = np.matmul(cov_inv, np.transpose([mean_0 - mean_1]))</span><br><span class="line">b = (<span class="number">-0.5</span>) * np.dot(mean_0, np.matmul(cov_inv, mean_0.T)) + (<span class="number">0.5</span>) * np.dot(mean_1, np.matmul(cov_inv, mean_1.T)) + np.log(float(X_train_0.shape[<span class="number">0</span>]) / X_train_1.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute accuracy on training set</span></span><br><span class="line">Y_train_pred = <span class="number">1</span> - _predict(X_train, w, b)</span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'\nTraining accuracy: &#123;&#125;\n'</span>.format(_accuracy(Y_train_pred, Y_train)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = <span class="number">1</span> - _predict(X_test, w, b)</span><br><span class="line"><span class="keyword">with</span> open(output_fpath.format(<span class="string">'generative'</span>), <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'id, label\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;, &#123;&#125;\n'</span>.format(i, label))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output the most significant weight</span></span><br><span class="line"><span class="keyword">with</span> open(X_test_fpath) <span class="keyword">as</span> f:</span><br><span class="line">    content = f.readline().strip(<span class="string">'\n'</span>).split(<span class="string">','</span>)</span><br><span class="line">content = content[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">ind = np.argsort(np.abs(np.concatenate(w)))[::<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">with</span> open(fpath, <span class="string">'a'</span>)<span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ind[<span class="number">0</span>:<span class="number">10</span>]:</span><br><span class="line">        f.write(<span class="string">'&#123;&#125;: &#123;&#125;\n'</span>.format(content[i], w[i]))</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>SVD原理，待补充</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章中，手刻实现了「机器学习-李宏毅」的HW2-Binary Income Prediction的作业。分别用Logistic Regression和Generative Model实现。&lt;br&gt;包括对数据集的处理，训练模型，可视化，预测等。&lt;br&gt;有关HW2的相关数据、源代码、预测结果等，欢迎光临小透明的&lt;a href=&quot;https://github.com/f1ed/ML-HW2&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习-李宏毅" scheme="https://f7ed.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"/>
    
    
      <category term="Machine-Learning" scheme="https://f7ed.com/tags/Machine-Learning/"/>
    
      <category term="Classification" scheme="https://f7ed.com/tags/Classification/"/>
    
  </entry>
  
</feed>
